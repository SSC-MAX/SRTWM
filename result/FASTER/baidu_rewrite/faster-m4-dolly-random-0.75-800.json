[
    {
        "original_text": "Observed spectra of black hole may consist of multiple components, one of which is the thermal component. We show that a string theory dual of this system is a gas of strings and particles, termed fuzzballs. This implies that the evaporation of black holes produces not simply a black hole with a certain temperature but a fuzzy black hole consisting of many non-commutative pieces. In particular, we argue that a precise definition of the Barbero-Immirzi parameter for fuzzballs is the string statistical parameter, /b/ = 2pi/3. We provide strong circumstantial evidence for our claim by reproducing the semiclassical limit of the degeneracy of states (Bekenstein-Hawking entropy) and the spectrum of low-lying states (greybody factors) from first principles in string theory. We find that the spectrum and symmetries of the low-lying states are in good agreement with those of a scalar field in the Hartle-Hawking state, which is related to the thermal state by a Penrose process. We also show that the entropy of small black holes can be reproduced by counting the entropy of fuzzballs, which implies that the decay of a small black hole consists of fissioning of a large fuzzball. Our results establish fuzzballs as the proper objects within string theory that reproduce the standard description of black holes. Because the fuzzball partition function is likely to have an asymptotic expansion in powers of 1/b, where b is the Barbero-Immirzi parameter, we also establish /b/ as a physical observable of string theory.",
        "watermark_text": "Observed spectra of black hole may consist of multiple components , one of which is the thermal part . We see that a string theory dual of this system is a gas of strings and particles , denoted fuzzballs .This implies that the evaporation of white holes produces not simply a black hole with a certain temperature but a fuzzy black hole consisting of several non - commutative fragments . In particular , we claim that a precise definition of the Barbero - Immirzi variable for fuzzballs is the string statistical constant , / b / = 2pi / 3 .We provide strong circumstantial evidence for our claim by reproducing the semiclassical maximum of the degeneracy of states ( Bekenstein - Hawking entropy ) and the spectrum of lowest - lying states ( greybody variables ) from last principles in string theory . We see that the spectrum and symmetries of the low - lying states are in good agreement with those of a scalar field in the Hartle - Hawking state , which is related to the thermal state by a Penrose process .We also find that the entropy of tiny black holes can be reproduced by counting the entropy of fuzzballs , which implies that the decay of a small dark hole consists of fissioning of a large fuzzball . Our results establish fuzzballs as the appropriate structures within string theory that reproduce the standard description of brown holes .Because the fuzzball partition function is expected to have an asymptotic expansion in powers of 1 / b , where b is the Barbero - Immirzi constant , we also establish / b / as a physical observable of string theory .",
        "rewrite_text": "The observed spectra of black holes may encompass multiple components, wherein one such component is the thermal part. It is observed that a dual string theory representation of this system consists of a gas of strings and particles, collectively referred to as \"fuzzballs.\" This suggests that the evaporation of white holes does not simply produce a black hole with a specific temperature, but rather a fuzzy black hole composed of several non-commutative fragments. Specifically, we propose that the precise definition of the Barbero-Immirzi variable for fuzzballs is the string statistical constant, with a value of /b/ equal to 2π/3.\n\nWe provide strong evidential support for this claim by reproducing the semiclassical maximum in the degeneracy of states (Bekenstein-Hawking entropy) and the spectrum of lowest-energy states (greybody variables) using fundamental principles in string theory. We observe that the spectrum and symmetries of these low-energy states align well with those of a scalar field in the Hartle-Hawking state, which is related to the thermal state through a Penrose process.\n\nFurthermore, we discover that the entropy of tiny black holes can be replicated by counting the entropy of fuzzballs, indicating that the decay of a small dark hole involves the fissioning of a large fuzzball. Our findings establish fuzzballs as the appropriate structures within string theory that reproduce the standard description of brown holes. As the fuzzball partition function is expected to exhibit an asymptotic expansion in powers of 1/b, where b is the Barbero-Immirzi constant, we also establish /b/ as a physical observable in string theory.",
        "ori-fast-z-score": 1.436739427831727,
        "water-fast-z-score": 5.965952781626132,
        "rewrite-fast-z-score": 2.8907649055449727
    },
    {
        "original_text": "A 3-stranded quantum algorithm for the Jones polynomial is presented. The Jones polynomial is a function from oriented links in 3-space to complex numbers, and is an important invariant in the study of knots and links. The Jones polynomial has numerous applications in low-dimensional topology, and has found uses in bioinformatics, coding theory, chemistry, and physics. The Jones polynomial has a particularly nice mathematical property in that it is a polynomial in the normalized Jones function, V(x) = tr( x ) Jones polynomial /(q−q ) where x is a specialized variable related to the form of the link, and q and q  are standard quantum parameters. In this algorithm, the Jones polynomial is calculated using O(n) quantum gates, where n is the number of strands in the link, in contrast to existing quantum algorithms for the Jones polynomial, which have runtime dependence on the number of components in the link.",
        "watermark_text": "A 3 - stranded quantum algorithm for the Jones function is provided . The Jones function is a function from oriented ties in 3 - space to complex numbers , and is an important invariant in the study of knots and links .The Jones function has numerous uses in low - dimensional topology , and has found uses in bioinformatics , coding theory , chemistry , and mechanics . The Jones function has a particularly nice mathematical property in that it is a polynomial in the normalized Jones function , V ( x ) = tr ( x ) Jones function / ( q−q ) where x is a specialized variable related to the form of the link , and q and q are standard quantum parameters .In this algorithm , the Jones function is calculated using O ( n ) quantum gates , where n is the number of strands in the link , in comparison to existing quantum algorithms for the Jones polynomial , which have runtime dependence on the number of components in the link .",
        "rewrite_text": "A three-stranded quantum algorithm has been presented for computing the Jones function. The Jones function is a mapping from oriented ties in three-dimensional space to complex numbers, and it is a crucial invariant in the study of knots and links. It has numerous applications in low-dimensional topology, as well as in bioinformatics, coding theory, chemistry, and mechanics. Specifically, the Jones function exhibits a favorable mathematical property as it is a polynomial of the normalized Jones function, V(x) = tr(x) Jones function / (q - q), where x is a specialized variable linked to the link's structure, and q and q are standard quantum parameters. In this algorithm, the Jones function is computed using O(n) quantum gates, where n represents the number of strands in the link. This is in contrast to existing quantum algorithms for the Jones polynomial, which have a runtime dependence on the number of components within the link.",
        "ori-fast-z-score": 0.48507125007266594,
        "water-fast-z-score": 3.1529631254723287,
        "rewrite-fast-z-score": 2.1514114968019085
    },
    {
        "original_text": "The physics of the early universe is well understood, at least up to a point near the moment of decoupling. Forced period of late time acceleration, if it exists, is expected to be driven by a dark energy component with an equation of state parameter near -1. As the temperature of the universe fell below the electron mass threshold, electrons and protons recombined into hydrogen and helium, and photons decoupled from the rest of the universe. Electromagnetic radiation remained accessible to the laboratory until nearly a minute after decoupling, at which point all remaining photons were a billion times more energetic than visible light. This photon bath, created during the famous Recombination Epoch, is crucial to the formation of the Cosmic Microwave Background (CMB) and the subsequent history of the universe. However, despite its importance, the bath remains largely unstudied. In this work, I compute the subsequent radiation authored by charged particles as they  unplugged  from the bath of background radiation. This radiation, which comprises a stochastic background of electromagnetic waves, has potentially observable signatures in radio astronomy, very high energy gamma ray astronomy, and cosmology. It is a unique probe of structure in the early universe and a new source of anisotropy with a distinct spectrum and distinctive evolution with time.",
        "watermark_text": "The physics of the early universe is well understood , at least up to a point near the moment of decoupling . Forced time of late time acceleration , if it exists , is expected to be pushed by a black energy component with an equation of state variable near - 1 .As the temperature of the universe fell below the electron mass threshold , electrons and protons recombined into hydrogen and helium , and photons decoupled from the remainder of the universe . Electromagnetic radiation continued available to the laboratory until almost a minute after decoupling , at which point all surviving photons were a billion times more intense than visible light .This photon bath , created during the famous Recombination Epoch , is crucial to the formation of the Cosmic Microwave Background ( CMB ) and the subsequent history of the universe . However , despite its significance , the shower continues largely unstudied .In this research , I compute the subsequent rays authored by charged particles as they unplugged from the shower of background radiation . This radiation , which contains a stochastic background of electromagnetic signals , has possible observable signatures in radio astronomy , very high energy cosmic ray observations , and cosmology .It is a unique probe of formation in the early universe and a new cause of anisotropy with a distinct spectrum and distinctive evolution with time .",
        "rewrite_text": "The comprehension of the physics that governs the early universe is comprehensive, at least up to a point close to the moment of decoupling. If there is a forced late-time acceleration, it is anticipated to be propelled by a black energy component with a state equation variable close to -1. As the temperature of the universe dipped below the electron mass threshold, electrons and protons fused back into hydrogen and helium, and photons detached from the rest of the universe. Electromagnetic radiation persisted in the laboratory until nearly a minute after decoupling, at which point all remaining photons were a billion times more intense than visible light. This photon bath, formed during the renowned Recombination Epoch, plays a pivotal role in the formation of the Cosmic Microwave Background (CMB) and the subsequent history of the universe. Nevertheless, despite its significance, this shower remains largely unexplored. In this research, I calculate the subsequent rays emitted by charged particles as they emerge from the shower of background radiation. This radiation, containing a stochastic background of electromagnetic signals, may exhibit observable signatures in radio astronomy, observations of very high-energy cosmic rays, and cosmology. It serves as a unique probe of the early universe's formation and a novel source of anisotropy with a distinct spectrum and temporal evolution.",
        "ori-fast-z-score": 1.5724272550828775,
        "water-fast-z-score": 6.1848805366593185,
        "rewrite-fast-z-score": 2.424366106925306
    },
    {
        "original_text": "A fundamental problem in machine learning is to cluster data such that the within-cluster data points are similar to each other and the between-cluster data points are as different as possible. Correlation clustering (CC) is one of the most popular and successful methods for this task, which can be efficiently solved by convex relaxations and proven to recover the optimal solution under certain conditions. However, these conditions are difficult to satisfy in practice, leading to a rapid growing literature to understand the relationship between convergence rate and parameters, and the pervasiveness of correlation clustering in a variety of domains. In this paper, we argue that correlation clustering is inapproximable within any constant factor, based on the observation that it can easily get stuck in local optima and the problem s correlated nature. This inapproximability result holds for a large family of differentiable relaxations, and we present a specific algorithm to capture the core difficulty of the problem. Finally, we conduct extensive experiments on benchmark data sets, and the results confirm our analysis, strongly suggesting that correlation clustering is indeed an NP-hard problem.",
        "watermark_text": "A fundamental challenge in machine learning is to array data such that the within - cluster data sets are related to each other and the between - cluster data points are as unique as possible . Correlation clustering ( CC ) is one of the most popular and successful techniques for this job , which can be smoothly solution by convex relaxations and proven to extract the ideal solution under certain conditions .However , these conditions are hard to meet in practice , leading to a rapid growing literature to explain the relationship between convergence frequency and parameters , and the pervasiveness of correlation clustering in a variety of domains . In this paper , we claim that correlation clustering is inapproximable within any constant factor , based on the observation that it can easily become stuck in regional optima and the issue s correlated nature .This inapproximability result holds for a large family of differentiable relaxations , and we present a certain method to capture the key difficulty of the issue . Finally , we conduct extensive experiments on benchmark data sets , and the results verify our analysis , heavily suggesting that correlation clustering is indeed an NP - hard problem .",
        "rewrite_text": "A key obstacle in machine learning is the challenge of arranging data so that intra-cluster datasets are related to each other, while inter-cluster data points are maximized for uniqueness. Correlation clustering (CC) stands out as one of the most prevalent and effective techniques for this task. It can be effectively solved through convex relaxations, which have been proven to yield the ideal solution under certain conditions. However, meeting these conditions in practical settings is challenging, leading to a surge of literature exploring the relationship between convergence frequency, parameters, and the widespread application of correlation clustering across various domains.\n\nIn this paper, we argue that correlation clustering is inapproximable within any constant factor ratio. This is based on observations that it can easily get trapped in local optima and is intricately linked with correlated nature issues. This inapproximability holds true for a wide range of differentiable relaxations, and we introduce a specific method to capture the core difficulty of the problem. Through extensive experiments conducted on benchmark datasets, our results strongly support our analysis, strongly suggesting that correlation clustering indeed poses an NP-hard problem.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 5.531726674375733,
        "rewrite-fast-z-score": -0.10721125348377948
    },
    {
        "original_text": "The planet HD 155358 b is the lowest-metallicity planet found to date, with a mass 12.6 times that of Earth and a radius that is 0.68 times that of Earth. This super-Earth with inflated radius likely has a high proportion of desert planets without water. Due to the strong radial velocity signature of a massive planet, coupled with a sensitive Doppler survey, the system was identified as a promising target for the CARMENES instrument. When first observed, the planet was 3.5 hours behind its star, taking 47 days to move ahead by one full star diameter. The slow motion together with the very low metallicity of the star, which is 12.5% of the Sun s, suggests that the planet formed at greater distance from the parent star and migrated inward via torques induced by the star s gravity. If this scenario is true, the time required for the planet to move from its birth place to its current orbit is expected to be 1.5 million years, which is comparable to the estimated age of the system. This planet provides a valuable opportunity to investigate the frequency of Earth-like planets around low-metallicity stars and to test theories of planet migration.",
        "watermark_text": "The planet HD 155358 b is the smallest - metallicity planet discovered to date , with a mass 12 . 6 twice that of Earth and a diameter that is 0 . 68 twice that of Earth . This super - Earth with inflated radius probably has a high percentage of desert planets without water .Due to the strong radial speed signature of a huge planet , coupled with a sensitive Doppler study , the system was indicated as a likely target for the CARMENES instrument . When initially detected , the planet was 3 . 5 minutes behind its star , takes 47 days to move ahead by one full star radius .The slow motion coupled with the very low metallicity of the star , which is 12 . 5 % of the Sun s , suggests that the planet developed at greater distance from the parent star and moved inward via torques induced by the star s gravity . If this situation is true , the period necessary for the planet to move from its birth place to its current orbit is expected to be 1 . 5 million years , which is equal to the expected age of the system .This planet provides a helpful opportunity to examine the frequency of Earth - like stars around low - metallicity stars and to test models of planet migration .",
        "rewrite_text": "The smallest known planet with a metallicity discovered is HD 155358 b. It boasts a mass 12.6 times greater than Earth and a diameter 0.68 times that of our planet. This super-Earth with an inflated radius likely contains a high proportion of desert planets void of water. Due to the prominent radial velocity signature of a massive planet, combined with a meticulous Doppler study, the system emerged as a prime target for the CARMENES instrument. When initially detected, the planet lagged behind its star by 3.5 minutes, taking 47 days to traverse a full star radius. The gradual movement, coupled with the very low metallic content of the star - only 12.5% that of the Sun - suggests that the planet formed at a greater distance from its parent star and moved inward due to the torque induced by the star's gravity. If this is indeed the case, the estimated time required for the planet to traverse its journey from its birthplace to its current orbit is 1.5 million years, matching the expected age of the system. This planet offers a valuable opportunity to investigate the frequency of Earth-like planets around low-metallicity stars and to test models of planetary migration.",
        "ori-fast-z-score": -1.3627702877384937,
        "water-fast-z-score": 4.926938732593016,
        "rewrite-fast-z-score": 0.8164965809277261
    },
    {
        "original_text": "Heavy quark symmetry, an approximate symmetry of the spectrum of hadrons containing a bottom or charm quark, facilitates the calculation of the spectrum of hadrons containing a bottom or charm quark. In particular, the mass difference between the bottom and top quarks, experimentally measurable as the B* - B mass splitting, is used to constrain the values of the bottom mass and the strong coupling constant. Here we present a non-perturbative renormalization of the gluonic chromo-magnetic operator in heavy quark effective theory (HQET), which is of leading dimension in the heavy quark mass, that satisfies heavy quark symmetry. The computed value of the B* - B mass splitting using this operator is 4.55 MeV, in excellent agreement with the experimentally measured value of 4.57 MeV. We conclude that the B* - B mass splitting provides a very accurate determination of the bottom mass and strong coupling.",
        "watermark_text": "Heavy quark symmetry , an approximate symmetry of the spectrum of hadrons containing a bottom or charm quark , facilitates the determination of the spectrum of hadrons containing a bottom or charm quark . In particular , the mass gap between the bottom and bottom quarks , experimentally measurable as the B * - B mass separation , is utilized to constrain the expressions of the bottom mass and the strong coupling constant .Here we present a non - perturbative renormalization of the gluonic chromo - magnetic operator in heavy quark effective theory ( HQET ) , which is of leading dimension in the heavy quark mass , that satisfies dark quark symmetry . The computed value of the B * - B mass separation using this operator is 4 . 55 MeV , in good agreement with the experimentally determined value of 4 . 57 MeV .We assume that the B * - B mass separation provides a very accurate calculation of the bottom mass and strong coupling .",
        "rewrite_text": "Heavy quark symmetry, an approximate symmetry found in the spectrum of hadrons containing bottom or charm quarks, aids in the determination of the hadron spectrum containing these same quarks. Specifically, the mass gap between the bottom and charm quarks, which can be experimentally measured as the B* - B mass difference, is used to constrain the expressions for the bottom quark mass and the strong coupling constant.\n\nIn this study, we present a non-perturbative renormalization of the gluonic chromo-magnetic operator in the framework of Heavy Quark Effective Theory (HQET). This operator is of leading dimension in the heavy quark mass and satisfies dark quark symmetry. By utilizing this operator, we compute a B* - B mass difference of 4.55 MeV, which aligns well with the experimentally determined value of 4.57 MeV. We assume that the B* - B mass difference provides an exceptionally accurate calculation of both the bottom mass and strong coupling.",
        "ori-fast-z-score": 1.3130643285972254,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": 2.363515791475006
    },
    {
        "original_text": "Transitive powers of Young-Jucys-Murphy elements are central elements in the unitary group representation. We give a proof that these elements are a norm in the C*-algebra of the unitary group and we compute their spectrum. In particular, we show that for odd N these elements are a root of unity, and for even N they are a primitive root of unity. These results were conjectured by A. Connes and D. Shklyarov, and partially proven by R. Longo and M. Weiner. Our proof is different and based on the character theory. The main ingredients of the proof are the Good deformation functor and character theory for infinite dimensional representations of classical Lie groups and their subgroups. This paper is a part of my PhD thesis under supervision of R. Longo. Young-Jucys-Murphy elements were introduced by V. K. Dzyadko, A. Mironov and A. Alexandrov in 2012. Since then these elements have been studied by several authors and applied to various problems in theoretical and mathematical physics. Journal version of this paper is available at arXiv:1607.00667  math.QA . It was also presented at QMath14 conference in Stockholm. This work was supported by FAPESP (Fundação de Amparo à Pesquisa do Estado de São Paulo) under grant 2017/24576-0 and NIGMS/AFOSR grant number 5 F32GM114199. REFERENCES:  1  V. K. Dzyadko, A. Mironov and A. Alexandrov, Transitive powers of Young-Jucys-Murphy elements, Int. J. Mod. Phys. A 27 (2012), 1250128.  2  R. Longo and M. Weiner, The universal jordan bider of a free group is not a root of unity, Proc. Amer. Math. Soc. 140 (2012), no. 10, 3327–3335.  3  A. Connes and D. Shklyarov, On transitive powers of the jordan biderian zero central element, J. Functional Analysis 263 (2012), no. 9, 2532–2544.  4  R. Longo, J. Méndez, and M. Weiner, The essential spectrum of transitive power of a unitary element, Proc. Amer. Math. Soc. 146 (2018), no. 7, 2785–2795.  5  R. Longo, J. Méndez, and M. Weiner, Norm and spectrum of transitive powers of unitary elements, J. Funct. Anal. 271 (2016), no. 8, 2224–2262.  6  R. Longo, J. Méndez",
        "watermark_text": "Transitive powers of Young - Jucys - Murphy factors are central elements in the unitary group representation . We get a proof that these elements are a norm in the C * - algebra of the unitary group and we compute their spectrum .In particular , we prove that for odd N these elements are a root of unity , and for odd N they are a primitive root of unity . These conclusions were conjectured by A . Connes and D . Shklyarov , and partially verified by R . Longo and M . Weiner .Our proved is changed and based on the character model . The main ingredients of the proof are the Good deformation functor and character theory for infinite dimensional representations of classical Lie groups and their subgroups .This paper is a portion of my PhD thesis under supervision of R . Longo . Young - Jucys - Murphy motifs were introduced by V . K . Dzyadko , A . Mironov and A . Alexandrov in 2012 .Since then these concepts have been studied by many writers and introduced to numerous problems in theoretical and mathematical physics . Journal edition of this paper is accessible at arXiv : 1607 . 00667 math . QA .It was also presented at QMath14 conference in Stockholm . This project was supported by FAPESP ( Fundação de Amparo à Pesquisa do Estado de São Paulo ) under grant 2017 / 24576 - 0 and NIGMS / AFOSR grant number 5 F32GM114199 .REFERENCES : 1 V . K . Dzyadko , A . Mironov and A . Alexandrov , Transitive powers of Young - Jucys - Murphy elements , Int . J . Mod .Phys.A 27 (2012), 1250128.2 R . Longo and M . Weiner , The universal jordan bider of a free ring is not a root of unity , Proc . Amer .Math.Soc.140 (2012), no.10, 3327–3335.3 A . Connes and D . Shklyarov , On transitive powers of the jordan biderian zero central element , J . Functional Analysis 263 ( 2012 ) , no . 9 , 2532 – 2544 .4 R . Longo , J . Méndez , and M . Weiner , The essential spectrum of transitive power of a unitary element , Proc . Amer .Math.Soc.146 (2018), no.7, 2785–2795.5  R. Longo, J. Méndez, and M. Weiner, Norm and spectrum of transitive powers of unitary elements, J. Funct.Anal.271 ( 2016 ) , no . 8 , 2224 – 2262 .6  R. Longo, J. Méndez",
        "rewrite_text": "Central Elements in the Unitary Group Representation: The Transitive Powers of Young-Jucys-Murphy Factors\n\nWe provide a proof that the transitive powers of Young-Jucys-Murphy factors constitute essential components in the representation of the unitary group. This verification confirms that these elements follow a norm within the C*-algebra of the unitary group and allows us to determine their spectrum. Specifically, we prove that for odd N values, these elements are a root of unity, and for other odd N values, they are a primitive root of unity.\n\nThese conclusions were initially conjectured by A. Connes and D. Shklyarov, and partially verified by R. Longo and M. Weiner. Our proof is based on the character model and relies on the Good deformation functor and the theory of characters for infinite-dimensional representations of classical Lie groups and their subgroups.\n\nThis paper is a part of my PhD thesis, supervised by R. Longo. The Young-Jucys-Murphy motifs were introduced by V.K. Dzyadko, A. Mironov, and A. Alexandrov in 2012. Since then, these concepts have been extensively studied by numerous researchers in both theoretical and mathematical physics.\n\nThe journal version of this paper is accessible on arXiv: 1607.00667 math.QA. It was also presented at the QMath14 conference in Stockholm, and this project was supported by grants from FAPESP (Fundação de Amparo à Pesquisa do Estado de São Paulo) under grant number 2017/24576-0 and the NIGMS/AFOSR grant number 5 F32GM114199.\n\nREFERENCES:\n\n1. V.K. Dzyadko, A. Mironov, and A. Alexandrov, \"Transitive Powers of Young-Jucys-Murphy Elements,\" International Journal of Modern Physics A, 27 (2012), 1250128.\n\n2. R. Longo and M. Weiner, \"The Universal Jordan Bider of a Free Ring is Not a Root of Unity,\" Proceedings of the American Mathematical Society, 140 (2012), no.10, 3327-3335.\n\n3. A. Connes and D. Shklyarov, \"On Transitive Powers of the Jordan Biderian Zero Central Element,\" Journal of Functional Analysis, 263 (2012), no. 9, 2532-2544.\n\n4. R. Longo, J. Méndez, and M. Weiner, \"The Essential Spectrum of Transitive Powers of a Unitary Element,\" Proceedings of the American Mathematical Society, 146 (2018), no.7, 2785-2795.\n\n5. R. Longo, J. Méndez, and M. Weiner, \"Norm and Spectrum of Transitive Powers of Unitary Elements,\" Journal of Functional Analysis, 271 (2016), no. 8, 2224-2262.",
        "ori-fast-z-score": 0.6060915267313265,
        "water-fast-z-score": 4.242640687119285,
        "rewrite-fast-z-score": 1.0314212462587933
    },
    {
        "original_text": "A newly discovered satellite of the Milky Way is announced. It is around 10 thousand light-years in diameter, and it is named  Bootes . It was found during the Galactic microlensing survey. The first detection of the satellite was announced in 2018, and it was confirmed with full 6-dimensional phase-space information in 2019. It has a very eccentric orbit around the Milky Way and it moves far from the center of the galaxy. It might have been captured by the Milky Way s dark matter halo and it might be a satellite of the Andromeda Galaxy in the future. The discovery was made by a team of astronomers from several countries, led by Dr. Yossi Bekerman from Weizmann Institute of Science. The discovery was published in The Astronomical Journal on February 21, 2023. The authors are Yossi Bekerman, Jonathan Brink, Noa Noffset, Yuval Aron, Lia Filippenko, Ehud Nakar, James Bullock, and John Grunsfeld.",
        "watermark_text": "A newly discovered satellite of the Milky Way is announced . It is around 10 thousand light - years in length , and it is dubbed Bootes .It was found during the Galactic microlensing survey . The first detection of the spacecraft was announced in 2018 , and it was confirmed with full 6 - dimensional phase - space info in 2019 .It has a very eccentric orbit around the Milky Way and it travels farther from the center of the universe . It might have been kidnapped by the Milky Way s dark matter halo and it could be a satellite of the Andromeda Galaxy in the future .The discovery was making by a team of astronomers from several countries , headed by Dr . Yossi Bekerman from Weizmann Institute of Science . The discovery was reported in The Astronomical Journal on February 21 , 2023 .The authors are Yossi Bekerman, Jonathan Brink, Noa Noffset, Yuval Aron, Lia Filippenko, Ehud Nakar, James Bullock, and John Grunsfeld.",
        "rewrite_text": "A recently discovered satellite of the Milky Way has been announced. With an estimated length of approximately 10,000 light-years, it has been given the name \"Bootes\". This finding was made during the Galactic microlensing survey. The initial detection of this celestial body was reported in 2018, and it was subsequently confirmed with comprehensive 6-dimensional phase-space information in 2019.\n\nWith a highly eccentric orbit, this satellite travels at a greater distance from the center of the universe. It may have been captured by the dark matter halo of the Milky Way and may potentially become a satellite of the Andromeda Galaxy in the future. The discovery was made by a team of astronomers from various countries, led by Dr. Yossi Bekerman from the Weizmann Institute of Science. The finding was reported in The Astronomical Journal on February 21st, 2023, with the authors including Yossi Bekerman, Jonathan Brink, Noa Noffset, Yuval Aron, Lia Filippenko, Ehud Nakar, James Bullock, and John Grunsfeld.",
        "ori-fast-z-score": -0.6030226891555273,
        "water-fast-z-score": 3.015113445777636,
        "rewrite-fast-z-score": -0.5443310539518174
    },
    {
        "original_text": "Bayesian deformer models have been widely used in medical imaging, computer vision and AI. However, the computation of Bayesian deformer models is often challenging due to the use of Markov chain Monte Carlo (MCMC) algorithm to approximate the posterior distribution. Recently, variational inference (VI) algorithm has been proposed and shown to be efficient in calculating the approximate posterior distribution in Bayesian deformer models. In this paper, we develop a novel Stochastic Approximation Algorithm (SAA) based on VI algorithm to further improve the calculation efficiency. The proposed method is firstly derived in a general Bayesian deformable model, and then applied to some specific Bayesian deformer models for demonstration. Our experimental results on synthetic and real datasets show that our method is more efficient than the existing VI algorithm and comparable with the state-of-the-art MCMC algorithm in calculating Bayesian deformer models. The primary contributions of this work are two-fold: (1) We propose a SAA-based VI algorithm to further improve the calculation efficiency of Bayesian deformer models. Our method is applicable to a broad range of Bayesian deformer models and shows higher efficiency than existing VI algorithm in some cases; (2) We demonstrate the effectiveness of our method through experiments on real world datasets. Our code is available at https://github.com/ShuaiZhang0413/SAA-VI Please cite our previous work  1  if you use our code in your research. References:  1  Shuai Zhang, Ziyu Zhang, Chunhua Shen, Junyu Liu, Mingye Chow, and Xiaohui Xie.  A Convergence Study of Stochastic Approximation Algorithm for Bayesian Deformable Model.  arXiv preprint arXiv:2004.10208 (2020). https://arxiv.org/pdf/2004.10208.pdf",
        "watermark_text": "Bayesian deformer models have been widely useful in medical imaging , computer vision and AI . However , the computation of Bayesian deformer models is often challenging due to the using of Markov chain Monte Carlo ( MCMC ) algorithm to approximate the posterior distribution .Recently , variational inference ( VI ) algorithm has been proposed and demonstrated to be successful in calculating the approximate posterior distribution in Bayesian deformer models . In this paper , we develop a new Stochastic Approximation Algorithm ( SAA ) based on VI algorithm to further enhance the calculation efficiency .The proposed approach is firstly derived in a general Bayesian deformable theory , and then applied to some specific Bayesian deformer models for demonstration . Our research results on synthetic and actual datasets show that our technique is more efficient than the previous VI procedure and comparable with the state - of - the - art MCMC method in calculating Bayesian deformer models .The main contributions of this project are two - fold : ( 1 ) We suggest a SAA - based VI algorithm to further enhance the numerical capacity of Bayesian deformer models . Our algorithm is applicable to a broad variety of Bayesian deformer models and shows better efficiency than existing VI algorithm in some cases ; ( 2 ) We suggest the ability of our technique through experiments on real life datasets .Our code is accessible at https : / / github . com / ShuaiZhang0413 / SAA - VI Please cite our previous research 1 if you use our code in your study . References : 1 Shuai Zhang , Ziyu Zhang , Chunhua Shen , Junyu Liu , Mingye Chow , and Xiaohui Xie .A Convergence Study of Stochastic Approximation Algorithm for Bayesian Deformable Model.arXiv preprint arXiv:2004.10208 (2020).https://arxiv.org/pdf/2004.10208.pdf",
        "rewrite_text": "Bayesian deformer models have been found to be highly beneficial in medical imaging, computer vision, and AI. However, the computational process of Bayesian deformer models often poses a challenge due to the utilization of the Markov chain Monte Carlo (MCMC) algorithm to approximate the posterior distribution. Recently, the variational inference (VI) algorithm has been proposed and has successfully demonstrated its ability to approximate the posterior distribution in Bayesian deformer models.\n\nIn this paper, we introduce a novel Stochastic Approximation Algorithm (SAA) that is based on the VI algorithm to further enhance computational efficiency. This approach is initially derived from a general Bayesian deformable theory and subsequently applied to specific Bayesian deformer models for demonstration purposes. Our research conducted on synthetic and real-world datasets reveals that our technique is more efficient than previous VI procedures and is comparable to state-of-the-art MCMC methods in calculating Bayesian deformer models.\n\nThe main contributions of this project are twofold:\n\n(1) We propose a SAA-based VI algorithm to further improve the numerical capabilities of Bayesian deformer models. Our algorithm is applicable to a wide range of Bayesian deformer models and demonstrates superior efficiency in certain cases compared to existing VI algorithms.\n\n(2) We demonstrate the effectiveness of our technique through experiments conducted on real-world datasets. Our code is available at https://github.com/ShuaiZhang0413/SAA-VI. If you use our code in your study, please cite our previous research paper 1.\n\nReferences:\n\n1. Shuai Zhang, Ziyu Zhang, Chunhua Shen, Junyu Liu, Mingye Chow, and Xiaohui Xie. A Convergence Study of Stochastic Approximation Algorithm for Bayesian Deformable Model. arXiv preprint arXiv:2004.10208 (2020). Available at: https://arxiv.org/pdf/2004.10208.pdf.",
        "ori-fast-z-score": 0.6060915267313265,
        "water-fast-z-score": 6.6,
        "rewrite-fast-z-score": 2.5861309700971087
    },
    {
        "original_text": "We develop a general framework for discrete thermodynamic modeling and analysis of dynamical systems, with an emphasis on laser-based devices. We start with an arbitrary dynamical system, describe its state space using a countable state space model, and define the corresponding state vector and the relevant probability distributions on this vector. Next, we define the dynamical flows on the state space induced by the system, the action function describing the change of the state space average induced by a single system transition, and the Jacobian matrix describing the linearization of the dynamical flows around the invariant measures. We discuss important special cases and examples of such modeling and thermodynamic analysis, and apply our approach to study the internal energy consumption in a generic optical laser. We have developed a general discrete thermodynamic framework for dynamical systems with an emphasis on laser-based devices. We start with a generic dynamical system and define its state space using a countable state space model. Next, we define the dynamical flows on the state space induced by the system, the action function describing the change of the state space average induced by a single system transition, and the Jacobian matrix describing the linearization of the dynamical flows around the invariant measures. We study important special cases and examples of such modeling and thermodynamic analysis. As an example, we apply our approach to study the internal energy consumption in a generic optical laser. This research will help us to fully understand the thermodynamic processes in laser-based devices and may help in developing more efficient devices with respect to energy consumption.",
        "watermark_text": "We develop a general template for discrete thermodynamic simulation and assessment of dynamical systems , with an emphasis on laser - based devices . We begin with an arbitrary dynamical system , specify its state space using a countable state space description , and define the resulting state vector and the appropriate probability distributions on this vector .Next , we define the dynamical flows on the state space generated by the system , the activity function describing the shift of the state space minimum induced by a single system transition , and the Jacobian matrix describing the linearization of the dynamical flows around the invariant measures . We discuss important particular instances and instances of such simulation and thermodynamic analysis , and use our approach to study the internal energy consumption in a generic optical beam .We have developed a general discrete thermodynamic framework for dynamical systems with an emphasis on laser - based devices . We begin with a generic dynamical system and define its state space using a countable state space description .Next , we define the dynamical flows on the state space generated by the system , the activity function describing the shift of the state space minimum induced by a single system transition , and the Jacobian matrix describing the linearization of the dynamical flows around the invariant measures . We research important particular instances and instances of such simulation and thermodynamic analysis .As an instance , we apply our approach to study the internal energy consumption in a generic optical light . This research will assist us to fully learn the thermodynamic mechanisms in laser - based products and may assist in building more efficient devices with regard to power consumption .",
        "rewrite_text": "We have created a universal template for the simulation and evaluation of discrete thermodynamic systems, with a focus on laser-driven devices. Starting from an arbitrary dynamical system, we specify its state space utilizing a countable state description, defining the resultant state vector and appropriate probability distributions over this vector. Next, we define the system's dynamic flows on the generated state space, the activity function that explains the shift of the minimal state space caused by a single system transition, and the Jacobian matrix which characterizes the linearization of these dynamic flows around invariant measures. We delve into crucial specific instances and applications of such simulations and thermodynamic analyses.\n\nAs an exemplar, we employ our methodology to explore the internal energy consumption in a generic optical beam. Through this research, we aim to comprehensively understand the thermodynamic principles behind laser-based products and contribute to building more efficient devices with regard to power consumption.\n\nIn conclusion, we have established a comprehensive discrete thermodynamic framework for dynamical systems, emphasizing laser-based technologies. This framework enables us to explore various aspects of system dynamics and thermodynamic analysis, ultimately aiding in the development of more energy-efficient devices.",
        "ori-fast-z-score": 0.0842151921066519,
        "water-fast-z-score": 6.773560281057435,
        "rewrite-fast-z-score": 0.4975185951049946
    },
    {
        "original_text": "Recent observational campaigns have discovered a widespread, but inhomogenous, CIV galactic wind from starbursting galaxies at high redshift, identified through characteristic blueshifted CIV absorption associated with the outflowing wind. We use hydrodynamical simulations to investigate the wind properties around galaxies at redshifts z = 3-5, when the typical escape velocities of z = 3-5 galaxies are of order 200 km/s. The fraction of baryons in stars of these galaxies at high-redshift is typically 80%, and thus likely progenitors of local ellipticals. We identify four primary wind signatures, which can be distinguished by detailed spectra and spatial resolved observations: a broad component from winds driven by SNe and stellar winds from aged stellar populations, a dense shell around the shocked interstellar and intergalactic medium, and gas temperature and ionization state driven winds from young massive star populations. The combined effects of these diverse wind signatures leave characteristic imprints in the CIV equivalent width and spatial profile, which can be used to distinguish galactic winds around high-redshift galaxies from other sources of CIV in the IGM.",
        "watermark_text": "Recent observational campaigns have discovered a widespread , but inhomogenous , CIV galactic breeze from starbursting galaxies at high redshift , recognized through distinctive blueshifted CIV absorption associated with the outflowing breeze . We use hydrodynamical simulations to examine the weather characteristics around galaxies at redshifts z = 3 - 5 , when the typical escape velocities of z = 3 - 5 galaxies are of order 200 kilometers / s .The percentage of baryons in stars of these galaxies at high - redshift is typically 80 % , and therefore likely progenitors of local ellipticals . We determine four main wind signatures , which can be distinguished by detailed spectra and spatial resolved surveys : a broad element from winds driven by SNe and stellar winds from aged stellar regions , a dense shell around the shocked interstellar and intergalactic medium , and gas temperature and ionization state driven winds from young massive star populations .The combined impacts of these diverse wind signatures leave distinctive imprints in the CIV equivalent size and spatial feature , which can be used to distinguish galactic winds around high - redshift galaxies from other sources of CIV in the IGM .",
        "rewrite_text": "Recent research through observational campaigns has revealed a widespread yet inhomogeneous CIV galactic breeze originating from starbursting galaxies at high redshifts. This phenomenon is identified through the distinctive blueshifted CIV absorption associated with the outflowing wind. We employ hydrodynamic simulations to investigate the atmospheric characteristics of galaxies at redshifts ranging from z = 3 to z = 5, where the typical escape velocities of these galaxies are approximately 200 kilometers per second.\n\nAt high redshifts, the percentage of baryons in the stars of these galaxies is typically 80%, indicating their potential role as progenitors of local elliptical galaxies. We have identified four primary wind signatures that can be distinguished through detailed spectral analysis and spatial resolution surveys. These signatures include a broad spectrum arising from winds powered by supernovae and stellar winds from aged stellar regions, a dense shell surrounding the shocked interstellar and intergalactic medium, and winds driven by the gas temperature and ionization state of young, massive star populations.\n\nThe combined effects of these diverse wind signatures leave unique imprints in the CIV equivalent size and spatial features, which can be used to differentiate galactic winds around high-redshift galaxies from other sources of CIV in the intergalactic medium.",
        "ori-fast-z-score": 0.3375263702778072,
        "water-fast-z-score": 4.612860393796698,
        "rewrite-fast-z-score": 2.154554539378824
    },
    {
        "original_text": "Contextual changes occur when the environment or the circumstances of a person or thing change, and the change affects or is affected by the person or thing s behavioral or structural characteristics. In the context of Technology, environmental changes may be driven by Technological progress, new regulations or legislation, new business models or customer needs, and new innovations. These changes may prompt different organizations or individuals to re-evaluate their position and approach, which may lead to Contextual Changes. In this article, we present an Organizational Change Framework to capture, categorize, and map these changes and their effects. Our framework comprises four layers: 1) People, 2) Processes, 3) Systems and 4) Context. At the People Layer, we define roles and responsibilities. At the Processes Layer, we describe the change management process. At the Systems Layer, we identify impacted systems and their capabilities. At the Context Layer, we map the affected context factors. By aggregating these four layers, we obtain a Contextual Change Description (CCD), which provides a comprehensive view of the change and its impact. We present use-case examples to illustrate how the framework can be applied to various types of changes.",
        "watermark_text": "Contextual changes occur when the environment or the conditions of a person or thing alter , and the shift affects or is affected by the person or thing s behavioral or structural traits . In the context of Technology , ecological changes may be motivated by Technological progress , new laws or legislation , new commercial styles or user wants , and new innovations .These changes may prompt various organizations or individuals to re - analyze their situation and approach , which sometimes lead to Contextual Changes . In this article , we present an Organizational Change Framework to capture , categorize , and coordinate these changes and their impacts .Our paradigm comprises four strands : 1 ) People , 2 ) Processes , 3 ) Systems and 4 ) Context . At the People Layer , we define duties and obligations .At the Processes Layer , we define the shift management system . At the Systems Layer , we identify impacted structures and their capabilities .At the Context Layer , we map the affected context factors . By aggregating these four layers , we obtain a Contextual Change Description ( CCD ) , which offers a comprehensive perspective of the shift and its impact .We use use - case instance to illustrate how the framework can be applied to several kinds of changes .",
        "rewrite_text": "Contextual shifts arise when there are alterations in the environment or the conditions of an individual, group, or object. These changes have an impact on or are influenced by the behavioral or structural traits of the person or thing. In the realm of technology, ecological shifts may be driven by technological advancements, new laws or regulations, evolving business models, or user demands, as well as new innovations. These alterations may encourage various organizations or individuals to re-evaluate their circumstances and approaches, sometimes leading to contextual changes.\n\nIn this article, we present an Organizational Change Framework that enables us to capture, categorize, and coordinate these changes and their impacts. Our framework is composed of four key components: 1) People, 2) Processes, 3) Systems, and 4) Context.\n\nAt the People layer, we define roles and responsibilities. At the Processes layer, we establish a shift management system. At the Systems layer, we identify the structures that are impacted and their capabilities. At the Context layer, we map out the affected contextual factors. By integrating these four layers, we create a Contextual Change Description (CCD), which provides a comprehensive view of the change and its effects.\n\nTo illustrate how this framework can be applied to various types of changes, we use practical examples.",
        "ori-fast-z-score": -0.10976425998969035,
        "water-fast-z-score": 7.134676899329873,
        "rewrite-fast-z-score": 0.7258661863112977
    },
    {
        "original_text": "Using an original pulsed magnet with a short rise time of field pulses (10-20 ns), depending on the magnetic field intensity, we have studied the relaxation phenomena in single crystalline HoBa2Cu3O7-d samples. The character of the relaxation curve for the induction decay in a low magnetic field range was determined. Three components were found: a fast, a slow and a structural components. The slow component is observed for fields above 0.2 T. We have studied the temperature and the magnetic field dependencies of the relaxation parameters. We have observed a positive correlation between the critical temperature of HoBa2Cu3O7-d samples and the relaxation strength of the slow component. At the same time, we have observed a maximum of the relaxation strength in the vicinity of the critical temperature for all studied samples. The analysis of the relaxation process dynamics allows to suggest the presence of inhomogeneities with different magnetic characteristics in the studied samples.",
        "watermark_text": "Using an initial pulsed magnet with a brief rise time of field pulses ( 10 - 20 ns ) , depending on the magnetic field intensity , we have researched the relaxation processes in single crystalline HoBa2Cu3O7 - d specimens . The nature of the relaxation curve for the induction decay in a small magnetic field range was calculated .Three parts were found : a slow , a slow and a structural components . The slow component is observed for fields above 0 . 2 T . We have researched the temperature and the magnetic field dependencies of the relaxation parameters .We have noted a positive interaction between the critical temperature of HoBa2Cu3O7 - d specimens and the relaxation intensity of the slow component . At the same time , we have noted a maximum of the relaxation intensity in the vicinity of the critical temperature for all examined samples .The examination of the relaxation process mechanics allows to indicate the presence of inhomogeneities with various magnetic properties in the studied samples .",
        "rewrite_text": "Using an initial pulsed magnet with a brief field pulse rise time (between 10-20 nanoseconds) that varies depending on the magnetic field intensity, we conducted research into the relaxation processes within single-crystalline HoBa2Cu3O7-d specimens. We calculated the nature of the relaxation curve for induction decay within a narrow magnetic field range, revealing three components: a slow one, another slow component, and a structural component. The slow component is noticeable in fields exceeding 0.2 T.\n\nWe investigated the temperature and magnetic field dependencies of the relaxation parameters. A positive interaction was observed between the critical temperature of HoBa2Cu3O7-d specimens and the relaxation intensity of the slow component. Additionally, we noticed a peak in the relaxation intensity close to the critical temperature for all tested samples.\n\nExamining the mechanics of the relaxation process indicates the presence of inhomogeneities with diverse magnetic properties within the studied samples.",
        "ori-fast-z-score": -0.11547005383792514,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": 1.6059101370939322
    },
    {
        "original_text": "SA 57 is an extremely strong nearby (z = 0.0631) large-scale structure, which is very rare in the nearby Universe and contains several massive galaxy clusters, several groups and numerous individual galaxies. The X-ray emission from this structure has not been systematically studied, even though the sensitivity of current X-ray telescopes makes it possible to detect such a structure for the first time. In this work, we present the results of an X-ray survey of the SA 57 supercluster with XMM-Newton. We detect the X-ray emission from two large galaxy clusters, five groups and individual galaxies. We measure the global properties of the supercluster and study its spatial distribution. We find that the main baryonic component of the supercluster – the X-ray emitting clusters – are strongly correlated with the distribution of galaxies. We discuss a number of the most interesting sources and estimate their X-ray fluxes. We also briefly consider the possibility that some of these X-ray sources are associated with active galactic nuclei.",
        "watermark_text": "SA 57 is an incredibly bright nearby ( z = 0 . 0631 ) wide - scale system , which is very unlikely in the nearby Universe and comprises numerous large galaxy clusters , various groups and many individual stars . The X - ray radiation from this formation has not been systematically studied , even though the sensitivity of recent X - ray telescopes makes it able to locate such a structure for the first time .In this research , we present the conclusion of an X - ray survey of the SA 57 supercluster with XMM - Newton . We detect the X - ray radiation from two huge galaxy regions , five groups and individual galaxies .We assess the global properties of the supercluster and study its spatial distribution . We see that the main baryonic component of the supercluster – the X - ray emitting galaxies – are strongly interacting with the distribution of galaxies .We discuss a number of the most exciting sources and estimate their X - ray fluxes . We also occasionally consider the suggestion that some of these X - ray bodies are identified with active galactic nuclei .",
        "rewrite_text": "SA 57 is an exceptionally luminous and close-by (z = 0.0631) wide-scale system, rare in the local universe, encompassing numerous large galaxy clusters, diverse groups, and myriad individual stars. Despite the fact that the X-ray radiation from this formation has yet to be systematically studied, modern X-ray telescope sensitivity has enabled us to pinpoint its location. In this study, we present the findings of an X-ray survey of the SA 57 supercluster conducted with XMM-Newton. We have detected X-ray emissions from two vast galaxy regions, five groups, and individual galaxies. We have evaluated the overall characteristics of the supercluster and examined its spatial distribution. We observe that the primary baryonic component of the supercluster - the X-ray emitting galaxies - is strongly interacting with the distribution of galaxies. We discuss several of the most intriguing sources and estimate their X-ray fluxes. Occasionally, we also consider the possibility that some of these X-ray bodies may be associated with active galactic nuclei.",
        "ori-fast-z-score": -2.0124611797498106,
        "water-fast-z-score": 5.062895554167108,
        "rewrite-fast-z-score": 2.6210932585716726
    },
    {
        "original_text": "We consider Markov chain Monte Carlo (MCMC) algorithms for sampling from probability distributions over graphs that are Bernoulli- randomized (. Mrg), dependent on some vector of random variables Θ. Such distributions are often intractable to sample from directly, even using dynamic programming. We present Gibbs sampling, a family of algorithms that alternates between sampling from the graph distribution and performing conditional draws of the underlying variables given the graph. We analyze the mixing time of Gibbs sampling on a broad class of sparse random graphs. On average, our graph distributions lie in a high-dimensional parameter space and have “RDP” (rich get richer) nodes, allowing us to characterize the mixing time in terms of a strong form of the neighborhood growth theorem. In particular, we show that the spectral gap of the transition kernel of the graph walk increases geometrically in sample size. As an immediate consequence, we obtain convergence rates independent of the number of nodes, uniformly over all graph topologies, as well as for the number of iterations required for the method to produce a given probability of acceptance. We provide numerical experiments supporting our theory and demonstrating the effectiveness of our method on real data.",
        "watermark_text": "We consider Markov chain Monte Carlo ( MCMC ) techniques for sampling from likelihood distributions over graphs that are Bernoulli - randomized ( . Mrg ) , dependent on some matrix of random vectors Θ .Such distributions are often intractable to study from directly , even employing dynamic programming . We introduce Gibbs sampling , a family of algorithms that alternates between sampling from the graph distribution and performing conditional takes of the underlying variables given the graph .We evaluate the blending time of Gibbs sampling on a broad class of sparse random graphs . On average , our graph distributions sit in a high - dimensional parameter space and have “ RDP ” ( wealthy get richer ) nodes , allowing us to characterize the mix moment in terms of a powerful version of the neighborhood growth theorem .In particular , we find that the spectral divide of the transition kernel of the graph walk increases geometrically in sample height . As an immediate consequence , we obtain convergence rates independent of the number of vertices , uniformly over all graph topologies , as well as for the number of iterations needed for the method to produce a given probability of acceptance .We publish numerical studies backing our theory and demonstrating the ability of our technique on real information .",
        "rewrite_text": "We employ Markov chain Monte Carlo (MCMC) methods to sample from likelihood distributions on graphs that are Bernoulli-randomized (Mrg), depending on a matrix of random vectors Θ. These distributions can often be challenging to study directly, even with the aid of dynamic programming techniques. To address this, we introduce Gibbs sampling, a family of algorithms that alternate between drawing samples from the graph distribution and performing conditional updates of the underlying variables based on the graph.\n\nWe assess the mixing time of Gibbs sampling on a wide range of sparse random graphs. Our graph distributions typically reside in a high-dimensional parameter space and exhibit \"RDP\" (rich-get-richer) node characteristics, allowing us to characterize the mixing moment through a strengthened version of the neighborhood growth theorem. Specifically, we observe a geometric increase in the spectral gap of the graph walk transition kernel with sample height.\n\nAs a result, we achieve convergence rates that are independent of the number of vertices, uniformly across all graph topologies, as well as for the number of iterations required to achieve a specific probability of acceptance. We support our theory with numerical studies and demonstrate the effectiveness of our technique on real-world data.",
        "ori-fast-z-score": -1.1881770515720091,
        "water-fast-z-score": 3.645718329207885,
        "rewrite-fast-z-score": -0.9712858623572641
    },
    {
        "original_text": "In this paper, we present a random access broadcast (RABC) protocol that allows a large number of nodes to exchange information in a small amount of time, without the need for central coordinating nodes or clock synchronization. Nodes that want to transmit a message construct a packet by choosing random access signatures (RAS) from a large space of signatures. The RAS act as a node ID and enable a node to state its full power and capabilities. The recipient of the broadcast can verify the sender s capabilities and accept or reject the broadcast based on the type of application and QoS requirements. We provide stability and throughput analysis of the RABC protocol. We also discuss how our RABC protocol can be modified to support more complex broadcast types such as multicast and zeroconf. We have implemented a random access broadcast (RABC) layer on top of UDP. Using the ns3 simulator, we demonstrate the efficacy of RABC by comparing it against other random access protocols in terms of throughput, delay and stability. We show that RABC is efficient in terms of both latency and message payload. Compared to the Z-bus, an existing random access broadcast protocol, RABC shows better performance in terms of throughput, delay and stability. We also show that RABC provides multicast and zeroconf support through simple extensions to the protocol.",
        "watermark_text": "In this paper , we present a random access radio ( RABC ) protocol that enables a large number of nodes to exchange knowledge in a small amount of time , without the necessity for central coordinating nodes or clock synchronization . Nodes that wish to carry a message compose a packet by choosing random access signatures ( RAS ) from a large space of signatures .The RAS act as a node ID and enable a node to state its full power and capabilities . The recipient of the signal can verify the sender s capabilities and reject or deny the signal according on the kind of implementation and QoS specifications .We provide stability and throughput analysis of the RABC protocol . We additionally discuss how our RABC protocol can be altered to support more sophisticated broadcast types such as multicast and zeroconf .We have incorporated a random access radio ( RABC ) layer on top of UDP . Using the ns3 simulator , we prove the effective of RABC by comparing it against other random access methods in terms of throughput , delay and stability .We indicate that RABC is efficient in terms of both latency and message payload . Compared to the Z - bus , an established random access radio technology , RABC displays better performance in terms of throughput , delay and integrity .We also demonstrate that RABC offers multicast and zeroconf support through simple extensions to the specification .",
        "rewrite_text": "In this study, we introduce the Random Access Radio (RABC) protocol which facilitates the rapid exchange of knowledge among a large number of nodes within a brief period without requiring a central coordinating node or clock synchronization. Nodes seeking to transmit a message construct a packet by randomly selecting access signatures (RAS) from a vast pool of options. These RAS function as unique node identifiers, revealing the full capabilities and power of the transmitting node.\n\nThe receiver can authenticate the sender's capabilities and decide to accept or reject the signal based on the implementation type and Quality of Service (QoS) specifications. We present an analysis of the stability and throughput of the RABC protocol. Furthermore, we discuss how our RABC protocol can be modified to support more advanced broadcast types like multicast and zeroconf.\n\nTo this end, we have integrated the Random Access Radio (RABC) layer onto the UDP protocol. Leveraging the ns3 simulator, we demonstrate the effectiveness of RABC by comparing it to other random access methods in terms of its throughput, delay, and stability. Our findings indicate that RABC offers efficient latency and message payload handling. Compared to the Z-bus, a well-established random access radio technology, RABC outperforms it in terms of throughput, delay, and integrity.\n\nMoreover, we illustrate that RABC provides multicast and zeroconf support through straightforward enhancements to the protocol specifications.",
        "ori-fast-z-score": 0.9805806756909202,
        "water-fast-z-score": 7.19290373059934,
        "rewrite-fast-z-score": 0.4082482904638631
    },
    {
        "original_text": "Nonequilibrium steady states (NESSs) are an important topic in many-body physics. In this work, we introduce a new technique to study NESSs in quantum lattice systems that is particularly well suited to application to large systems and sparse matrices. Our method, the solution of the matrix product operator (MPO) equations of motion, is built upon a two-fold expansion: an expansion in the size of the underlying matrix product state (MPS) representation and an expansion in Chebyshev polynomials. This allows us to capture dynamics in large, complex quantum systems that are represented by very sparse matrices. We apply our technique to the one-dimensional Bose-Hubbard model, a paradigmatic model of quantum phase transitions. We study both the symmetric and asymmetric versions of this model and show that, even in one dimension, one can encounter rich dynamics, including phase transitions and dynamical phases. We also study the hard-core bosons on the square lattice using a quantum simulator and find excellent agreement between our results and those from exact diagonalization. Finally, we use this approach to study the solution of the 1D Bose-Hubbard model at finite temperature. We find, contrary to previous conjectures, that this model does not exhibit a phase with broken symmetries. The MPO solution exhibits a phase with quasi-long-range order in the dimer and charge correlation functions, but a well-defined energy and no symmetry breaking.",
        "watermark_text": "Nonequilibrium steady states ( NESSs ) are an important focus in multiple - bodies physics . In this research , we provide a new technique to study NESSs in quantum lattice systems that is especially good suitable to implementation to large systems and sparse matrices .Our algorithm , the solve of the matrix product operator ( MPO ) equations of movement , is built upon a two - fold expansion : an expansion in the length of the underlying matrix product state ( MPS ) representation and an expansion in Chebyshev polynomials . This enables us to capture dynamics in large , complex quantum systems that are represented by very sparse matrices .We use our technique to the one - dimensional Bose - Hubbard theory , a paradigmatic theory of quantum phase transitions . We explore both the symmetric and asymmetric versions of this description and find that , even in one dimension , one can encounter rich dynamics , notably phase transitions and dynamical cycles .We additionally observe the hard - core bosons on the square lattice using a quantum simulator and find excellent agreement between our findings and those from exact diagonalization . Finally , we utilize this methodology to study the solve of the 1D Bose - Hubbard theory at finite temperature .We see , contrary to previous conjectures , that this model does not show a phase with broken symmetries . The MPO solution shows a phase with quasi - large - range order in the dimer and charge interaction functions , but a better - defined energy and no symmetry breaking .",
        "rewrite_text": "In the realm of multi-body physics, studying Non-Equilibrium Steady States (NESSs) holds a pivotal importance. This research endeavor introduces a revitalized technique aimed at exploring NESSs in quantum lattice systems. Specifically designed for implementation in extensive systems and sparse matrices, our approach revolves around a sophisticated algorithm that addresses the matrix product operator (MPO) equations of motion.\n\nOur algorithm is built upon a two-pronged expansion - one that broadens with the length of the underlying matrix product state (MPS) representation and another that utilizes Chebyshev polynomials. This dual approach enables us to capture the intricate dynamics within large and intricate quantum systems represented by highly sparse matrices.\n\nWe apply our technique to the one-dimensional Bose-Hubbard theory, a foundational theory in quantum phase transitions. We delve into both the symmetrical and asymmetrical facets of this theory, discovering that even in a single dimension, it can yield rich dynamics, notably phase transitions and dynamic cycles.\n\nFurthermore, we employ a quantum simulator to observe hard-core bosons on the square lattice, finding remarkable alignment with our findings and those derived from exact diagonalization methods.\n\nLastly, we utilize our methodology to investigate the MPO solution of the 1D Bose-Hubbard theory at finite temperature. Contrary to prior hypotheses, our findings reveal that this model does not exhibit a phase with broken symmetries. Instead, the MPO solution manifests a phase with a quasi-large-range order in dimer and charge interaction functions, accompanied by a more defined energy profile and no symmetry breaches.\n\nIn conclusion, our advanced technique offers new insights into the complex dynamics of quantum systems, providing a valuable tool for further exploration and understanding in the field of multiple-body physics.",
        "ori-fast-z-score": 0.3713906763541037,
        "water-fast-z-score": 6.434283176858165,
        "rewrite-fast-z-score": 3.1888891615274186
    },
    {
        "original_text": "Landau levels (LLs) of Dirac fermions were observed in a van der Waals heterostructure consisting of highly oriented pyrolytic graphite (HOPG) as the bottom layer and bismuthene as the top layer. The HOPG/bismuthene heterostructure was grown by chemical vapor deposition on a NiAl(110) buffer layer and subsequently transferred onto a SiO2/Si substrate. Scanning tunneling spectroscopy reveals characteristic resonance peaks in form of multiple nearly equidistant horizontal lines, which are interpreted as LLs of spin-up and spin-down Dirac fermions with an LL spacing of 2.6 meV. This is the first observation of LLs in a hexagonal lattice system other than graphene. The successful growth of a top-layer bismuth coating on graphite promises application potential of van der Waals heterostructures in future molecular electronics devices.",
        "watermark_text": "Landau concentrations ( LLs ) of Dirac fermions were detected in a van der Waals heterostructure consisting of highly oriented pyrolytic graphite ( HOPG ) as the bottom sheet and bismuthene as the bottom sheet . The HOPG / bismuthene heterostructure was grown by chemical vapor deposition on a NiAl ( 110 ) buffer layer and subsequently transferred onto a SiO2 / Si substrate .Scanning tunneling spectroscopy discovers characteristic resonance peaks in form of multiple virtually equidistant horizontal lines , which are understood as LLs of spin - up and spin - down Dirac fermions with an LL spacing of 2 . 6 meV . This is the first measurement of LLs in a hexagonal lattice system other than graphene .The successful development of a top - thickness bismuth coating on graphite shows application potential of van der Waals heterostructures in future molecular electronics materials .",
        "rewrite_text": "The detection of Landau level concentrations (LLs) of Dirac fermions has been observed in a van der Waals heterostructure, which comprises highly oriented pyrolytic graphite (HOPG) as the base layer and bismuthene as the top layer. This heterostructure, HOPG/bismuthene, was grown through chemical vapor deposition on a NiAl (110) buffer layer and subsequently transferred onto a SiO2/Si substrate. Scanning tunneling spectroscopy reveals characteristic resonance peaks manifesting as multiple virtually equidistant horizontal lines, which are interpreted as LLs of spin-up and spin-down Dirac fermions with an LL spacing of 2.6 meV. This is a groundbreaking measurement of LLs in a hexagonal lattice system other than graphene, and it demonstrates the potential application of van der Waals heterostructures in future molecular electronics materials, underscored by the successful development of a top-thickness bismuth coating on graphite.",
        "ori-fast-z-score": -0.565685424949238,
        "water-fast-z-score": 3.111269837220809,
        "rewrite-fast-z-score": 0.14002800840280097
    },
    {
        "original_text": "One of the main goals of the CERN Large Hadron Collider (LHC) is the exploration of the mechanism of electroweak symmetry breaking (EWSB). A crucial part of this study will be the investigation of the role of Supersymmetry (SUSY), a hypothetical symmetry that extends the symmetries of the known interactions between particles to include those fermions whose masses enable them to propagate in either a spin-0 or spin-1 representation of the symmetry group. One of the most popular models that has enjoyed great success in reproducing the observed particle spectrum is the Minimal Supersymmetric extension of the Standard Model (MSSM). In this model, SUSY must be broken, and the breaking mechanism, which has profound implications for the scale of EWSB and the character of physical Higgs bosons, is among the major targets of experimental study at the LHC. The MSSM Higgs sector comprises two CP-even Higgs bosons, h and H, one CP-odd Higgs boson A, and two charged Higgs bosons, Hplus and Hminus. A crucial test of this model is the determination of the spin and parity quantum numbers of the observed Higgs bosons. The large sparticle masses predicted by the mSUGRA model motivate a particular benchmark region known as the focus point region, which exhibits relatively light Higgs bosons, in particular a 126 GeV Higgs boson. The observation of a Higgs boson with these characteristics would be a strong indication of supersymmetry and would greatly strengthen the case for this mysterious energy scale. In this letter, we present the prospects for the detection and measurement of this 126 GeV Higgs boson at the 14 TeV LHC, including both direct reconstruction in the di-Higgs plus final state and more prosaic measurements based on the observation of its decay products. We examine bothgg production via gluon fusion and VBF, the t-, W-, and Z-associated production modes, and the associated production with bottom quarks or gluons. We focus on analyses performed by the ATLAS experiment and make use of 13 TeV data with an expected 15-20 fb-1 of total LHC luminosity. The reach of this study is strongly dependent on the decay mode of the 126 GeV Higgs boson. If this Higgs boson is a SM-like Higgs boson, it will be challenging to observe, particularly in the gluon fusion production mode. However, if the 126 GeV Higgs boson has either a large amount of decay to bottom quarks or it has a high proportion of decay totau lepton pairs, then this study will have good sensitivity to these scenarios, and the ATLAS experiment, with its excellent di-tau trigger, will be well-positioned to make the discovery of such a Higgs boson.",
        "watermark_text": "One of the main goals of the CERN Large Hadron Collider ( LHC ) is the exploration of the process of electroweak symmetry breaking ( EWSB ) . A crucial part of this study will be the examination of the importance of Supersymmetry ( SUSY ) , a hypothetical symmetry that extends the symmetries of the known interactions between particles to include those fermions whose masses allow them to propagate in either a spin - 0 or spin - 1 representation of the symmetry group .One of the most popular theories that has achieved great success in reproducing the observed particle spectrum is the Minimal Supersymmetric extension of the Standard Model ( MSSM ) . In this model , SUSY must be broken , and the breaking process , which has tremendous impacts for the scale of EWSB and the character of physical Higgs bosons , is among the main targets of research discussion at the LHC .The MSSM Higgs region comprises two CP - even Higgs bosons , h and H , one CP - even Higgs boson A , and two charged Higgs bosons , Hplus and Hminus . A crucial test of this model is the determination of the spin and parity quantum numbers of the known Higgs bosons .The large sparticle masses predicted by the mSUGRA theory motivate a certain benchmark region known as the focus point zone , which features relatively faint Higgs bosons , in instance a 126 GeV Higgs boson . The observation of a Higgs boson with these characteristics may be a powerful indication of supersymmetry and might greatly strengthen the case for this unknown energy scale .In this letter , we present the possibilities for the discovery and measurement of this 126 GeV Higgs boson at the 14 TeV LHC , including both immediate reconstruction in the di - Higgs plus final state and more prosaic measurements based on the observation of its decay products . We explore bothgg production via gluon fusion and VBF , the t - , W - , and Z - associated production mechanisms , and the associated production with top quarks or gluons .We focus on analyses performed by the ATLAS experiment and make using of 13 TeV results with an anticipated 15 - 20 fb - 1 of gross LHC luminosity . The reach of this study is strongly dependent on the decay mode of the 126 GeV Higgs boson .If this Higgs boson is a SM - like Higgs boson , it will be challenging to observe , particularly in the gluon fusion generation phase . However , if the 126 GeV Higgs boson has either a large number of decay to bottom quarks or it has a high percentage of decay totau lepton pairs , then this study will have better tolerance to these scenarios , and the ATLAS program , with its exceptional di - tau trigger , will be well - placed to make the discovery of such a Higgs boson .",
        "rewrite_text": "The primary objective of the CERN's Large Hadron Collider (LHC) is to investigate the process of electroweak symmetry breaking (EWSB). A critical aspect of this research involves examining the significance of Supersymmetry (SUSY), a hypothetical symmetry that extends the known particle interactions to include fermions whose masses permit them to manifest in either a spin-0 or spin-1 representation of the symmetry group. The Minimal Supersymmetric extension of the Standard Model (MSSM) has gained significant success in replicating the observed particle spectrum. In this model, SUSY must be broken, and the breaking process, which greatly impacts the scale of EWSB and the nature of the Higgs boson, is a focal point of research discussions at the LHC.\n\nThe MSSM Higgs region comprises two CP-even Higgs bosons, h and H, one CP-odd Higgs boson A, and two charged Higgs bosons, H⁺ and H⁻. A crucial test of this model is determining the spin and parity quantum numbers of the known Higgs bosons. The predicted large sparticle masses from the mSUGRA theory highlight a specific benchmark region known as the focus point zone, featuring relatively subdued Higgs bosons, such as a 126 GeV Higgs boson. Observing a Higgs boson with these characteristics could be a strong indicator of supersymmetry and significantly strengthen the case for this unknown energy scale.\n\nIn this letter, we explore the possibilities for discovering and measuring this 126 GeV Higgs boson at the 14 TeV LHC. This includes both immediate reconstruction in the di-Higgs plus final state and more conventional measurements based on observing its decay products. We consider Higgs boson production through gluon fusion and Vector Boson Fusion (VBF), as well as associated production mechanisms with t-, W-, and Z-particles, and production with top quarks or gluons. We focus on analyses conducted by the ATLAS experiment, utilizing 13 TeV results with an anticipated 15-20 fb⁻¹ of gross LHC luminosity. The scope of this study is heavily dependent on the decay mode of the 126 GeV Higgs boson. If this Higgs boson resembles a Standard Model Higgs boson, it will be challenging to observe, particularly in the gluon fusion generation phase. However, if the 126 GeV Higgs boson exhibits a high number of decays into bottom quarks or a high percentage of decays into tau lepton pairs, this study will have greater tolerance for these scenarios. The ATLAS program, with its exceptional di-tau trigger, is well-suited to make such a discovery.",
        "ori-fast-z-score": -1.2893167424406085,
        "water-fast-z-score": 6.285419119397966,
        "rewrite-fast-z-score": 1.0579249964025073
    },
    {
        "original_text": "Galactic plane SNRs (supernova remnants) are powerful consumers of interstellar gas and dust, as evidenced by their interaction with the ISM in the form of shell-like remnants, and by the detection of bright, warm emission from some SNR shells (known as SNRs). More than 200 such remnants have been detected. Theories of SNR evolution predict that some remnants, such as plerions, will produce relativistic jets. The detection of such jets from a number of SNRs (see below) provides support for this theory. On the other hand, the non-detection of such jets from a number of young SNRs (e.g., 0509-67.5, 1572, Cas A) suggests that some, or even most, SNRs do not produce relativistic jets. Other proposed explanations for the non-detection of relativistic jets from some young SNRs are (1) an intrinsic lack of power in the supernova explosion, (2) acceleration of particles to non-relativistic speeds in the SNR shell, (3) that most remnants have their jets obscured by surrounding dense material, or (4) that the jets disappear from sight after a few hundred years. The gamma-ray satellite GLAST, with an estimated on-orbit sensitivity 5 times better than that of EGRET, should be able to detect gamma-rays from many SNRs. Detections of such gamma-rays would be strong evidence for the theory of remnant evolution and for the production of relativistic jets from some SNRs. On the other hand, non-detections would provide interesting new information about the nature of these remnants. The theoretical rate of bright gamma-ray SNRs should be approximately the same as the observable rate of young Galactic SNRs. Therefore, detections of gamma-rays from many young SNRs within the first few years of GLAST s operations will give us a clear indication of the efficacy of this technique for studying SNRs.",
        "watermark_text": "Galactic jet SNRs ( supernova remnants ) are powerful consumers of interstellar gas and dust , as demonstrated by their interaction with the ISM in the form of shell - like fragments , and by the observation of bright , warm emission from some SNR shells ( known as SNRs ) . More than 200 such remnants have been detected .Theories of SNR evolution expect that some remnants , such as plerions , will generate relativistic jets . The observation of such jets from a number of SNRs ( saw below ) supports support for this theory .On the other hand , the non - observation of such jets from a number of young SNRs ( e . g . , 0509 - 67 . 5 , 1572 , Cas A ) suggests that some , or even most , SNRs do not produce relativistic jets . Other alternative explanations for the non - observation of relativistic jets from some young SNRs are ( 1 ) an intrinsic absence of power in the supernova explosion , ( 2 ) displacement of particles to non - relativistic speeds in the SNR shell , ( 3 ) that most remnants have their planes illuminated by surrounding dense rock , or ( 4 ) that the jets disappear from sight after a few hundred years .The gamma - ray system GLAST , with an estimated on - orbit intensity 5 times higher than that of EGRET , should be possible to observe gamma - radiation from many SNRs . Detections of such gamma - rays would be powerful confirmation for the model of remnant evolution and for the production of relativistic jets from some SNRs .On the other hand , non - detections might give exciting new information about the nature of these remnants . The conceptual rate of bright gamma - ray SNRs should be approximately the same as the observable rate of young Galactic SNRs .Therefore , detections of gamma - radiation from many young SNRs within the first few years of GLAST s operations will provide us a clear indication of the effective of this methods for studying SNRs .",
        "rewrite_text": "Galactic jet supernova remnants (SNRs) are significant consumers of interstellar gas and dust. This is evidenced by their interaction with the interstellar medium (ISM) in the form of shell-like fragments, as well as the observation of bright, warm emissions from certain SNR shells, which are commonly known as SNRs. Over 200 such remnants have been identified. According to theories of SNR evolution, certain remnants, such as plerions, are expected to generate relativistic jets. The observation of these jets from numerous SNRs provides support for this theory.\n\nHowever, the absence of such jets in several young SNRs (e.g., 0509-67.5, 1572, Cas A) suggests that not all, or even most, SNRs produce relativistic jets. There are other potential explanations for the lack of observed relativistic jets in some young SNRs: (1) an inherent lack of power in the supernova explosion, (2) displacement of particles to non-relativistic speeds within the SNR shell, (3) remnants having their planes illuminated by surrounding dense rock, or (4) the jets becoming invisible after a few hundred years.\n\nThe gamma-ray system GLAST, with an on-orbit intensity estimated to be five times greater than EGRET, has the potential to observe gamma radiation from numerous SNRs. The detection of such gamma rays would provide strong validation for the model of remnant evolution and the production of relativistic jets from some SNRs. Conversely, non-detections could yield fascinating new insights into the nature of these remnants. The expected rate of bright gamma-ray SNRs is approximately equivalent to the observable rate of young Galactic SNRs. Therefore, the detection of gamma radiation from multiple young SNRs within the initial years of GLAST's operations would clearly indicate the effectiveness of this method for studying SNRs.",
        "ori-fast-z-score": 1.0864289525102224,
        "water-fast-z-score": 6.7552448758970485,
        "rewrite-fast-z-score": 3.8890872965260113
    },
    {
        "original_text": "The Sunyaev-Zel dovich (SZ) effect is a potentially powerful cosmological tool, as it can be used to detect clusters of galaxies and measure the degree to which their inner regions have been thermalized by Weakly Interacting Massive Particles (WIMPs) or other cold structures. The thermalization of the ICM gas, due to its mergers, plays an important rôle in the SZ detectability of clusters and the accuracy of their WIMP mass limits. We carry out the study of the merging process of galaxy clusters using the SZ effect and show that it is crucial to properly model the SZ signal coming from these systems in order to accurately study their merger state. We provide a new and complete description of the SZ signal, allowing us to accurately characterize clusters that are likely to be undergoing a merger. We also investigate the effect of applying a naive de-mergerization method on the SZ signal, which will introduce a strong bias in the measurement of the projected mass of merging clusters and the distribution of the redshift space distortion parameter, ultimately affecting the cluster dynamical state reconstruction.",
        "watermark_text": "The Sunyaev - Zel dovich ( SZ ) effect is a potentially powerful cosmological technique , as it can be used to identify clusters of stars and estimate the degree to which their internal regions have been thermalized by Weakly Interacting Massive Particles ( WIMPs ) or other warm bodies . The thermalization of the ICM gas , owing to its mergers , plays an important rôle in the SZ detectability of clusters and the accuracy of their WIMP mass limits .We take out the study of the merging process of galaxy galaxies using the SZ phenomenon and suggest that it is crucial to properly model the SZ signal coming from these systems in order to correctly analyze their merger state . We create a new and complete characterization of the SZ signal , allowing us to correctly characterize clusters that are likely to be experiencing a unification .We additionally probe the impact of using a naive de - mergerization technique on the SZ signal , which will create a powerful bias in the measurement of the projected mass of combining complexes and the distribution of the redshift space distortion function , ultimately affecting the cluster dynamical state reconstruction .",
        "rewrite_text": "The Sunyaev-Zel'dovich (SZ) effect is a potentially significant cosmological technique. It can be utilized to identify clusters of stars and estimate the extent of thermalization within their internal regions caused by Weakly Interacting Massive Particles (WIMPs) or other heat-generating bodies. The thermalization of intercluster medium (ICM) gas, often resulting from mergers, plays a crucial role in the detectability of clusters through the SZ effect and the accuracy of WIMP mass limits.\n\nWe investigate the merging process of galaxies using the SZ phenomenon and emphasize the importance of accurately modeling the SZ signal originating from these systems for a proper analysis of their merger status. We establish a comprehensive and novel characterization of the SZ signal, enabling us to accurately classify clusters that may be undergoing a merger process. Furthermore, we explore the consequences of using a basic de-mergerization technique on the SZ signal, which can introduce significant biases in the measurement of projected masses of merging complexes and the distribution of redshift space distortion functions. Ultimately, this can affect the reconstruction of the cluster's dynamical state.",
        "ori-fast-z-score": 0.3418817293789138,
        "water-fast-z-score": 6.495752858199363,
        "rewrite-fast-z-score": 1.9126494315742406
    },
    {
        "original_text": "The Comoros Effect, named after the formerly independent country of Comoros in the Indian Ocean, is the belief that brushing your teeth regularly reduces the number of cavities you acquire. This has been scientifically disproven, yet many people maintain the belief to this day. To further examine the psychology of the Comoros Effect, I present the first quantifiable measurement of the free energy of activation for the abrasive action of toothbrushing. Using a detailed biophysical model of a tooth in motion, I demonstrate that the energy cost of toothbrushing is tolerable and is therefore easily allowed for in the total free energy cost of diet, leading to the erroneous belief that brushing regularly would reduce cavities. This is the first known scientific measurement of the free energy of activation for the abrasive action of toothbrushing, and serves to educate the public on the actual energy costs of various common practices.",
        "watermark_text": "The Comoros Effect , named after the previously independent nation of Comoros in the Indian Ocean , is the belief that brushing your teeth regularly reduces the quantity of cavities you obtain . This has been scientifically disproven , however several people retain the belief to this day .To further examine the psychology of the Comoros Effect , I offer the first quantifiable calculation of the free energy of activation for the abrasive behavior of toothbrushing . Using a detailed biophysical description of a tooth in motion , I test that the electricity cost of toothbrushing is tolerable and is consequently easily allowed for in the total free energy cost of foods , leading to the erroneous view that brushing frequently would improve cavities .This is the first known research assessment of the free energy of activation for the abrasive effect of toothbrushing , and serves to educate the public on the actual power costs of several common practices .",
        "rewrite_text": "The Comoros Effect, named after the former independent nation in the Indian Ocean, is a belief that regular tooth brushing can reduce the occurrence of dental caries. However, this notion has been scientifically disproved, yet many people still hold onto this belief even today. To delve deeper into the psychology behind the Comoros Effect, I present the first quantifiable calculation of the activation free energy associated with the abrasive behavior of toothbrushing. Utilizing a comprehensive biophysical description of a tooth in motion, I examine the electrical cost of toothbrushing, which is deemed tolerable and can be easily accounted for in the overall free energy cost of food consumption. This erroneously leads to the belief that frequent brushing will enhance dental health. This is the initial research evaluation known for calculating the activation free energy related to the abrasive effects of toothbrushing, and it aims to educate the public on the actual power requirements of various common dental hygiene practices.",
        "ori-fast-z-score": -0.4923659639173309,
        "water-fast-z-score": 5.0854241181575475,
        "rewrite-fast-z-score": 0.23249527748763857
    },
    {
        "original_text": "Over several decades, the hypothesis that dark matter is made up of some form of extended object has been widely investigated. These objects are predicted to have formed via the collision and merger of smaller objects, a process which can lead to the formation of stable macroscopic structures. The most concrete evidence for the dark matter hypothesis to date is a cosmological model which incorporates these stable macroscopic structures, known as the Cold Dark Matter model. This model successfully predicts the large-scale structure of the universe we observe today, prompting comparison to similar models in the particulate physics domain, where a similar level of agreement has been achieved through the use of quantum zero-point energy. Despite its success, there are still aspects of the Cold Dark Matter model which lack explanation. Chief amongst these is the formation of stable macroscopic structures at late times, which does not occur in simulations which ignore these zero-point fluctuations. This Letter will introduce a way in which dark matter may self-organise into spherical structures, without the need to appeal to dark energy or other non-particle forces. Spherical collapse experiments demonstrate that spherical structures are indeed a stable outcome of these experiments, and this Letter presents empirical evidence for the hypothesis that dark matter is self-organising into these structures. The methodology of the Letter is as follows. Dark matter is introduced to a simulation which includes zero-point fluctuations, and the resulting large-scale structure is examined. The letter s hypothesis is then tested by introducing dark matter to a simulation which does not include zero-point fluctuations, and observing the resulting structure. The letter concludes by outlining future work in which the letter s hypothesis will be tested against observed large-scale structure, in order to provide further evidence for the hypothesis that dark matter is self-organising into spherical structures. The methodology of this Letter is original, and whilst similar work has previously been conducted on the subject of dark matter self-organisation, to the best of the author s knowledge, this Letter is the first to empirically demonstrate the self-organisation of dark matter into stable macroscopic structures. This Letter is the author s Master s thesis, and was written under the supervision of Professor Ian Robertson.",
        "watermark_text": "Over multiple years , the notion that dark matter is made up of some kind of extended object has been widely examined . These structures are expected to have formed via the interaction and merger of tiny particles , a process which can lead to the formation of stable macroscopic objects .The most formal evidence for the dark matter hypothesis to date is a cosmological model which includes these stable macroscopic properties , known as the Cold Dark Matter theory . This theory fully predicts the huge - scale configuration of the universe we perceive nowadays , inviting analogy to similar models in the particulate science domain , where a similar level of agreement has been achieved through the using of quantum zero - point energy .Despite its success , there are still aspects of the Cold Dark Matter theory which require explanation . Chief amongst these is the formation of stable macroscopic structures at late times , which does not occur in simulations which ignore these zero - point fluctuations .This Letter will provide a way in which black material may self - organise into spherical objects , without the necessity to appeal to dark energy or other non - particle forces . Spherical collapse tests prove that spherical systems are indeed a steady outcome of these experiments , and this Letter offers experimental evidence for the notion that dark matter is self - organising into these structures .The technique of the Letter is as follows . Dark matter is applied to a simulation which includes zero - point fluctuations , and the resulting high - scale system is investigated .The letter s hypothesis is then evaluated by bringing dark matter to a simulation which does not include zero - point fluctuations , and observing the resulting structure . The letter concludes by outlining future research in which the letter s hypothesis will be evaluated against observed large - scale structure , in order to provide further evidence for the assumption that dark matter is self - organising into spherical systems .The technique of this Letter is new , and whilst comparable research has formerly been performed on the subject of bright mind self - organisation , to the best of the writer s knowledge , this Letter is the first to empirically prove the self - organisation of bright matter into stable macroscopic objects . This Letter is the writer s Master s thesis , and was written under the guidance of Professor Ian Robertson .",
        "rewrite_text": "Over multiple years, the concept that dark matter consists of some form of extended object has been extensively examined in the scientific community. These structures are anticipated to have formed through the interaction and merging of tiny particles, a process that can lead to the formation of stable macroscopic objects. The most established evidence for the dark matter hypothesis so far is a cosmological model known as the Cold Dark Matter theory, which incorporates these stable macroscopic properties.\n\nThis theory comprehensively predicts the vast configuration of the universe we observe today, mirroring similar models in particle science where quantum zero-point energy has achieved a comparable level of agreement. Despite its success, there are still aspects of the Cold Dark Matter theory that require further explanation. A primary concern is the formation of stable macroscopic structures at later stages, which does not occur in simulations that ignore these zero-point fluctuations.\n\nThis letter presents a method through which dark matter may self-organize into spherical objects, without relying on dark energy or other non-particle forces. Experimental tests on spherical collapse demonstrate that spherical systems are a consistent outcome of these experiments, providing empirical evidence that dark matter self-organizes into these structures.\n\nThe technique described in this letter involves applying dark matter to a simulation that incorporates zero-point fluctuations and examining the resulting high-scale system. The hypothesis is then evaluated by introducing dark matter into a simulation without zero-point fluctuations and observing the resulting structure.\n\nThe letter concludes by outlining future research that will evaluate the letter's hypothesis against observed large-scale structures, aiming to provide further evidence for the assumption that dark matter self-organizes into spherical systems. This technique is novel, and while similar research has been conducted on the self-organization of bright matter, to the best of my knowledge, this letter is the first to empirically demonstrate the self-organization of bright matter into stable macroscopic objects. This letter represents the author's Master's thesis, written under the guidance of Professor Ian Robertson.",
        "ori-fast-z-score": 0.07474350927519359,
        "water-fast-z-score": 8.994380267950337,
        "rewrite-fast-z-score": 4.851827478904982
    },
    {
        "original_text": "Self-organized GeMn nano-columns were studied by means of both, scanning and transmission electron microscopy and extended X-ray absorption spectroscopy. The GeMn columns were grown by chemical vapor deposition on Ge(100) substrates at 300 °C. The average diameter of the columns is about 30 nm. High-angle annular dark-field scanning transmission electron microscopy imaging demonstrates that the GeMn columns are formed by accumulation of smaller grains at the end of larger ones. Transmission electron microscopy tomography analysis shows that these small grains are enriched in Mn with respect to the substrate composition. The GeMn composition strongly varies across the columns. Extended X-ray absorption fine structure spectroscopy measurements show that GeMn columns incorporate significant amounts of interstitial Mn atoms, that are probably responsible for the ferromagnetic character of the material. The self-organization process during GeMn column growth allows tailoring of the GeMn composition and structure, which strongly influences the magnetic properties of the material. In particular, the columns incorporate significant amounts of interstitial Mn atoms that are probably responsible for their ferromagnetic behavior.",
        "watermark_text": "Self - organized GeMn nano - columns were studied by means of both , scanning and transmission electron microscopy and extended X - ray diffusion spectroscopy . The GeMn columns were grown by chemical vapor precipitation on Ge ( 100 ) substrates at 300 °C .The estimated size of the pillars is about 30 nm . High - angle annular dark - field scan transmission electron microscopy imaging reveals that the GeMn columns are created by accumulation of tiny grains at the end of bigger ones .Transmission electron microscopy tomography analysis shows that these little grains are enriched in Mn with regard to the substrate structure . The GeMn structure strongly depends across the columns .Extended X - ray absorption fine structure spectroscopy observations show that GeMn columns incorporate significant amounts of interstitial Mn atoms , that are probably controlling for the ferromagnetic character of the metal . The self - organization reaction during GeMn column growth allows tailoring of the GeMn structure and shape , which strongly effects the magnetic properties of the metal .In particular , the pillars contain significant amounts of interstitial Mn atoms that are probably controlling for their ferromagnetic activity .",
        "rewrite_text": "Self-organized GeMn nano-columns were investigated utilizing scanning and transmission electron microscopy, as well as extended X-ray diffusion spectroscopy. These GeMn columns were grown through chemical vapor deposition on Ge (100) substrates at a temperature of 300°C. An estimated size of the columns is approximately 30 nanometers. High-angle annular dark-field scan transmission electron microscopy imaging reveals that the GeMn columns are formed by the accumulation of tiny grains at the ends of larger ones. Tomographic analysis using transmission electron microscopy demonstrates that these small grains are enriched in Mn compared to the substrate structure.\n\nThe GeMn structure exhibits significant variations across the columns. Observations from extended X-ray absorption fine structure spectroscopy reveal that the GeMn columns contain significant amounts of interstitial Mn atoms, which likely play a crucial role in determining the ferromagnetic properties of the metal. The self-organization process during GeMn column growth allows for the tailoring of the GeMn structure and shape, which significantly impacts the magnetic properties of the metal. Specifically, the columns are rich in interstitial Mn atoms that likely control their ferromagnetic activity.",
        "ori-fast-z-score": -1.6269784336399213,
        "water-fast-z-score": 4.1812388858673994,
        "rewrite-fast-z-score": 1.4757295747452437
    },
    {
        "original_text": "The InterHourly-Variability (IHV) Index of Geomagnetic Activity and its Use in Deriving the Long-term Variation of Solar Wind Speed. The InterHourly-variability (IHV) index of Geomagnetic Activity is introduced and its characteristics are defined. This is then used to characterize the long-term variation of Solar Wind Speed (SW speed) that has occurred over the past 450 years. The SW speed variation is shown to comprise two component parts, a primary component (periodic wave with a period of around 11 years) and a secondary component (a clear long-term increase). The IHV index is shown to track the SW speed variation very closely, with the change in SW speed driving the change in IHV index. The SW speed variation is shown to have an overall increasing trend, at a rate of around 20 km/s/century. This has important implications for Space Weather, as the increased SW speed directly influences how much energy the Solar Wind carries.",
        "watermark_text": "The InterHourly - Variability ( IHV ) Index of Geomagnetic Activity and its Use in Deriving the Long - term Variation of Solar Wind Speed . The InterHourly - variability ( IHV ) index of Geomagnetic Activity is adopted and its characteristics are specified .This is then used to characterize the long - term variation of Solar Wind Speed ( SW speed ) that has occurred over the previous 450 years . The SW speed difference is demonstrated to contain two constituent parts , a primary part ( periodic wave with a period of around 11 weeks ) and a secondary component ( a clear long - term increase ) .The IHV indicator is demonstrated to track the SW speed difference very closely , with the shift in SW speed driving the shift in IHV index . The SW speed change is demonstrated to have an overall increasing growth , at a rate of around 20 km / s / century .This has crucial consequences for Space Weather , as the increased SW speed directly influences how many heat the Solar Wind carries .",
        "rewrite_text": "The InterHourly Variability (IHV) Index for Geomagnetic Activity and Its Application in Determining Long-Term Solar Wind Speed Variations:\n\nThe IHV index for assessing geomagnetic activity has been selected and its features have been precisely defined. This index is then utilized to characterize the long-term variations in solar wind speed (SW speed) that have occurred over the past 450 years. It is demonstrated that the difference in SW speed comprises two components: a primary periodic wave with a period of approximately 11 weeks and a distinct long-term increase. The IHV indicator closely tracks the SW speed difference, with shifts in SW speed leading to corresponding shifts in the IHV index. The observed changes in SW speed exhibit an overall increasing trend, with a rate of approximately 20 km/s per century. This has significant implications for space weather, as the increased SW speed directly affects the amount of heat carried by the solar wind.",
        "ori-fast-z-score": 1.1523319193960637,
        "water-fast-z-score": 6.017733356846111,
        "rewrite-fast-z-score": 2.3570226039551585
    },
    {
        "original_text": "For the problem of weighted matching in general (arboreal) graphs, we show that the continuous relaxation of the linear programming (LP) dual (which is often used in practice) is equivalent to solving a max-product algorithm. We then generalize this result to graphs that contain a tree-subgraph, which covers a large class of arboreal networks. In particular, this covers all graphs that can be matched with a polynomial-time algorithm when all edge weights are 1. For such graphs, we also show that the continuous relaxation is in strong duality with the original discrete optimization problem, and provide a primal feasibility guarantee. Finally, we present some empirical evidence that the max-product algorithm performs better than the LP relaxation on arboreal instances. Here is the link to the paper on arXiv.org: https://arxiv.org/abs/1905.06538 Here is the abstract of the paper: For the problem of weighted matching in general (arboreal) graphs, we show that the continuous relaxation of the linear programming (LP) dual (which is often used in practice) is equivalent to solving a max-product algorithm. We then generalize this result to graphs that contain a tree-subgraph, which covers a large class of arboreal networks. In particular, this covers all graphs that can be matched with a polynomial-time algorithm when all edge weights are 1. For such graphs, we also show that the continuous relaxation is in strong duality with the original discrete optimization problem, and provide a primal feasibility guarantee. Finally, we present some empirical evidence that the max-product algorithm performs better than the LP relaxation on arboreal instances. Here is the link to the paper: https://arxiv.org/abs/1905.06538 The complete paper is available here: https://arxiv.org/abs/1905.06538 The paper is authored by Jiehui Lu, Xingye Lu, and Nan Zhang. The paper is related to Linear Programming, Weighted Matching, General Graphs, Tree Subgraphs. Thank you for reading. - Authors of the paper Jiehui Lu, Xingye Lu, and Nan Zhang School of Computer Science University of California, Davis Davis, CA, 95616, USA jiehui@ucdavis.edu, xy Lu @ ucdavis.edu, nnz@ucdavis.edu https://ucdavis.edu/jiehui Department of Computer Science and Engineering Tsinghua University Beijing, 100084, China lyf@tsinghua.edu.cn https://www.tsinghua.edu.cn/lyf Institute of Information Engineering Sun Yat-sen University Guangzhou, 510006, China",
        "watermark_text": "For the question of weighted matching in general ( arboreal ) graphs , we prove that the smooth relaxation of the linear programming ( LP ) dual ( which is often employed in practice ) is analogous to solving a max - product algorithm . We then generalize this fact to graphs that include a tree - subgraph , which covers a large class of arboreal systems .In particular , this encompasses all graphs that can be paired with a polynomial - time algorithm when all edge weights are 1 . For such graphs , we also prove that the discrete relaxation is in good duality with the previous discrete optimization problem , and provide a primal feasibility guarantee .Finally , we present some empirical evidence that the max - product algorithm performs better than the LP relaxation on arboreal instances . Here is the link to the paper on arXiv . org : https : / / arxiv . org / abs / 1905 . 06538 Here is the abstract of the paper : For the problem of weighted matching in general ( arboreal ) graphs , we show that the continuous relaxation of the linear programming ( LP ) dual ( which is often used in practice ) is equivalent to solving a max - product algorithm .We then generalize this consequence to graphs that include a tree - subgraph , which covers a large class of arboreal systems . In particular , this encompasses all graphs that can be paired with a polynomial - time algorithm when all edge weights are 1 .For such graphs , we also demonstrate that the discrete relaxation is in good duality with the previous discrete optimization problem , and provide a primal feasibility guarantee . Finally , we present some experimental evidence that the max - product algorithm performs better than the LP relaxed on arboreal instances .Here is the link to the paper : https : / / arxiv . org / abs / 1905 . 06538 The complete paper is accessible here : https : / / arxiv . org / abs / 1905 . 06538 The paper is authored by Jiehui Lu , Xingye Lu , and Nan Zhang . The paper is related to Linear Programming , Weighted Matching , General Graphs , Tree Subgraphs .Thank you for reading . - Authors of the paper Jiehui Lu , Xingye Lu , and Nan Zhang School of Computer Science University of California , Davis Davis , CA , 95616 , USA jiehui @ ucdavis . edu , xy Lu @ ucdavis . edu , nnz @ ucdavis . edu https : / / ucdavis . edu / jiehui Department of Computer Science and Engineering Tsinghua University Beijing , 100084 , China lyf @ tsinghua . edu . cn https : / / www . tsinghua . edu . cn / lyf Institute of Information Engineering Sun Yat - sen University Guangzhou , 510006 , China",
        "rewrite_text": "Regarding the weighted matching issue within general (or arboreal) graphs, we establish that the smooth relaxation of the linear programming (LP) dual, which is frequently employed in practical applications, bears similarities to solving a max-product algorithm. This observation is then extended to graphs containing a tree-subgraph, encompassing a broad range of arboreal systems. Specifically, it covers all graphs that can be paired with polynomial-time algorithms when all edge weights are set to 1. For these graphs, we further demonstrate that discrete relaxation maintains a strong duality with previous discrete optimization problems, and provides a guarantee for primal feasibility.\n\nEmpirical evidence suggests that the max-product algorithm outperforms the LP relaxation when applied to arboreal instances. The link to the paper on arXiv.org is provided below: https://arxiv.org/abs/1905.06538.\n\nThe abstract of the paper is as follows: We show that the continuous relaxation of the linear programming (LP) dual, which is commonly used in practice, is equivalent to solving a max-product algorithm for weighted matching problems in general (arboreal) graphs. This equivalence is then generalized to graphs that include a tree-subgraph, encompassing a wide range of arboreal systems. Notably, this includes all graphs that can be paired with polynomial-time algorithms when all edge weights are set to 1. For these graphs, we demonstrate the duality between discrete relaxation and previous discrete optimization problems and provide a guarantee for primal feasibility. Furthermore, our experimental results indicate that the max-product algorithm performs better than the LP relaxation in arboreal instances.\n\nThe complete paper can be accessed here: https://arxiv.org/abs/1905.06538. The authors of this paper are Jiehui Lu, Xingye Lu, and Nan Zhang. It is affiliated with Linear Programming, Weighted Matching, General Graphs, and Tree Subgraphs.\n\nThank you for reading. - The authors of the paper, Jiehui Lu, Xingye Lu, and Nan Zhang from the School of Computer Science at the University of California, Davis, CA 95616, USA. You can reach us at jiehui@ucdavis.edu, xylu@ucdavis.edu, and nnz@ucdavis.edu. Our departmental websites are: https://ucdavis.edu/jiehui and https://www.tsinghua.edu.cn/lyf. We also have affiliates at the Department of Computer Science and Engineering, Tsinghua University in Beijing, China and the Institute of Information Engineering, Sun Yat-sen University in Guangzhou, China.",
        "ori-fast-z-score": 0.8451542547285166,
        "water-fast-z-score": 5.0709255283711,
        "rewrite-fast-z-score": 0.5107539184552492
    },
    {
        "original_text": "Star-forming dwarf galaxies (SFDG) are important systems in the early stages of galaxy formation. These small systems are efficient in their ability to form stars, therefore they are ideal laboratories for understanding the initial conditions of galaxy evolution. In this paper we present optical spectroscopy and spatially resolved analysis of 12 SFDG. We determine oxygen abundances and chemical abundances patterns using both direct method and strong line methods. We also explore the spatial distribution of the chemical elements and study the relationships between different parameters. Our main results can be summarised as follows: 1. We found extremely low oxygen abundances with higher values of 12 + log(O/H) estimated using the R23 method compared to the Pettini & Pagel’s (2004) strong line method. 2. We did not find any correlation between oxygen abundances and kinematic radial velocities. 3. Calcium abundance seems to decrease with the increasing radius. In contrast, sodium and aluminum abundances seem to increase at large distances from the centers. 4. We did not find any correlation between chemical abundances and structural parameters of the galaxies. However, we noticed a correlation between SFR and chemical abundances. We also found a very high fraction of SF regions with 12 + log(O/H) values lower than 8.2, which is a threshold used by many authors to distinguish between normal and low-metallicity regions.",
        "watermark_text": "Star - creating dwarf stars ( SFDG ) are important structures in the early stages of galaxy formation . These tiny systems are successful in their potential to form galaxies , therefore they are excellent labs for studying the early conditions of galaxy evolution .In this paper we present imaging spectroscopy and spatially resolved analysis of 12 SFDG . We determine oxygen abundances and biological abundances patterns use both direct technique and strong line methods .We additionally discuss the spatial distribution of the chemical elements and study the relationships between various variables . Our main results can be summarised as follows : 1 .We identified extremely poor oxygen abundances with higher values of 12 + log ( O / H ) assumed using the R23 method compared to the Pettini & Pagel ’ s ( 2004 ) weak line model . 2 .We did not find any correlation between oxygen abundances and kinematic transverse velocities . 3 .Calcium concentrations seems to reduce with the increasing diameter . In comparison , sodium and aluminum abundances seem to expand at large distances from the centers .4 . We did not find any correlation between chemical abundances and physical properties of the galaxies .However , we saw a correlation between SFR and material abundances . We additionally found a very high fraction of SF regions with 12 + log ( O / H ) values lower than 8 . 2 , which is a threshold utilized by many writers to distinguish between normal and low - metallicity areas .",
        "rewrite_text": "Stars - the creation of dwarf star systems (SFDG) play a pivotal role in the initial phases of galaxy formation. These small systems possess remarkable potential for shaping galaxies, making them invaluable laboratories for studying the early conditions of galaxy evolution. In this paper, we present imaging spectroscopy and spatially resolved analysis of 12 SFDG systems. We employ both direct techniques and strong line methods to determine oxygen and biological abundance patterns. Furthermore, we discuss the spatial distribution of chemical elements and explore the relationships between various variables. Our key findings can be summarized as follows:\n\n1. We identified significantly low oxygen abundances with higher values of 12 + log (O/H) using the R23 method compared to the weaker line model proposed by Pettini & Pagel (2004).\n2. No correlation was found between oxygen abundances and kinematic transverse velocities.\n3. Calcium concentrations seem to decrease with increasing diameter, whereas sodium and aluminum abundances expand at greater distances from the galaxy centers.\n4. We did not detect any correlation between chemical abundances and the physical properties of galaxies. However, a correlation was observed between star formation rates (SFR) and material abundances. Additionally, we found a notably high proportion of SF regions with 12 + log (O/H) values below 8.2, a threshold often used to distinguish between normal and low-metallicity regions in the literature.",
        "ori-fast-z-score": -0.7770286898858113,
        "water-fast-z-score": 7.637218263460742,
        "rewrite-fast-z-score": 1.873171623163388
    },
    {
        "original_text": "Orbital-Free Density Functional Theory (OF-DFT) is an approach to electronic structure theory that combines the advantages of local density and wavefunction-based theories. The theory is formally obtained from conventional Density Functional Theory (DFT) by partitioning the total energy into a linear combination of local energy functionals, each depending on the density and one additional parameter. The parameters, called potentials, are determined by solving a constrained minimization problem. Thus, in contrast to DFT, the orbital-free OF-DFT does not assume a single-electron wavefunction and can therefore directly handle molecules and solids. We provide an implementation of the theory based on a numerical implementation of the Schrödinger equation in Lagrangian form. The theory is validated on molecules and solids and shown to give excellent results for several test cases. Finally, we present a case study of graphene on Au(111), where we find that interaction-induced physis and chemisn plays a significant role in determining the structures.",
        "watermark_text": "Orbital - Free Density Functional Theory ( OF - DFT ) is an approach to electronic structure theoretical that combines the advantages of local density and wavefunction - based theories . The theory is fully obtained from standard Density Functional Theory ( DFT ) by partitioning the total energy into a linear mixture of local power functionals , each varying on the density and one additional parameter .The parameters , called potentials , are decided by solving a constrained minimization problem . Thus , in comparison to DFT , the orbital - free OF - DFT does not assume a single - ion wavefunction and can thus effectively handle molecules and solids .We provide an implementation of the principle relying on a numerical formulation of the Schrödinger equation in Lagrangian form . The theory is validated on molecules and solids and demonstrated to give good results for numerous test situations .Finally , we present a case study of graphene on Au ( 111 ) , where we find that interaction - mediated physis and chemisn plays a major part in identifying the structures .",
        "rewrite_text": "Orbital-Free Density Functional Theory (OF-DFT) is a method in electronic structure theory that combines the strengths of both local density and wavefunction-based theories. This theory is derived entirely from the standard Density Functional Theory (DFT) by partitioning the total energy into a linear combination of local power functionals, each of which varies with density and an additional parameter. These parameters, known as potentials, are determined through the solution of a constrained minimization problem. In contrast to DFT, OF-DFT does not rely on a single-ion wavefunction, making it capable of effectively handling both molecules and solids.\n\nWe have implemented this principle using a numerical formulation of the Schrödinger equation in Lagrangian form. The theory has been validated on molecules and solids, demonstrating excellent results in numerous testing scenarios. As a final case study, we present an examination of graphene on Au (111), where we found that interaction-mediated physics and chemistry play a significant role in identifying the structure.",
        "ori-fast-z-score": -1.5650160901149996,
        "water-fast-z-score": 3.491189739487307,
        "rewrite-fast-z-score": -1.0834726777719228
    },
    {
        "original_text": "J 0903+0357 is a nearby L dwarf observed by the Gaia satellite in the preliminary data release. It has a radial velocity and two new optical spectra, one from GTC and one from LBT. J 0903+0357 is one of the most distant and faintest L dwarfs yet observed. J 0903+0357 s radial velocity is 22.2 km/s, indicating it is a low-mass member of the local solar neighborhood. The spectra are consistent with L dwarf characteristics, but the JHK colors are unusual for L dwarfs, especially its red color. This color cannot be explained by the late-M spectral classification, so it may be a binary or a very low surface gravity object. If it is a binary, it would be the most distant and most tightly bound system of any L dwarf. Alternatively, a late-L companion to a mid-M dwarf could explain the red color. It will be important to observe J 0903+0357 over multiple epochs to determine which of these scenarios is correct.",
        "watermark_text": "J 0903 + 0357 is a nearby L dwarf seen by the Gaia satellite in the preliminary images release . It has a radial speed and two new optical spectra , one from GTC and one from LBT .J 0903 + 0357 is one of the most distant and faintest L dwarfs yet observed . J 0903 + 0357 s radial speed is 22 . 2 km / s , suggests it is a small - weight member of the local solar neighborhood .The spectra are compatible with L dwarf characteristics , but the JHK colors are peculiar for L dwarfs , particularly its red color . This color cannot be understood by the early - M spectral classification , so it could be a binary or a very low surface gravity object .If it is a binary , it would be the most distant and most closely bound system of any L dwarf . Alternatively , a late - L companion to a middle - M dwarf might explain the red color .It will be crucial to observe J 0903 + 0357 over numerous epochs to find which of these scenarios is accurate .",
        "rewrite_text": "J 0903+0357 is an L-type dwarf situated close to Earth, identified by the Gaia satellite's preliminary image release. This celestial body boasts a radial velocity of 22.2 km/s and boasts two novel optical spectra; one sourced from the GTC, while the other is from LBT. Intriguingly, J 0903+0357 stands out as one of the most distant and faint L dwarfs ever observed.\n\nThe spectra of J 0903+0357 align with the characteristics of L dwarfs, yet its JHK colors deviate from the norm for L dwarfs, particularly its pronounced red hue. This color cannot be explained by the early-M spectral classification, suggesting it could either be a binary system or a highly low-surface gravity object. If it is a binary system, it would be the most distant and tightly bound pair among all L dwarfs. Alternatively, a late-L companion to a middle-M dwarf could account for the red color.\n\nTo determine which of these scenarios is accurate, it is imperative to observe J 0903+0357 over multiple epochs.",
        "ori-fast-z-score": -0.8551861104941365,
        "water-fast-z-score": 4.275930552470682,
        "rewrite-fast-z-score": 1.8325416653445783
    },
    {
        "original_text": "Twenty three globular clusters (GCs) in the inner region of the Milky Way are called the bulge globular clusters. These clusters are very old and have low metallicities. Consequently they have red horizontal branches (RHBs). Terzan 5, Liller 1, UKS 1 and Terzan 4 are very luminous and massive, and are known to have extended cores. HST observations in the F110W and F160W bands were used to obtain their relative proper motions and consequently their 3D space positions. These four clusters are on average 31.2 kpc from the Galactic center. Terzan 5 is at a distance of 25.3 kpc from the Galactic center, which is closer than the Galactic center. This cluster is the closest to the center of the Milky Way among the known globular clusters. The other clusters are at distances of 32.4 kpc (Liller 1), 33.7 kpc (UKS 1) and 38.1 kpc (Terzan 4). The spatial distribution of the bulge globular clusters has important consequences for the formation and evolution of the Galaxy. Terzan 5 is a possible globular cluster progenitor that may have been disrupted by the central supermassive black hole. The other clusters are likely to have formed during the early stages of the Galaxy s formation.",
        "watermark_text": "Twenty three globular complexes ( GCs ) in the inner region of the Milky Way are called the bulge globular clusters . These clusters are very ancient and have poor metallicities .Consequently they have red horizontal branches ( RHBs ) . Terzan 5 , Liller 1 , UKS 1 and Terzan 4 are very luminous and massive , and are known to have extended cores .HST observations in the F110W and F160W bands were used to obtain their relative proper motions and consequently their 3D space positions . These four galaxies are on average 31 . 2 kpc from the Galactic center .Terzan 5 is at a distance of 25 . 3 kpc from the Galactic center , which is closer than the Galactic center . This cluster is the nearest to the center of the Milky Way among the known globular galaxies .The other complexes are at distances of 32 . 4 kpc ( Liller 1 ) , 33 . 7 kpc ( UKS 1 ) and 38 . 1 kpc ( Terzan 4 ) . The geographic distribution of the bulge globular complexes has significant implications for the formation and evolution of the Galaxy .Terzan 5 is a possible globular cluster progenitor that might have been disrupted by the main supermassive black hole . The other complexes are likely to have formed during the early stages of the Galaxy s formation .",
        "rewrite_text": "In the inner region of the Milky Way, there are twenty-three globular complexes (GCs) collectively referred to as the bulge globular clusters. These clusters are extremely ancient and exhibit poor metallicities, resulting in the presence of red horizontal branches (RHBs). Notably, Terzan 5, Liller 1, UKS 1, and Terzan 4 are exceptionally luminous and massive, with extended cores that are well-documented.\n\nHST observations in the F110W and F160W bands were employed to determine their relative proper motions, subsequently revealing their three-dimensional spatial positions. These four galaxies are situated at an average distance of 31.2 kpc from the Galactic center. Specifically, Terzan 5 is located at a distance of 25.3 kpc from the center, making it closer than the center itself. This cluster stands as the nearest to the center of the Milky Way among all known globular galaxies.\n\nThe other complexes are situated at distances of 32.4 kpc (Liller 1), 33.7 kpc (UKS 1), and 38.1 kpc (Terzan 4). The spatial distribution of these bulge globular complexes holds significant implications for the formation and evolution of the Galaxy. Terzan 5 is a potential progenitor of a globular cluster that may have been disrupted by the primary supermassive black hole. On the other hand, the other complexes likely formed during the early stages of the Galaxy's formation.",
        "ori-fast-z-score": 0.2626128657194451,
        "water-fast-z-score": 4.03585624040554,
        "rewrite-fast-z-score": 1.709408646894569
    },
    {
        "original_text": "The characterization of instrumental phase stability is necessary for several applications in astronomy such as the detection of meteors, the study of the atmosphere or for the detection of artificial satellites. In the field of radio astronomy, phase stability is necessary to achieve the coherence needed to conduct precision astronomical observations. The coherence is obtained by referencing the signal to an external source whose stability is known with great precision (a hydrogen maser for instance). The stability of the astronomical instrument itself is the amount of jitters that does not degrade the coherence of the signal. To characterize the phase stability of the IRAM 30-meter telescope, we implemented a phase referenced signal at 10.5 cm in the form of a dual channel Bracewell discriminator. By doing this, we could measure the phase stability between two channels every 20 minutes with a 1.1 millas RMS. We have also shown how to compute the equivalent closed loop transfer function of the system. This is relevant for the control community because this kind of measurements are often used as a probe for the stability of an Airbus A300 airplane or a satellite. This stability is very important for their reliability.",
        "watermark_text": "The characterization of instrumental phase stability is required for numerous uses in science such as the observation of meteors , the observation of the atmosphere or for the observation of artificial satellites . In the field of radio astronomy , phase stability is required to achieve the coherence necessary to conduct precision astronomical measurements .The coherence is achieved by referencing the signal to an external source whose stability is known with great skill ( a hydrogen maser for instance ) . The stability of the astronomical instrument itself is the quantity of jitters that does not degrade the coherence of the signal .To characterize the phase safety of the IRAM 30 - meter telescope , we implemented a phase referenced signal at 10 . 5 centimetres in the form of a twin channel Bracewell discriminator . By doing this , we could assess the phase stability between two signals every 20 minutes with a 1 . 1 millas RMS .We have also shown how to compute the equivalent closed loop transfer function of the device . This is relevant for the control community because this form of measurements are often employed as a probe for the stability of an Airbus A300 aircraft or a spacecraft .This integrity is very important for their reliability .",
        "rewrite_text": "Characterizing the stability of instrumental phases is crucial for various scientific applications, such as observing meteorites, monitoring the atmosphere, or detecting artificial satellites. In the realm of radio astronomy, phase stability is essential to achieve the necessary coherence for conducting precise astronomical measurements. This coherence is achieved by referencing the signal to an external source with a well-known stability, such as a hydrogen maser. The stability of the astronomical instrument itself is a measure of any jitter that does not compromise the coherence of the signal.\n\nTo assess the phase stability of the IRAM 30-meter telescope, we implemented a phase-referenced signal at 10.5 centimeters using a twin-channel Bracewell discriminator. By doing this, we can evaluate the phase stability between two signals every 20 minutes with a precision of 1.1 millas RMS. We have also demonstrated how to calculate the equivalent closed-loop transfer function of the device, which is significant for control systems as measurements like these are often used to assess the stability of aircraft like the Airbus A300 or spacecraft. This integrity is crucial for their reliability.",
        "ori-fast-z-score": -1.2222222222222223,
        "water-fast-z-score": 3.888888888888889,
        "rewrite-fast-z-score": 0.3375263702778072
    },
    {
        "original_text": "Pappus s theorem in plane geometry states that three lines in the plane intersecting in a triad of points, there exists a fourth line not in the plane, containing one of the triad of points. There have been many proofs of this theorem since it was first stated in the 2nd century AD. In this short note we present a simple and beautiful proof of Pappus s theorem that has some interesting geometric and algorithmic applications. Our approach is to discretize Pappus s configuration of lines in the plane and solve an arrangement problem on a gridded plane. The theorem follows as a special case of the theorem for this problem. We also present an alternate, but more complex and less direct, proof of Pappus s theorem, which may be of independent interest. esk A simple and beautiful proof of Pappus s theorem We present a simple and beautiful proof of Pappus s theorem that has some interesting geometric and algorithmic applications. Our approach is to discretize Pappus s configuration of lines in the plane and solve an arrangement problem on a gridded plane. The theorem follows as a special case of the theorem for this problem.",
        "watermark_text": "Pappus s theorem in plane geometry says that three lines in the plane intersecting in a triad of points , there exists a fourth line not in the plane , containing one of the triad of points . There have been many proofs of this theorem since it was first stated in the 2nd millennium AD .In this short note we present a simple and beautiful proof of Pappus s theorem that has some interesting geometric and algorithmic use . Our solution is to discretize Pappus s composition of lines in the plane and solve an arrangement task on a gridded plane .The theorem follows as a special case of the theorem for this question . We additionally offer an alternate , but more complex and less formal , proof of Pappus s theorem , which may be of independent importance .esk A simple and beautiful proof of Pappus s theorem We create a simple and beautiful proof of Pappus s theorem that has some interesting geometric and algorithmic use . Our perspective is to discretize Pappus s composition of lines in the plane and solve an arrangement issue on a gridded plane .The theorem follows as a special case of the theorem for this question .",
        "rewrite_text": "In plane geometry, Pappus's theorem states that given three lines intersecting at a set of three points, there always exists a fourth line not within the plane that contains one of the points from the set. This theorem has been proven numerous times since its initial statement in the 2nd millennium AD. In this brief communication, we offer a straightforward and captivating proof of Pappus's theorem that offers intriguing geometric and algorithmic applications.\n\nOur approach involves discretizing Pappus's line composition in the plane and resolving an arrangement task on a gridded surface. This theorem naturally follows as a specific case of the theorem related to this problem. Additionally, we present an alternative, albeit more complex and less formal, proof of Pappus's theorem that may hold independent significance.\n\nA simple and elegant proof of Pappus's theorem has been developed. This proof offers both geometric and algorithmic significance and is achieved by discretizing the composition of lines in the plane according to Pappus's theory and addressing an arrangement challenge on a gridded plane. As a particular instance of a broader theorem, it becomes a logical consequence of our approach. Moreover, we provide another variation of proof, albeit more intricate and less structured, which could have independent value.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": -0.20203050891044214
    },
    {
        "original_text": "Massive stars are formed in clusters. Their massive radiative and mechanical energies profoundly influence their environment. Despite their prevalence and crucial impact, the massive star formation process, particularly in its earliest phases, is difficult to observe due to the high spatial resolution and sensitivity needed. The water masers arise only in actively forming massive stars and gas reservoir, and so provide an excellent probe of this earliest phase. Using new Very Long Baseline Interferometry (VLBI) observations, we detect 23 new radio water masers associated with the high-mass star formation in the Nuclear Disk of the Milky Way. The distribution, velocities, and dispersion of the water masers are consistent with them tracing an inclined rotating disk. The dynamical mass of the disk is estimated to be (3.6±0.7)×10^5 M⊙, comparable to the previous upper limit of the Nuclear Disk. This, along with the coincident far-infrared and γ-ray emissions, suggests the current high-mass star formation in the Nuclear Disk is in an early stage with substantial amounts of gas and dust.",
        "watermark_text": "Massive stars are created in clusters . Their huge radiative and physical energies profoundly affect their environment .Despite their prevalence and crucial influence , the huge star formation transition , particularly in its earliest periods , is complex to observe due to the high visual clarity and sensitivity required . The water masers occur only in actively creating huge stars and gas reservoir , and so give an excellent probe of this earliest stage .Using new Very Long Baseline Interferometry ( VLBI ) observations , we locate 23 newest radio freshwater masers associated with the high - mass star formation in the Nuclear Disk of the Milky Way . The distribution , velocities , and dispersion of the water masers are compatible with them tracing an inclined spinning disk .The dynamical mass of the disk is estimated to be ( 3 . 6±0 . 7 ) ×10 ^ 5 [UNK] , comparable to the previous upper limit of the Nuclear Disk . This , along with the coincident far - infrared and γ - ray emissions , suggests the current high - mass star formation in the Nuclear Disk is in an early stage with substantial amounts of gas and dust .",
        "rewrite_text": "Giant stars are formed in clusters, exerting profound impacts on their surroundings through their immense radiative and physical energies. Despite their widespread occurrence and significant influence, observing the transition of massive star formation, especially in its earliest phases, is challenging due to the high visual clarity and sensitivity required. Water masers only manifest in actively creating environments of large stars and gas reservoirs, making them an excellent probe of this early stage. Utilizing recent Very Long Baseline Interferometry (VLBI) observations, we have identified 23 new radio-freshwater masers linked to high-mass star formation within the Nuclear Disk of the Milky Way. The distribution, velocities, and dispersion of these water masers align with them tracing an inclined rotating disk. The estimated dynamical mass of the disk is (3.6 ± 0.7) × 10^5 units, comparable to the previous upper limit of the Nuclear Disk's mass. This, coupled with concurrent far-infrared and γ-ray emissions, suggests that the current high-mass star formation in the Nuclear Disk is in its early stage, with significant amounts of gas and dust present.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 4.2808061395979236,
        "rewrite-fast-z-score": 0.5241424183609592
    },
    {
        "original_text": "In this paper, we construct supersymmetric solitons of QKdV equation (also known as mKdV equation in short) in arbitrary genus Riemann surfaces. The spectral problem for QKdV equation is first reduced to the spectral problem for the mKdV equation by using the Riemann-Hilbert problem. Then we use modified Riemann-Hilbert problem to construct the multi-solitons of QKdV equation. The corresponding spectral data for this solitons includes some genus 0 spectral data and some genus N with N solitons spectral data. We name these solitons of arbitrary genus. It is shown that the multi-solitons of arbitrary genus constructed here correspond to some Hamiltonian motion on the Jacobi varieties of genus N non-hyperelliptic curves. This article is posted on arXiv.org website URL: https://arxiv.org/abs/1902.06723 Author: Qiong Yu Title: Supersymmetric Q Solitons of Arbitrary Genus Country: China Status: Accepted URL: https://arxiv.org/abs/1902.06723 arXiv ID: 1902.06723 In this paper, we construct supersymmetric solitons of QKdV equation (also known as mKdV equation in short) in arbitrary genus Riemann surfaces. The spectral problem for QKdV equation is first reduced to the spectral problem for the mKdV equation by using the Riemann-Hilbert problem. Then we use modified Riemann-Hilbert problem to construct the multi-solitons of QKdV equation. The corresponding spectral data for this solitons includes some genus 0 spectral data and some genus N with N solitons spectral data. We name these solitons of arbitrary genus. It is shown that the multi-solitons of arbitrary genus constructed here correspond to some Hamiltonian motion on the Jacobi varieties of genus N non-hyperelliptic curves. This article has been reviewed by Qing Yang of Peking University. January 10, 2021 Abstract In this paper, we construct supersymmetric solitons of QKdV equation (also known as mKdV equation in short) in arbitrary genus Riemann surfaces. The spectral problem for QKdV equation is first reduced to the spectral problem for the mKdV equation by using the Riemann-Hilbert problem. Then we use modified Riemann-Hilbert problem to construct the multi-solitons of QKdV equation. The corresponding spectral data for this solitons includes some genus 0 spectral data and some genus N with N solitons spectral data. We name these solitons of arbitrary genus. It is shown that the multi-solitons of arbitrary genus constructed here correspond to some Hamiltonian motion on the Jacobi varieties of genus N non-hyperelliptic curves. This article is protected by Ten",
        "watermark_text": "In this paper , we create supersymmetric solitons of QKdV function ( also known as mKdV formula in brief ) in arbitrary genus Riemann surfaces . The spectral problem for QKdV equation is first reduced to the spectral problem for the mKdV formula by using the Riemann - Hilbert problem .Then we utilize modified Riemann - Hilbert problem to create the multi - solitons of QKdV equation . The corresponding spectral information for this solitons encompasses some genera 0 spectral information and some genera N with N solitons spectral information .We name these solitons of arbitrary genus . It is seen that the multi - solitons of arbitrary genus constructed here correspond to some Hamiltonian movement on the Jacobi varieties of genus N non - hyperelliptic curves .This section is posted on arXiv . org web URL : https : / / arxiv . org / abs / 1902 . 06723 Author : Qiong Yu Title : Supersymmetric Q Solitons of Arbitrary Genus Country : China Status : Accepted URL : https : / / arxiv . org / abs / 1902 . 06723 arXiv ID : 1902 . 06723 In this paper , we create supersymmetric solitons of QKdV formula ( also known as mKdV formula in brief ) in arbitrary genus Riemann surfaces . The spectral problem for QKdV equation is first reduced to the spectral problem for the mKdV equation by using the Riemann - Hilbert problem .Then we utilize modified Riemann - Hilbert problem to create the multi - solitons of QKdV equation . The corresponding spectral information for this solitons encompasses some genera 0 spectral information and some genera N with N solitons spectral information .We name these solitons of arbitrary genus . It is seen that the multi - solitons of arbitrary genus constructed here correspond to some Hamiltonian movement on the Jacobi varieties of genus N non - hyperelliptic curves .This page has been discussed by Qing Yang of Peking University . January 10 , 2021 Abstract In this paper , we create supersymmetric solitons of QKdV equation ( commonly known as mKdV equation in brief ) in arbitrary genus Riemann surfaces .The spectral problem for QKdV formula is first reduced to the spectral problem for the mKdV formula by using the Riemann - Hilbert problem . Then we using modified Riemann - Hilbert problem to create the multi - solitons of QKdV formula .The equivalent spectral information for this solitons encompasses some genera 0 spectral information and some genera N with N solitons spectral information . We name these solitons of arbitrary genus .It is demonstrated that the multi - solitons of arbitrary genus constructed here correspond to some Hamiltonian movement on the Jacobi varieties of genus N non - hyperelliptic curves . This section is protected by Ten",
        "rewrite_text": "In this study, we construct supersymmetric solitons of the QKdV function (also referred to as the mKdV formula in brief) on Riemann surfaces of arbitrary genus. To achieve this, we first simplify the spectral problem for the QKdV equation to the spectral problem for the mKdV formula through the utilization of the Riemann-Hilbert problem. Subsequently, we employ a modified Riemann-Hilbert problem to generate multi-solitons of the QKdV equation. The corresponding spectral information for these solitons encompasses both genera 0 and genera N with N solitons' spectral data. We term these solitons of arbitrary genus. It is evident that the multi-solitons of arbitrary genus created in this study are linked to certain Hamiltonian movements on the Jacobi varieties of non-hyperelliptic curves of genus N.\n\nThis section has been posted on the arXiv.org website with the following URL: https://arxiv.org/abs/1902.06723. The author is Qiong Yu, and the title is \"Supersymmetric Q Solitons of Arbitrary Genus\" originating from China. The submission status is accepted. Additionally, this content has been discussed by Qing Yang from Peking University.\n\nAbstract: In this paper, we establish supersymmetric solitons of the QKdV equation (commonly known as the mKdV equation in brief) on Riemann surfaces of varying genus. Initially, we reduce the spectral problem for the QKdV function to that of the mKdV formula by utilizing the Riemann-Hilbert problem. Then, we utilize a modified version of the Riemann-Hilbert problem to create multiple solitons of the QKdV equation. These solitons' corresponding spectral information encompasses both genera 0 and genera N with N solitons' worth of spectral data. We label these solitons as having arbitrary genus. Our findings reveal that these multi-solitons of arbitrary genus are linked to certain Hamiltonian movements on the Jacobi varieties of non-hyperelliptic curves of genus N. This study is protected by Ten's copyright.",
        "ori-fast-z-score": -1.6550318531021113,
        "water-fast-z-score": 6.599663291074443,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Dirichlet sigma models and mean curvature flow are widely studied in geometry and physics, with important applications in surface and interface motion, phase transitions, and material science. In this paper we show how to derive sigma models from the more fundamental theories of quantum gravity. We use the background independent quantization of 2+1 gravity, Vasiliev theory, to derive sigma models for interfaces in (2+1)-dimensional quantum gravity. This provides a geometrical interpretation of the (2+1)-dimensional quantum gravity dynamical variables as an (2+1)-dimensional analogue of the relativistic membrane. We show that the mean curvature flow is the $0$-th order perturbative expansion of the gradient flow for the quantum gravity action, and describe the higher order corrections in the flow. This theory provides a geometrical interpretation of higher order corrections to the mean curvature flow. By analogy with the example of the relativistic membrane, this may give us new insights into the behavior of these flows in general, and the behavior of solutions near singularities in particular. The theory also has implications for the potential experimental tests of this theory: In various regimes of physical interest, the gravitational interaction between probes is weak and may be described by a sigma model with a world-sheet metric independent of the target space geometry. This means that many proposed experiments to test two-dimensional quantum gravity cannot distinguish this theory from sigma models based on otherworldly targets. However, there are regimes where the theory deviates from a sigma model, such as at strong coupling or near a critical point, and in these cases gravity may give a detectable signal. This work was performed within the program of theoretical physics of fundamental interactions of the Russian Academy of Sciences, and was supported in part by the Russian Found for Fundamental Research, Grant No. 16-32-00137. The abstract has been approved by the arXiv.org moderators and may be found here: https://arxiv.org/abs/1709.07497",
        "watermark_text": "Dirichlet sigma models and average curvature flow are widely understood in geometry and mechanics , with important use in surface and interface movement , phase transitions , and material science . In this paper we explain how to derive sigma models from the more fundamental theories of quantum gravitational .We use the background independent quantization of 2 + 1 gravity , Vasiliev theory , to derive sigma models for interfaces in ( 2 + 1 ) - dimensional quantum gravitational . This offers a geometrical interpretation of the ( 2 + 1 ) - dimensional quantum gravitational dynamical variables as an ( 2 + 1 ) - dimensional analogue of the relativistic membrane .We see that the mean curvature flow is the $ 0 $ - th order perturbative expansion of the gradient flow for the quantum gravitational action , and describe the higher order corrections in the flow . This theory provides a geometrical interpretation of greater order corrections to the mean curvature flow .By comparison with the example of the relativistic membrane , this might give us new understanding into the dynamics of these flows in general , and the dynamics of solutions near singularities in particular . The theory also has implications for the potential experimental tests of this theory : In many regimes of physical concern , the gravitational interaction between probes is weak and may be described by a sigma description with a world - sheet metric independent of the target space geometry .This implies that numerous proposed experiments to test two - dimensional quantum gravitational cannot distinguish this theory from sigma systems relying on otherworldly targets . However , there are regimes where the model deviates from a sigma description , such as at weak interaction or near a critical position , and in these instance gravitational might give a detectable signal .This research was done within the program of theoretical physics of fundamental interactions of the Russian Academy of Sciences , and was supported in part by the Russian Found for Fundamental Research , Grant No . 16 - 32 - 00137 .The abstract has been approved by the arXiv . org moderators and may be found here : https : / / arxiv . org / abs / 1709 . 07497",
        "rewrite_text": "Dirichlet sigma models and the average curvature flow are extensively understood in geometry and mechanics, playing a significant role in surface and interface motion, phase transitions, and material science. In this paper, we elaborate on how to derive sigma models from the more fundamental theories of quantum gravity. We utilize the background-independent quantization of 2 + 1 gravity, namely the Vasiliev theory, to derive sigma models for interfaces in (2 + 1)-dimensional quantum gravity. This offers a geometric interpretation of the (2 + 1)-dimensional quantum gravitational dynamic variables as an analog of the relativistic membrane in a three-dimensional space.\n\nWe observe that the mean curvature flow represents the zero-th order perturbative expansion of the gradient flow for the quantum gravitational action, and we describe the higher-order corrections in the flow. This theory provides a geometric interpretation of the corrections to the mean curvature flow at various orders. In comparison to the example of the relativistic membrane, this may provide us with new insights into the general dynamics of these flows and particularly into the dynamics of solutions near singularities.\n\nFurthermore, this theory has implications for potential experimental tests. In various physical contexts, the gravitational interaction between probes is weak and can be described using a sigma description with a world-sheet metric independent of the target space geometry. This implies that several proposed experiments designed to test two-dimensional quantum gravity cannot distinguish this theory from other sigma systems with extraterrestrial targets. However, there are scenarios where the model deviates from a sigma description, such as at weak interactions or near a critical point, where gravity may produce a detectable signal.\n\nThis research was conducted within the program of fundamental interactions in theoretical physics at the Russian Academy of Sciences and was partially supported by the Russian Foundation for Fundamental Research, Grant No. 16-32-00137. The abstract of this research has been approved by the arXiv.org moderators and can be found here: https://arxiv.org/abs/1709.07497.",
        "ori-fast-z-score": 0.9135002783911397,
        "water-fast-z-score": 7.0,
        "rewrite-fast-z-score": 2.848259605699058
    },
    {
        "original_text": "Clues, or cluesets, are used to guide the sampling process. A clueset contains a seed, a sampling range, and a sampling density function. Starting from the seed, the sampling range is divided into equal sized subsamples. The subsamples are scored according to the sampling density function, and the highest scoring subsample is chosen as the next sample. This process is repeated until the goal sample size is reached. Clues can be weighted according to their probability, so that biased samples can be generated. For example, if it is required to sample only Pathumthani people, Thailand, then Pathumthani is the seed, Thailand is the sampling range, and the probability of each Thai province being Pathumthani people is the sampling density function. This can be extended by creating a bank of clues, which can be selected from and reused for various sampling tasks. The bank can contain several hundred thousand clues and still fit in memory.",
        "watermark_text": "Clues , or cluesets , are using to guide the sampling method . A clueset contains a seed , a sampling range , and a sampling density function .Starting from the seed , the sampling range is separated into equal sized subsamples . The subsamples are scored according to the sampling density function , and the highest ranked subsample is chosen as the second sample .This process is repeated until the target sample capacity is reached . Clues can be weighted according to their likelihood , so that biased samples can be derived .For instance , if it is required to survey only Pathumthani persons , Thailand , then Pathumthani is the seed , Thailand is the sampling range , and the probability of each Thai country being Pathumthani persons is the sampling density function . This can be enhanced by creating a bank of clues , which can be selected from and reused for various sampling challenges .The bank can contain many hundred thousand clues and still fit in memory .",
        "rewrite_text": "Sampling methods are guided by clues or clue sets, which consist of a seed, a sampling range, and a sampling density function. Starting from the seed, the sampling range is divided into uniformly sized subsamples. Each subsample is then scored based on the sampling density function, and the highest-ranked subsample is chosen as the subsequent sample. This process is iterated until the desired sample capacity is achieved. Clues can be weighted based on their likelihood, enabling the generation of biased samples.\n\nFor instance, if a survey is required to target only individuals from Pathumthani, Thailand, then Pathumthani becomes the seed, Thailand becomes the sampling range, and the probability of any individual in Thailand being a Pathumthani resident becomes the sampling density function. This approach can be enhanced by creating a repository of clues, which can be selected from and reused for various sampling challenges. This repository can contain hundreds of thousands of clues and still remain easily accessible in memory.",
        "ori-fast-z-score": 0.12216944435630522,
        "water-fast-z-score": 3.8805700005813275,
        "rewrite-fast-z-score": 3.3941932686877867
    },
    {
        "original_text": "Nonlinear oscillators based on semiconductor microcavities provide an excellent testbed for studying chaos, bifurcations, and non-linear effects1-3. An external periodic force, which can be applied either optically or electrically, can induce complicated and interesting behaviors4-7. In this Letter, we report a dynamics-controlled truncation scheme to find high-dimensional chaotic attractors in the semiconductor laser with a saturable absorber. By introducing a dynamics-dependent truncation procedure, long transient chaotic orbits are truncated by chaotic solutions of low-dimensional truncation systems. With this truncation scheme, we are able to observe novel dynamical behaviors in semiconductor laser with a saturable absorber, such as intermittency and bifurcations from torus to strange attractor. Our method can also be applied to any high-dimensional nonlinear system. We study the semiconductor laser with saturable absorber and external periodic force (SA+EFP). The laser dynamics can be described by the following equations7: +j0=σ(I+Iref)−(1−σ)I−j(γ+β|A|2)|A|^2+η(t);  1  where A is the complex field, and Iref, γ, β, σ, and η(t) are the reference current, gain loss, feedback gain, feedback loss, and time dependent perturbation, respectively. |A| is the magnitude of |A| and is proportional to the intensity of the optical field. By setting the external pump at the desired frequency, the laser dynamics can be reduced to a one dimensional map (1DMA), which is composed of two terms. The first term describes the phase dynamics, while the second term describes the intensity dynamics. In our simulation, we choose the parameters γ = 2 and β = 4, and the saturable loss coefficient σ = 0.8. By setting the modulation of external periodic force to be a limit-cycle oscillation, a chaotic attractor in 1DMA can be obtained. If the dynamics of the full model is truncated at a particular dimension, it is possible to observe long transient chaotic orbits in 1DMA. In addition, we can still observe the existence of limit-cycle oscillation in the truncated system. However, the truncation dimension controls the transient time of the chaotic orbit. If the truncation dimension is high, the transient chaotic orbit is long. If the transient chaotic orbit is long, we are able to observe more interesting dynamical behaviors. For example, if the transient chaotic orbit is long enough, it is possible to observe intermittency and bifurcations from torus to strange attractor. In contrast, if the transient chaotic orbit is too short, the dynamical behavior of the laser can not be observed.",
        "watermark_text": "Nonlinear oscillators built on semiconductor microcavities provide an excellent testbed for studying chaos , bifurcations , and non - discrete effects1 - 3 . An external periodic force , which can be applied either optically or electrically , can induce complicated and exciting behaviors4 - 7 .In this Letter , we report a dynamics - controlled truncation scheme to find high - dimensional chaotic attractors in the semiconductor laser with a saturable absorber . By introducing a dynamics - dependent truncation procedure , long transient turbulent orbits are truncated by turbulent solutions of low - dimensional truncation systems .With this truncation scheme , we are able to observe new dynamical interactions in semiconductor laser with a saturable absorber , such as intermittency and bifurcations from torus to odd attractor . Our techniques can also be applied to any large - dimensional nonlinear system .We consider the semiconductor laser with saturable absorber and external periodic force ( SA + EFP ) . The laser dynamics can be described by the following equations7 : + j0 = σ ( I + Iref ) − ( 1−σ ) I−j ( γ + β | A | 2 ) | A | ^ 2 + η ( t ) ; 1 where A is the complex field , and Iref , γ , β , σ , and η ( t ) are the reference current , gain loss , feedback gain , feedback loss , and period dependent perturbation , respectively .| A | is the magnitude of | A | and is proportional to the frequency of the optical field . By sets the external pump at the desired wavelength , the laser dynamics can be reduced to a one dimensional map ( 1DMA ) , which is composed of two terms .The first term describes the phase dynamics , while the second term describes the intensity dynamics . In our modeling , we choose the variables β = 2 and β = 4 , and the saturable failure coefficient σ = 0 . 8 .By sets the modulation of external periodic force to be a limit - cycle oscillation , a chaotic attractor in 1DMA can be obtained . If the dynamics of the full simulation is truncated at a certain dimension , it is easy to observe long transient turbulent orbits in 1DMA .In addition , we can always observe the existence of limit - process oscillation in the truncated system . However , the truncation dimension controls the transient moment of the chaotic orbit .If the truncation dimension is high , the transient turbulent orbit is long . If the transient turbulent orbit is long , we are able to observe more exciting dynamical interactions .For instance , if the transient turbulent orbit is long enough , it is easy to observe intermittency and bifurcations from torus to odd attractor . In comparison , if the transient turbulent orbit is too longer , the dynamical behavior of the laser can not be observed .",
        "rewrite_text": "Nonlinear oscillators constructed on semiconductor microcavities offer an exceptional platform for studying chaos, bifurcations, and non-discrete effects.1-3 An externally applied periodic force, either through optical or electrical means, can induce intricate and captivating behaviors.4-7\n\nIn this letter, we present a dynamically controlled truncation method to identify high-dimensional chaotic attractors in semiconductor lasers with saturable absorbers. By introducing a truncation procedure that depends on dynamics, long transient turbulent orbits are truncated by solutions of low-dimensional truncation systems. This truncation approach enables us to observe novel dynamic interactions in the semiconductor laser, such as intermittency and bifurcations from torus to odd attractors.\n\nOur techniques can be applied to any large-dimensional nonlinear system. We specifically consider a semiconductor laser with a saturable absorber and an external periodic force (SA + EFP). The laser dynamics can be described by specific equations:\n\nThe complex field is represented by A, while Iref, γ, β, σ, and η(t) denote the reference current, gain loss, feedback gain, feedback loss, and period-dependent perturbation, respectively. By setting the external pump at a specific wavelength, the laser dynamics can be reduced to a one-dimensional map (1DMA) comprising two terms: one describing phase dynamics and the other intensity dynamics.\n\nIn our modeling, we choose β values of 2 and 4 and set the saturable failure coefficient σ to 0.8. By modulating the external periodic force as a limit-cycle oscillation, a chaotic attractor can be achieved in 1DMA. If the full simulation's dynamics are truncated at a certain dimension, it becomes straightforward to observe long transient turbulent orbits in 1DMA. Additionally, the truncated system always exhibits limit-process oscillation. However, the truncation dimension governs the chaotic orbit's transient moment. A higher truncation dimension results in a longer transient turbulent orbit. This longer orbit permits us to witness more exciting dynamic interactions. For instance, prolonged transient turbulent orbits are conducive to observing intermittency and bifurcations from torus to odd attractors. Conversely, an excessively long transient turbulent orbit may obscure the laser's dynamic behavior.",
        "ori-fast-z-score": 0.4714045207910317,
        "water-fast-z-score": 6.599663291074443,
        "rewrite-fast-z-score": 3.522819383711917
    },
    {
        "original_text": "Local purity distillation aims to remotely transmit unknown quantum states to a local server without disclosing any classical information to the server. Purity of the states to be transferred is required to be at least as high as the server’s capability to synthesize. In this paper, we propose and analyze a novel protocol Local Purity Distillation (LPD) with Bounded Classical Communication (BCC). LPD not only guarantees the remote state transfer with high fidelity, but also limits the server to a finite number of classical queries. In particular, for a 2-qubit state, LPD with BCC only requires the server to perform one query; for a 3-qubit state, the number of queries is two; and for an n-qubit state, the number of queries is O(n). In addition, we further analyze the secure quantum channel condition under LPD and present a tighter bound in the general case. Finally, we also give a real-world application example in digital signature to demonstrate the feasibility of the protocol.",
        "watermark_text": "Local purity distillation seeks to remotely send unknown quantum states to a local client without disclosing any classical knowledge to the server . Purity of the states to be transferred is required to be at least as great as the computer ’ s capability to synthesize .In this paper , we propose and analyze a novel procedure Local Purity Distillation ( LPD ) with Bounded Classical Communication ( BCC ) . LPD not only provides the remote state transfer with high fidelity , but also limits the client to a finite number of classical queries .In particular , for a 2 - qubit state , LPD with BCC only needs the client to conduct one query ; for a 3 - qubit state , the number of queries is two ; and for an n - qubit state , the number of queries is O ( n ) . In addition , we further analyze the secure quantum channel situation under LPD and present a tighter bound in the general case .Finally , we also get a real - time application instance in digital signature to test the feasibility of the protocol .",
        "rewrite_text": "Remote purity distillation aims to transmit unknown quantum states to a local client without revealing any classical information to the server. The purity of the states to be transferred must be at least equivalent to the computer's capability to synthesize. In this paper, we introduce and analyze a novel procedure called Local Purity Distillation (LPD) with Bounded Classical Communication (BCC). LPD not only ensures high-fidelity remote state transfer but also limits the client's classical query count to a finite number. Specifically, for a 2-qubit state, LPD with BCC requires only one query from the client; for a 3-qubit state, the number of queries is two; and for an n-qubit state, the number of queries is O(n). Furthermore, we delve into the secure quantum channel scenario under LPD and present a tighter bound in the general case. Lastly, we provide a real-time application example in digital signature to test the feasibility of the protocol.",
        "ori-fast-z-score": -0.6108472217815261,
        "water-fast-z-score": 3.6380343755449944,
        "rewrite-fast-z-score": 0.24618298195866545
    },
    {
        "original_text": "The emission from high energy particles in gamma rays and very high energy (VHE, E>100 GeV) particles in particles in accelerators like the Large Hadron Collider is known to be strongly dependent on the physical conditions near the acceleration sites. The Very Energetic Radiation Imaging Telescope Array System (VERITAS), a system of major atmospheric Cherenkov telescopes located in southern Arizona, has been observing a sample of active galaxies (called BL Lac objects) that are commonly thought to be located near the center of their hosts  gravitational wells. To date, these observations have shown that the spectrum of these objects does not harden, i.e., the slope of the energy spectrum (d log I/d log E) does not change as the flux (I) increases. New observations with VERITAS spanning 9 years of data (from 2008 to 2017) suggest that the spectrum may harden at fluxes exceeding 10(-11) cm(-2) s(-1) GeV. While this finding seems to contradict what is known about gamma-ray emission from other sources, there are several aspects of the sample and analysis that may account for this. Further analysis of the full data set is needed to confirm this surprising finding.",
        "watermark_text": "The emission from high energy molecules in gamma radiation and very high energy ( VHE , E > 100 GeV ) electrons in particles in accelerators like the Large Hadron Collider is known to be highly dependent on the physical conditions near the acceleration sites . The Very Energetic Radiation Imaging Telescope Array System ( VERITAS ) , a system of key atmospheric Cherenkov telescopes located in southern Arizona , has been monitoring a sample of active galaxies ( named BL Lac objects ) that are often said to be found near the center of their hosts gravitational wells .To date , these observations have shown that the spectrum of these objects does not harden , i . e . , the slope of the energy spectrum ( d log I / d log E ) does not change as the flux ( I ) increases . New observations with VERITAS spanning 9 years of data ( from 2008 to 2017 ) suggest that the spectrum may harden at fluxes exceeding 10 ( - 11 ) cm ( - 2 ) s ( - 1 ) GeV .While this finding appears to contradict what is known about gamma - ray radiation from other sources , there are several parts of the sample and investigation that might account for this . Further examination of the full data set is required to confirm this surprising finding .",
        "rewrite_text": "The emission resulting from high-energy molecules in gamma radiation and electrons with very high energy (VHE; E > 100 GeV) within accelerators, such as the Large Hadron Collider, is highly dependent on the physical conditions in the vicinity of the acceleration sites. The Very Energetic Radiation Imaging Telescope Array System (VERITAS), a network of crucial atmospheric Cherenkov telescopes situated in southern Arizona, has been monitoring a selection of active galaxies, commonly found near the centers of their host gravitational wells, known as BL Lac objects.\n\nTo this day, observations have indicated that the spectrum of these objects remains unhardened, meaning that the slope of the energy spectrum (d log I/d log E) remains unchanged as the flux (I) increases. However, new observations conducted by VERITAS over a nine-year period spanning from 2008 to 2017 suggest that the spectrum may harden at fluxes exceeding 10^-11 cm^-2 s^-1 GeV. Although this finding seems to contradict existing knowledge about gamma-ray radiation from other sources, there are various aspects of the sample and investigation that could explain this discrepancy. A further examination of the complete dataset is necessary to verify this unexpected finding.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.712790073055879,
        "rewrite-fast-z-score": 3.092082730095703
    },
    {
        "original_text": "Distributed network topologies are commonly established by some central authority according to geographic proximity, link availability, or other arbitrary constraints. In this work, we design a decentralized, localized approach to network topology formation, whereby each sensor node randomly and independently forms links to other sensor nodes. We present conditions under which this decentralized approach leads to substantially denser networks compared to existing centralized approaches based on similar assumptions. We also present two versions of the proposed protocol for partially and fully random networks. In partially random networks, some connectivity constraints, such as a minimum number of neighbors, are fulfilled; in fully random networks, all constraints are fulfilled. We evaluate our protocol through extensive simulations and illustrate its applicability to sensor networks in large venues such as a college campus.  We envision a wide range of applications for our approach. For example, it can be used to generate ad hoc networks for emergency services with the goal of expediting the delivery of emergency services communications, or for wireless sensor networks deployed in large venues such as a college campus, where the generated networks have the desired properties of high density and full randomness.",
        "watermark_text": "Distributed system topologies are often established by some central authority according to geographic proximity , link allocation , or other arbitrary limitations . In this research , we design a decentralized , localized approach to system topology structure , whereby each sensor node randomly and independently creates connections to other sensor nodes .We present terms under which this decentralized method results to substantially denser networks relative to existing centralized methods using on similar assumptions . We additionally offer two variants of the suggested protocol for partially and fully random networks .In partially random networks , some connectivity restrictions , such as a minimum amount of neighbors , are fulfilled ; in fully random networks , all constraints are fulfilled . We evaluate our protocol through numerous simulations and illustrate its applicability to sensor networks in large venues such as a college campus .We envision a broad variety of applications for our approach . For instance , it can be used to create ad hoc networks for emergency services with the objective of expediting the supply of urgent personnel communications , or for wireless sensor networks deployed in large sites such as a college campus , where the produced channels have the desired properties of high density and complete randomness .",
        "rewrite_text": "The topologies of distributed systems are frequently established by a central authority based on geographic proximity, link allocation, or other arbitrary constraints. In our research, we have designed a decentralized and localized approach for the structure of system topology. This approach involves each sensor node independently creating random connections with other sensor nodes. \n\nWe have identified conditions under which this decentralized approach results in substantially denser networks compared to existing centralized methods assuming similar conditions. Furthermore, we offer two variants of the proposed protocol for partially and fully random networks. In partially random networks, certain connectivity restrictions, such as a minimum number of neighbors, are met, while in fully random networks, all constraints are met. \n\nOur protocol has been evaluated through numerous simulations and has demonstrated its applicability in large-scale sensor networks, such as on a college campus. We envision a wide range of potential applications for our approach. For instance, it can be used to create ad-hoc networks for emergency services to expedite urgent personnel communications or for wireless sensor networks deployed in large sites like a college campus, where the resulting channels exhibit desired properties such as high density and complete randomness.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.471832459560074,
        "rewrite-fast-z-score": 2.030146626995893
    },
    {
        "original_text": "The complexity of model checking higher-order logic has been a long-standing open problem. Since the 1980s, model checking has been reduced to decision problems for several fragments of higher-order logic. However, in the worst case, checking whether a given formula is satisfied by all fixpoints of a fixpoint logic formula requires solving a Picard iteration, an optimal fixpoint algorithm, for every projection of the logic formula. Thus, even when the logic formula is over a small set of variables, model checking Higher-Order Fixpoint Logic (HOF logic) is np-hard. We study HOF logic in the parameterized complexity framework, where the complexity of a problem is bound by a function of the size of the formula. We give two parameterizations: tree decomposition, and block decomposition. We show that model checking HOF logic with respect to tree decompositions is FPT, and we prove that model checking HOF logic with respect to block decompositions is not FPT. Our parameterizations leave plenty of room for further parameterizations. We also give a 3d-Printable structure, a width-bounded syntactic abstraction, that preserves FPT parameterized complexity.",
        "watermark_text": "The complexity of model checking upper - order logic has been a long - standing open challenge . Since the 1980s , model checking has been reduced to decision difficulties for numerous pieces of greater - order logic .However , in the worst case , searching whether a given formula is fulfilled by all fixpoints of a fixpoint reasoning formula involves solving a Picard iteration , an appropriate fixpoint algorithm , for every projection of the logic formula . Thus , even when the logic formula is over a small set of variables , model searching Higher - Order Fixpoint Logic ( HOF reasoning ) is np - hard .We explore HOF reasoning in the parameterized complexity formulation , where the complexity of a problem is bound by a function of the size of the formula . We get two parameterizations : tree transformation , and block decomposition .We see that model checking HOF reasoning with regard to tree decompositions is FPT , and we prove that model checking HOF reasoning with regard to block decompositions is not FPT . Our parameterizations leave lot of room for further parameterizations .We also get a 3d - Printable form , a width - bounded syntactic abstraction , that preserves FPT parameterized complexity .",
        "rewrite_text": "The long-standing open challenge of the complexity in model checking higher-order logic has persisted for decades. Since the 1980s, the task of model checking has been narrowed down to decision-making challenges associated with various higher-order logical constructs. However, in the most challenging scenarios, verifying if a given formula is satisfied by all fixpoints of a fixpoint reasoning formula entails solving a Picard iteration, an appropriate fixpoint algorithm, for each projection of the logic formula. Consequently, even when dealing with a logic formula involving a small set of variables, model searching in Higher-Order Fixpoint Logic (HOF reasoning) remains NP-hard.\n\nWe delve into HOF reasoning within the framework of parameterized complexity, where the complexity of a problem is constrained by a function that depends on the size of the formula. This exploration yields two parameterization approaches: tree transformation and block decomposition. Our findings indicate that model checking HOF reasoning with respect to tree decompositions is Fixed-Parameter Tractable (FPT), while we prove that model checking with regard to block decompositions is not FPT. Our parameterization approaches leave ample room for further refinements. Additionally, we have developed a 3D printable form and a width-bounded syntactic abstraction that preserves the parameterized complexity of FPT.",
        "ori-fast-z-score": -3.709704134011871,
        "water-fast-z-score": 2.494700264914546,
        "rewrite-fast-z-score": 0.8432740427115678
    },
    {
        "original_text": "Aiming at reducing the metallicity in a ferromagnetic material, we have fabricated a bilayer film with a thin layer of NiMnSb sandwiched between two thin layers of NiMnSb with a gradient of NiMnSb thickness. While the outer two layers have a constant composition of Ni50Mn50Sb20, the inner layer has a reduced Ni content of Ni40Mn50Sb20. As a result, the whole film becomes Ni40Mn30Sb20, with the Mn content increasing gradually from the inner to the outer layer, thus achieving a gradient of metallicity across the film. Experiments and first-principles calculations show that this gradient in the Mn content induces a gradient in the Ni3+ and Mn3+ concentrations, giving rise to a nearly full metallicity for the film. Because of the great potential of gradient metallicity for novel spintronic applications, this work may attract broad attention.",
        "watermark_text": "Aiming at lowering the metallicity in a ferromagnetic material , we have synthesized a bilayer film with a thin layer of NiMnSb sandwiched between two thin layers of NiMnSb with a gradient of NiMnSb thickness . While the inner two sheets have a constant structure of Ni50Mn50Sb20 , the inner layer has a reduced Ni content of Ni40Mn50Sb20 .As a result , the whole film becomes Ni40Mn30Sb20 , with the Mn content rising gradually from the inner to the exterior layer , thus producing a gradient of metallicity across the film . Experiments and first - principles measurements show that this gradient in the Mn content induces a gradient in the Ni3 + and Mn3 + concentrations , giving rise to a nearly full metallicity for the film .Because of the great potential of gradient metallicity for novel spintronic use , this research could attract wide focus .",
        "rewrite_text": "With the objective of reducing the metallic content in a ferromagnetic material, we have synthesized a bilayer film that features a thin layer of NiMnSb sandwiched between two layers with a gradient in NiMnSb thickness. The inner two layers maintain a consistent composition of Ni50Mn50Sb20, whereas the inner layer exhibits a reduced Ni content at Ni40Mn50Sb20. Consequently, the entire film composition is Ni40Mn30Sb20, with a gradual increase in Mn content from the inner to the outer layer, resulting in a gradient of metallic content across the film. Experiments and first-principles measurements indicate that this Mn content gradient leads to a variation in the concentrations of Ni3+ and Mn3+, ultimately achieving nearly complete metallicity for the film. Given the significant potential of this gradient metallic property in novel spintronic applications, this research is expected to garner widespread attention.",
        "ori-fast-z-score": 0.9438798074485389,
        "water-fast-z-score": 5.258758927213289,
        "rewrite-fast-z-score": 2.794002794004191
    },
    {
        "original_text": "ANALYSIS OF SINGLE SPECTRA IN THE QUIET SUN SPECTRA FROM HMI AND MDI A single spectrum from the magnetic diagnostics on board SOHO and NASA s Solar and Heliospheric Observatory (SUN / HDต Backasch et al. (1995): Two photospheric lines with opposite magnetic polarity are clearly visible in the intensity profiles. A possible explanation is the Zeeman effect. The result is in disagreement with the notion that the quiet Sun has no magnetic field. The signal to noise ratio in the quiet Sun, obtained with current technologies, is not sufficient to detect the weak fields of a few Gauss. The weakest fields in the solar atmosphere are observed in the photospheric layers, where the gas pressure and temperature are high. In particular, two photospheric lines with opposite magnetic polarity are clearly visible in the intensity profiles. A possible explanation is the Zeeman effect. The result is in disagreement with the notion that the quiet Sun has no magnetic field. The quiet Sun has a magnetic field of several hundred Gauss, although it is much weaker than the fields of active regions.",
        "watermark_text": "ANALYSIS OF SINGLE SPECTRA IN THE QUIET SUN SPECTRA FROM HMI AND MDI A single spectrum from the magnetic diagnostics on board SOHO and NASA s Solar and Heliospheric Observatory ( SUN / [UNK] Backasch et al . ( 1995 ) : Two photospheric lines with opposite magnetic polarity are readily noticeable in the intensity profiles .A potential explanation is the Zeeman phenomenon . The result is in disagreement with the notion that the quiet Sun has no magnetic force .The signal to noise proportion in the quiet Sun , obtained with current technologies , is not sufficient to locate the weak fields of a few Gauss . The weakest fields in the solar atmosphere are observed in the photospheric structures , where the gas pressure and temperature are high .In particular , two photospheric lines with opposite magnetic polarity are directly evident in the intensity profiles . A potential explanation is the Zeeman phenomenon .The result is in disagreement with the notion that the quiet Sun has no magnetic force . The calm Sun has a magnetic force of several hundred Gauss , although it is much weaker than the fields of active regions .",
        "rewrite_text": "ANALYSIS OF INDIVIDUAL SPECTRA IN THE SPECTRUM OF THE QUIET SUN DERIVED FROM HMI AND MDI\n\nIn the analysis of a single spectrum from the magnetic diagnostics on the SOHO and NASA's Solar and Heliospheric Observatory (SUN), as reported by Backasch et al. (1995), two photospheric lines with contrasting magnetic polarity are readily apparent in the intensity profiles. A possible explanation for this phenomenon is the Zeeman effect. This finding contradicts the notion that the quiet Sun lacks magnetic force.\n\nWith current technological advancements, the signal-to-noise ratio in the quiet Sun is insufficient to detect weak fields of only a few Gauss. The weakest fields in the solar atmosphere are typically observed in photospheric structures where gas pressure and temperature are high. Specifically, two photospheric lines with opposing magnetic polarity are clearly visible in the intensity profiles, suggesting the Zeeman effect as a potential explanation. This result challenges the notion that the quiet Sun lacks any magnetic influence, even if it is significantly weaker compared to the fields found in active regions, with a magnetic force of several hundred Gauss.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": 1.5428161556520092
    },
    {
        "original_text": "In this paper we give the two-loop differential beta function in the minimal-subtraction scheme for non-abelian gauge theories in the asymptotic freedom region. We present the result in terms of the so-called evolved constant of the theory, defined as the solution to a certain differential equation. We comment on the structure of the result and compare it to previous results in the literature.  1  G. Burgio, D. Carturan, S. Morisi, J. starcefeld, and G. tran, JHEP 11, 025 (2016).  2  L.V. Atroshchenko, D.I. Kazakov, and A.V. Kovalenko, Theor. Math. Phys. 185, 1762 (2015).  3  N.V. Prokushkin and M.A. Vasiliev, Fortsch. Phys. 53, 741 (2005).  4  S.L. Lukyanov, Int. J. Mod. Phys. A 29, 1450186 (2014).  5  V.E. Tarasov, L.G. Aarts, and A.V. Salnikov, Phys. Lett. B 339, 374 (1994).  6  D.R. Hofman and J. Smit, J. Math. Phys. 48, 122301 (2007).  7  S. Carrozza, B. Hambly, F. Hassani, and D. Weiskopf, “The beta function of gauge theories,” arXiv:1610.09827 (2016).  8  L.V. Atroshchenko, D.I. Kazakov, and A.V. Kovalenko, Theor. Math. Phys. 185, 1874 (2015).  9  N.V. Prokushkin and M.A. Vasiliev, Phys. Lett. B 389, 45 (1996).  10  N.N. Bogolyubov and D.V. Shirkov, “Introduction to the theory of quantum fields,” v. 2 (1980).  11  G. Leibbrandt, Rev. Mod. Phys. 59, 1067 (1987).  12  S.M. Ryabchenko, Theor. Math. Phys. 149, 326 (2006).  13  F.J. Dyson, Am. J. Math. 58, 209 (1936).  14  H.D. Dahmen, “The beta function in renormalization group theory,” v. 1 (1993).  15  M.E. Agido and G. Grignani, Nucl. Phys. B 844, 326 (2011).  16  S.M.",
        "watermark_text": "In this paper we give the two - loop differential beta function in the reduced - subtraction scheme for non - abelian gauge theories in the asymptotic freedom region . We present the result in terms of the so - called evolved constant of the theory , defined as the solve to a certain differential equation .We comment on the structure of the result and link it to previous findings in the book . 1 G . Burgio , D . Carturan , S . Morisi , J . starcefeld , and G . tran , JHEP 11 , 025 ( 2016 ) .2  L.V.Atroshchenko, D.I.Kazakov, and A.V.Kovalenko, Theor.Math.Phys.185, 1762 (2015).3  N.V. Prokushkin and M.A.Vasiliev, Fortsch.Phys.53, 741 (2005).4  S.L.Lukyanov, Int.J. Mod.Phys.A 29, 1450186 (2014).5  V.E.Tarasov, L.G.Aarts, and A.V.Salnikov, Phys.Lett.B 339, 374 (1994).6  D.R.Hofman and J. Smit, J.Math.Phys.48 , 122301 ( 2007 ) . 7 S . Carrozza , B . Hambly , F . Hassani , and D . Weiskopf , “ The beta function of gauge fields , ” arXiv : 1610 . 09827 ( 2016 ) .8  L.V.Atroshchenko, D.I.Kazakov, and A.V.Kovalenko, Theor.Math.Phys.185, 1874 (2015).9  N.V. Prokushkin and M.A.Vasiliev, Phys.Lett.B 389, 45 (1996).10  N.N.Bogolyubov and D . V . Shirkov , “ Introduction to the principle of quantum fields , ” v . 2 ( 1980 ) .11  G. Leibbrandt, Rev.Mod.Phys.59, 1067 (1987).12  S.M.Ryabchenko, Theor.Math.Phys.149, 326 (2006).13  F.J. Dyson, Am.J.Math.58, 209 (1936).14  H.D.Dahmen, “The beta function in renormalization group theory,” v. 1 (1993).15  M.E.Agido and G. Grignani, Nucl.Phys.B 844, 326 (2011).16  S.M.",
        "rewrite_text": "In this study, we present the two-loop differential beta function within the reduced-subtraction scheme for non-Abelian gauge theories in the asymptotic freedom region. We express the result in terms of the theory's evolved constant, which is defined as a solution to a specific differential equation.\n\nWe comment on the structure of this result and connect it to previous findings in various publications. For instance, in the book by 1 G. Burgio et al. (JHEP 11, 025, 2016), 2 L.V. Atroshchenko et al. (Theor. Math. Phys. 185, 1762, 2015), 3 N.V. Prokushkin and M.A. Vasiliev (Fortsch. Phys. 53, 741, 2005), and so on.\n\nAdditionally, we refer to other notable works such as 4 S.L. Lukyanov's (Int. J. Mod. Phys. A 29, 1450186, 2014) exploration of related topics and the studies by 5 V.E. Tarasov et al. (Phys. Lett. B 339, 374, 1994) which provided valuable insights into gauge fields' beta function. Furthermore, we mention works such as 6 D.R. Hofman and J. Smit's (J. Math. Phys. 48, 122301, 2007) contributions and 7 S. Carrozza et al.'s (arXiv: 1610.09827, 2016) research on the beta function of gauge fields.\n\nMoreover, we also draw references to other important studies such as 8 L.V. Atroshchenko et al.'s (Theor. Math. Phys. 185, 1874, 2015), 9 N.V. Prokushkin and M.A. Vasiliev's (Phys. Lett. B 389, 45, 1996) research, as well as the classic works by 10 N.N. Bogolyubov and D.V. Shirkov's \"Introduction to the principle of quantum fields\" (v. 2, 1980) and other pioneering works in the field such as 11 G. Leibbrandt's (Rev. Mod. Phys. 59, 1067, 1987) reviews and more recent works like 12 S.M. Ryabchenko's (Theor. Math. Phys. 149, 326, 2006).\n\nFurthermore, we also acknowledge the seminal contributions made by early works such as 13 F.J. Dyson's (Am J Math., volume and page numbers omitted due to limited information), as well as other references like H.D Dahmen's work on \"The beta function in renormalization group theory\" (v. 1, 1993). Additionally, we mention the recent advancements made by M.E Agido and G Grignani (Nucl Phys B 844, 326, 2011) and other notable researchers in the field of non-Abelian gauge theories and their beta functions in general.\"",
        "ori-fast-z-score": 1.2857142857142858,
        "water-fast-z-score": 3.1754264805429417,
        "rewrite-fast-z-score": 1.5716505559714824
    },
    {
        "original_text": "The UKIRT Infrared Deep Sky Survey (UKIDSS; Lucas et al. 2008) uses the United Kingdom Infrared Telescope (UKIRT) to survey the whole sky visible from Mauna Kea in Hawaii, in five optical bands and two infrared bands. In April 2010, UKIDSS completed its Early Data Release (EDR), including images of some areas that had not been observed by the UKIRT Wide Field Survey (uwfS; Collins et al. 2004). In this paper we present a new method of searching for distant galaxies by using these new UKIDSS EDR images of the Galactic Plane. We use the UKIDSS Galactic Plane Survey (GPLS; Hambly et al. 2008) to select two stellar populations, A and B, with different colours and ages. We then apply a colour-colour diagram, constructed using these two stellar populations and star-forming galaxies with known redshifts, to find regions in colour space where the distant galaxies can be found. We apply this technique to two separate areas of the sky, labelled L7 and L11, and find a total of 66 galaxies at redshifts greater than z = 1.5, with a mean distance of 92.2 ± 5.4 million light-years. We conclude that this new technique is a potentially useful method of selecting distant galaxies, and discuss the implications of this result for studies of the epoch of reionisation.  — RMCGP Team",
        "watermark_text": "The UKIRT Infrared Deep Sky Survey ( UKIDSS ; Lucas et al . 2008 ) uses the United Kingdom Infrared Telescope ( UKIRT ) to survey the whole sky visible from Mauna Kea in Hawaii , in five optical bands and two infrared bands .In April 2010 , UKIDSS completed its Early Data Release ( EDR ) , incorporating photos of some regions that had not been observed by the UKIRT Wide Field Survey ( uwfS ; Collins et al . 2004 ) .In this paper we present a new method of searching for distant galaxies by using these new UKIDSS EDR photographs of the Galactic Plane . We use the UKIDSS Galactic Plane Survey ( GPLS ; Hambly et al .2008 ) to select two stellar regions , A and B , with varying colours and periods . We then use a colour - colour graph , constructed using these two stellar colonies and galaxy - creating stars with recorded redshifts , to find regions in colour room where the distant galaxies can be found .We use this methodology to two separate areas of the sky , labelled L7 and L11 , and find a total of 66 galaxies at redshifts greater than z = 1 . 5 , with a mean distance of 92 . 2 ± 5 . 4 million light - years . We determine that this new technique is a potentially important method of determining distant galaxies , and consider the implications of this effect for research of the epoch of reionisation .— RMCGP Team",
        "rewrite_text": "The United Kingdom Infrared Deep Sky Survey (UKIDSS), coined by Lucas et al. in 2008, utilizes the United Kingdom Infrared Telescope (UKIRT) to conduct a comprehensive survey of the sky visible from Mauna Kea in Hawaii. This survey spans five optical bands and two infrared bands. In April 2010, UKIDSS released its Early Data Release (EDR), which included images of certain regions not previously observed by the UKIRT Wide Field Survey (uwfS; Collins et al., 2004).\n\nIn this study, we introduce a novel approach for detecting distant galaxies utilizing the new UKIDSS EDR images of the Galactic Plane. We leverage the UKIDSS Galactic Plane Survey (GPLS; Hambly et al., 2008) to select two stellar regions, designated as A and B, with diverse colors and periods. Subsequently, we construct a color-color graph using these two stellar populations and galaxy-forming stars with recorded redshifts to identify color spaces where distant galaxies may be found.\n\nApplying this methodology to two distinct sky areas, labeled L7 and L11, we discover a total of 66 galaxies with redshifts exceeding z = 1.5. These galaxies are estimated to be at a mean distance of 92.2 ± 5.4 million light-years. We conclude that this new technique holds significant potential for identifying distant galaxies and consider the implications of this technique for researching the epoch of reionization.\n\n- The RMCGP Team's Report",
        "ori-fast-z-score": 1.7457431218879391,
        "water-fast-z-score": 6.764754597315764,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "We present the discovery of 11 new T dwarfs in theTwo Micron All-Sky Survey (2MASS). We describe the process of matching 2MASS PSC point sources to WISE full-frame data, and present the resulting list of 22 new T dwarf counterparts. New T dwarfs found include the 23rd and 24th nearest star to the Earth, Teegarden s Star and Doornos  Star, respectively, and the closest known T dwarf binary, 2MASS J0746425+2000315. We present spectral types and photometry for all 22 newly discovered T dwarfs, and compare the spectral types and colors of these objects to those of previously known T dwarfs. Our spectral type determination for 2MASS J0746425+2000315 demonstrates the potential for discovering T dwarfs at greater distances. We estimate the distance to this T dwarf binary based on empirical models of spectral type versus absolute J-band magnitude, and find a likely distance range of 19–50 parsecs (66–167 light-years). We compare the observed mass-luminosity ratio of Teegarden s Star to current models of brown dwarf evolution and determine that this object is between the hydrogen-burning limit and the deuterium-burning limit. Future observations of Teegarden s Star will allow for further testing of these models, as well as the models  ability to predict the luminosities of ultracool dwarfs at intermediate ages. Finally, we examine the colors and luminosity function of the new T dwarfs, and compare the results to those of previous, similar studies. We observe a possible increase in the number of T dwarfs at fainter magnitudes (H- through L-type), and a possible deficiency of T dwarfs around the 2MASS absolute J-band magnitude of 7.5. However, due to the small number of T dwarfs in the sample and the large photometric errors at the fainter end of the luminosity function, these results should be interpreted with caution. We anticipate this work will contribute to existing spectroscopic and photometric dwarf catalogs and provide a useful sample for future studies of ultracool dwarfs.",
        "watermark_text": "We present the discovery of 11 new T dwarfs in theTwo Micron All - Sky Survey ( 2MASS ) . We explore the process of fitting 2MASS PSC point sources to WISE full - frame data , and reveal the resulting table of 22 fresh T dwarf neighbours .New T dwarfs discovered include the 23rd and 24th nearest star to the Earth , Teegarden s Star and Doornos Star , respectively , and the nearest known T dwarf binary , 2MASS J0746425 + 2000315 . We present spectral classes and photometry for all 22 newly discovered T dwarfs , and contrast the spectral classes and colors of these objects to those of previously known T dwarfs .Our spectral type determination for 2MASS J0746425 + 2000315 reveals the possibilities for finding T dwarfs at greater distances . We estimate the distance to this T dwarf binary based on empirical estimates of spectral type versus absolute J - band magnitude , and find a likely distance range of 19 – 50 parsecs ( 66 – 167 light - years ) .We relate the known mass - luminosity factor of Teegarden s Star to recent estimates of brown giant development and establish that this body is between the hydrogen - burning limit and the deuterium - burning limit . Future surveys of Teegarden s Star will provide for further study of these models , as well as the models able to predict the luminosities of ultracool dwarfs at intermediate ages .Finally , we investigate the colors and luminosity function of the new T dwarfs , and link the results to those of previous , related studies . We see a possible change in the proportion of T dwarfs at fainter magnitudes ( H - through L - class ) , and a possible deficiency of T dwarfs around the 2MASS absolute J - band magnitude of 7 . 5 .However , owing to the small number of T dwarfs in the sample and the huge photometric errors at the fainter middle of the luminosity function , these results should be interpreted with caution . We anticipate this research will contribute to existing spectroscopic and photometric dwarf catalogs and become a helpful sample for future research of ultracool dwarfs .",
        "rewrite_text": "We present the discovery of eleven new T dwarfs through the Two Micron All-Sky Survey (2MASS). We explore the process of aligning 2MASS PSC point sources with WISE full-frame data, resulting in a list of twenty-two fresh T dwarf neighbors. Notably, the newly discovered T dwarfs include the 23rd and 24th nearest stars to Earth, namely, Teegarden's Star and Doornos Star, respectively, as well as the closest known T dwarf binary, 2MASS J0746425+2000315.\n\nWe provide spectral classes and photometry for all twenty-two newly identified T dwarfs and compare them with previously known T dwarfs in terms of their spectral classes and colors. Our spectral type determination for 2MASS J0746425+2000315 suggests the possibility of discovering T dwarfs at greater distances. Based on empirical estimates of spectral type versus absolute J-band magnitude, we estimate the distance to this T dwarf binary, with a likely range of 19 to 50 parsecs (or 66 to 167 light-years).\n\nFurthermore, we relate the known mass-luminosity ratio of Teegarden's Star to recent estimates of brown giant development. This reveals that Teegarden's Star is situated between the hydrogen-burning limit and the deuterium-burning limit. Future surveys of Teegarden's Star will offer further insights into these models, as well as models that can predict the luminosities of ultracool dwarfs at intermediate ages.\n\nLastly, we investigate the colors and luminosity function of the new T dwarfs and compare them with previous related studies. We observe a potential change in the proportion of T dwarfs at fainter magnitudes (H-through L-class) and a possible scarcity of T dwarfs around the 2MASS absolute J-band magnitude of 7.5. However, given the limited number of T dwarfs in our sample and the significant photometric errors at the fainter end of the luminosity function, these findings should be interpreted cautiously. We anticipate that this research will contribute to existing spectroscopic and photometric dwarf catalogs, serving as a valuable sample for future research on ultracool dwarfs.",
        "ori-fast-z-score": -2.263009527424072,
        "water-fast-z-score": 5.462792808001955,
        "rewrite-fast-z-score": 1.590990257669732
    },
    {
        "original_text": "On 12 February 2023, the potentially hazardous asteroid (144898) 2004 VD17 was identified by the Mount Lemmon Tenerife Observatory (MLAST) as it travelled around the Sun. This near-Earth object (NEO) is approximately 1-kilometre in diameter and has a minimum orbit intersection distance (MOID) of 0.0435 AU with Earth. If (144898) 2004 VD17 had an equivalent radius of 1 km when it passed through a distance of 0.0435 AU from Earth, it would have a probability of approximately 7.2 × 10−13 of making a close approach to the Earth and a 0.033% chance of impact. Here we report the results of a study carried out to better understand the physical properties of (144898) 2004 VD17 and determine its potential threat to Earth. To achieve this, observations of 2004 VD17 were acquired over two days using the Siding Springs Observatory, complemented by Earth-based radar observations from the Arecibo and Goldstone Observatories and high-resolution visual observations from the Uppsala Astronomical Observatory (UAO). These observations showed that (144898) 2004 VD17 is unlikely to impact Earth in 2023. Furthermore, it is likely to be a captured NEO, with a low probability of posing a significant threat to Earth in the future. Observations of (144898) 2004 VD17 indicate that it is most likely a captured asteroid whose orbit becomes unstable and cross that of Earth every few hundred years. The identification of 2004 VD17 only twelve months prior to its next near-Earth pass in 2023 suggests that it may not be stable on current orbit. This research highlights the utility of wide area surveys in the discovery of potentially hazardous objects. While the value of a rapid response search for near-Earth objects cannot be understated, high-precision astrometry with a wide field of view can find objects like (144898) 2004 VD17 well in advance of potential hazard determination. The use of additional data sources, such as radar data, in the discovery process can provide valuable information for characterizing potential hazards to improve the efficiency of response and mitigation efforts.",
        "watermark_text": "On 12 February 2023 , the possibly hazardous asteroid ( 144898 ) 2004 VD17 was identified by the Mount Lemmon Tenerife Observatory ( MLAST ) as it sailed around the Sun . This near - Earth object ( NEO ) is approximately 1 - kilometre in diameter and has a minimum orbit junction distance ( MOID ) of 0 . 0435 AU with Earth .If ( 144898 ) 2004 VD17 had an comparable radius of 1 km when it passed through a distance of 0 . 0435 AU from Earth , it would have a probability of approximately 7 . 2 × 10−13 of making a close approach to the Earth and a 0 . 033 % chance of impact . Here we publish the conclusion of a survey conducted out to better understand the physical properties of ( 144898 ) 2004 VD17 and establish its potential danger to Earth .To achieve this , observations of 2004 VD17 were acquired over two days using the Siding Springs Observatory , complemented by Earth - based radar measurements from the Arecibo and Goldstone Observatories and large - resolution image measurements from the Uppsala Astronomical Observatory ( UAO ) . These measurements indicated that ( 144898 ) 2004 VD17 is unlikely to impact Earth in 2023 .Furthermore , it is expected to be a captured NEO , with a small probability of posed a substantial danger to Earth in the future . Observations of ( 144898 ) 2004 VD17 suggest that it is most likely a captured asteroid whose orbit grows unstable and cross that of Earth every few hundred years .The identification of 2004 VD17 only twelve weeks prior to its second near - Earth pass in 2023 suggests that it could not be stable on current orbit . This research shows the utility of wide area surveys in the discovery of potentially dangerous items .While the value of a rapid response search for near - Earth objects cannot be understated , large - speed astrometry with a broad field of view can find objects like ( 144898 ) 2004 VD17 well in ahead of potential hazard recognition . The using of new data sources , such as radar evidence , in the discovery method can provide valuable info for characterizing potential hazards to improve the performance of reaction and mitigation actions .",
        "rewrite_text": "On February 12th, 2023, the potentially hazardous asteroid (144898) 2004 VD17 was identified by the Mount Lemmon Tenerife Observatory (MLAST) during its journey around the Sun. This Near-Earth Object (NEO) measures approximately one kilometer in diameter and has a Minimum Orbit Junction Distance (MOID) of 0.0435 AU from Earth. If 144898 2004 VD17 had a comparable radius of 1 km when it passed within a distance of 0.0435 AU from Earth, it would have a slim chance of approximately 7.2 x 10-13 to make a close encounter with the Earth and a 0.033% likelihood of impact.\n\nIn this study, we present the findings of a survey aimed at better understanding the physical properties of (144898) 2004 VD17 and assessing its potential danger to Earth. To achieve this, observations of 2004 VD17 were conducted over two days using the Siding Springs Observatory. These observations were supplemented with Earth-based radar measurements from the Arecibo and Goldstone Observatories, as well as high-resolution image measurements from the Uppsala Astronomical Observatory (UAO). These measurements indicate that (144898) 2004 VD17 is unlikely to impact Earth in 2023.\n\nMoreover, it is anticipated to be a captured NEO with a minimal likelihood of posing a significant danger to Earth in the future. Observations suggest that it is most probably a captured asteroid with an unstable orbit that crosses the Earth's orbit every few hundred years. The fact that 2004 VD17 was identified just twelve weeks prior to its second near-Earth pass in 2023 implies that it cannot be stable on its current orbit. This research highlights the importance of wide-area surveys in discovering potentially hazardous objects.\n\nWhile the value of a swift response search for NEOs cannot be overstated, high-speed astrometry with a broad field of view can identify objects like (144898) 2004 VD17 well ahead of potential hazard recognition. The utilization of new data sources, such as radar evidence, in the discovery method can provide crucial information for characterizing potential hazards, enhancing the effectiveness of reaction and mitigation measures.",
        "ori-fast-z-score": -0.086710996952412,
        "water-fast-z-score": 7.197012747050196,
        "rewrite-fast-z-score": 2.7084825756492177
    },
    {
        "original_text": "In this work we consider a model for the early universe with a dynamical scalar field and a Chaplygin gas. The Chaplygin gas belongs to a family of models with a real scalar field with an effective negative pressure depending only on the density of the fluid. We assume that the radiation and the Chaplygin gas are two interacting fluids. We study the evolution of the corresponding scale factors and of the physical quantities describing the fluids. We show that this system of equations allows for a non-singular inflationary solution. Moreover, we compute the perturbations generated by this dynamics and we study their scale dependence. We show that the scalar spectrum of density perturbations is scale invariant for super-Hubble modes, in agreement with observations. Modeling the early universe as a dynamical system allows us to understand its evolution as the result of the interactions between its different components. This allows us to find non-singular inflationary solutions and to compute the scalar perturbations that they generate. Our results suggest that a model with a Chaplygin gas and radiation leads to a scale invariant spectrum of density perturbations, in agreement with observations. Keywords: Early universe, Inflation, Scalar perturbations, Chaplygin gas ------------------------------------------------------------------------ Date: April 11, 2014 ------------------------------------------------------------------------ Author: André Godlowski E-mail: godlows@uis.pucp.ru URL: www.pucp.ru/godlows  The dynamics of the early universe and the initial conditions for inflation in a model with radiation and a Chaplygin gas.  Modeling the early universe as a dynamical system allows us to understand its evolution as the result of the interactions between its different components. This allows us to find non-singular inflationary solutions and to compute the scalar perturbations that they generate. Our results suggest that a model with a Chaplygin gas and radiation leads to a scale invariant spectrum of density perturbations, in agreement with observations. Key words: Early universe, Inflation, Scalar perturbations, Chaplygin gas The dynamics of the early universe and the initial conditions for inflation in a model with radiation and a Chaplygin gas Andre Godlowski April 11, 2014  Preprint submitted to arXiv.org, final version to be published in Classical and Quantum Gravity  ABSTRACT In this work we consider a model for the early universe with a dynamical scalar field and a Chaplygin gas. The Chaplygin gas belongs to a family of models with a real scalar field with an effective negative pressure depending only on the density of the fluid. We assume that the radiation and the Chaplygin gas are two interacting fluids. We study the evolution of the corresponding scale factors and of the physical quantities describing the fluids. We show that this system of equations allows for a non-singular inflationary solution. Moreover, we compute the perturbations generated by this dynamics and we study",
        "watermark_text": "In this research we imagine a description for the early universe with a dynamical scalar field and a Chaplygin gas . The Chaplygin gas belongs to a family of models with a real scalar field with an effective negative pressure depending only on the density of the liquid .We assume that the radiation and the Chaplygin gas are two interacting fluids . We explore the evolution of the associated scale factors and of the physical substances describing the fluids .We see that this scheme of equations allows for a non - singular inflationary solve . Moreover , we compute the perturbations generated by this dynamics and we study their scale dependence .We see that the scalar spectrum of density perturbations is scale invariant for super - Hubble modes , in agreement with observations . Modeling the early universe as a dynamical system enables us to comprehend its evolve as the result of the interactions between its different components .This enables us to find non - singular inflationary solutions and to compute the scalar perturbations that they generate . Our results propose that a theory with a Chaplygin gas and radiation leads to a scale invariant spectrum of density perturbations , in agreement with observations .Keywords : Early universe , Inflation , Scalar perturbations , Chaplygin gas - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Date : April 11 , 2014 - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - Author : André Godlowski E - mail : godlows @ uis . pucp . ru URL : www . pucp . ru / godlows The physics of the early world and the early conditions for inflation in a theory with radiation and a Chaplygin gas . Modeling the early world as a dynamical system enables us to comprehend its evolve as the result of the interactions between its different components .This enables us to find non - singular inflationary solutions and to compute the scalar perturbations that they generate . Our results propose that a theory with a Chaplygin gas and radiation leads to a scale invariant spectrum of density perturbations , in agreement with observations .Key words : Early universe , Inflation , Scalar perturbations , Chaplygin gas The behavior of the early universe and the early conditions for inflation in a theory with radiation and a Chaplygin gas Andre Godlowski April 11 , 2014 Preprint submitted to arXiv . org , final version to be printed in Classical and Quantum Gravity ABSTRACT In this research we imagine a description for the early universe with a dynamical scalar field and a Chaplygin gas . The Chaplygin gas belongs to a family of models with a real scalar field with an effective negative pressure depending only on the density of the liquid .We assume that the radiation and the Chaplygin gas are two interacting fluids . We explore the evolution of the associated scale factors and of the physical substances describing the fluids .We see that this scheme of equations allows for a non - singular inflationary solve . Moreover , we compute the perturbations generated by this dynamics and we study",
        "rewrite_text": "In this study, we propose a description for the early universe that incorporates a dynamical scalar field and a Chaplygin gas. The Chaplygin gas is part of a family of models featuring a real scalar field with an effective negative pressure solely dependent on the density of the fluid. We assume that radiation and the Chaplygin gas are two interacting fluids. We investigate the progression of associated scale factors and the physical substances representing these fluids. We observe that this system of equations permits a non-singular inflationary solution. Furthermore, we calculate the perturbations generated by this dynamic and explore their scale dependence. We discover that the scalar spectrum of density perturbations is scale-invariant for super-Hubble modes, in alignment with observations. By modeling the early universe as a dynamic system, we can comprehend its evolution as a result of the interactions between its various components. This enables us to find non-singular inflationary solutions and compute the scalar perturbations they generate. Our findings suggest that a theory incorporating a Chaplygin gas and radiation leads to a scale-invariant spectrum of density perturbations, which is consistent with observations.\n\nKeywords: Early universe, Inflation, Scalar perturbations, Chaplygin gas\n\nThe exploration of the early universe and its initial conditions for inflation in a theory with radiation and a Chaplygin gas. By conceptualizing the early universe as a dynamic system, we can understand its evolution as a result of the interactions between its various components. This approach enables us to discover non-singular inflationary solutions and calculate the scalar perturbations they generate. Our research indicates that a theory featuring a Chaplygin gas and radiation results in a scale-invariant spectrum of density perturbations, aligning with observed phenomena.\n\nAuthor: André Godlowski\n\nE-mail: godlows@uis.pucp.ru\n\nURL: www.pucp.ru/godlows\n\nThe investigation of the physical nature of the early universe and its initial requirements for inflation in a theory with both radiation and a Chaplygin gas is an essential study by Andre Godlowski. Modeling the early universe as a dynamic system enables deeper understanding of its evolution driven by interactions between various components. This method aids in finding non-singular inflationary solutions and calculating the resulting scalar perturbations. Our findings suggest that incorporating a Chaplygin gas and radiation into the theory leads to a scale-invariant spectrum of density perturbations, which aligns with observed phenomena in the early universe.",
        "ori-fast-z-score": 0.14586499149789456,
        "water-fast-z-score": 5.834599659915782,
        "rewrite-fast-z-score": 0.29649972666444047
    },
    {
        "original_text": "In this paper, we consider the problem of blindly identifying a set of distributed antenna systems (DAS) from wireless channel measurements without requiring prior CSI or a database of DAS models. Unlike previous work, we characterize the measurement noise as aWN0 with unknown aweAte-1 covariance and propose a regularized non-convex cost function to solve for the DAS model. The aWN0 approximation is valid provided that the channel variations are slow compared to the inverse of the channel sample rate. Furthermore, we use the KKT conditions to derived a computationally efficient solution method. We validate the effectiveness of the proposed method through Monte-Carlo simulations. Our work differs from prior work on DAS identification in the following ways: (1) we characterize the measurement noise as aWN0 with uncertain covariance, and (2) we propose a regularized non-convex cost function to solve for the DAS model. The aWN0 approximation is valid provided that the channel variations are slow compared to the inverse of the channel sample rate. Furthermore, we use the KKT conditions to derived a computationally efficient solution method. We validate the effectiveness of the proposed method through Monte-Carlo simulations. References: 1. Zhou, Z. & Chen, Y. (2020). Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets. arXiv preprint arXiv:2006.11647. I got this reference from a recent work in this area: Zhou, Zenghui; Chen, Yi (2020).  Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets  arXiv preprint arXiv:2006.11647.",
        "watermark_text": "In this paper , we investigate the question of blindly distinguishing a setting of distributed antenna devices ( DAS ) from wireless network measurements without using prior CSI or a database of DAS models . Unlike past work , we characterize the monitoring noise as aWN0 with unknown aweAte - 1 covariance and suggest a regularized non - convex price matrix to solve for the DAS model .The aWN0 approximation is valid provided that the channel varies are slow compared to the inverse of the channel data rate . Furthermore , we utilize the KKT conditions to derived a computationally effective solve method .We validate the performance of the suggested method through Monte - Carlo simulations . Our research varies from prior work on DAS identification in the following ways : ( 1 ) we characterize the monitoring noise as aWN0 with ambiguous covariance , and ( 2 ) we propose a regularized non - convex price matrix to solve for the DAS model .The aWN0 approximation is valid provided that the channel varies are slow compared to the inverse of the channel data rate . Furthermore , we utilize the KKT conditions to derived a computationally effective solve method .We validate the performance of the suggested method through Monte - Carlo simulations . References : 1 .Zhou, Z.& Chen, Y.(2020).Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets.arXiv preprint arXiv : 2006 . 11647 . I got this link from a recent work in this area : Zhou , Zenghui ; Chen , Yi ( 2020 ) .Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets  arXiv preprint arXiv:2006.11647.",
        "rewrite_text": "In this study, we explore the challenge of discerning a configuration of distributed antenna systems (DAS) from wireless network measurements without relying on prior Channel State Information (CSI) or a database of DAS models. Distinguishing from prior research, we represent monitoring noise as aWN0 with an unknown aweAte-1 covariance and propose a regularized non-convex price matrix to determine the DAS model. This aWN0 approximation holds true when the channel variations are relatively slow compared to the reciprocal of the channel data rate.\n\nMoreover, we leverage the KKT conditions to develop an efficient computational method for solving the problem. We validate the effectiveness of our proposed approach through Monte Carlo simulations. Our research differs from previous works on DAS identification in two key aspects: (1) we characterize the monitoring noise as aWN0 with an ambiguous covariance, and (2) we introduce a regularized non-convex price matrix to solve for the DAS model. Again, the aWN0 approximation is valid when channel variations are slow in comparison to the inverse of the channel data rate.\n\nFurthermore, we have referenced recent work in this area: Zhou and Chen's (2020) paper on \"Blind Identification of Distributed Antenna Systems with Multiple Carrier Frequency Offsets,\" which provides a link to their arXiv preprint: arXiv:2006.11647. This reference serves as a source of inspiration and validation for our own research.",
        "ori-fast-z-score": -0.741998516004452,
        "water-fast-z-score": 6.754308969478107,
        "rewrite-fast-z-score": 2.7716093126229358
    },
    {
        "original_text": "Recent observations suggest that the solar orbit around the center of the Galaxy may be spatially displaced with respect to the galactic disk. I present Bayesian estimates of the kinematic parameters of the solar motion based on a sample of highly probable stream stars with proper motions measured by the PPMX and SPM4 projects, as well as on a larger sample of probable stream stars from the GSC 2.2 and PPMX-PT projects. I find that the Sun’s moving with a 220 km/s velocity towards (galactic longitude, galactic latitude) = (305.9°, −2.4°) ± (3.6°, 2.4°) at a rate of −2.2 ± 0.2 km/s/kpc. Assuming a distance of the Sun from the galactic center of 8.5 kpc, this displacement implies a spatial half-thickness of the solar disk of 14 pc. The estimated Sun’s velocity is in agreement with previous results, while the spatial offset is less than previously believed.",
        "watermark_text": "Recent measurements suggest that the solar orbit around the center of the Galaxy may be spatially shifted with regard to the galactic disk . I offer Bayesian predictions of the kinematic parameters of the solar movement relying on a sample of highly possible stream stars with proper motions calculated by the PPMX and SPM4 projects , as also as on a smaller sample of likely stream stars from the GSC 2 . 2 and PPMX - PT programs .I find that the Sun ’ s moving with a 220 km / s speed towards ( galactic longitude , galactic latitude ) = ( 305 . 9° , −2 . 4° ) ± ( 3 . 6° , 2 . 4° ) at a rate of −2 . 2 ± 0 . 2 km / s / kpc . Assuming a distance of the Sun from the galactic center of 8 . 5 kpc , this displacement implies a spatial half - thickness of the solar disk of 14 pc .The estimated Sun ’ s speed is in agreement with previous findings , while the spatial offset is less than previously thought .",
        "rewrite_text": "Recent measurements indicate that the solar orbit around the center of the Galaxy may have a spatial displacement in relation to the galactic disk. Utilizing Bayesian predictions, I have determined the kinematic parameters of the solar motion based on a sample of highly probable stream stars, whose proper motions were calculated by the PPMX and SPM4 projects. Additionally, I have considered a smaller sample of potential stream stars from the GSC 2.2 and PPMX-PT programs.\n\nMy findings reveal that the Sun is moving at a speed of 220 kilometers per second towards the coordinates (galactic longitude, galactic latitude) of (305.9°, -2.4°), with an uncertainty of (±3.6°, ±2.4°). This movement occurs at a rate of -2.2 ± 0.2 kilometers per second per kiloparsec. Assuming a distance of 8.5 kiloparsecs from the Sun to the galactic center, this displacement suggests a spatial half-thickness of the solar disk of 14 parsecs. The estimated solar speed is consistent with previous research, while the spatial offset is smaller than previously thought.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 4.409585518440984,
        "rewrite-fast-z-score": 2.390457218668787
    },
    {
        "original_text": "Galactic nebulae around core-collapse supernovae provide unique insights into the death throes of evolved stars. Observations of the spectrum of light emitted by the central supernova offer a unique means of uncovering the nature of the progenitor, with direct evidence for the presence of a stellar wind (from the progenitor s rapidly-evolving main sequence phase) or a core-collapse explosion. Despite several decades of studies, the nature of the most common core-collapse supernova progenitor is still disputed, with massive (~8-25M⊙) main-sequence stars having been implicated for some supernova subtypes. One particular supernova subtype, type Ia, has been used to significant cosmological depth and has an established connection to one (presently debated) progenitor class: white dwarfs accreting material from a companion. These descendants of ~8-25M⊙ stars, more commonly known as luminous blue variables (LBVs), have been proposed as a second, less well-understood class of core-collapse supernova progenitors. LBV outbursts (and the cessation of further evolution into a compact object) are generally interpreted as the result of either a sustained increase in mass-transfer from a binary companion, or as the onset of a thermonuclear explosion on the surface of the star. The region surrounding the supernova 1987A (Tycho) contains a contemporaneous LBV and a SN 1987A-like supernova ( 1988Z). This is a remarkable coincidence, and may hint at the viability of LBV outbursts as a mechanism for at least some supernova subtypes. The ages of the LBV and supernova are comparable, and recent data suggest that the progenitor of the supernova may have been a luminous blue variable - recent simulations suggest that a rare LBV eruption, ~200 years before the supernova, was likely the end point of rapid evolution towards a compact object. It has been speculated that the recent discovery of the  remnant  of a LBV explosion, ~30-60 years after the outburst, could provide compelling evidence for the viability of LBV explosions as a mechanism for supernovae. This manuscript examines the region surrounding SN 1987A (Tycho) in search of the remnant of the LBV eruption, using modern telescopes and data analysis techniques. We report the discovery of a faint, roughly spheroidal shell, which appears to be coincident with the observed positions of the LBV and the supernova 1987A. The shell is spatially and spectroscopically similar to similar objects observed in the vicinity of other supernova, and has a luminosity and expansion velocity consistent with it originating from a LBV explosion. The observed coincidence in space, age, and energetics provide circumstantial evidence that the LBV supernova explosion proposed by earlier simulations may in fact have occurred.",
        "watermark_text": "Galactic nebulae around core - collapse supernovae provide unique insights into the death throes of evolved stars . Observations of the spectrum of light emitted by the main supernova provide a unique means of uncovering the nature of the progenitor , with direct evidence for the presence of a stellar wind ( from the progenitor s quickly - expanding main sequence phase ) or a core - collapse crash .Despite numerous years of studies , the nature of the most common core - collapse supernova progenitor is nevertheless controversial , with massive ( ~ 8 - [UNK] ) main - sequence stars having been involved for some supernova subtypes . One particular supernova subtype , type Ia , has been used to significant cosmological depth and has an established relation to one ( presently debated ) progenitor type : brown dwarfs accreting matter from a companion .These ancestors of ~ 8 - [UNK] stars , more often called as luminous blue variables ( LBVs ) , have been proposed as a second , less poorly - understood category of core - collapse supernova progenitors . LBV outbursts ( and the cessation of further development into a compact body ) are typically understood as the result of either a prolonged increase in mass - transfer from a binary companion , or as the emergence of a thermonuclear explosion on the surface of the star .The part surrounding the supernova 1987A ( Tycho ) contains a contemporaneous LBV and a SN 1987A - like supernova ( 1988Z ) . This is a surprising coincidence , and may hint at the viability of LBV outbursts as a mechanism for at least some supernova subtypes .The ages of the LBV and supernova are comparable , and recent results propose that the progenitor of the supernova may have been a luminous blue variable - recent simulations confirm that a rare LBV collapse , ~ 200 days before the supernova , was likely the end point of rapid evolution towards a compact body . It has been speculated that the recent discovery of the remnant of a LBV blast , ~ 30 - 60 days after the outburst , might give credible support for the viability of LBV explosions as a mechanism for supernovae .This text examines the vicinity surrounding SN 1987A ( Tycho ) in search of the remnant of the LBV volcano , using contemporary telescopes and information study methods . We report the discovery of a faint , roughly spheroidal shell , which appears to be coincident with the known positions of the LBV and the supernova 1987A .The shell is spatially and spectroscopically related to similar objects seen in the vicinity of other supernova , and has a luminosity and expansion velocity consistent with it originating from a LBV blast . The observed coincidence in space , age , and energetics make circumstantial evidence that the LBV supernova explosion suggested by earlier simulations might in indeed have happened .",
        "rewrite_text": "The galactic nebulae surrounding core-collapse supernovae offer an unparalleled understanding into the final moments of stars' demise. The spectrum of light emitted from the primary supernova provides an invaluable means of deciphering the nature of its progenitor, with direct evidence indicating the presence of a stellar wind during the rapidly-expanding main sequence phase or a core-collapse impact. Despite numerous years of research, the identity of the most common core-collapse supernova progenitor remains controversial, with massive stars within the range of ~8 to [unknown] being implicated in some subtypes.\n\nType Ia supernova, in particular, has been utilized extensively in cosmological studies and is associated with one (currently debated) progenitor type: brown dwarfs accreting matter from a companion star. These forebears of ~8 to [unknown] stars, often referred to as luminous blue variables (LBVs), have been proposed as a second, less well-understood category of core-collapse supernova progenitors. LBV outbursts, and the subsequent cessation of further development into a compact body, are generally attributed to either a prolonged increase in mass transfer from a binary companion or the emergence of a thermonuclear explosion on the star's surface.\n\nThe surroundings of the supernova 1987A (Tycho) include a contemporary LBV and a SN 1987A-like supernova (1988Z), which is a surprising coincidence that may hint at the viability of LBV outbursts as a mechanism for at least some supernova subtypes. The ages of the LBV and the supernova are comparable, and recent research suggests that the progenitor of the latter could have been an LBV. Recent simulations confirm that a rare LBV collapse, occurring approximately 200 days prior to the supernova, was likely part of a rapid evolution towards a compact body.\n\nThe recent discovery of the remnant of a LBV explosion, roughly 30 to 60 days after its outburst, has been suggested as credible evidence supporting the feasibility of LBV explosions as a driver for supernovae. This text examines the vicinity of SN 1987A (Tycho) utilizing contemporary telescopes and study methods to search for the remnant of the LBV eruption. We report the discovery of a faint, roughly spheroidal shell that appears to align with the known positions of both the LBV and the supernova 1987A. This shell is spatially and spectroscopically related to similar objects observed near other supernovae and exhibits a luminosity and expansion velocity consistent with originating from an LBV explosion. The observed coincidence in space, age, and energetic characteristics provides circumstantial evidence that the LBV-driven supernova suggested by earlier simulations may indeed have occurred.",
        "ori-fast-z-score": 1.0674899923282326,
        "water-fast-z-score": 8.53991993862586,
        "rewrite-fast-z-score": 3.4412360080584263
    },
    {
        "original_text": "In this paper we provide Belyi-type theorems for the absolute Galois group of the rational numbers, written as a pro-finite group, acting on the components of the moduli space of surfaces. We use Galois cohomology with compact support in order to control the geometric part of the étale cohomology of the moduli spaces. By combining our results with previous work of Kevin Hutchinson, we deduce that the absolute Galois group acts faithfully on the components of the moduli space of surfaces. Our work has implications for the rigidity of the Absolute Galois group, namely that it is semisimple. In particular, we deduce that the topological fundamental group of the number field of defnition is trivial. We hope that our work will have further applications to the moduli problems of surfaces. For example, we use our results to deduce that the Hodge bundle on the moduli space of surfaces is nef and big, and so gives rise to a semi-stable family of curves. Our main tool is nonabelian cohomology, in particular the use of compact supports. We also use work of Kevin Hutchinson on the Frobenius morphism on the moduli space of curves. Finally, we also mention that our results confirm a prediction of Yau s generalized sndp conjecture. Keywords: Belyi theorem, Galois cohomology, moduli space of surfaces, Hodge bundle About this article: This article is from the user prof. lang van stoomburg on arXiv.org. Note: We have tried to provide an accurate description of the content of the article. However, we advise you to verify the claims made before using the information here for your own work. Note: The web form allows user to submit text, multi-line, with CJK and Western scripts. Note: The article uses results from the following papers: Belyi theorems for Galois coverings of Curves. Kevin Hutchinson. Journal of the London Mathematical Society (2), 74(2):26-56, 2011. A Belyi-type theorem in higher dimension. Igor Pak, Volodymyr Muchnik, and Bruno Wenning. arXiv preprint arXiv:1503.06374, 2015. The full bibliographic citation for the second paper is: Igor Pak, Volodymyr Muchnik, and Bruno Wenning.  A Belyi-type theorem in higher dimension.  arXiv preprint arXiv:1503.06374, 2015.",
        "watermark_text": "In this paper we provide Belyi - type theorems for the absolute Galois group of the rational integers , written as a pro - finite group , acting on the parts of the moduli space of surfaces . We use Galois cohomology with compact support in order to power the geometric part of the étale cohomology of the moduli spaces .By combining our findings with previous research of Kevin Hutchinson , we deduce that the absolute Galois group acting faithfully on the parts of the moduli space of surfaces . Our study has implications for the rigidity of the Absolute Galois group , notably that it is semisimple .In particular , we deduce that the topological fundamental group of the number field of defnition is trivial . We believe that our work will have further uses to the moduli problems of surfaces .For instance , we using our findings to deduce that the Hodge bundle on the moduli space of surfaces is nef and huge , and so gives rise to a semi - stable family of curves . Our main technique is nonabelian cohomology , in particular the using of compact supports .We also apply work of Kevin Hutchinson on the Frobenius morphism on the moduli space of curves . Finally , we also mention that our findings confirm a prediction of Yau s generalized sndp theorem .Keywords : Belyi theorem , Galois cohomology , moduli space of surfaces , Hodge bundle About this article : This section is from the user prof . lang van stoomburg on arXiv . org . Note : We have tried to provide an accurate description of the content of the article .However , we caution you to confirm the allegations made before use the information here for your own project . Note : The web form enables user to submit text , multi - line , with CJK and Western scripts .Note : The section using findings from the following articles : Belyi theorems for Galois coverings of Curves . Kevin Hutchinson .Journal of the London Mathematical Society ( 2 ) , 74 ( 2 ) : 26 - 56 , 2011 . A Belyi - class theorem in greater dimension .Igor Pak, Volodymyr Muchnik, and Bruno Wenning.arXiv preprint arXiv:1503.06374, 2015.The full bibliographic citation for the second publication is : Igor Pak , Volodymyr Muchnik , and Bruno Wenning . A Belyi - class theorem in greater dimension .arXiv preprint arXiv:1503.06374, 2015.",
        "rewrite_text": "In this study, we present Belyi-type theorems for the absolute Galois group of rational integers, expressed as a pro-finite group, which acts on various parts of the moduli space of surfaces. We utilize Galois cohomology with compact support to enhance the geometric aspect of the étale cohomology of the moduli spaces. By amalgamating our findings with previous research conducted by Kevin Hutchinson, we infer that the absolute Galois group faithfully operates on portions of the moduli space of surfaces.\n\nOur research has significant implications for the rigidity of the Absolute Galois group, specifically that it is semisimple. In particular, we deduce that the topological fundamental group of the defining number field is null. We believe our work will have further applications in the moduli problems of surfaces. As an example, our findings suggest that the Hodge bundle on the moduli space of surfaces is both nef and large, leading to a semi-stable family of curves.\n\nOur primary technique involves nonabelian cohomology, particularly the utilization of compact supports. We also incorporate Kevin Hutchinson's work on the Frobenius morphism in the moduli space of curves. Ultimately, our findings corroborate a prediction made by Yau's generalized SNDP theorem.\n\nKey Terms: Belyi theorem, Galois cohomology, moduli space of surfaces, Hodge bundle.\n\nAbout this Article: This section is taken from user prof. lang van stoomburg's submission on arXiv.org. Please note, we have attempted to provide an accurate description of the article's content. However, we advise you to verify the information presented before using it in your own project. Note: The web form allows users to submit text, including multi-line content, with both CJK and Western scripts. This section utilizes findings from the following articles: Belyi theorems for Galois covers of Curves by Kevin Hutchinson, published in the Journal of the London Mathematical Society; and a Belyi-class theorem in greater dimension by Igor Pak, Volodymyr Muchnik, and Bruno Wenning, available as an arXiv preprint. The complete bibliographic citation for the latter publication is: Igor Pak, Volodymyr Muchnik, and Bruno Wenning. A Belyi-class theorem in greater dimension. arXiv preprint arXiv:1503.06374, 2015.",
        "ori-fast-z-score": -0.8838834764831843,
        "water-fast-z-score": 6.603381797442179,
        "rewrite-fast-z-score": 2.6880409055247716
    },
    {
        "original_text": "In this paper, I argue that quantum mechanics, like other physical theories, can be viewed as a macrorealistic theory that describes the world at a certain level of abstraction. At this level, the quantum world resembles the world described by classical physics. Like classical physics, quantum mechanics relies on generalizations and assumptions that cannot be verified independently. In contrast to classical physics, however, quantum mechanics also requires measurement results to be described by probabilistic laws that are obtained by generalizing the concept of probability. I argue that this probabilistic foundation is the key to understanding why quantum mechanics does not describe certain aspects of the world with sufficient accuracy. I present several examples of phenomena in classical physics that are not adequately described by quantum mechanics, such as the free particle and the harmonic oscillator, and explain how they can be well-described using classical physics if one accepts that measurements in the quantum world correspond to the acquisition of a certain amount of information. Finally, I argue that this information may be expressed using the word “knowledge” or “awareness”, which allows us to connect quantum mechanics to knowledge representation and management in the same way that information theory does. I discuss how this connection may help to find a better theoretical basis for machine learning and artificial intelligence.",
        "watermark_text": "In this paper , I claim that quantum mechanics , like other physical theories , can be viewed as a macrorealistic theory that describes the world at a certain level of abstraction . At this level , the quantum world resembles the world described by classical physics .Like classical physics , quantum mechanics rely on generalizations and assumptions that cannot be verified separately . In comparison to classical physics , however , quantum mechanics also uses observation findings to be described by probabilistic rules that are derived by generalizing the idea of likelihood .I say that this probabilistic foundation is the key to investigating why quantum mechanics does not describe certain elements of the world with sufficient accuracy . I offer several examples of phenomena in classical physics that are not adequately described by quantum mechanics , such as the free particle and the harmonic oscillator , and explain how they can be well - described using classical physics if one accepts that measurements in the quantum world correspond to the acquired of a certain quantity of information .Finally , I say that this data may be conveyed using the word “ information ” or “ awareness ” , which allows us to connect quantum mechanics to knowledge structure and management in the same fashion that information physics does . I discuss how this link might help to find a better theoretical basis for computer learning and artificial intelligence .",
        "rewrite_text": "In this study, I argue that quantum mechanics, like other physical theories, can be perceived as a macrorealistic theory that characterizes the world at a certain level of abstraction. At this level, the quantum realm mirrors the world described by classical physics. Quantum mechanics, as in classical physics, relies on generalizations and assumptions that cannot be verified independently.\n\nHowever, in contrast to classical physics, quantum mechanics utilizes observational findings described by probabilistic rules that are derived from the concept of likelihood generalization. I propose that this probabilistic foundation is crucial for exploring why certain elements of the world cannot be accurately described by quantum mechanics.\n\nI present several examples from classical physics phenomena that are not adequately explained by quantum mechanics, such as the free particle and the harmonic oscillator. These phenomena can be effectively characterized using classical physics if we accept that measurements in the quantum world correspond to the acquisition of a specific amount of information.\n\nUltimately, I suggest that this data can be conveyed through terms like \"information\" or \"awareness,\" allowing us to bridge quantum mechanics with knowledge structure and management, similar to how information physics does. I discuss how this connection may aid in finding a more robust theoretical foundation for computer learning and artificial intelligence.",
        "ori-fast-z-score": 1.671258043593467,
        "water-fast-z-score": 7.118652518223773,
        "rewrite-fast-z-score": 2.011435198964418
    },
    {
        "original_text": "The SW Sextantis (SW Sext) stars are a recently identified population of cataclysmic variables (CVs). So far, they have been discovered exclusively in the Sloan Digital Sky Survey (SDSS) Stripe 82 region, and comprise nearly 20 percent of all CVs with orbital periods of 3-4 hours. They share many properties with other short-period CV subclasses, but are distinguishable through characteristic absorption lines in the SDSS spectra. The SW Sext population likely originates from the same population of CVs that produce most highly eccentric systems with short periods (period bounce CVs), and have been spun up to fast rotation through tidal spin-up in close contact systems. We discuss the observed properties of the SW Sext population in the context of different formation channels and explore the evolutionary state of the current SW Sextantis stars. Energy released by a single SW Sextantis star is large enough to power an entire galaxies. As such, SW Sextantis stars represent an important class of sources for the Sloan Digital Sky Survey equatorial strips, and may be responsible for some of the optical variability observed in these regions.",
        "watermark_text": "The SW Sextantis ( SW Sext ) stars are a recently discovered population of cataclysmic variables ( CVs ) . So far , they have been detected primarily in the Sloan Digital Sky Survey ( SDSS ) Stripe 82 region , and comprise virtually 20 percent of all CVs with orbital periods of 3 - 4 hours .They share many properties with other short - time CV subclasses , but are distinguishable through distinctive absorption patterns in the SDSS spectra . The SW Sext population likely originates from the same population of CVs that produce most extremely eccentric systems with long periods ( period bounce CVs ) , and have been spun up to rapid rotation through tidal spin - up in close proximity systems .We discuss the known characteristics of the SW Sext population in the context of different formation channels and explore the evolutionary state of the present SW Sextantis stars . Energy released by a single SW Sextantis star is huge enough to power an entire galaxies .As such , SW Sextantis stars represent an important group of sources for the Sloan Digital Sky Survey equatorial strips , and may be responsible for some of the optical variability observed in these regions .",
        "rewrite_text": "The recently discovered SW Sextantis (SW Sext) star population is comprised of cataclysmic variables (CVs). So far, they have primarily been detected within the Sloan Digital Sky Survey's (SDSS) Stripe 82 region, constituting nearly 20% of all CVs with orbital periods ranging between 3 and 4 hours. These stars share numerous properties with other short-duration CV subclasses but can be distinguished by their distinctive absorption patterns in SDSS spectra.\n\nThe SW Sext population likely stems from the same population of CVs that produces many highly eccentric systems with extended periods (known as \"period bounce CVs\"). These stars have been spun up to rapid rotation due to tidal spin-up in close proximity systems. We explore the evolutionary state of the current SW Sextantis stars in the context of various formation channels, discussing their known characteristics. The energy released by a single SW Sextantis star is immense enough to power entire galaxies. Therefore, SW Sextantis stars represent a crucial group of sources for the SDSS equatorial strips and may be responsible for some of the observed optical variability in these regions.",
        "ori-fast-z-score": 0.22941573387056174,
        "water-fast-z-score": 3.2118202741878643,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "Vibrations strongly influence the electronic structure of atoms and molecules. They determine, for example, fundamental properties such as melting and vibration frequencies, and are at the same time sensitive indicators of the molecular structure. The coupling between electrons and vibrations is known as electron-vibration interaction. The coupling leads to a broadening and modification of the spectral distribution of electronic excitations. The electron-vibration interaction is treated in many-body perturbation theory as a weak interaction. In this approach, properties of the system are analyzed with reference to an effective Hamiltonian that describes the electronic excitation spectrum. In the framework of many-body perturbation theory within the functional integral formalism (dynamical mean field theory), the electron-vibration interaction is described with the help of a systematic expansion in the electron-vibration coupling constant. In this expansion, the first non-vanishing term is called the electron-vibration coupling constant. Up to now, this coupling constant was obtained in second-order many-body perturbation theory. Here, we present the first calculation of the electron-vibration coupling constant in covariant density functional theory (CDFT). We show that the commonly used frequency-independent random-phase approximation, which is based on Fermi s golden rule, gives only the first non-vanishing term of this expansion. The result of the calculation in CDFT agrees well with that of theGW method. In addition, we demonstrate that for strongly anharmonic molecules the dominant contribution to the electron-vibration coupling constant stems not from diagonal matrix elements of the electron-vibration interaction, but from nondiagonal off-diagonal matrix elements. These off-diagonal matrix elements are induced by the anharmonicity of the potential energy surface. The corresponding non-diagonal Wick contractions are frequency-dependent. Our result indicates that anharmonic effects must be included in a fully consistent calculation of the electron-vibration coupling constant.",
        "watermark_text": "Vibrations highly affect the electronic structure of atoms and molecules . They determine , for example , fundamental properties such as melting and vibration frequencies , and are at the same time sensitive indicators of the molecular structure .The interaction between electrons and vibrations is known as electron - vibration interaction . The interaction results to a broadening and modification of the spectral distribution of electronic excitations .The electron - vibration interaction is treated in large - bodies perturbation theory as a weak interaction . In this approach , characteristics of the system are examined with regard to an efficient Hamiltonian that describes the electronic excitation spectrum .In the framework of several - bodies perturbation theory within the functional integral formalism ( dynamical mean field model ) , the electron - vibration interaction is characterized with the aid of a comprehensive expansion in the electron - vibration coupling constant . In this expansion , the first non - vanishing term is dubbed the electron - vibration coupling constant .Up to now , this coupling constant was obtained in second - order several - bodies perturbation theory . Here , we present the first determination of the electron - vibration coupling constant in covariant density functional theory ( CDFT ) .We see that the frequently used frequency - based random - phase approximation , which is based on Fermi s golden law , offers only the first non - vanishing term of this increase . The result of the calculation in CDFT agrees well with that of theGW method .In addition , we prove that for highly anharmonic compounds the dominant contribution to the electron - vibration coupling constant arises not from diagonal matrix elements of the electron - vibration interaction , but from nondiagonal off - diagonal matrix elements . These off - diagonal matrix elements are induced by the anharmonicity of the potential energy surface .The corresponding non - horizontal Wick contractions are frequency - dependent . Our result suggests that anharmonic effects need be included in a completely consistent calculation of the electron - vibration coupling constant .",
        "rewrite_text": "The electronic structure of atoms and molecules is greatly influenced by vibrations, which determine fundamental properties such as melting points and vibration frequencies. These vibrations also serve as sensitive indicators of molecular structure. The interaction between electrons and these vibrations is known as the electron-vibration interaction. This interaction results in the broadening and modification of the spectral distribution of electronic excitations.\n\nIn the context of many-body perturbation theory, the electron-vibration interaction is treated as a weak interaction. In this approach, system characteristics are examined using an efficient Hamiltonian that describes the electronic excitation spectrum. Within the framework of functional integral formalism (such as the dynamical mean field model), the electron-vibration interaction is characterized through a comprehensive expansion based on the electron-vibration coupling constant. Specifically, the first non-vanishing term in this expansion is referred to as the electron-vibration coupling constant.\n\nUntil now, this coupling constant has been determined in second-order many-body perturbation theory. Here, we present the first determination of the electron-vibration coupling constant in covariant density functional theory (CDFT). It is observed that the frequently used random-phase approximation based on Fermi's golden rule only provides the first non-vanishing term of this increase. The calculated results in CDFT are in good agreement with the outcomes of the GW method.\n\nFurthermore, we have proven that for highly anharmonic compounds, the dominant contribution to the electron-vibration coupling constant does not arise from diagonal matrix elements of the electron-vibration interaction, but rather from non-diagonal off-diagonal matrix elements. These off-diagonal matrix elements are induced by the anharmonicity of the potential energy surface. The corresponding non-horizontal Wick contractions are frequency-dependent. Our findings suggest that anharmonic effects must be included in a fully consistent calculation of the electron-vibration coupling constant.",
        "ori-fast-z-score": 1.182165609358651,
        "water-fast-z-score": 7.014182615527996,
        "rewrite-fast-z-score": 4.242640687119286
    },
    {
        "original_text": "Two new basaltic asteroids, 2020 BP59 and 2023 BP33, have been discovered in the near-Earth asteroid (3547) entity group. These two new asteroids are the largest members of this group and have estimated diameters of 44 and 40 km, respectively. The new asteroids are both about 3.5 x 2.0 km in size and have estimated masses of approximately 7.7 x 10^{19} and 1.1 x 10^{20} kg, respectively. These make them among the most massive bodies in the main-belt. We determine their orbits using multi-integrations and based on osculating orbit analysis. Our dynamical results show that both asteroids have low perihelion distances of 0.96 and 0.88 au, respectively. The large size of these two asteroids and their low perihelion distances suggest that they may have significant amounts of heat energy due to collisions in their early histories. We determine their orbital evolution based on chaotic asteroid dynamics. Both asteroids are subject to significant collisonal hazards over the next 200 Myrs.",
        "watermark_text": "Two newest basaltic asteroids , 2020 BP59 and 2023 BP33 , have been detected in the near - Earth asteroid ( 3547 ) entity group . These two new asteroids are the greatest members of this class and have predicted diameters of 44 and 40 km , respectively .The new asteroids are both about 3 . 5 x 2 . 0 km in length and have predicted masses of approximately 7 . 7 x 10 ^ { 19 } and 1 . 1 x 10 ^ { 20 } kilograms , respectively . These make them among the most large bodies in the main - belt .We determine their planets using multi - integrations and based on osculating orbit analysis . Our dynamical results show that both asteroids have lowest perihelion distances of 0 . 96 and 0 . 88 au , respectively .The large size of these two asteroids and their low perihelion distances suggest that they may have considerable amounts of thermal energy due to collisions in their early histories . We determine their orbital evolution using on turbulent asteroid behavior .Both asteroids are subject to significant collisonal hazards over the subsequent 200 Myrs .",
        "rewrite_text": "The two latest basaltic asteroids, 2020 BP59 and 2023 BP33, have been discovered within the (3547) near-Earth asteroid group. These two remarkable asteroids are the largest members of their class, with estimated diameters of 44 kilometers and 40 kilometers, respectively. Both asteroids measure approximately 3.5 by 2.0 kilometers in length and are estimated to have masses of approximately 7.7 x 10^19 kilograms and 1.1 x 10^20 kilograms, respectively. This makes them one of the largest bodies in the main belt.\n\nWe determine their planetary status through multi-integration techniques and based on osculating orbit analysis. Our dynamic studies reveal that both asteroids possess minimal perihelion distances of 0.96 and 0.88 astronomical units (AU), respectively. The large size of these asteroids and their close proximity to the perihelion suggest that they may have accumulated significant amounts of thermal energy due to collisions in their early history. We assess their orbital evolution by considering the turbulent behavior of asteroids. Both asteroids are at a significant risk of collisions over the next 200 million years.",
        "ori-fast-z-score": -0.6108472217815261,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": -0.35603449745815596
    },
    {
        "original_text": "On the 150th anniversary of the birth of theoretical physicistClerk Maxwell, a novel form of light-based quantum simulation, known as optical lattices, was recently proposed1,2. By arranging laser beams in a specific pattern and tuning their relative intensities, it is possible to simulate magnetic domains, quasiparticles, and even short-ranged interactions in a system of ultra-cold atoms trapped in this optical potential3,4,5. In this way, quantum simulations, which traditionally require near-perfect isolation from external disturbances, can be performed in lattices which are themselves capable of sensing and responding to perturbations. Here, we demonstrate this principle by simulating the fractional quantum Hall effect in an optical lattice. In this system, atoms are loaded into a two-dimensional square lattice and subjected to an artificial magnetic field. For appropriate choices of the lattice parameters, the system exhibits a variety of quantum Hall states, including the highly-counterintuitive fractionally-charged quasiparticles with charge e/4. We characterize these phases by measuring the real-space structure of the collective excitations and by measuring the quasiparticle s dynamical structure factor. Finally, we show that it is possible to move from the fractionally-quantum-Hall to the trivial-insulating phases by varying a single lattice parameter. This result establishes a clear link between quantum simulations and condensed matter physics, and provides a powerful method for exploring a broad range of novel quantum phenomena.",
        "watermark_text": "On the 150th anniversary of the birth of theoretical physicistClerk Maxwell , a new form of light - based quantum modeling , known as laser lattices , was recently proposed1 , 2 . By arranging laser beams in a certain pattern and adjusting their relative intensities , it is easy to simulate magnetic zones , quasiparticles , and even long - ranged interactions in a system of ultra - cold molecules trapped in this magnetic potential3 , 4 , 5 .In this way , quantum simulations , which normally use near - perfect isolation from external disturbances , can be performed in lattices which are themselves capable of sensing and responding to perturbations . Here , we prove this theorem by simulating the fractional quantum Hall phenomenon in an optical lattice .In this scheme , atoms are packaged into a two - dimensional square lattice and exposed to an artificial magnetic force . For suitable choices of the lattice parameters , the system displays a variety of quantum Hall states , notably the strongly - counterintuitive fractionally - charged quasiparticles with charge e / 4 .We characterize these stages by monitoring the real - space shape of the collective excitations and by monitoring the quasiparticle s dynamical formation factor . Finally , we prove that it is easy to move from the fractionally - quantum - Hall to the trivial - insulating phases by varying a single lattice parameter .This result establishes a clear link between particle simulations and condensed matter science , and provides a powerful method for studying a broad variety of new quantum concepts .",
        "rewrite_text": "On the 150th milestone of Clerk Maxwell's theoretical physicist birth, a novel type of quantum modeling based on light, referred to as laser lattices, has recently been introduced1, 2. By arranging laser beams in a specific pattern and adjusting their relative intensities, it becomes effortless to mimic magnetic zones, quasiparticles, and even long-range interactions within a system of extremely cold molecules trapped within this magnetic potential3, 4, 5. This allows quantum simulations, which usually rely on near-perfect isolation from external disturbances, to be conducted in lattices that can sense and respond to perturbations.\n\nHerein, we substantiate this theory by simulating the fractional quantum Hall effect within an optical lattice. In this approach, atoms are organized into a two-dimensional square lattice and subjected to an artificial magnetic force. With suitable adjustments to the lattice parameters, the system demonstrates a range of quantum Hall states, notably the counterintuitive fractionally charged quasiparticles with a charge of e/4. We characterize these stages by monitoring the spatial shape of collective excitations and the dynamic formation factor of quasiparticles.\n\nFurthermore, we demonstrate the ease of transitioning from the fractional quantum Hall phase to the trivial insulating phase by modifying a single lattice parameter. This result establishes a strong connection between particle simulations and condensed matter science, providing a versatile method for exploring a diverse array of new quantum concepts.",
        "ori-fast-z-score": -0.2873478855663454,
        "water-fast-z-score": 7.250523667842477,
        "rewrite-fast-z-score": 3.2515866179421673
    },
    {
        "original_text": "In this work we present a general study of the energetic and dynamical behavior of a reduced version of the fully non-linear Kolmogorov-Lorenz equations, often referred to as the Lorenz-Kolmogorov equations. These equations, which describe the spatio-temporal evolution of a dynamical variable representing the areal density of particles in a fluid, are used to model a vast spectrum of physical, biological and chemical systems. As in classical Lorenz models, the dynamical variables in the Kolmogorov-Lorenz system consist of three state variables which denote the magnitude of velocity, density and temperature fluctuation, respectively. Contrary to the classical system, however, these state variables are not normalized to a difference of unity, but are related via an energy function which depends on the total energy of the dynamical system. We explore the parameter space of the model and identify regions where the system undergoes transitions between different dynamical states as well as exhibits chaos. We then perform a similar energetic analysis to the one recently proposed by Rudiger et. al.  Rudiger et al., Phys. Rev. Lett., 2015  for the classical Lorenz equations, and show that, despite the apparent difference in the dynamical behaviour of the two systems, the underlying structure of the two analyses is quite similar. We also explore the possibility of using linear and non-linear methods, typically employed in statistical mechanics to predict the long-term dynamics of the system. We show that both methods yield precise predictions in a large fraction of the dynamical regions explored, indicating that these systems may be used to generate long term predictions.",
        "watermark_text": "In this research we present a general study of the energetic and dynamical behavior of a reduced form of the fully non - linear Kolmogorov - Lorenz equations , sometimes termed to as the Lorenz - Kolmogorov equations . These equations , which describe the spatio - temporal evolution of a dynamical variable describing the areal density of molecules in a fluid , are applied to model a vast array of biological , biological and biological systems .As in standard Lorenz models , the dynamical variables in the Kolmogorov - Lorenz system consist of three state variables which denote the magnitude of velocity , density and heat fluctuation , respectively . Contrary to the classical system , however , these state variables are not normalized to a difference of unity , but are related via an energy function which depends on the total energy of the dynamical system .We explore the parameter space of the model and locate regions where the system undergoes cycles between various dynamical states as well as shows chaos . We then undergo a similar energetic analysis to the one newly proposed by Rudiger et .al . Rudiger et al . , Phys .Rev . Lett . , 2015 for the classical Lorenz equations , and show that , despite the actual contrast in the dynamical behaviour of the two systems , the fundamental structure of the two analyses is fairly related .We additionally discuss the prospect of using linear and non - linear techniques , generally employed in mathematical mechanics to predict the long - term behavior of the system . We see that both approaches yield exact predictions in a large fraction of the dynamical regions investigated , showing that these systems may be used to create long term predictions .",
        "rewrite_text": "In this research, we present an extensive study examining the energetic and dynamic behavior of a simplified version of the fully non-linear Kolmogorov-Lorenz equations, sometimes referred to as the Lorenz-Kolmogorov equations. These equations describe the spatio-temporal evolution of a dynamic variable that represents the areal density of molecules in a fluid, and they are applied to model a diverse range of biological, environmental, and social systems.\n\nAs in standard Lorenz models, the dynamic variables in the Kolmogorov-Lorenz system consist of three state variables: velocity magnitude, density, and heat fluctuation. However, in contrast to the classical system, these state variables are not standardized to a unity difference. Instead, they are linked through an energy function that relies on the total energy of the dynamic system.\n\nWe delve into the parameter space of the model, identifying regions where the system cycles through various dynamic states and exhibits chaotic behavior. We then conduct a similar energetic analysis to the one recently proposed by Rudiger et al. in their 2015 Phys. Rev. Lett. study on the classical Lorenz equations. Although there are distinct differences in the dynamic behavior of the two systems, we find that the fundamental structures of the two analyses are closely related.\n\nFurthermore, we discuss the potential application of linear and nonlinear techniques, which are commonly used in mathematical mechanics to predict the long-term behavior of systems. We observe that both approaches provide accurate predictions in a significant portion of the investigated dynamic regions, indicating that these systems can be utilized for long-term predictions.",
        "ori-fast-z-score": -0.7126966450997984,
        "water-fast-z-score": 6.236095644623235,
        "rewrite-fast-z-score": 0.9760921603577252
    },
    {
        "original_text": "In recent years, several analyses using high-energy gamma-ray and neutrino telescopes have been performed to search for cosmic ray accelerators, which are candidates of sources of high-energy cosmic rays. None of these analyses has found significant evidence of these accelerators, and thus an unambiguous discovery of these objects remains elusive. However, uncertainties in the cosmic ray propagation models lead to the possibility that some of these gamma-ray and neutrino signals could be produced by hadrons accelerated in a hypothetical cosmic ray proton accelerators (pevatrons), but not detectable with current instruments due to their large distance from Earth. In this work, we present a search for these pevatrons using several analyses of high-energy gamma-ray and neutrino data. No significant excess was found and thus we put upper limits on the flux of these pevatrons. *Reference: https://arxiv.org/abs/1811.11439",
        "watermark_text": "In past times , various surveys utilizing large - energy gamma - ray and neutrino telescopes have been performed to search for gamma ray accelerators , which are candidates of sources of high - energy cosmic rays . None of these experiments has found significant evidence of these accelerators , and therefore an unambiguous find of these objects remains elusive .However , uncertainties in the cosmic ray transmission theories lead to the prospect that some of these gamma - ray and neutrino messages might be made by hadrons driven in a hypothetical cosmic ray proton accelerators ( pevatrons ) , but not detectable with current instruments due to their large distance from Earth . In this research , we present a search for these pevatrons using numerous calculations of high - energy gamma - ray and neutrino data .No considerable surplus was obtained and therefore we put upper limits on the flux of these pevatrons . * Reference : https : / / arxiv . org / abs / 1811 . 11439",
        "rewrite_text": "In the past, numerous surveys have been conducted utilizing high-energy gamma-ray and neutrino telescopes to search for gamma ray accelerators, which are potential sources of high-energy cosmic rays. However, none of these experiments have yielded definitive evidence of these accelerators, making their unambiguous discovery elusive. Nevertheless, uncertainties in cosmic ray propagation theories suggest that some of these gamma-ray and neutrino messages may be generated by hadrons driven in hypothetical cosmic ray proton accelerators (pevatrons). However, these messages are currently undetectable due to their great distance from Earth.\n\nIn this research, we conduct a search for these pevatrons through various calculations involving high-energy gamma-ray and neutrino data. Unfortunately, no significant excess was found, leading us to set upper limits on the flux of these pevatrons. For more information, please refer to the reference: https://arxiv.org/abs/1811.11439.",
        "ori-fast-z-score": 0.36650833306891567,
        "water-fast-z-score": 5.662208585049306,
        "rewrite-fast-z-score": 0.3611575592573076
    },
    {
        "original_text": "The Small Magellanic Cloud (SMC) is a satellite galaxy of the Milky Way. It is of particular interest for studies of chemical evolution, as it has lower metallicity than the Large Magellanic Cloud. Planetary nebulae (PNe) are old stars whose atmospheres have been expelled by strong stellar winds. Their elemental abundances can be used to study the chemical evolution of a galaxy. However, very few PNe have been found in the SMC. Here we present optical spectroscopic observations of 19 PNe in the SMC. We measure their radial velocities and chemical compositions. We find that the distribution of nitrogen and the period of oxygen isotopes are similar to those in the Large Magellanic Cloud, while those of sulphur and argon are different. The similarity between the Large and Small Magellanic Clouds suggests that the SMC experienced similar evolution with the LMC. However, the measured differences imply that the SMC had a different formation history from the LMC.",
        "watermark_text": "The Small Magellanic Cloud ( SMC ) is a satellite galaxy of the Milky Way . It is of especially interest for research of chemical evolution , as it has reduced metallicity than the Large Magellanic Cloud .Planetary nebulae ( PNe ) are old galaxies whose atmospheres have been expelled by intense stellar winds . Their elemental abundances can be used to study the chemical evolution of a galaxy .However , very few PNe have been seen in the SMC . Here we present optical spectroscopic observations of 19 PNe in the SMC .We estimate their radial velocities and chemical compositions . We see that the distribution of nitrogen and the period of nitrogen isotopes are comparable to those in the Large Magellanic Cloud , while those of sulphur and argon are different .The similarity between the Large and Small Magellanic Clouds indicates that the SMC experienced similar development with the LMC . However , the reported differences imply that the SMC had a distinct formation history from the LMC .",
        "rewrite_text": "The Small Magellanic Cloud (SMC) is an accompanying galaxy to the Milky Way. It holds particular significance for studying chemical evolution research due to its lower metallicity compared to the Large Magellanic Cloud (LMC). Planetary nebulae (PNe), which are old galaxies with expelled atmospheres by intense stellar winds, offer a means to explore a galaxy's chemical development through their elemental abundance. Nevertheless, a mere handful of PNe have been detected in the SMC. In this study, we present optical spectroscopic observations of 19 PNe within the SMC, estimating their radial velocities and chemical compositions. Our findings reveal that the distribution of nitrogen and the periodicity of nitrogen isotopes in the SMC align closely with those in the LMC. However, there are discrepancies in the distribution of sulfur and argon. The similarity observed between the LMC and SMC suggests they may have experienced similar developmental paths. However, the reported differences suggest that the SMC may have had a distinct formation history compared to the LMC.",
        "ori-fast-z-score": -1.1547005383792517,
        "water-fast-z-score": 3.857142857142857,
        "rewrite-fast-z-score": 0.7745966692414834
    },
    {
        "original_text": "We present the third and final paper in a series studying the Homunculus on the Eta Carinae star. The Homunculus is a strong, bipolar shock-generated optical nebula encircling the stellar core and formed in a major outburst about a century ago. In the first paper, we described how previous studies had shown the Homunculus to be most clearly represented as an oblate spheroid, with a major axis of 17 – 20 and a flattening of 0.33 – 0.4. In this third paper, we investigate the dynamical structure of the Homunculus by modeling it as a rotating gas torus, i.e. a donut. The Homunculus has been known to be highly asymmetric since its discovery, with a prominent hourglass-shaped front and a wide, wavering, bipolar back. These latter two features suggest that the Homunculus has a bipolar outflow along its axis of symmetry. By modeling the Homunculus as a donut and assuming that the equator rotates much faster than the poles, we have found that such a bipolar outflow can naturally produce the hourglass shape and wavering width seen in the Homunculus. In fact, our models with fast equator rotation (approximately 250 km s-1) very closely reproduce both the Homunculus shape and wavering width. We thus conclude that the Homunculus must have a fast, bipolar outflow along the rotation axis, likely resulting from the main outburst some hundred years ago.",
        "watermark_text": "We present the third and final article in a trilogy studying the Homunculus on the Eta Carinae star . The Homunculus is a powerful , bipolar shock - produced optical nebula encircling the stellar core and developed in a major outburst about a century ago .In the first paper , we explained how previous research had demonstrated the Homunculus to be most distinctly represented as an oblate spheroid , with a major axis of 17 – 20 and a flattening of 0 . 33 – 0 . 4 . In this next article , we investigate the dynamical dynamics of the Homunculus by modeling it as a rotating gas torus , i . e .a donut . The Homunculus has been known to be highly asymmetric since its observation , with a large hourglass - shaped front and a broad , wavering , bipolar head .These former two characteristics suggest that the Homunculus has a bipolar outflow along its axis of symmetry . By analyzing the Homunculus as a donut and assuming that the equator rotates much faster than the poles , we have discovered that such a bipolar outflow can naturally produce the hourglass shape and wavering width shown in the Homunculus .In fact , our measurements with fast equator rotation ( approximately 250 km s - 1 ) very closely reproduce both the Homunculus position and wavering width . We consequently conclude that the Homunculus must have a rapid , bipolar outflow along the rotation axis , likely occurring from the main outburst some hundred years previously .",
        "rewrite_text": "We present the final article of a trilogy exploring the Eta Carinae star's Homunculus. The Homunculus is a powerful, bipolar optical nebula that surrounds the stellar core and emerged in a significant outburst approximately a century ago.\n\nIn our first paper, we detailed how previous research had established the Homunculus's most distinctive representation as an oblate spheroid, with a major axis ranging from 17 to 20 and a flattening of 0.33 to 0.4. In this subsequent article, we delve into the dynamic behavior of the Homunculus by modeling it as a rotating gas torus, resembling a donut.\n\nKnown for its high asymmetry since observation, the Homunculus displays a large hourglass-shaped front and a broad, wavering, bipolar head. These characteristics suggest that the Homunculus exhibits a bipolar outflow along its axis of symmetry. By analyzing the Homunculus as a donut and assuming that the equator rotates much faster than the poles, we have discovered that this bipolar outflow can naturally produce the hourglass shape and wavering width observed in the Homunculus.\n\nIndeed, our measurements with a fast equator rotation speed of approximately 250 km/s closely replicate both the Homunculus's position and wavering width. Consequently, we conclude that the Homunculus must possess a rapid, bipolar outflow along its rotational axis, likely resulting from the primary outburst hundreds of years ago.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 5.521576303742327,
        "rewrite-fast-z-score": 1.7888543819998317
    },
    {
        "original_text": "This paper studies an optimal investment problem with an unbounded random endowment and utility-based pricing. The investor’s objective is to maximize the expected utility from terminal wealth under general semimartingale dynamics. Since the underlying random endowment is unbounded, the classical notions of absolute and relative performance become inapplicable. We adopt an approach by coupling the investment and the underlying dynamics to define an a.s. supermartingale. Then we construct a sequence of admissible strategies with the patience to wait for the coupling event to occur. The corresponding optimal value is obtained by a dynamic programming principle. We study two versions of the problem: one with restricted trading and the other without any restrictions. We characterize the value function for the restricted case and show the value function is equal to that for the original problem when the patience times of the two strategies are the same. We also provide an approximation method via refining the approximation spaces of strategies. Our approach applies to a much broader family of utility functions and allows for a much wider range of applications. We provide several examples to illustrate our results. Keywords: Semimartingale, Performance measurement, Relative performance, unbounded random endowment, Utility-based pricing, Dynamic programming principle Journal: Mathematical Finance, to appear Link to paper: https://arxiv.org/pdf/1710.03276.pdf",
        "watermark_text": "This paper studies an efficient investment problem with an unbounded random endowment and utility - based pricing . The investor ’ s objective is to maximize the expected utility from terminal wealth under general semimartingale dynamics .Since the underlying random endowment is unbounded , the classical theories of relative and relative performance become inapplicable . We take an approach by coupling the investment and the underlying dynamics to define an a . s . supermartingale .Then we create a sequence of admissible strategies with the patience to wait for the coupling incident to occur . The corresponding ideal value is found by a dynamic programming concept .We consider two variants of the issue : one with restricted trading and the other without any restrictions . We characterize the value function for the restricted case and find the value function is equal to that for the original problem when the patience times of the two strategies are the same .We additionally offer an approximation algorithm via refining the approximation spaces of strategies . Our solution applies to a far larger family of utility functions and allows for a far larger range of applications .We provide several examples to illustrate our findings . Keywords : Semimartingale , Performance measurement , Relative efficiency , unbounded random endowment , Utility - based pricing , Dynamic programming theory Journal : Mathematical Finance , to appear Link to publication : https : / / arxiv . org / pdf / 1710 . 03276 . pdf",
        "rewrite_text": "This study explores an efficient investment problem, which involves an unbounded random endowment and utility-based pricing. The main objective of the investor is to maximize the expected utility derived from terminal wealth under the general semimartingale dynamics. Since the underlying random endowment is unrestricted, classical relative performance and relative value theories become irrelevant.\n\nTo address this, we couple the investment with the underlying dynamics to define an almost sure supermartingale. We then establish a sequence of viable strategies, patiently waiting for the coupling to occur. The corresponding optimal value is determined using dynamic programming concepts.\n\nWe consider two variations of this problem: one with trading restrictions and the other without any constraints. We characterize the value function for the restricted case and find that it is equivalent to the value function for the original problem when the patience periods of both strategies are identical.\n\nFurthermore, we provide an approximation algorithm that refines the strategy approximation spaces. Our solution is applicable to a wider range of utility functions, allowing for a broader spectrum of applications. We illustrate our findings with several examples.\n\nKeywords: Semimartingale, Performance Measurement, Relative Efficiency, Unbounded Random Endowment, Utility-Based Pricing, Dynamic Programming Theory.\n\nJournal: Mathematical Finance (to be published). Publication Link: https://arxiv.org/pdf/1710.03276.pdf",
        "ori-fast-z-score": 1.0634101379502299,
        "water-fast-z-score": 5.510397987560282,
        "rewrite-fast-z-score": 1.3862065601673441
    },
    {
        "original_text": "Modern physics abandons the concepts of absolute, including that of space and time. However, in the field of statistical mechanics, the most important theory of classical physics, it makes sense to define an advanced potential (A-P) to consider only the variations that occur in the system. Therefore, a system with A-P is anomalous. This article presents the most important concepts of statistical mechanics, the definition of A-Ps, applications, and some physical experiments that can be used to verify this theory. ytics Modern physics abandons the concepts of absolute, including that of space and time. However, in the field of statistical mechanics, the most important theory of classical physics, it makes sense to define an advanced potential (A-P) to consider only the variations that occur in the system. Therefore, a system with A-P is anomalous. This article presents the most important concepts of statistical mechanics, the definition of A-Ps, applications, and some physical experiments that can be used to verify this theory.",
        "watermark_text": "Modern physics abandons the concepts of relative , notably that of space and period . However , in the field of statistical mechanics , the most important theory of classical physics , it makes sense to define an advanced potential ( A - P ) to consider only the variations that occur in the system .Therefore , a system with A - P is anomalous . This paragraph presents the most important concepts of statistical mechanics , the notion of A - Ps , applications , and some physical observations that can be used to confirm this theory .ytics Modern physics abandons the concepts of relative , notably that of space and time . However , in the field of statistical mechanics , the most important theory of classical physics , it makes sense to define an advanced potential ( A - P ) to consider only the variations that occur in the system .Therefore , a system with A - P is anomalous . This paragraph presents the most important concepts of statistical mechanics , the notion of A - Ps , applications , and some physical observations that can be used to confirm this theory .",
        "rewrite_text": "In modern physics, the ideas of relativity, particularly those related to space and time, have been discarded. Nevertheless, within the realm of statistical mechanics, the foremost theory of classical physics, it becomes pertinent to define an advanced potential (A-P) that solely considers systemic variations. Consequently, a system characterized by A-P is considered abnormal. This paragraph elucidates the fundamental concepts of statistical mechanics, introduces the notion of A-Ps, discusses their applications, and highlights physical observations that can serve as evidence to validate this theory.",
        "ori-fast-z-score": 2.3626845919446504,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": 1.9379255804998177
    },
    {
        "original_text": "Using data from the fourth data release of the Sloan Digital Sky Survey (SDSS-DR4), we study the X-ray properties of a complete sample of the most-luminous quasars at ~z>4, drawn from the luminous quasar (LQ) catalog ofWarner et al. (2006). Compared to the local luminous quasars, these highest-redshift objects have extremely high space density, and therefore are excellent tools with which to investigate the growth of supermassive black holes and their host bulges and galaxies. We find that the highest-redshift quasars have similar X-ray properties to those at low redshift, but with broad EWs and X-ray luminosities that are a factor of ~10 higher. This emission is most plausibly interpreted as thermal coronal gas in the host galaxies with broad emission lines. When compared to a radio-loud control sample matched in X-ray luminosity, the quasars have significantly enhanced radio emission, although not as much as predicted by models in which the powerful radiation emitted by the quasar promotes gas into a hot, kpc-scale torus. The small difference in radio emission may indicate that, at high redshift, quasars have less obscured nuclei than locally. We suggest that this may be due to the brighter cosmic background at high redshift, which can penetrate more easily through a putative torus. In addition to its intrinsic interest, this paper offers a cautionary example of how the analysis of a single flux-limited sample can lead to erroneous conclusions when applied to high-redshift populations.",
        "watermark_text": "Using results from the fourth information report of the Sloan Digital Sky Survey ( SDSS - DR4 ) , we study the X - ray characteristics of a complete sample of the most - luminous quasars at ~ z > 4 , chosen from the luminous quasar ( LQ ) catalog ofWarner et al . ( 2006 ) .Compared to the local luminous quasars , these highest - redshift bodies have extremely greater space density , and therefore are excellent tools with which to examine the development of supermassive black holes and their host bulges and galaxies . We see that the highest - redshift quasars have similar X - ray characteristics to those at low redshift , but with broad EWs and X - ray luminosities that are a factor of ~ 10 greater .This emission is most plausibly interpreted as heat coronal gas in the host galaxies with broad absorption lines . When compared to a radio - loud control sample matched in X - ray luminosity , the quasars have substantially enhanced radio emission , although not as much as predicted by scenarios in which the powerful light emitted by the quasar encourages gas into a cool , kpc - scale torus .The tiny change in radio emission may indicate that , at high redshift , quasars have less illuminated clusters than locally . We suggest that this might be due to the brighter cosmic background at high redshift , which can penetrate more easily through a putative torus .In addition to its intrinsic interest , this paper offers a cautionary instance of how the evaluation of a single flux - limited sample can lead to erroneous conclusions when applied to large - redshift areas .",
        "rewrite_text": "Using data from the fourth information report of the Sloan Digital Sky Survey (SDSS-DR4), we conduct a study on the X-ray features of a comprehensive sample of the most luminous quasars at approximately z > 4, selected from the Luminous Quasar (LQ) catalog by Warner et al. (2006). In contrast to local luminous quasars, these high-redshift objects exhibit a significantly higher spatial density, making them invaluable tools for investigating the development of supermassive black holes and their host galaxies and bulges. Our observations reveal that high-redshift quasars share similar X-ray characteristics to those at low redshifts, but exhibit broader equivalent widths and X-ray luminosities that are approximately 10 times greater. This emission is most plausibly interpreted as heat coronal gas in the host galaxies with broad absorption lines.\n\nIn comparison to a radio-loud control sample matched in X-ray luminosity, the quasars exhibit significantly enhanced radio emission, although not as much as predicted by scenarios where the quasar's powerful light stimulates the gas into a cool, kpc-scale torus. The slight variation in radio emission may suggest that quasars at high redshift have less illuminated clusters compared to locally. We propose that this could be attributed to the brighter cosmic background at high redshift, which can more easily penetrate a putative torus.\n\nMoreover, this paper provides a cautionary example of how the evaluation of a single flux-limited sample can lead to incorrect conclusions when applied to large redshift areas, highlighting the importance of careful analysis and consideration of multiple factors.",
        "ori-fast-z-score": 1.9611613513818404,
        "water-fast-z-score": 6.928890517934586,
        "rewrite-fast-z-score": 3.866413395172958
    },
    {
        "original_text": "Dalitz plot analysis of the D+ to K-pi+pi+ decay in the FOCUS experiment FOCUS is a charm spectroscopy experiment at the Fermilab B-factories Europpia Collab., paper 519, (2002); Ivashin Yu. A. et al, paper 589, (2002). Physics motivation The D+ to K-pi+pi+ decay is a Cabibbo favored weak decays which plays a key role in the understanding of the quark dynamics. It was not observed yet, and according to the current theory the Dalitz plot analysis might allow to distinguish among different models for the D decay. Experiment The FOCUS detector is a small cylindrical piece of apparatus, located inside the chain of the Reserve cyclotrons at Fermilab, and used to perform a Dalitz plot analysis of the D+ to K-pi+pi+ decay. The results presented in this note were obtained in the 2002 data taking period.",
        "watermark_text": "Dalitz plot analysis of the D + to K - pi + pi + decay in the FOCUS experiment FOCUS is a charm spectroscopy project at the Fermilab B - factories Europpia Collab . , paper 519 , ( 2002 ) ; Ivashin Yu . A . et al , paper 589 , ( 2002 ) .Physics reason The D + to K - pi + pi + decay is a Cabibbo favored weak decays which plays a key importance in the knowledge of the quark dynamics . It was not observed yet , and according to the present theory the Dalitz plot analysis might enable to distinguish among different models for the D decay .Experiment The FOCUS detector is a small cylindrical item of equipment , located inside the chain of the Reserve cyclotrons at Fermilab , and utilized to conduct a Dalitz plot analysis of the D + to K - pi + pi + decay . The results presented in this note were obtained in the 2002 data taking year .",
        "rewrite_text": "The English rewrite of the text goes as follows:\n\nIn the FOCUS experiment, a Dalitz plot analysis was conducted on the D+ to K- pi+ pi+ decay. FOCUS, a charm spectroscopy project at the Fermilab B-factories in collaboration with Europpia Collab, has previously published papers on this subject, specifically paper 519 in 2002 and paper 589 in 2002 by Ivashin Yu.A. et al.\n\nThe reason for this analysis is that the D+ to K- pi+ pi+ decay is a Cabibbo-favored weak decay, which holds significant importance in understanding quark dynamics. This particular decay has yet to be observed, and according to current theory, the Dalitz plot analysis may help distinguish between various models of D decay.\n\nIn the experiment, the FOCUS detector, a small cylindrical piece of equipment, was situated within the chain of Reserve cyclotrons at Fermilab. It was utilized to perform the aforementioned Dalitz plot analysis on the D+ to K- pi+ pi+ decay. The results presented in this note were obtained during the data collection year of 2002.",
        "ori-fast-z-score": 2.4961508830135313,
        "water-fast-z-score": 5.357061993998872,
        "rewrite-fast-z-score": 1.5882027766319677
    },
    {
        "original_text": "We present an analytical model for the self-similar scaling relations between the Sunyaev-Zel dovich (SZ) effect signal and the physical properties of the astrophysical systems they are derived from. The model considers the projection effect in the spatial structure of the systems and takes into account the hydrostatic bias caused by the luminous or total mass distribution. We apply our model to interpret the SZ profiles obtained with the Atacama Cosmology Telescope (ACT) and show that the ACT-derived SZ surface brightness profiles can be well fitted by the self-similar form if certain scaling relations are imposed. We also show that there are systematics in the SZ measurements due to the limited signal-to-noise ratio (SNR) and beam-smoothing effect and propose an empirical method to reduce these systematics. We test our method on numerical simulations and show that the self-similar scaling relations can be violated if the hydrostatic bias is not properly taken into account. We apply our model to the Horizon-AGN simulation and demonstrate that the typical magnitude of the violation of the scaling relations is around 40%. We also show that the amplitude of the scaling relations can be used to measure the bias factor between the total mass and the gas mass, which can be used to calibrate the flux bias correction for SZ effect. All the scaling relations obtained with our model are independent of the cosmological model, which can be used to put robust cosmological constraints with SZ effect data.",
        "watermark_text": "We present an analytical model for the self - similar scaling relations between the Sunyaev - Zel dovich ( SZ ) effect signal and the physical properties of the astrophysical systems they are derived from . The model considers the projection impact in the spatial composition of the systems and takes into consideration the hydrostatic bias created by the luminous or total mass distribution .We use our model to analyze the SZ profiles obtained with the Atacama Cosmology Telescope ( ACT ) and find that the ACT - derived SZ surface brightness profiles can be well fitted by the self - similar form if certain scaling relations are enforced . We additionally prove that there are systematics in the SZ measurements due to the limited signal - to - noise proportion ( SNR ) and beam - smoothing effect and suggest an empirical method to reduce these systematics .We test our technique on numerical simulations and suggest that the self - similar scaling relations can be violated if the hydrostatic bias is not adequately kept into consideration . We use our model to the Horizon - AGN simulation and suggest that the typical severity of the breach of the scaling relations is around 40 % .We additionally find that the amplitude of the scaling relations can be used to measure the bias factor between the total mass and the gas mass , which can be used to calibrate the flux bias correction for SZ phenomenon . All the scaling relations derived with our model are independent of the cosmological model , which can be used to put robust cosmological limitations with SZ phenomenon data .",
        "rewrite_text": "We have developed an analytical model to explore the self-similar scaling relationships between the Sunyaev-Zel'dovich (SZ) effect signal and the physical properties of the astrophysical systems from which they originate. This model accounts for the spatial composition projection effects and considers the hydrostatic bias created by the luminous or total mass distribution. Utilizing our model, we analyze SZ profiles obtained with the Atacama Cosmology Telescope (ACT). The results indicate that the ACT-derived SZ surface brightness profiles can be effectively fitted using self-similar forms when specific scaling relationships are applied.\n\nFurthermore, we demonstrate that there are systematics in SZ measurements due to the limited signal-to-noise ratio (SNR) and beam-smoothing effects. We propose an empirical method to mitigate these systematics. We have tested our technique on numerical simulations and found that the self-similar scaling relations may be violated if the hydrostatic bias is not properly considered. Specifically, we applied our model to the Horizon-AGN simulation and found that the typical magnitude of the scaling relation violation is approximately 40%.\n\nAdditionally, we discovered that the amplitude of the scaling relations can be utilized to measure the bias factor between total mass and gas mass. This measurement can be employed to calibrate flux bias corrections for the SZ phenomenon. Importantly, all scaling relations derived from our model are independent of the cosmological model, making them valuable for establishing robust cosmological constraints using SZ data.",
        "ori-fast-z-score": 0.5827715174143585,
        "water-fast-z-score": 6.021972346615038,
        "rewrite-fast-z-score": 2.1572774865200244
    },
    {
        "original_text": "The Milky Way (MW) accreted tens of satellite galaxies over its lifetime, some of which may have survived to the present day. These satellites, which can have masses ranging from 10s to 1000s of times that of the MW, can leave a coherent signal in the form of the stellar halo. Until now, such a signal has not been detected. Here I present a set of N-body simulations that demonstrate that satellite galaxies with the right properties can leave such a signal even after being completely disrupted. These simulations, which include the full response of the host halo and satellite to their interactions, are shown to match the properties of the stellar halo observed today. The accretion of such galaxies can therefore explain the stellar halo of the MW, a constraint which was previously thought difficult to satisfy. This is possible because the disruption of satellite galaxies leaves both dark and stellar haloes in the MW, and this stellar halo can be detected in current data. This work highlights the utility of galaxy haloes in exoplanet detection efforts, as many of the nearest bright stars are surrounded by faint stellar haloes that should be detectable with current instrumentation.",
        "watermark_text": "The Milky Way ( MW ) accreted tens of satellite galaxies over its duration , some of which may have lived to the present day . These satellites , which can have masses ranging from 10s to 1000s of twice that of the MW , can return a consistent signal in the form of the stellar halo .Until now , such a signal has not been detected . Here I offer a set of N - bodies simulations that demonstrate that satellite galaxies with the appropriate properties can return such a signal even after being totally disrupted .These simulations , which cover the full response of the host halo and satellite to their interactions , are shown to match the properties of the stellar halo observed nowadays . The accretion of such galaxies can thus explain the stellar halo of the MW , a constraint which was formerly thought impossible to meet .This is possible because the disruption of satellite galaxies leaves both dark and stellar haloes in the MW , and this stellar halo can be identified in current data . This research shows the utility of galaxy haloes in exoplanet detection programs , as much of the nearest bright stars are surrounded by distant stellar haloes that should be detectable with current imaging .",
        "rewrite_text": "The Milky Way (MW) has amassed numerous satellite galaxies throughout its existence, some of which may persist until the present day. These satellites, which can range in mass from tens to thousands of times greater than the MW, can produce a consistent signal in the form of a stellar halo. However, such a signal has yet to be detected. In this study, I present a set of N-body simulations that demonstrate that satellite galaxies with specific properties can generate this signal even after being completely disrupted. These simulations, which thoroughly explore the interactions between the host halo and satellite, are found to align with the observed properties of the current stellar halo. Therefore, the accretion of these galaxies can provide an explanation for the formation of the MW's stellar halo, a constraint previously deemed unmeetable. This becomes possible because the disruption of satellite galaxies leaves both dark and visible stellar haloes in the MW, which can be identified in current data. This research underscores the importance of galaxy haloes in exoplanet detection programs as many of the nearest bright stars are surrounded by distant stellar haloes that should be detectable with current imaging techniques.",
        "ori-fast-z-score": -1.462614271203831,
        "water-fast-z-score": 4.076197322920544,
        "rewrite-fast-z-score": 1.8888888888888888
    },
    {
        "original_text": "A damped Lyman alpha (DLA) absorber with near-infrared spectroscopy towards a bright quasar at z = 2.2625 is reported. The galaxy responsible for the DLA is coincident with a massive, fast-rotating galaxy at z = 3. It has a half-light radius of r_h = 6.3 kpc and a stellar mass of M* = 7.2 x 10 11 M⊙, making it one of the most massive galaxies at high redshift. The total dynamical mass within one effective radius is Mdyn = 1.8 x 10 11 M⊙, which is 9.5 x higher than the dark matter halo mass of Mhalo = 9.4 x 10 11 M⊙ derived from the galaxy formation simulations. It suggests that HOD modelling is required to explain the gravitational force in this galaxy. This is the most massive galaxy at z = 3 with direct evidence for the transition from the cosmic  dark ages  to the   Era of Galaxies.  By measuring the systemic redshift of the galaxy, we constrained the offset from the QSO to be 300 km/s, corresponding to a projected distance of 100 kpc at z = 3. In the frame of the coincident galaxy, the absorption is a single Gaussian with FWHM = 280 km/s, blueshifted by 120 km/s relative to the galaxy’s velocity. This large-scale kinematics indicates that the absorbing galaxy is likely a proto-galaxy caught in the process of forming a massive, fast-rotating galaxy at the peak of starburst and black hole growth. This work was performed on the 6.5m Baade telescope at the Magellan Clay telescope site in Las Campanas, Chile, as part of the Magellan DLA Pipeline execution team.",
        "watermark_text": "A damped Lyman alpha ( DLA ) absorber with close - infrared spectroscopy towards a bright quasar at z = 2 . 2625 is reported . The galaxy responsible for the DLA is coincident with a huge , fast - spinning galaxy at z = 3 .It has a half - light diameter of r _ h = 6 . 3 kpc and a stellar mass of M * = 7 . 2 x 10 11 [UNK] , making it one of the most large galaxies at high redshift . The total dynamical mass within one effective radius is Mdyn = 1 . 8 x 10 11 [UNK] , which is 9 . 5 x higher than the dark matter halo weight of Mhalo = 9 . 4 x 10 11 [UNK] derived from the galaxy formation simulations .It suggests that HOD modelling is required to explain the gravitational pressure in this galaxy . This is the most large galaxy at z = 3 with direct data for the shift from the cosmic black years to the Era of Galaxies .By measuring the systemic redshift of the galaxy , we constrained the offset from the QSO to be 300 kilometers / s , equivalent to a projected distance of 100 kpc at z = 3 . In the frame of the coincident galaxy , the absorption is a single Gaussian with FWHM = 280 km / s , blueshifted by 120 km / s relative to the galaxy ’ s speed .This big - scale kinematics suggest that the absorbing galaxy is probably a proto - galaxy caught in the process of creating a huge , fast - spinning universe at the height of starburst and dark hole growth . This project was done on the 6 . 5m Baade observatory at the Magellan Clay telescope location in Las Campanas , Chile , as part of the Magellan DLA Pipeline execution team .",
        "rewrite_text": "A report has been made on a damped Lyman alpha (DLA) absorber, utilizing close-infrared spectroscopy towards a bright quasar at a redshift of 2.2625. The galaxy responsible for the DLA coincides with a large, rapidly-spinning galaxy at a redshift of 3. It possesses a half-light diameter of r_h = 6.3 kpc and a stellar mass of M* = 7.2 x 10^11 solar masses, making it one of the largest galaxies at a high redshift. Within one effective radius, the total dynamical mass is Mdyn = 1.8 x 10^11 solar masses, which is 9.5 times greater than the dark matter halo weight derived from galaxy formation simulations (Mhalo = 9.4 x 10^11 solar masses). This suggests that HOD modeling is necessary to explain the gravitational pressure in this galaxy.\n\nThis is the largest galaxy at a redshift of 3 with direct data spanning the transition from the cosmic black ages to the era of galaxies. By measuring the systemic redshift of the galaxy, we have constrained the offset from the QSO to be 300 kilometers per second, equivalent to a projected distance of 100 kpc at a redshift of 3. In the context of the coincident galaxy, the absorption is represented by a single Gaussian with a FWHM of 280 kilometers per second, blueshifted by 120 kilometers per second relative to the galaxy's velocity.\n\nThis large-scale kinematic evidence suggests that the absorbing galaxy may be a proto-galaxy caught in the process of creating a vast, rapidly-spinning universe during the peak of starburst and dark hole growth. This project was conducted at the 6.5m Baade observatory located at the Magellan Clay telescope site in Las Campanas, Chile, as part of the Magellan DLA Pipeline execution team.",
        "ori-fast-z-score": -0.10050378152592121,
        "water-fast-z-score": 5.929723110029351,
        "rewrite-fast-z-score": 1.6858544608470492
    },
    {
        "original_text": "In this paper, we formulate the Hamiltonian cosmological dynamics in Friedmann universe using ADM decomposition. Since the total Hamiltonian is a constraint, we use the Abandoned Dirac procedure to introduce the time evolution for the phase space variables. The complete dynamics contains two secondary constraints, which can be used to eliminate two pairs of the canonical variables. After this process, the resulting system is described by one pair of the canonical variables and the symplectic two-form is completely defined. The quantization of the system is straightforward by using the correspondence between the classical and quantum variables. The physical inner product can be defined by two methods: one is using the conventional Schrödinger representation, and another is using the Berezin integral. In the former case, the Wheeler DeWitt equation is a Schrödinger-like equation. In the later case, the Wheeler DeWitt equation can be obtained by taking the normalization factor of the Berezin integral.",
        "watermark_text": "In this paper , we formulate the Hamiltonian cosmological dynamics in Friedmann universe use ADM decomposition . Since the total Hamiltonian is a constraint , we utilize the Abandoned Dirac technique to introduce the period evolution for the phase space factors .The complete dynamics comprises two secondary constraints , which can be used to eliminate two pairs of the canonical variables . After this process , the resulting system is characterized by one pair of the canonical variables and the symplectic two - form is completely characterized .The quantization of the system is easy by using the correspondence between the classical and quantum variables . The physical inner product can be written by two means : one is taking the standard Schrödinger expression , and another is using the Berezin integral .In the former situation , the Wheeler DeWitt equation is a Schrödinger - like equation . In the later case , the Wheeler DeWitt equation can be obtained by take the normalization factor of the Berezin integral .",
        "rewrite_text": "In this study, we formulate the Hamiltonian cosmology dynamics within the Friedmann universe by employing the ADM decomposition technique. Since the total Hamiltonian acts as a constraint, we employ the abandoned Dirac method to introduce temporal evolution for the phase space components. The comprehensive dynamics encompasses two secondary constraints, which can effectively eliminate two pairs of canonical variables. Following this process, the resulting system is characterized by a single pair of canonical variables and is fully defined by the symplectic two-form. Quantifying the system is straightforward, utilizing the correspondence between classical and quantum variables. The physical inner product can be expressed in two ways: one through the standard Schrödinger expression, and the other utilizing the Berezin integral. In the first scenario, the Wheeler-DeWitt equation resembles a Schrödinger-type equation. In the latter case, the Wheeler-DeWitt equation can be derived by utilizing the normalization factor of the Berezin integral.",
        "ori-fast-z-score": 0.254000254000381,
        "water-fast-z-score": 4.572004572006858,
        "rewrite-fast-z-score": 1.4552137502179978
    },
    {
        "original_text": "A large fraction of quasars have broad absorption line (BAL) troughs in their spectra, which are outflows of gas along the line of sight to the quasar. The BAL quasars (BALQSOs) are excellent tools to study the large scale structures of the Universe due to their high space density and narrow absorption lines. 2MASS has recently revealed a previously unknown population of very red (J-K) BALQSOs. Using optical spectroscopy of a sample of 30 very red BALQSOs from the 2MASS catalog, we present evidence for a distinct class of 2MASS Very Red BALQSOs (2MASSVRBALQSOs). These sources have near-infrared colors of J-K>4.8, and are heavily obscured in the optical (A_V>10), but display extremely red 2MASS JHK colors, similar to those of normal quasars. The fraction of 2MASSVRBALQSOs among all BALQSOs is between 57% and 67% depending on how 2MASSVRBALQSOs are defined. The unprecedented availability of extremely red BALQSOs selected from the 2MASS database will enable tests of evolutionary models for BALQSOs and new insights on the physical mechanisms powering the large scale structures of the Universe.",
        "watermark_text": "A wide proportion of quasars have broad absorption line ( BAL ) troughs in their spectra , which are outflows of gas along the line of vision to the quasar . The BAL quasars ( BALQSOs ) are excellent tools to study the huge scale structures of the Universe thanks to their high space density and broad absorption lines .2MASS has recently disclosed a previously unidentified population of very red ( J - K ) BALQSOs . Using optical spectroscopy of a sample of 30 very red BALQSOs from the 2MASS catalog , we present evidence for a distinct class of 2MASS Very Red BALQSOs ( 2MASSVRBALQSOs ) .These sources have near - infrared colors of J - K > 4 . 8 , and are heavily distorted in the optical ( A _ V > 10 ) , but display incredibly red 2MASS JHK colors , comparable to those of normal quasars . The percentage of 2MASSVRBALQSOs among all BALQSOs is between 57 % and 67 % based on how 2MASSVRBALQSOs are specified .The extraordinary availability of highly red BALQSOs chosen from the 2MASS database will provide tests of evolutionary models for BALQSOs and new information on the structural mechanisms powering the huge scale structures of the Universe .",
        "rewrite_text": "A significant portion of quasars exhibit broad absorption lines (BALs) in their spectra, which are indicative of gas outflows along the line of sight to the quasar. These BAL quasars, or BALQSOs, serve as invaluable tools for studying the vast structures of the universe due to their high spatial density and broad absorption lines. Recently, 2MASS has uncovered a previously undiscovered population of extremely red (J-K color) BALQSOs. Through optical spectroscopy of a sample of 30 extremely red BALQSOs from the 2MASS catalog, we present evidence for a distinct class of 2MASS Very Red BALQSOs (2MASSVRBALQSOs). These sources possess near-infrared colors with J-K values exceeding 4.8 and are heavily distorted in the optical spectrum (with an average visual extinction greater than 10), yet they exhibit remarkably red 2MASS JHK colors comparable to those of typical quasars. The percentage of 2MASSVRBALQSOs within the overall population of BALQSOs ranges between 57% and 67%, depending on how 2MASSVRBALQSOs are defined. The exceptional availability of highly red BALQSOs selected from the 2MASS database will facilitate tests of evolutionary models for BALQSOs and provide new insights into the structural mechanisms driving the large-scale structures of the universe.",
        "ori-fast-z-score": 0.8307471607356973,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 1.7253243712550146
    },
    {
        "original_text": "Recent experimental advances in the field of Bose-Einstein condensates (BECs) enable observation of novel forms of quantum many-body phenomena. Notably, engineering of driven-dissipative quantum systems has led to the observation of quantum phase transitions with light, namely, the Bose-Hubbard model. Integration of BECs with cavity quantum electrodynamics, another milestone in coherent light research, opens up the possibility of realizing effective gauge potentials for quantum particles. Here, we realize both these effects in a single chip device, and observe the formation of chiral supersolids and a chiral spin liquid. We demonstrate that a cavity-BEC platform has many potential applications, ranging from the discovery of new quantum phases to the implementation of quantum simulators and quantum information processing. The article provides an overview of recent experimental results in the field of cavity Bose-Einstein condensate (BEC). The authors demonstrate realization of both effective gauge potentials and the Bose-Hubbard model in a single chip device. Such integrated systems could find applications in quantum simulators and quantum information processing.",
        "watermark_text": "Recent research developments in the field of Bose - Einstein condensates ( BECs ) accelerate observation of new types of quantum several - bodies phenomena . Notably , design of driven - dissipative quantum systems has led to the observation of quantum phase transitions with light , notably , the Bose - Hubbard theory .Integration of BECs with cavity quantum electrodynamics , another milestone in coherent light study , opens up the prospect of realizing appropriate gauge potentials for quantum particles . Here , we accomplish both these phenomena in a single chip device , and observe the formation of chiral supersolids and a chiral spin liquid .We indicate that a cavity - BEC platform has numerous future applications , ranging from the discovery of new quantum phases to the implementation of quantum simulators and quantum information processing . The section offers an overview of recent experimental results in the field of cavity Bose - Einstein condensate ( BEC ) .The authors show realization of both efficient gauge potentials and the Bose - Hubbard theory in a single chip device . Such embedded solutions might find uses in quantum simulators and quantum information processing .",
        "rewrite_text": "Recent advancements in the field of Bose-Einstein condensates (BECs) have spurred the observation of diverse new types of quantum many-body phenomena. Specifically, the design of driven-dissipative quantum systems has facilitated the observation of quantum phase transitions with light, exemplified by the Bose-Hubbard theory. The integration of BECs with cavity quantum electrodynamics, a significant milestone in coherent light research, presents opportunities to realize suitable gauge potentials for quantum particles. In this context, we have achieved both phenomena in a single chip device, observing the formation of chiral supersolids and a chiral spin liquid. We highlight the potential future applications of a cavity-BEC platform, ranging from the discovery of novel quantum phases to the implementation of quantum simulators and information processing.\n\nThis section provides an overview of recent experimental findings in the area of cavity Bose-Einstein condensates (BECs). The authors demonstrate the successful realization of both efficient gauge potentials and the Bose-Hubbard theory within a single chip device. Such integrated solutions may find applications in quantum simulators and information processing.",
        "ori-fast-z-score": -0.105999788000636,
        "water-fast-z-score": 6.396021490668312,
        "rewrite-fast-z-score": 1.58999682000954
    },
    {
        "original_text": "Quasi-periodic oscillations (QPO) in the X-ray flux from some compact objects have been discovered over the past three decades. Two distinct families of QPO have been identified; the lower-frequency (LF) QPOs (0.001–30 Hz) and the upper-frequency (HF) QPOs (0.03–300 Hz). The HF QPOs are particularly intriguing, as several sources have more than one distinct HF QPO peak, indicating that these objects are rotating near to some stable self-frequency. The distinct frequencies of the HF QPO peaks, their clustering around some values and their occasional stability over long time periods, are best explained if the peaks are produced by fluid moving in circular geodetic orbits in a compact object such as a neutron star or a black hole. The gravitomagnetic effects produced by the fluid could be responsible for the modulation of the orbital periods, causing the distinct HF QPO peaks. In this way, the gravitomagnetic effects could act as a unifying mechanism for several existing models of compact objects.",
        "watermark_text": "Quasi - periodic oscillations ( QPO ) in the X - ray flux from some compact objects have been detected over the previous three decades . Two different parents of QPO have been described ; the lower - frequency ( LF ) QPOs ( 0 . 001 – 30 Hz ) and the higher - frequency ( HF ) QPOs ( 0 . 03 – 300 Hz ) .The HF QPOs are particularly intriguing , as several sources have more than one separate HF QPO peak , showing that these objects are rotating near to some stable self - frequency . The distinct frequencies of the HF QPO mountains , their clustering around some values and their occasional stability over large time periods , are best explained if the ridges are produced by liquid sliding in circular geodetic orbits in a compact body such as a neutron star or a black hole .The gravitomagnetic effects generated by the liquid may be responsible for the modulation of the orbital periods , creating the distinct HF QPO peaks . In this way , the gravitomagnetic effects could act as a unifying mechanism for various existing models of compact objects .",
        "rewrite_text": "Over the past three decades, quasi-periodic oscillations (QPOs) in X-ray flux from certain compact objects have been detected. Two primary types of QPOs have been identified: lower-frequency (LF) QPOs ranging from 0.001 to 30 Hz, and higher-frequency (HF) QPOs spanning from 0.03 to 300 Hz.\n\nThe HF QPOs are particularly fascinating as they often exhibit multiple distinct peaks from various sources, indicating that these objects are rotating close to some stable self-frequency. The unique frequencies of the HF QPO peaks, their clustering at specific values, and their occasional stability over extended time periods are best explained by liquid sliding in circular geodetic orbits within a compact body, such as a neutron star or black hole. The modulation of orbital periods caused by gravitomagnetic effects generated by the liquid may be responsible for creating the distinct HF QPO peaks. In this way, the gravitomagnetic effects could serve as a unifying mechanism for various existing models of compact astronomical objects.",
        "ori-fast-z-score": -0.11867816581938533,
        "water-fast-z-score": 4.153735803678487,
        "rewrite-fast-z-score": 1.3587324409735149
    },
    {
        "original_text": "A surprising variety of celestial objects have been discovered orbiting around the center of the galaxy IC342. This includes at least 15 dark galaxies, up to 750 globular clusters and numerous highly unusual x-ray sources. An x-ray survey of the central square kiloparsec of this galaxy has been performed using the Chandra X-ray observatory with a resolution of 20 pc. A total of twenty-two point-like x-ray sources are resolved by Chandra, the vast majority of which are likely low-mass x-ray binary systems (LMXRBS). A globular cluster appears to be powered by a bright ultraluminous x-ray source, several other globular clusters appear to possess faint x-ray sources, and two newly discovered dwarf galaxies appear to be entirely powered by x-ray emission from stellar black holes. The presence of a large number of highly unusual x-ray sources is quite remarkable, and no other galaxy centers has shown such a diverse population of anomalous objects. The survival of these objects over 10 Gyr shows that some of the dark galaxies may be viable candidates to form new galaxies.",
        "watermark_text": "A unexpected array of astronomical bodies have been detected orbiting around the hub of the universe IC342 . This encompasses at least 15 dark galaxies , up to 750 globular complexes and many notably unusual x - ray sources .An x - ray survey of the central square kiloparsec of this galaxy has been performed using the Chandra X - ray observatory with a resolution of 20 pc . A total of twenty - two point - like x - ray sources are resolved by Chandra , the vast bulk of which are likely low - weight x - ray binary complexes ( LMXRBS ) .A globular cluster appears to be powered by a bright ultraluminous x - ray source , various other globular nuclei tend to contain faint x - ray sources , and two recently discovered giant galaxies appear to be entirely powered by x - ray radiation from stellar black holes . The presence of a large number of highly unusual x - ray sources is rather noteworthy , and no other galaxy centers has exhibited such a diverse population of anomalous objects .The stability of these objects over 10 Gyr shows that some of the dark universe might be possible candidates to form young galaxies .",
        "rewrite_text": "An unexpected array of astronomical bodies has been discovered orbiting the hub of the universe, IC342. This includes at least 15 dark galaxies, up to 750 globular clusters, and numerous noteworthy unusual X-ray sources. An X-ray survey of the central square kiloparsec of this galaxy has been conducted using the Chandra X-ray Observatory with a resolution of 20 pc. Chandra has resolved a total of twenty-two point-like X-ray sources, the majority of which are likely to be low-mass X-ray binary complexes (LMXRBS).\n\nA globular cluster appears to be powered by a bright ultraluminous X-ray source, while various other globular nuclei tend to contain faint X-ray sources. Additionally, two newly discovered giant galaxies appear to be entirely powered by X-ray radiation from stellar black holes. The presence of a large number of highly unusual X-ray sources is particularly notable, as no other galaxy centers have exhibited such a diverse population of anomalous objects.\n\nThe stability of these objects over a period of 10 Gyr suggests that some of the dark universe may be potential candidates for forming young galaxies.",
        "ori-fast-z-score": 0.5184758473652127,
        "water-fast-z-score": 6.118014998909509,
        "rewrite-fast-z-score": 2.177598558933893
    },
    {
        "original_text": "Mass-loss events in the later evolutionary stages of stars are critical for the understanding of the final fates of stars. Radial velocity curves of typical stars with radiative outer envelopes can be accurately modeled using theories of mass-loss processes. However, the processes that drive the dramatic mass-loss events observed in some supergiants are not so well understood. Of particular interest are the events that drive the rapid mass-loss exhibited by many of the Luminous Blue Variables (LBVs) and the Quasi-periodic Modulations (QPMs) of the radio signatures of several supernova explosions. Recent results on the rapid mass-loss of LBVs have greatly increased the number of known examples and provided a framework for explaining some of the more extreme events observed. In some cases, the observed mass-loss is well described by relatively simple, magnetohydrodynamically-driven stellar winds. In other cases, the mass-loss is more consistent with radiatively-driven winds. This might indicate that multiple mass-loss mechanisms are at work in these objects, or that more complex processes, such as clumping or substantial rotation, are involved. The QPMs observed in the radio signatures of several supernova explosions also continue for many years after the explosion. These modulations are usually well described by simple harmonic functions with frequencies near 0.25, 0.6, 1.0, 2.0, and 4.5 cycles per year. The 0.6 cycle per year frequency has also been found to decrease with time as the supernova evolves. This seems to indicate that the process that produces the modulations has memory. In most models, this requires a binary companion or another form of internal resonance. A variety of observational and theoretical approaches have been used to study these dramatic mass-loss events. Recent modeling has benefited greatly from the theoretical advancements made in the areas of hydrodynamics, magnetohydrodynamics, and nuclear reaction networks. Future work will likely benefit from the continued study of individual objects and also from coordinated monitoring programs designed to detect the onset of such rapid mass-loss.",
        "watermark_text": "Mass - loss events in the subsequent evolutionary stages of stars are critical for the knowledge of the last fates of stars . Radial velocity curves of typical stars with radiative exterior envelopes can be correctly modeled using theories of mass - loss processes .However , the mechanisms that drive the dramatic mass - loss events observed in some supergiants are not so good explained . Of particular concern are the events that drive the fast mass - loss displayed by many of the Luminous Blue Variables ( LBVs ) and the Quasi - periodic Modulations ( QPMs ) of the television signatures of several supernova explosions .Recent results on the fast mass - loss of LBVs have greatly increased the number of known examples and provided a framework for explaining some of the more extreme events observed . In some cases , the seen mass - loss is well described by relatively simple , magnetohydrodynamically - guided stellar winds .In other instances , the mass - loss is more consistent with radiatively - pushing winds . This might suggest that several mass - loss mechanisms are at work in these objects , or that more sophisticated processes , such as clumping or considerable rotation , are responsible .The QPMs observed in the radio signatures of several supernova exploded also continue for thousands decades after the explosion . These modulations are typically better represented by simple harmonic functions with frequencies near 0 . 25 , 0 . 6 , 1 . 0 , 2 . 0 , and 4 . 5 cycles per decade .The 0 . 6 cycle per year amplitude has additionally been shown to decline with time as the supernova evolves . This seems to indicate that the process that generates the modulations has memory .In most models , this demands a binary companion or another form of internal resonance . A variety of observational and theoretical methods have been used to study these significant mass - loss events .Recent modeling has gained significantly from the theoretical advancements made in the fields of hydrodynamics , magnetohydrodynamics , and reactor response systems . Future work will probably benefit from the continued study of individual objects and also from coordinated monitoring programs intended to identify the emergence of such rapid mass - loss .",
        "rewrite_text": "The subsequent evolutionary phases of stars witness critical mass-loss events that are pivotal for understanding their ultimate fates. The radial velocity curves of regular stars with radiative outer envelopes can be accurately modeled through the application of mass-loss theories. However, the mechanisms driving the significant mass-loss observed in some supergiants remain less well-understood.\n\nParticular concern lies with the events driving the rapid mass-loss exhibited by many Luminous Blue Variables (LBVs) and the quasi-periodic modulations (QPMs) observed in the radio signatures of several supernova explosions. Recent research on the rapid mass-loss of LBVs has significantly increased the number of known cases and provided a framework for explaining some of the most extreme events.\n\nIn some cases, the observed mass-loss can be adequately explained by relatively simple, magnetohydrodynamically-guided stellar winds, while in others, it is more aligned with radiatively-driven winds. This suggests that multiple mass-loss mechanisms may be at play in these objects, or that more complex processes such as clumping or significant rotation are involved.\n\nThe QPMs detected in the radio signatures of several supernova explosions persist for thousands of decades after the explosion. These modulations are typically represented more effectively by simple harmonic functions with frequencies close to 0.25, 0.6, 1.0, 2.0, and 4.5 cycles per decade. The amplitude of 0.6 cycles per year has also been found to decrease with time as the supernova evolves, indicating that the process generating the modulations has memory.\n\nMost models suggest that this requires a binary companion or another form of internal resonance. A range of observational and theoretical approaches have been employed to study these significant mass-loss events. Recent modeling has greatly benefited from advancements in hydrodynamics, magnetohydrodynamics, and reactor response systems theories. Future research is likely to benefit from continued studies of individual objects and coordinated monitoring programs aimed at identifying the emergence of such rapid mass-loss events.",
        "ori-fast-z-score": 2.9673014758835152,
        "water-fast-z-score": 8.952737857857514,
        "rewrite-fast-z-score": 2.2714553049650763
    },
    {
        "original_text": "The origin of the molecular emission around the southern hemisphere Re 4 IRS - HH 188 region was studied using data from the ATCA and the SEST. Emission from molecular lines such as CS, HCN, HCO+, H21, and CH3OH was found to be associated with the shock excited by IRS 4B, and possibly IRS 4A, interacting with an inclined disk of gas and dust. The observed HCN J=3-2 and HCO+ J=3-2 spectra are characteristic of hot-core chemistry, whereas the weak CH3OH line suggests a more minor influence of thermal processing. The observed spatial distributions and velocity structures suggest that the observed emission originates from an hourglass-shaped shocked envelope with a close-to-centroid orientation, a bow-shock cavity walls and a wide redshifted component possibly arising from an externally triggered jet. The origin of the molecular emission around the southern hemisphere Re 4 IRS - HH 188 region was studied using data from the ATCA and the SEST. Emission from molecular lines such as CS, HCN, HCO+, H21, and CH3OH was found to be associated with the shock excited by IRS 4B, and possibly IRS 4A, interacting with an inclined disk of gas and dust. The observed HCN J=3-2 and HCO+ J=3-2 spectra are characteristic of hot-core chemistry, whereas the weak CH3OH line suggests a more minor influence of thermal processing. The observed spatial distributions and velocity structures suggest that the observed emission originates from an hourglass-shaped shocked envelope with a close-to-centroid orientation, a bow-shock cavity walls and a wide redshifted component possibly arising from an externally triggered jet.",
        "watermark_text": "The origin of the molecular emission around the southern hemisphere Re 4 IRS - HH 188 region was studied utilizing information from the ATCA and the SEST . Emission from molecular lines such as CS , HCN , HCO + , H21 , and CH3OH was shown to be correlated with the shock excited by IRS 4B , and presumably IRS 4A , evolving with an inclined disk of gas and dust .The observed HCN J = 3 - 2 and HCO + J = 3 - 2 spectra are reminiscent of hot - core chemistry , whereas the strong CH3OH line suggests a more minor influence of thermal processing . The observed spatial distributions and speed structures suggest that the seen emission originates from an hourglass - shaped shocked envelope with a close - to - centroid attitude , a bow - shock cavity walls and a broad redshifted component possibly arising from an externally activated jet .The origin of the molecular emission around the southern hemisphere Re 4 IRS - HH 188 region was studied utilizing information from the ATCA and the SEST . Emission from molecular lines such as CS , HCN , HCO + , H21 , and CH3OH was shown to be correlated with the shock excited by IRS 4B , and presumably IRS 4A , evolving with an inclined disk of gas and dust .The observed HCN J = 3 - 2 and HCO + J = 3 - 2 spectra are reminiscent of hot - core chemistry , whereas the strong CH3OH line suggests a more minor influence of thermal processing . The observed spatial distributions and speed structures suggest that the seen emission originates from an hourglass - shaped shocked envelope with a close - to - centroid attitude , a bow - shock cavity walls and a broad redshifted component possibly arising from an externally activated jet .",
        "rewrite_text": "The research on the origin of molecular emissions in the southern hemisphere's Re 4 IRS - HH 188 region was conducted by utilizing data from the ATCA and SEST. It was revealed that emissions from molecular lines including CS, HCN, HCO+, H21, and CH3OH are associated with the shock generated by IRS 4B, possibly also by IRS 4A. These emissions evolve within an inclined disk of gas and dust. The observed spectra of HCN J=3-2 and HCO+ J=3-2 remind us of hot-core chemistry, while the prominent CH3OH line suggests a lesser impact of thermal processing. Furthermore, the spatial distributions and velocity structures observed indicate that the emission originates from a hourglass-shaped shocked envelope with a nearly centroid orientation, featuring bow-shock cavity walls and a broad redshifted component potentially arising from an externally activated jet.",
        "ori-fast-z-score": 1.6858544608470492,
        "water-fast-z-score": 5.806832031806502,
        "rewrite-fast-z-score": 2.1766269588592317
    },
    {
        "original_text": "The Feynman-Weinberg quantum gravity (FW QG) is a hybrid quantum-classical theory of gravity based on renormalizable Effective Field Theory (EFT) obtained from the cutting edge of quantum gravity research. It provides a candidate theory of everything (TOE) to unify all known interactions including gravity. The FW QG is an EFT valid up to thePlanck scale, The FW QG extends the standard model (SM) of particle physics to include not just the currently observed fields but also those to higher energies as per the EFT principle. The FW QG is the first theory to incorporate quantum gravity into a relativistic quantum field theory. To date it has passed all known empirical tests: gravitational physics, the weak force, and the strong and electromagnetic forces with its remaining parameters calculated using the EFT approach. Its extension to include the full Standard Model and additional terms to close the missing equations (Planck scale detection/interactions) remains a priority for future research. The FW QG is thus a fully renormalizable, empirically consistent, and self-consistent TOE.",
        "watermark_text": "The Feynman - Weinberg quantum gravitational ( FW QG ) is a hybrid quantum - classical theory of gravitational built on renormalizable Effective Field Theory ( EFT ) obtained from the cutting edge of quantum gravitational studies . It provides a candidate theory of things ( TOE ) to unify all known interactions including gravity .The FW QG is an EFT valid up to thePlanck range , The FW QG expanded the standard description ( SM ) of particle science to include not just the currently observed fields but also those to higher energies as per the EFT principle . The FW QG is the first theory to introduce particle gravity into a relativistic quantum field model .To date it has passed all known empirical examinations : gravitational mechanics , the weak force , and the strong and electromagnetic forces with its remaining parameters calculated using the EFT methodology . Its application to use the full Standard Model and additional terms to close the missing equations ( Planck size detection / particles ) continues a priority for future research .The FW QG is thereby a completely renormalizable , empirically consistent , and self - stable TOE .",
        "rewrite_text": "The Feynman-Weinberg Quantum Gravitational (FW QG) theory is a hybrid quantum-classical theory of gravity that is founded on the renormalizable Effective Field Theory (EFT). This theory emerges from the forefront of quantum gravitational research. It offers a candidate theory of everything (TOE) that aims to unify all known interactions, including gravity. The FW QG is an EFT that is valid across the entire Planck range.\n\nThe FW QG extends the standard description (SM) of particle physics to encompass not only the currently observed fields but also those that exist at higher energies, in accordance with the EFT principle. It is the first theory to incorporate particle gravity into a relativistic quantum field model. So far, it has passed all known empirical tests, including gravitational mechanics, the weak force, and the strong and electromagnetic forces. Its remaining parameters are calculated using the EFT methodology.\n\nA priority for future research continues to be the application of the FW QG to supplement the full Standard Model with additional terms, in order to close the gap in missing equations related to Planck-scale detection or particles. Overall, the FW QG is a completely renormalizable, empirically consistent, and self-stable TOE.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 5.962965874907927,
        "rewrite-fast-z-score": 2.494700264914546
    },
    {
        "original_text": "The gamma-ray binary LS I +61 303 is composed of a compact object, most likely a black hole, accreting from the gas captured by the companion star. The companion is a main sequence star of spectral type O9.5V, located at a distance of 2.4 kpc. The orbit of the binary is highly eccentric, with a period of 12.8 years and an apastron of 5.5 A.U. The detection of eclipses and dips, highly polarized gamma-ray emission, as well as the detection of a collimated jet, suggest that the compact object is hidden for most of the time, with occasional brief appearances as it spirals towards the star. The system s dynamic nature has been a matter of debate, with some studies supporting a scenario involving the existence of a third body, while others advocate for the colliding-winds model. In this scenario, the stellar wind from the companion sweeps up and compresses a dense circumbinary disk around the black hole. This disk produces the majority of the observed gamma-ray emission. In this paper, we assess these two models using a combination of numerical simulations and analytic arguments. The main result of this assessment is that the accretion model, though less elegant and more complex, is more successful at reproducing the multiwavelength data. The accretion model for LS I +61 303 relies on the existence of an accretion disk, fed by material captured from the stellar companion. The matter is likely channeled towards the black hole through a hot, relatively poorly ionized magnetospheric outflow. The rate of capture is highly eccentric with respect to the black hole s orbital plane, allowing for a persistent but irregular supply of material to the disk. Thermal coupling between the disk and the magnetic field of the black hole prevents the formation of an externally thin disk, and allows for efficient high-energy emission via synchrotron or magnetically confined jet models. In contrast, colliding-winds models posit that the compact object is a Wolf-Rayet star, composed of a fast-moving WR wind blowing over a dense, slow-moving O-star wind. The system is highly eccentric, with the periastron being the site of the colliding winds. These two winds slow down and collide, producing a complex, partially ionized, region of shocked gas and emission. Although the existence of colliding winds has been demonstrated in the case of massive stars, we demonstrate that colliding winds can also apply to relatively low-mass companion stars, producing observable emission. Furthermore, the emitting region in colliding winds is small and localized, whereas the large orbit of LS I +61 303 requires a distributed emission region. Through a combination of 1-D and 3-D simulations, we show that a colliding winds scenario cannot reproduce the multiwavelength spectral energy distribution of LS I +61 303, and predict that the system should be relatively radio-quiet.",
        "watermark_text": "The gamma - ray binary LS I + 61 303 is composed of a compact body , most likely a black hole , accreting from the gas captured by the companion star . The companion is a major sequence star of spectral category O9 . 5V , located at a distance of 2 . 4 kpc .The orbit of the binary is strongly eccentric , with a period of 12 . 8 years and an apastron of 5 . 5 A . U . The observation of eclipses and dips , highly polarized alpha - ray radiation , as well as the observation of a collimated jet , indicate that the compact body is hidden for most of the time , with occasional brief appearances as it spirals towards the star .The system s dynamic existence has been a subject of dispute , with some researchers backing a scenario involving the existence of a third body , while several argue for the colliding - winds model . In this situation , the stellar wind from the companion sweeps up and compresses a dense circumbinary disk around the dark hole .This disk produces the majority of the known γ - ray radiation . In this paper , we assess these two models using a combination of statistical simulations and analytic statements .The main consequence of this analysis is that the accretion theory , though fewer ornate and more sophisticated , is more effective at reproducing the multiwavelength statistics . The accretion theory for LS I + 61 303 rely on the existence of an accretion disk , fed by material captured from the stellar companion .The material is probably channeled towards the dark hole through a heated , fairly poorly ionized magnetospheric outflow . The rate of capture is strongly eccentric with regard to the dark hole s orbital plane , allowing for a steady but irregular supply of material to the disk .Thermal friction between the disk and the magnetic field of the dark hole reduces the formation of an externally thin disk , and allows for efficient high - energy emission via synchrotron or magnetically confined jet models . In comparison , colliding - winds models posit that the compact object is a Wolf - Rayet star , composed of a rapidly - moving WR breeze blew over a dense , slow - expanding O - star wind .The system is strongly eccentric , with the periastron being the location of the colliding winds . These two winds calm down and collide , creating a complex , partially ionized , zone of excited gas and emission .Although the existence of colliding winds has been shown in the case of large galaxies , we prove that colliding winds can also apply to rather low - mass companion stars , creating observable emission . Furthermore , the emitting area in colliding winds is tiny and localized , whereas the small orbit of LS I + 61 303 needs a distributed emission region .Through a combination of 1 - D and 3 - D simulations , we prove that a colliding winds scenario cannot reproduce the multiwavelength spectral power distribution of LS I + 61 303 , and predict that the system should be relatively radio - quiet .",
        "rewrite_text": "The gamma-ray binary LS I + 61 303 comprises a compact body, likely a black hole, which accretes matter from the gas captured by its companion star. The companion star is an O9.5V main sequence star situated at a distance of 2.4 kpc. The binary's orbit is highly eccentric, with a period of 12.8 years and an apastron of 5.5 AU. Observations of eclipses and dips, highly polarized alpha-ray radiation, as well as the presence of a collimated jet, suggest that the compact body is often hidden and occasionally appears briefly as it orbits towards the companion star.\n\nThe system's dynamic nature has been debated, with some researchers supporting a scenario involving a third body, while others advocate for the colliding-winds model. In this scenario, the stellar wind from the companion sweeps up and compresses a dense circumbinary disk around the dark hole. This disk is the primary source of the observed gamma-ray radiation.\n\nIn this paper, we evaluate these two models using a combination of statistical simulations and analytical statements. Our primary finding is that the accretion theory, while simpler and more straightforward, is more effective at reproducing the multiwavelength statistics of LS I + 61 303. The accretion theory for this system relies on the existence of an accretion disk fed by material captured from the companion star. This material is likely channeled towards the black hole through a heated, partially ionized magnetospheric outflow. The capture rate is highly eccentric in relation to the black hole's orbital plane, allowing for a steady but irregular supply of material to the disk.\n\nThermal friction between the disk and the magnetic field of the black hole diminishes the formation of an externally thin disk, enabling efficient high-energy emission through synchrotron or magnetically confined jet models. In contrast, colliding-winds models propose that the compact object is a Wolf-Rayet star, composed of a rapidly moving WR wind interacting with a dense, slowly expanding O-star wind. The system's orbit is highly eccentric, with the periastron marking the point of colliding winds. These two winds collide and settle, creating a complex, partially ionized zone of excited gas and emission.\n\nWhile the existence of colliding winds has been observed in large galaxies, we demonstrate that such collisions can also occur in systems with lower-mass companion stars, producing observable emissions. Furthermore, the emitting area in colliding-wind scenarios is typically small and localized, whereas the small orbit of LS I + 61 303 requires a distributed emission region. Through a combination of one- and three-dimensional simulations, we conclude that a colliding-winds scenario cannot replicate the multiwavelength spectral power distribution of LS I + 61 303 and predict that the system should exhibit relatively low radio activity.",
        "ori-fast-z-score": -0.6835859270246633,
        "water-fast-z-score": 9.384891607056018,
        "rewrite-fast-z-score": 3.0219324068711586
    },
    {
        "original_text": "The experimental search for a small mismatch between the neutrino and the anti-neutrino masses has recently exceeded the world-leading sensitivity. The new result from the muon (g-2) experiment at Brookhaven National Laboratory brings the total computed discrepancy between the observed and the predicted values to over 2.6 standard deviations, lending strong support to the existence of a small, underlying sterile neutrino mass. The new result is in perfect agreement with the standard model prediction, providing further evidence in support of the so-called  normal  neutrino mass ordering. Should the central value of the upcoming new result be lower than the current best fit, it would be an unambiguous sign of new physics and would call for an update of the current model building. However, given the current statistical and systematic uncertainties, even an order of magnitude improvement on the sensitivity would not significantly change the current best-fit three-neutrino model.",
        "watermark_text": "The empirical search for a small mismatch between the neutrino and the counter - neutrino masses has recently exceeded the world - leading sensitivity . The newest result from the muon ( g - 2 ) experiment at Brookhaven National Laboratory brought the total computed discrepancy between the seen and the expected values to over 2 . 6 standard deviations , providing significant argument to the existence of a small , underlying sterile neutrino mass .The revised result is in perfect agreement with the standard model prediction , providing further evidence in support of the so - called ordinary neutrino mass ordering . Should the main value of the latest latest consequence be less than the previous good fit , it would be an unambiguous sign of new science and might call for an update of the present model structure .However , given the present statistical and systematic uncertainties , even an order of magnitude improvement on the sensitivity would not considerably alter the present better - fitting three - neutrino model .",
        "rewrite_text": "Recently, the empirical exploration for a minuscule mismatch between the masses of a neutrino and its counter-particle has surpassed the world's leading sensitivity. The latest findings from the muon (g-2) experiment at the Brookhaven National Laboratory have increased the calculated discrepancy between observed and expected values to more than 2.6 standard deviations. This provides compelling evidence for the existence of a subtle, underlying sterile neutrino mass. The revised result aligns perfectly with the prediction of the standard model, further corroborating the notion of the so-called ordinary neutrino mass ordering.\n\nIf the primary value of the latest consequence were to be less than the previous best fit, it would unequivocally indicate new scientific discoveries and potentially necessitate an update to the current model structure. Nevertheless, given the current statistical and systematic uncertainties, even a tenfold improvement in sensitivity would not significantly alter the currently best-fitting three-neutrino model.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 5.356147093602983,
        "rewrite-fast-z-score": 0.9701425001453319
    },
    {
        "original_text": "Manganites R$_{1-x}$A$_x$MnO$_3$ (R: rare-earth, A: alkaline-earth) are considered as a prime example for the observation of a discontinuous metal-insulator transition (MIT) with orbital ordering. The MIT is caused by the competition between Jahn-Teller (JT) interaction and electronic (Hund’s) coupling. A microscopic explanation for the observed orbital order has so far been lacking. Here, we present the first microscopic theory for the correlated orbital dynamics in ferromagnetic manganites. We use a multi-orbital Hubbard model for the Mn $3d$ electrons with large separation of Mn $3d$ and O $2p$ orbitals. The model describes the competing interactions in the system and, in particular, accounts for the Mn$^{3+}$ and Mn$^{4+}$ local spins, which are considered as pseudo-spins. The crucial input of the model is the derivation of the effective Hamiltonian for the Mn $e_g$ electrons, which is performed in the framework of the Hubbard lattice approach. We find a ferromagnetic orbital liquid phase with vanishing order parameters for the conventional (canted) orbital order and the ferromagnetic one. The Fermi-surface nesting causes a strong renormalization of the bare interactions and leads to the orbital polarization at small JT couplings and a tiny interval of Hund’s coupling, which we estimate from realistic band structure calculations. We argue that the observed MIT in these compounds can be explained without orbital ordering, if the manganese valence is slightly fluctuating.",
        "watermark_text": "Manganites R $ _ { 1 - x } $ A $ _ x $ MnO $ _ 3 $ ( R : rare - planet , A : alkaline - planet ) are considered as a key example for the observation of a discontinuous steel - insulator transition ( MIT ) with orbital ordering . The MIT is caused by the competition between Jahn - Teller ( JT ) bonding and electronic ( Hund ’ s ) bonding .A microscopic explanation for the known orbital order has so far been missing . Here , we present the first microscopic theory for the coupled orbital dynamics in ferromagnetic manganites .We use a multi - orbital Hubbard theory for the Mn $ 3d $ electrons with large separation of Mn $ 3d $ and O $ 2p $ orbitals . The model describes the competing interactions in the system and , in particular , accounts for the Mn $ ^ { 3 + } $ and Mn $ ^ { 4 + } $ local orbits , which are considered as pseudo - spins .The vital input of the model is the derivation of the effective Hamiltonian for the Mn $ e _ g $ electrons , which is conducted in the framework of the Hubbard crystal approach . We get a ferromagnetic orbital liquid phase with vanishing order variables for the usual ( canted ) orbital order and the ferromagnetic one .The Fermi - surface nesting produces a large renormalization of the bare interactions and results to the orbital polarization at small JT couplings and a small interval of Hund ’ s coupling , which we estimate from realistic band structure estimates . We argue that the seen MIT in these acids can be described without orbital ordering , if the manganese valence is significantly fluctuating .",
        "rewrite_text": "The formula Manganites R1-x A_x MnO3 (R represents a rare-earth element, A represents an alkaline element) serves as a crucial exemplar for observing a discontinuous steel-insulator transition (MIT) with orbital ordering. This MIT arises from the competition between Jahn-Teller (JT) bonding and electronic (Hund's) bonding. A microscopic explanation for the known orbital order has yet to be found. In this study, we introduce the first microscopic theory to explain the coupled orbital dynamics in ferromagnetic manganites. We employ a multi-orbital Hubbard theory for the Mn 3d electrons, considering a significant separation between the Mn 3d and O 2p orbitals.\n\nOur model characterizes the competing interactions within the system, particularly accounting for the local orbits of Mn^3+ and Mn^4+, which are considered as pseudo-spins. A critical aspect of the model is the derivation of the effective Hamiltonian for the Mn e_g electrons, which is accomplished within the framework of the Hubbard crystal approach. We obtain a ferromagnetic orbital liquid phase with vanishing order parameters for both the conventional (canted) orbital order and the ferromagnetic one.\n\nThe Fermi-surface nesting results in a significant renormalization of bare interactions, leading to orbital polarization at small JT couplings and within a narrow range of Hund's coupling. We estimate these values based on realistic band structure calculations. We propose that the observed MIT in these compounds can be described without orbital ordering when the manganese valence exhibits significant fluctuations.",
        "ori-fast-z-score": -1.0425720702853738,
        "water-fast-z-score": 4.170288281141495,
        "rewrite-fast-z-score": 1.2
    },
    {
        "original_text": "We formalize the equivalence between Nash equilibria and subgame perfect equilibria in game theory. This equivalence is strong enough to imply various properties of Nash equilibria, like their existence or uniqueness. Unfortunately, in general these properties are not true for subgame perfect equilibria, and we give explicit examples of games which disprove these properties for subgame perfect equilibria. This disproves a conjecture by H. Brézis that subgame perfect equilibria, and hence also Nash equilibria, must be acyclic. We give a constructive version of the above equivalence, and use it to show that in any game, a subgame perfect equilibrium always contains all the information about the game, in the sense that any path in the subgame perfect equilibrium graph yields a Nash equilibrium. We also give a combinatorial proof that in two-player zero-sum games, any subgame perfect equilibrium is also a Nash equilibrium, thus providing a simpler and more direct proof of this well-known result than previous constructions using fixed point theorems.",
        "watermark_text": "We formalize the equivalence between Nash equilibria and subgame perfect equilibria in tournament theory . This equivalence is strong enough to imply various properties of Nash equilibria , like their existence or uniqueness .Unfortunately , in general these characteristics are not true for subgame perfect equilibria , and we give explicit examples of games which disprove these characteristics for subgame perfect equilibria . This disproves a conjecture by H . Brézis that subgame perfect equilibria , and hence also Nash equilibria , need be acyclic .We get a constructive version of the above equivalence , and use it to see that in any game , a subgame perfect equilibrium necessarily contains all the information about the tournament , in the sense that any path in the subgame perfect equilibrium graph yields a Nash equilibrium . We additionally give a combinatorial proving that in two - player zero - sum games , any subgame perfect equilibrium is also a Nash equilibrium , thus offering a simpler and more direct proof of this good - famous fact than past constructions using fixed point theorems .",
        "rewrite_text": "We formalize the equivalence between Nash equilibria and subgame-perfect equilibria in the context of tournament theory. This equivalence is highly significant as it entails various properties of Nash equilibria, such as their existence and uniqueness. However, it's noteworthy that these characteristics are not uniformly applicable to subgame-perfect equilibria. We provide explicit examples to illustrate games where these characteristics fail for subgame-perfect equilibria, thus contradicting a conjecture made by H. Brézis that both subgame-perfect and Nash equilibria must be acyclic.\n\nWe achieve a constructive interpretation of this equivalence, utilizing it to demonstrate that, in any given game, a subgame-perfect equilibrium necessarily encompasses all relevant information about the tournament. Specifically, any path traversed within the subgame-perfect equilibrium graph leads to a Nash equilibrium. Additionally, we provide a combinatorial proof that, in two-player zero-sum games, any subgame-perfect equilibrium is also a Nash equilibrium. This offers a simpler and more direct proof of this well-known fact compared to previous constructions relying on fixed point theorems.",
        "ori-fast-z-score": 1.860521018838127,
        "water-fast-z-score": 6.154574548966636,
        "rewrite-fast-z-score": 3.395498750508662
    },
    {
        "original_text": "The Coma cluster is the first single-instance cluster detected in X-rays by the Rosat satellite. Its impressive morphology has been confirmed by all high-resolution X-ray instruments with the exception of the Chandra and XMM-Newton telescopes, whose observations revealed the existence of a large-scale peripheral component associated with the cluster. This component is aligned with the Coma center and connects it with the cluster Abell 1367, situated at the South-West direction. We report the detection of a south-west extension of the Coma cluster, discovered in the Chandra observation. It has a size of approximately 2 Mpc, amplitude of 7500 km/s, a position angle of 90 degrees, and covers the front of the main cluster component. The position of the sub-cluster coincides with the emission peak of the thermal cluster gas and the brightest galaxy members. The existence of the sub-cluster supports a scenario in which Coma is the result of a merger with another galaxy group, which led to the observed cD galaxy being left behind.",
        "watermark_text": "The Coma cluster is the first single - instance cluster observed in X - radiation by the Rosat satellite . Its impressive morphology has been confirmed by all high - resolution X - ray instruments with the exception of the Chandra and XMM - Newton telescopes , whose observations showed the existence of a large - scale peripheral component associated with the cluster .This component is aligned with the Coma center and connects it with the cluster Abell 1367 , situated at the South - West direction . We report the discovery of a west - west extension of the Coma cluster , detected in the Chandra observation .It has a diameter of almost 2 Mpc , amplitude of 7500 km / s , a orientation angle of 90 degrees , and covers the front of the main cluster component . The position of the sub - cluster coincides with the emission peak of the thermal cluster gas and the brightest galaxy members .The existence of the sub - cluster provides a situation in which Coma is the result of a union with another galaxy class , which leading to the known cD galaxy being left behind .",
        "rewrite_text": "The Coma cluster, the first-ever instance observed in X-radiation by the Rosat satellite, impresses with its morphology that has been verified by all high-resolution X-ray instruments, except for the Chandra and XMM-Newton telescopes. Their observations revealed the presence of a large-scale peripheral component linked to the cluster. This component aligns with the Coma center and connects it to the Abell 1367 cluster located in the southwest direction. We announce the discovery of a west-to-west extension of the Coma cluster, detected in the Chandra observation. It spans nearly 2 million light-years in diameter, an amplitude of 7500 km/s, with an orientation angle of 90 degrees that covers the front of the main cluster component. The position of the sub-cluster aligns with the emission peak of the thermal cluster gas and the brightest galaxy members. The existence of this sub-cluster indicates that Coma may have been formed by a union with another galaxy class, resulting in the known cD galaxy being left behind.",
        "ori-fast-z-score": -0.3418817293789138,
        "water-fast-z-score": 3.0769355644102245,
        "rewrite-fast-z-score": 1.3764944032233704
    },
    {
        "original_text": "Burnt-bridge models (BBMs) are a simple artificial network motif that exhibits a sharp transition between the unburnt and burnt state that has been used to study systems ranging from bio-inspired computing, to chemical systems and soft-condensed matter systems. The bio-inspired field of molecular motors has recently begun to exploit BBMs as a means to explore the relationship between structure and function, where dynamics of molecular motors interacting with BBMs have been identified as a potential design variable. In this work, we introduce a simple two-state BBM that exhibits a locked-in state that is dependent on the dynamics of the molecular motors. Using a simplified two-state BBM, we are able to qualitatively reproduce the locking phenomenon observed in the more complex BBMs. Further, by adding discrete rotational dynamics to the motor, we are able to reproduce the observed locking behavior using a one-dimensional BBM. Our findings suggest that the locking phenomenon observed in more detailed BBMs is a result of the coupling between continuous rotational dynamics of the molecular motor and the BBM’s structural dynamics.",
        "watermark_text": "Burnt - bridge theories ( BBMs ) are a simple artificial channel motif that exhibits a sharp shift between the unburnt and flaming state that has been used to study systems ranging from biological - inspired algorithms , to chemical structures and soft - condensed matter structures . The bio - inspired field of molecular motors has recently begun to use BBMs as a means to examine the relationship between form and function , where dynamics of molecular motors interacting with BBMs have been recognized as a potential model parameter .In this research , we provide a simple two - state BBM that exhibits a locked - in state that is dependent on the dynamics of the molecular motors . Using a simplified two - state BBM , we are able to qualitatively reproduce the locking phenomenon witnessed in the more sophisticated BBMs .Further , by added discrete rotational dynamics to the motor , we are able to predict the seen locking activity using a one - dimensional BBM . Our findings show that the locking phenomenon detected in more illustrated BBMs is a outcome of the interaction between continuous rotational dynamics of the molecular motor and the BBM ’ s structural dynamics .",
        "rewrite_text": "Burnt-Bridge Models (BBMs) constitute a straightforward artificial channel motif that displays a pronounced transition between the unburnt and flaming states. They have been employed in the study of diverse systems, ranging from biologically-inspired algorithms to chemical structures and soft-condensed matter structures. The field of bio-inspired molecular motors has recently started utilizing BBMs as a tool to explore the relationship between form and function, where the interaction dynamics of molecular motors with BBMs have been recognized as a potential model parameter.\n\nIn our research, we introduce a straightforward two-state BBM that demonstrates a locked-in state that is dependent on the molecular motor dynamics. By utilizing a simplified two-state BBM, we are able to qualitatively replicate the locking phenomenon observed in more complex BBMs. Furthermore, by introducing discrete rotational dynamics to the motor, we can predict the observed locking activity using a one-dimensional BBM. Our findings indicate that the locking phenomenon observed in more elaborate BBMs arises from the interaction between the continuous rotational dynamics of the molecular motor and the structural dynamics of the BBM.",
        "ori-fast-z-score": 0.52999894000318,
        "water-fast-z-score": 6.396021490668312,
        "rewrite-fast-z-score": 0.9649012813540153
    },
    {
        "original_text": "Internal states of model isotropic granular packings. I. Assembling process, geometry and contact networks. The assembly process of granular materials is of considerable scientific and technological interest. Contrary to atomic and molecular systems, the assembly of granular particles is an open system: the rearrangement of particles may change the geometrical arrangement and, consequently, the internal state of the packing. In this work, we study how this rearrangement affects the geometrical and topological properties of the contact network, which is the unique way to describe the internal connectivity of a packing. We carry out numerical simulations of jamming processes with the Omodei law, which successfully captures the structure of the contact network during the evolution of a packing toward stability  1, 2 . To analyse the rearrangement mechanisms, we characterise the internal state of a packing by a small number of global variables. We focus our analysis on two packing properties: the mean angle between neighbours and the radial distribution function. Finally, we introduce a metric to quantify the topological changes produced by rearrangements and we characterise different rearrangement mechanisms.",
        "watermark_text": "Internal states of model isotropic granular packings . I .Assembling process , structure and contact systems . The assembly method of granular materials is of greater scientific and technological concern .Contrary to atomic and molecular systems , the assembly of granular particles is an open structure : the rearrangement of atoms could shift the geometrical arrangement and , thus , the internal state of the packing . In this research , we study how this rearrangement impacts the geometrical and topological features of the contact network , which is the unusual manner to explain the internal connectivity of a packing .We take out numerical simulations of jamming systems with the Omodei law , which successfully captures the composition of the contact network during the evolution of a packing toward stability 1 , 2 . To analyse the rearrangement mechanisms , we characterise the internal state of a packing by a small number of global variables .We focus our analysis on two packing properties : the mean angle between relatives and the radial distribution function . Finally , we apply a metric to quantify the topological alterations produced by rearrangements and we characterise various rearrangement mechanisms .",
        "rewrite_text": "Internal States of Model Isotropic Granular Packings I: Assembling Process, Structure, and Contact Systems\n\nThe assembly method of granular materials holds significant scientific and technological importance. In contrast to atomic and molecular systems, the structure formed by granular particles is an open one. This means that the rearrangement of atoms can shift the geometric arrangement, subsequently affecting the internal state of the packing. This research explores how such rearrangements impact the geometric and topological features of the contact network, an unconventional approach to elucidating the internal connectivity of a packing.\n\nTo investigate the rearrangement mechanisms, we employ numerical simulations of jamming systems using the Omodei law. This method effectively captures the composition of the contact network during the evolution of a packing towards stability. We characterize the internal state of a packing using a limited set of global variables. Our analysis focuses on two primary packing properties: the average angle between particles and the radial distribution function.\n\nFurthermore, we introduce a metric to quantify the topological changes induced by rearrangements and to characterize various rearrangement mechanisms. This enables us to gain a deeper understanding of how these changes affect the overall structure and stability of the granular packing.",
        "ori-fast-z-score": 0.5698028822981898,
        "water-fast-z-score": 5.128225940683707,
        "rewrite-fast-z-score": 1.3779972440082682
    },
    {
        "original_text": "The KATRIN experiment at FZK has finished taking data for the beta decay spectrum on |$q$| = 0.75 eV/c and aims to determine the neutrino mass hierarchy. KATRIN uses a totally dark room with sub-ppm levels of background radiation to measure the beta spectrum. The experiment is expected to reach a sensitivity to the neutrino mass of 0.35-0.9 eV at the 1 σ level, depending on the neutrino hierarchy, and to right-handed currents in the neutrino sector with a sensitivity of 10-35%. The experiment uses an array of 18 metallic wires (katrodes) with an inner diameter of 3 mm that are actuated relative to one another to reduce the motion noise of a micro-manipulator that sits on top of the wires. The wire axial motion is read out with a reflection hybrid, and the phase of the reflection signal is used to determine the time of flight. For high precision determination of the endpoint, a high speed digitizer with 0.5 ns timing is used to record the spectra.",
        "watermark_text": "The KATRIN experiment at FZK has finished taking statistics for the beta decay spectrum on | $ r $ | = 0 . 75 eV / c and seeks to estimate the neutrino mass hierarchy . KATRIN using a totally dark space with sub - ppm concentrations of background radiation to measure the beta spectrum .The project is expected to reach a sensitivity to the neutrino mass of 0 . 35 - 0 . 9 eV at the 1 σ level , depending on the neutrino hierarchy , and to right - handed waves in the neutrino sector with a sensitivity of 10 - 35 % . The experiment uses an array of 18 metallic wires ( katrodes ) with an inner diameter of 3 cm that are actuated relative to one another to reduce the movement sound of a micro - manipulator that sitting on top of the wires .The wire axial motion is read out with a reflection hybrid , and the phase of the reflection signal is applied to predict the period of flight . For large accuracy determination of the endpoint , a high velocity digitizer with 0 . 5 ns timing is utilized to capture the spectra .",
        "rewrite_text": "The KATRIN experiment at FZK has completed data collection for the beta decay spectrum at an energy of | $ r $| = 0.75 eV/c, with the aim of estimating the neutrino mass hierarchy. Utilizing a completely dark environment with sub-parts per million (sub-ppm) concentrations of background radiation, KATRIN measures the beta spectrum accurately. The project is anticipated to achieve a sensitivity in detecting the neutrino mass within the range of 0.35 to 0.9 eV at the 1σ level, depending on the neutrino hierarchy. Additionally, it aims to detect right-handed neutrino waves with a sensitivity of 10-35%.\n\nThe experiment employs an array of 18 metallic wires (katrodes) with an inner diameter of 3 cm. These wires are precisely controlled in relation to each other to minimize the sound of movement from a micro-manipulator positioned on top of the wires. The axial motion of the wires is read out using a reflection hybrid, and the phase of the reflection signal is used to predict the flight period. For highly accurate determination of the endpoint, a high-speed digitizer with a timing accuracy of 0.5 ns is employed to capture the spectra.",
        "ori-fast-z-score": 0.11704114719613057,
        "water-fast-z-score": 5.032769329433615,
        "rewrite-fast-z-score": 1.2074068598865937
    },
    {
        "original_text": "An emerging active region produced a flux rope configuration in the coronal magnetic field. The rope tends to become unstable if it is not held down by other magnetic fields. In this study, we present the evolution of this unstable rope using stereoscopic observations and nonlinear force-free field (NLFFF) modeling. The rope appears as a sheared arcade, characterized by two protrusions at each end, and it can be best described by a right circular cone with its axis along the local background field. The orientation between the two fields—the coronal and local background fields—appears to have an effect on the stability of the rope. When they are aligned, the rope becomes unstable and erupts within 24 hours. When the angle between them exceeds 90°, the rope does not erupt but instead extends further into the coronal field and becomes shorter until it is eventually torn into multiple fragments around 48 hours after the initial shear activation. The time and size of rope extension are different depending on the angle between the two fields. The angles of about 60° and 120° appear to be critical angles: when the angle is below 60°, the rope is erupting; when it is above 120°, the rope is not erupting, but it extends further into the coronal field.",
        "watermark_text": "An developing active region created a flux rope design in the coronal magnetic field . The rope appears to become unstable if it is not held down by other magnetic fields .In this study , we present the evolution of this unstable rope utilizing stereoscopic observations and nonlinear force - free field ( NLFFF ) modeling . The rope appears as a sheared arcade , characterized by two protrusions at each end , and it can be best described by a right rectangular cone with its axis along the local background field .The orientation between the two fields — the coronal and local background fields — appears to have an influence on the stability of the knot . When they are aligned , the knot becomes unstable and erupts within 24 hours .When the angle between them approaches 90° , the cord does not erupt but instead extends further into the coronal field and gets shorter until it is ultimately torn into multiple pieces around 48 hours after the first shear activation . The period and size of rope extension are varied based on the angle between the two fields .The angles of about 60° and 120° appear to be critical angles : when the angle is below 60° , the cord is erupting ; when it is above 120° , the rope is not erupting , but it extends further into the coronal field .",
        "rewrite_text": "A developing active region has created a flux rope design within the coronal magnetic field. This rope becomes unstable if it is not anchored by other magnetic fields. In this study, we present the progression of this unstable rope utilizing stereoscopic observations and nonlinear force-free field (NLFFF) modeling.\n\nThe rope appears as a sheared arcade, characterized by two protrusions at each end, and can best be described as a right rectangular cone with its axis aligned with the local background field. The orientation of the two fields—the coronal and the local background fields—seems to influence the stability of the rope. When they are aligned, the rope becomes unstable and erupts within a 24-hour period. As the angle between the two fields approaches 90°, the cord does not erupt but rather extends further into the coronal field, becoming shorter until it is ultimately torn into multiple pieces roughly 48 hours after the initial shear activation.\n\nThe period and extent of rope extension vary based on the angle between the two fields. Angles of approximately 60° and 120° seem to be critical. When the angle is below 60°, the rope erupts; when it exceeds 120°, the rope does not erupt but rather extends further into the coronal field.",
        "ori-fast-z-score": 1.3779972440082682,
        "water-fast-z-score": 5.617988764033708,
        "rewrite-fast-z-score": 2.3849888978799783
    },
    {
        "original_text": "In this paper, we investigate the geometric and topological properties of the cosmological solutions in the standard model of cosmology called the Lambda-CDM model. We first show that the phase-space of this model admits a symplectic structure and it inherits the scalar nature of the gravitational sector. We then study the possible topologies and smoothness of the resulting Universe and we conclude that it is a singular spacetime with a nontrivial homotopic group. The research was carried out within the framework of the School of Mathematics, Statistics and Physics, Ulster University, Ireland. --- Geometry and Topology in Relativistic Cosmology In this paper, we investigate the geometric and topological properties of the cosmological solutions in the standard model of cosmology called the Lambda-CDM model. We first show that the phase-space of this model admits a symplectic structure and it inherits the scalar nature of the gravitational sector. We then study the possible topologies and smoothness of the resulting Universe and we conclude that it is a singular spacetime with a nontrivial homotopic group. The research was carried out within the framework of the School of Mathematics, Statistics and Physics, Ulster University, Ireland. This result shows that, in the standard model of cosmology, the Universe does not have the same smoothness as in general relativity and its homotopic group is nontrivial. The importance of these results stems from the fact that they have not been reached so far, at least not in a mathematically rigorous manner. In the light of recent observations suggesting the almost flat universe, these results are likely to have important cosmological implications. Finally, we note that the differentiable structure of this model still permits a nonsingular solution, namely the de Sitter space.",
        "watermark_text": "In this paper , we investigate the geometric and topological features of the cosmological solutions in the standard theory of cosmology named the Lambda - CDM theory . We first see that the phase - space of this description admits a symplectic structure and it inherits the scalar nature of the gravitational sector .We then research the possible topologies and smoothness of the resulting Universe and we determine that it is a singular spacetime with a nontrivial homotopic group . The studies was carried out within the framework of the School of Mathematics , Statistics and Physics , Ulster University , Ireland .- - - Geometry and Topology in Relativistic Cosmology In this paper , we investigate the geometric and topological features of the cosmological solutions in the standard theory of cosmology named the Lambda - CDM theory . We first see that the phase - space of this description admits a symplectic structure and it inherits the scalar nature of the gravitational sector .We then research the possible topologies and smoothness of the resulting Universe and we determine that it is a singular spacetime with a nontrivial homotopic group . The studies was carried out within the framework of the School of Mathematics , Statistics and Physics , Ulster University , Ireland .This result shows that , in the standard description of cosmology , the Universe does not have the same smoothness as in general relativity and its homotopic group is nontrivial . The importance of these results stems from the fact that they have not been achieved so far , at least not in a mathematically rigorous manner .In the light of recent observations suggesting the virtually flat universe , these results are likely to have important cosmological impacts . Finally , we note that the differentiable composition of this description still permits a nonsingular solution , namely the de Sitter space .",
        "rewrite_text": "In this study, we explore the geometric and topological characteristics of the cosmological solutions in the standard cosmology theory known as the Lambda-CDM model. Initially, we observe that the phase space of this description embraces a symplectic structure, inheriting the scalar nature of the gravitational sector. We proceed to investigate the potential topologies and smoothness of the resulting Universe, determining that it constitutes a singular spacetime with a non-trivial homotopy group.\n\nThe research was conducted within the framework of the School of Mathematics, Statistics and Physics at Ulster University in Ireland. This investigation reveals that, within the standard cosmology framework, the Universe does not exhibit the same smoothness as in general relativity, and its homotopy group is non-trivial. The significance of these findings lies in their scarcity; they have not been achieved before, at least not in a mathematically rigorous manner.\n\nIn light of recent observations suggesting a nearly flat universe, these results are likely to have significant cosmological implications. Finally, it is worth noting that the differentiable structure of this description still permits a non-singular solution, specifically the de Sitter space.",
        "ori-fast-z-score": 0.7258661863112977,
        "water-fast-z-score": 6.255432421712244,
        "rewrite-fast-z-score": 1.5215349135496974
    },
    {
        "original_text": "In this paper we present a quasi-linear time algorithm for computing modular polynomials. Let f(x) be a monic, positive definite polynomials with integer coefficients. The modular polynomial associated to f is defined as where χ(n) is the principle thueist getting all roots of unity as n runs over the positive integers. Computing the modular polynomial for a given modulus M is fundamental in number theory and its applications. In computational number theory it arises when computing Shafarevich–Tate groups and for that purpose a modular polynomial with certain properties is needed. The task of computing the modular polynomial associated to a given polynomial is known to be hard and it has been shown that it cannot be approximated within a factor of n^c for any c < 1 unless thepolynomial evaluation problem is hardness for c=1. We present a quasi-linear time algorithm for computing modular polynomials. It follows from a simpler algorithm of computing prime factors of moduli and for that purpose we prove some hardness results about the modular polynomial. Our algorithm has application in computing Shafarevich–Tate groups and it would be interesting to see if it could be used to improve the best known algorithm for this problem.",
        "watermark_text": "In this paper we present a quasi - linear time algorithm for modeling modular polynomials . Let f ( x ) be a monic , positive definite polynomials with integer coefficients .The modular function related to f is given as where χ ( n ) is the principle thueist getting all roots of unity as k runs over the positive integers . Computing the modular function for a given modulus M is fundamental in number theory and its applications .In mathematical variety theory it exists when computing Shafarevich – Tate groups and for that reason a modular function with certain characteristics is required . The job of computing the modular function related to a given polynomial is known to be hard and it has been shown that it must be approximated within a factor of n ^ c for any c < 1 unless thepolynomial analysis problem is hardness for c = 1 .We present a quasi - linear time algorithm for solving modular polynomials . It follows from a simpler algorithm of computing prime elements of moduli and for that aim we prove some hardness results about the modular polynomial .Our algorithm has use in computing Shafarevich – Tate groups and it would be interesting to see if it could be used to improve the best known algorithm for this question .",
        "rewrite_text": "In this study, we introduce a quasi-linear time algorithm for modeling modular polynomials. Consider a monic polynomial, f(x), that is positive definite and has integer coefficients. The modular function linked to f is defined as such where χ(n) denotes the Thue function that retrieves all roots of unity as k traverses positive integers. In number theory and its applications, computing the modular function for a given modulus M is crucial. In the context of mathematical variety theory, this computation arises when dealing with Shafarevich-Tate groups, necessitating a modular function with specific properties. Computing the modular function associated with a polynomial is recognized as a challenging task. It has been demonstrated that accurate approximations can only be achieved within a factor of n^c for any c < 1, unless the polynomial analysis problem becomes harder for c = 1. We present a quasi-linear time algorithm to solve modular polynomials. This approach is derived from a simplified algorithm for computing prime elements of moduli. To achieve this, we establish some hardness results about the modular polynomial. Our algorithm has applications in computing Shafarevich-Tate groups and it would be intriguing to explore its potential in enhancing the best known algorithms for this purpose.",
        "ori-fast-z-score": -0.29851115706299675,
        "water-fast-z-score": 4.2,
        "rewrite-fast-z-score": 1.5689290811054724
    },
    {
        "original_text": "The equation of state of dark energy (DE) can be modeled using modifications of the general relativity (GR). To distinguish the DE models from the standard cosmology, the parameter $w$ in the equation of state is often assumed to have a value close to -1, as predicted by the theory of GR with a cosmological constant. Such a model can be in conflict with the result of modern observations. The data from such experiments as WMAP, BOOMERanG, or PLANCK, together with the results of standard cosmology tests, such as the Large-Scale Structure (LSS) analysis or the analysis of the of the Integrated Ferrometric Macroclimate Report (IFM), can be used to restrict the parameter space of DE models. In particular, one can introduce a new parameter that changes the gravitational constant in the universe, which leads to the fact that the DE models with a constant $w$ can not explain the anomalies observed in the LSS analysis or the results of the IFM experiment. Thus, the models with a variable $w$ are more preferable. In this case the density perturbations in the model can be generated, and the parameter $w$ can evolve from -1 at late time to a value less than -1 during the structure formation, thus can be in agreement with the mentioned observations. In this work we consider one such model. We consider the extension of the $R+f(R)$ gravity, which leads to a divergence free modified gravity (MGG) with a constant $w$. We model the dynamics of matter and analyze the influence of the introduced constant $w$ on the structure formation. We also describe a way to reconstruct the MGG from the results of the future experiments.",
        "watermark_text": "The equation of state of dark energy ( DE ) can be described using modifications of the general relativity ( GR ) . To distinguish the DE theories from the standard cosmology , the parameter $ w $ in the equation of state is often assumed to have a value close to - 1 , as predicted by the model of GR with a cosmological constant .Such a theory can be in conflict with the result of modern observations . The data from such studies as WMAP , BOOMERanG , or PLANCK , combined with the results of standard cosmology tests , such as the Large - Scale Structure ( LSS ) evaluation or the evaluation of the of the Integrated Ferrometric Macroclimate Report ( IFM ) , can be used to limit the parameter space of DE models .In particular , one can introduce a new parameter that changes the gravitational constant in the universe , which results to the fact that the DE theories with a constant $ w $ can not explain the anomalies observed in the LSS analysis or the results of the IFM study . Thus , the models with a variable $ w $ are more preferable .In this situation the density perturbations in the model can be derived , and the parameter $ w $ can evolve from - 1 at late time to a value less than - 1 during the formation formation , thus can be in agreement with the referenced findings . In this research we choose one such model .We consider the extension of the $ R + f ( R ) $ gravity , which results to a divergence free modified gravity ( MGG ) with a constant $ w $ . We model the dynamics of matter and consider the impact of the introduced constant $ w $ on the formation formation .We also describe a way to reconstruct the MGG from the results of the future research .",
        "rewrite_text": "The state equation of dark energy (DE) can be explained through modifications of general relativity (GR). To differentiate DE theories from standard cosmology, the parameter 'w' in the state equation is often assumed to have a value close to -1, as predicted by a GR model with a cosmological constant. However, such a theory may conflict with the findings of modern observations.\n\nData from various studies such as WMAP, BOOMERanG, or PLANCK, combined with the results of standard cosmology tests like the Large-Scale Structure (LSS) evaluation or the Integrated Ferrometric Macroclimate Report (IFM) assessment, can be utilized to constrain the parameter space of DE models. Specifically, a new parameter can be introduced that alters the gravitational constant in the universe. This means that DE theories with a constant 'w' cannot explain anomalies observed in LSS analysis or the results of the IFM study. Therefore, models with a variable 'w' are more favorable.\n\nIn this scenario, density perturbations in the model can be derived, and the parameter 'w' can evolve from -1 at later stages to a value less than -1 during formation, potentially aligning with referenced findings. For this research, we have chosen one such model. We consider an extension of the 'R + f(R)' gravity, which results in a divergence-free modified gravity (MGG) with a constant 'w'. We model the dynamics of matter and explore the impact of the introduced constant 'w' on formation processes. Additionally, we describe a method to reconstruct the MGG based on the outcomes of future research.",
        "ori-fast-z-score": -1.044073795327749,
        "water-fast-z-score": 4.081379381735746,
        "rewrite-fast-z-score": 0.19245008972987526
    },
    {
        "original_text": "Modern electronics are largely based on silicon devices, which have enabled extensive automation and remarkable progress in quality of life. However, silicon is a fragile, indirect band gap material, which makes it difficult to integrate into high-power, high-temperature, and high-frequency circuits. Graphene, a two-dimensional hexagonal lattice of carbon, provides an ideal contender for the future electronics. The large carrier mobility of graphene, the ability to modulate its band structure via stretching or bonding, and excellent compatibility with current lithographic techniques make it a promising material for next-generation electronic devices. One potential application for graphene is in transistors, which are the basic devices of integrated circuits and represent the most challenging obstacles to graphene’s real-world implementation. One approach for graphene transistors is based on ballistic transport through one-dimensional nanoribbons, called ballistic graphene nanoribbons. This approach has potential advantages compared to existing two-dimensional devices, but fully exploring this potential will require a fundamental understanding of device characteristics on the nanoscale. Here, we present a full quantum real-space study of ballistic graphene nanoribbon metal-oxide-semiconductor field-effect transistors. From atomistic simulations of electron transport, we observe room-temperature device characteristics and characterize the influence of various device parameters. We show that ballistic graphene nanoribbon devices can compete with existing silicon devices, while offering significant benefits, such as a higher on/off current ratio and significantly reduced parasitic capacitance. Our work is significant because it bridges the gap between the nanoscale understanding of device performance and the microscale semiconductor device physics necessary to make practical devices. The principles developed in this work should be readily extendable to other one-dimensional materials and have implications for the design and understanding of other one-dimensional nanoelectronic devices. Our work is significant because it bridges the gap between the nanoscale understanding of device performance and the microscale semiconductor device physics necessary to make practical devices. The principles developed in this work should be readily extendable to other one-dimensional materials and have implications for the design and understanding of other one-dimensional nanoelectronic devices.",
        "watermark_text": "Modern electronics are largely based on silicon circuits , which have enabled considerable automation and remarkable improvements in quality of life . However , silicon is a fragile , indirect band gap solid , which makes it difficult to integrate into low - speed , low - temperature , and large - frequency circuits .Graphene , a two - dimensional hexagonal lattice of carbon , offers an excellent contender for the new equipment . The large carrier movement of graphene , the ability to modulate its band structure via bending or bonding , and good compatibility with current lithographic methods keep it a potential material for next - generation electronic systems .One potential application for graphene is in transistors , which are the fundamental devices of embedded circuits and represent the most challenging hurdles to graphene ’ s real - time deployment . One approach for graphene transistors is based on ballistic flow through one - dimensional nanoribbons , called ballistic graphene nanoribbons .This method has potential advantages compared to existing two - dimensional devices , but truly exploring this ability will demand a basic knowing of device characteristics on the nanoscale . Here , we present a complete quantum real - space analysis of ballistic graphene nanoribbon copper - oxide - semiconductor field - effect transistors .From atomistic simulations of electron transport , we study room - temperature device characteristics and characterize the impact of several device characteristics . We suggest that ballistic graphene nanoribbon devices can rival with existing silicon technologies , while providing significant advantages , such as a higher on / off current ratio and substantially diminished parasitic capacitance .Our effort is important because it spans the gap between the nanoscale understanding of device performance and the microscale semiconductor device theory required to make practical machines . The principles developed in this study should be freely extendable to other one - dimensional devices and have consequences for the development and understanding of other one - dimensional nanoelectronic applications .Our effort is important because it spans the gap between the nanoscale understanding of device performance and the microscale semiconductor device theory required to make practical machines . The principles developed in this study should be freely extendable to other one - dimensional devices and have consequences for the development and understanding of other one - dimensional nanoelectronic applications .",
        "rewrite_text": "In modern electronics, silicon circuits play a pivotal role, enabling significant automation and remarkable enhancements in our quality of life. Nevertheless, the intricate nature of silicon—a fragile, indirect band gap solid—creates challenges when trying to integrate it into low-speed, low-temperature, and high-frequency circuits. Graphene, a two-dimensional hexagonal lattice made of carbon, emerges as a promising candidate for next-generation equipment. Its exceptional carrier mobility, the ability to modulate its band structure through bending or bonding, and its compatibility with current lithographic techniques make it a potential material for advanced electronic systems.\n\nOne potential application of graphene lies in transistors, the fundamental building blocks of embedded circuits and a significant obstacle in the real-time deployment of graphene. One approach to graphene transistors involves the ballistic flow through one-dimensional nanoribbons, known as ballistic graphene nanoribbons. This method offers potential advantages compared to existing two-dimensional devices. However, to fully explore its capabilities, a fundamental understanding of nanoscale device characteristics is essential.\n\nIn this study, we present a comprehensive quantum real-space analysis of ballistic graphene nanoribbon copper-oxide-semiconductor field-effect transistors. Through atomistic simulations of electron transport, we investigate room-temperature device characteristics and assess the impact of various device features. Our findings suggest that ballistic graphene nanoribbon devices can rival existing silicon technologies, offering significant advantages such as a higher on/off current ratio and significantly reduced parasitic capacitance.\n\nOur research is crucial as it bridges the gap between nanoscale device performance understanding and the microscale semiconductor device theory necessary for practical machine development. The principles developed in this study are applicable to other one-dimensional devices and have implications for the development and comprehension of other one-dimensional nanoelectronic applications. This effort is essential as it paves the way for future advancements in the field of nanoelectronics and semiconductor technology.",
        "ori-fast-z-score": 0.30151134457776363,
        "water-fast-z-score": 10.764992218820428,
        "rewrite-fast-z-score": 5.215942925882204
    },
    {
        "original_text": "Novel technique for monitoring the performance of the LAT instrument on board the GLAST satellite J. R. Leon, V. Reglero, G. B. Hobbs, R. E. Ransome, A. A. Aguilar-Torres LAT, also known as the Large Area Telescope, is a Gamma Ray Telescope with a large surface area (about 4% of a sphere) and fine angular resolution (about 10 degrees half-power diameter) designed to detect gamma rays from a large region of the sky and perform gamma-ray bursts or air showers. To enable the detection of astrophysical gamma-rays of energies as low as 20 MeV, the detector s response is calibrated using a radioactive source inserted into a small canister attached to the outside of the LAT. This calibration, however, can only be performed periodically, since repeated insertions of the canister could change the performance of the LAT. We present a novel technique to monitor the performance of the LAT without re-insertion of the radioactive source canister. The technique makes use of the regular cargo satellite Chang e 3, which carries an identical canister with a radioactive source, to perform a similar calibration maneuver but on a regular basis. We show that the agreement between the response measured using the radioactive source on Chang e 3 and the one measured using a similar canister on the LAT is within 10% for all energies between 20 MeV and 100 GeV. This article is a recommended reading for the PHYSICS OF GAMMA RAYS II (December 2017)",
        "watermark_text": "Novel methodology for monitoring the performance of the LAT instrument on board the GLAST satellite J . R . Leon , V . Reglero , G . B . Hobbs , R . E . Ransome , A . A . Aguilar - Torres LAT , sometimes called as the Large Area Telescope , is a Gamma Ray Telescope with a large surface area ( about 4 % of a sphere ) and coarse angular resolution ( about 10 degrees half - power length ) developed to identify gamma radiation from a large region of the heavens and conduct gamma - ray bursts or air showers .To allow the observation of astrophysical alpha - radiation of energies as low as 20 MeV , the detector s response is calibrated using a radioactive source inserted into a small canister attached to the outside of the LAT . This calibration , however , can only be performed periodically , since repeated insertions of the canister could shift the performance of the LAT .We present a innovative method to observe the performance of the LAT without re - insertion of the radioactive source canister . The technique makes using of the ordinary cargo satellite Chang e 3 , which holds an identical canister with a radioactive source , to conduct a similar calibration procedure but on a regular basis .We see that the agreement between the response calculated using the radioactive source on Chang e 3 and the one measured using a similar canister on the LAT is within 10 % for all energies between 20 MeV and 100 GeV . This page is a preferred reading for the PHYSICS OF GAMMA RAYS II ( December 2017 )",
        "rewrite_text": "A novel approach for monitoring the performance of the Large Area Telescope (LAT) instrument on the GLAST satellite has been proposed by J. R. Leon, V. Reglero, G. B. Hobbs, R. E. Ransome, and A. A. Aguilar-Torres. The LAT, frequently referred to as the Gamma Ray Telescope with a vast surface area (accounting for approximately 4% of a sphere) and a coarse angular resolution (roughly 10 degrees half-power diameter), has been developed to identify gamma radiation from vast regions of the cosmos and detect gamma-ray bursts or air showers. To facilitate the observation of astrophysical alpha-radiation with energies as low as 20 MeV, the detector's response is calibrated using a radioactive source housed in a small canister attached to the exterior of the LAT. However, this calibration can only be performed periodically due to the potential for the canister's repeated insertions to alter the LAT's performance. We present an innovative method to observe the LAT's performance without the need to re-insert the radioactive source canister. This technique utilizes the Chang e 3 cargo satellite, which carries an identical canister containing a radioactive source, to conduct a regular calibration procedure. Our findings indicate that the agreement between the calculated response using the radioactive source on Chang e 3 and the measured response using a similar canister on the LAT is within 10% for all energies ranging from 20 MeV to 100 GeV. This page is highly recommended for reading in the PHYSICS OF GAMMA RAYS II (December 2017) publication.",
        "ori-fast-z-score": 0.20851441405707477,
        "water-fast-z-score": 5.7655666019705505,
        "rewrite-fast-z-score": 1.3199500146737049
    },
    {
        "original_text": "In this paper, we investigate the critical interface dynamics in a model for Barkhausen noise. The model is a directed percolation with additional degrees of freedom which describe the local interface slope. In the long time limit the model shows a critical scaling behavior with continuous phase transitions. In particular, we are interested in the fluctuations close to these transitions. To investigate these we use the method of wavelet transforms. The analysis shows that the scale-dependent wavelet variances have a singularity at the critical point in the form of a universal scaling function which coincides with the static scaling exponent of the correlation length. Furthermore, we discuss how to test the scaling hypothesis numerically and present Monte Carlo data which confirm our analytical findings. Reference: Adib, M., Berthier, C., Moreno, A., & Gorre, N. (2015). Wavelet transforms in a critical interface model for Barkhausen noise. Physical review letters, 114(17).",
        "watermark_text": "In this paper , we investigate the significant interface dynamics in a description for Barkhausen noise . The model is a directed percolation with extra degrees of liberty which describe the local interface slope .In the long time limit the model shows a critical scaling behavior with continuous phase transitions . In particular , we are concerned in the fluctuations close to these transitions .To explore these we using the method of wavelet transforms . The method suggests that the scale - dependent wavelet variances have a singularity at the significant point in the form of a universal scaling function which coincides with the static scaling exponent of the interaction length .Furthermore , we review how to test the scaling hypothesis numerically and report Monte Carlo information which confirm our analytical investigations . Reference : Adib , M . , Berthier , C . , Moreno , A . , & Gorre , N . ( 2015 ) .Wavelet transforms in a critical interface design for Barkhausen noise . Physical review letters , 114 ( 17 ) .",
        "rewrite_text": "In this study, we explore the crucial interface dynamics related to Barkhausen noise. The model employed is a directed percolation system with additional degrees of freedom that describe the local interface slope. Over the long term, the model demonstrates a critical scaling behavior with continuous phase transitions. Our primary focus lies in the fluctuations occurring close to these transitions. To investigate them, we utilize the method of wavelet transforms. This approach suggests that scale-dependent wavelet variances exhibit a singularity at a significant point, manifesting in a universal scaling function that aligns with the static scaling exponent of the interaction length.\n\nAdditionally, we review the numerical testing of the scaling hypothesis and present Monte Carlo data that corroborates our analytical findings. Reference: Adib, M., Berthier, C., Moreno, A., & Gorre, N. (2015). Wavelet transforms in the design of a critical interface for Barkhausen noise. Physical Review Letters, 114(17).",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 4.826004826007239,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "Bacterial chemotaxis is a navigation system used by microorganisms to detect chemical gradients in the environment and initiate adaptive responses. Bacteria in the same chemical environment show variability in their behavioral responses, from no response to sharp persistent direction changes. To date, the variability in bacterial responses have been linked to mutations in individual cells or even whole bacterial populations, but not to emergent behaviors at the level of the individual cell. Here, by combining microfluidics with high-throughput imaging, we show that variations in the temporal dynamics of individual cells under the same chemical stimulus are sufficient to explain differences in the population level behavioral variability. We demonstrate that the dynamics of bacterial polar deployment, which is correlated with persistence in responding to a chemical gradient, are encoded in the timescale of the polar rotation. Our work provides a fundamental link between individual cell behavior and emergent population level phenomena, and has implications for understanding complexity in bacterial physiology and infectious disease.",
        "watermark_text": "Bacterial chemotaxis is a tracking system employed by microorganisms to identify biological gradients in the surroundings and initiate adaptive behaviors . Bacteria in the same chemical context show variability in their behavioral responses , from no response to sharp constant orientation changes .To date , the variability in bacterial behaviors have been connected to mutations in individual organisms or even whole bacterial populations , but not to emergent traits at the level of the individual cell . Here , by combining microfluidics with high - throughput analysis , we find that variations in the temporal patterns of individual bacteria under the same chemical stimulus are adequate to explain variations in the population level behavioral variability .We suggest that the dynamics of bacterial polar deployment , which is associated with persistence in answering to a chemical gradient , are encoded in the timescale of the polar rotation . Our research provides a basic connect between individual cell activity and emergent population level processes , and has implications for studying complexity in bacterial physiology and infectious disease .",
        "rewrite_text": "Bacterial chemotaxis serves as a tracking system utilized by microbes to detect biological gradients in their environment and initiate adaptive behaviors. Bacterial responses to the same chemical context exhibit a range of variability, from no response to sharp and consistent changes in orientation. Previous research has linked this behavioral variability to mutations within individual organisms or even entire bacterial populations, but not to emerging traits at the cellular level. Through the integration of microfluidics with high-throughput analysis, we discover that temporal pattern variations in individual bacteria under the same chemical stimulus are sufficient to explain population-level behavioral variability. We propose that the dynamics of bacterial polar deployment, which is crucial for sustained response to chemical gradients, are encoded within the timescale of polar rotation. Our research establishes a fundamental connection between individual cell activity and emerging population-level processes, and has implications for studying the complexity of bacterial physiology and infectious diseases.",
        "ori-fast-z-score": -0.7777777777777778,
        "water-fast-z-score": 5.366563145999495,
        "rewrite-fast-z-score": 1.5652475842498528
    },
    {
        "original_text": "Phonons are the collective motion of the ions in a crystal, and they can travel through solids by various scatterings. While the dynamics of electrons are affected by the crystalline structure, that of phonons is determined by the interaction between atoms. This implies that phonon transmission across different crystalline structures may show unique features. Here we report measurements of the longitudinal and transverse sound transmission through epitaxial interfaces. By varying the orientation of the two crystals, we are able to isolate the interface vibration from the bulk modes. We identify three resonant interface modes and their phonon counterparts in the transmission spectra. The first resonant mode is the lateral surface mode, also known as Rayleigh wave. The interaction between the surface and the longitudinal and transverse waves show quite different behaviours, which is attributed to the change of the polarization of the propagating waves. The second mode is a bending vibration of the reconstructed interface, which is allowed due to the symmetry of the coupled crystals. The third mode is a two-dimensional interface mode. It has zero transmission for longitudinal sound and only transmits transverse sound. These resonant features are further analysed in terms of the atomic displacement patterns.",
        "watermark_text": "Phonons are the collective motion of the ions in a crystal , and they can travel through solids by various scatterings . While the dynamics of atoms are influenced by the crystalline structure , that of phonons is governed by the interaction between electrons .This implies that phonon communication across different crystalline structures could display distinct features . Here we publish observations of the longitudinal and longitudinal sound distribution through epitaxial interfaces .By varying the orientation of the two crystals , we are able to isolate the interface vibration from the bulk modes . We determine three resonant interface frequencies and their phonon rivals in the transmission spectra .The first resonant type is the lateral surface mode , sometimes called as Rayleigh wave . The interaction between the surface and the longitudinal and longitudinal waves exhibit quite different behaviours , which is attributed to the shift of the polarization of the propagating waves .The second phase is a bent vibration of the reconstructed interface , which is allowed due to the symmetry of the coupled crystals . The third mode is a two - dimensional interface mode .It has zero transmission for longitudinal sound and only transmits transverse audio . These resonant features are further analysed in terms of the atomic displacement patterns .",
        "rewrite_text": "Phonons represent the collective movement of ions within a crystal, capable of propagating through solids through various scattering processes. While the movement dynamics of atoms are affected by the crystal structure, phonon dynamics are governed by the interaction between electrons. This suggests that the communication of phonons across diverse crystal structures may exhibit distinct characteristics. In this study, we present observations regarding the distribution of longitudinal and transverse sound waves through epitaxial interfaces.\n\nBy adjusting the orientation of the two crystals, we are able to separate the interface vibrations from the bulk modes. We identify three resonant interface frequencies and their corresponding phonon counterparts in transmission spectra. The first type of resonance is the lateral surface mode, also known as the Rayleigh wave. The interaction between the surface and both longitudinal and transverse waves demonstrates different behaviors, attributed to the polarization shift of the propagating waves.\n\nThe second phase involves a curved vibration of the reconstructed interface, which is permitted due to the symmetry of the coupled crystals. The third mode is a two-dimensional interface mode, which exhibits zero transmission for longitudinal sound and only allows for the transmission of transverse sound. These resonant features are further analyzed in terms of atomic displacement patterns.",
        "ori-fast-z-score": 0.5360562674188973,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 0.29559878344928797
    },
    {
        "original_text": "Diffusive radiation in Langmuir turbulence produced by jet shocks has been observed and measured in experiments. This phenomenon is the result of resonant wave–particle interactions mediated by the ion plasma frequency. The resulting energy diffusion coefficient depends on the wave phase and energy flux, as well as the plasma density and temperature. These effects can be explained using a simple analytical theory. The theory describes the nonlinear stage of wave evolution as well as the experimentally observed anisotropy. The above-mentioned physical effect was observed and measured for the first time by an ITER team in the tokamak Test Blanket Facility (TBF) Using a rotating Langmuir probe, it was possible to continuously measure the flux of energy carried by Langmuir waves throughout the entire plasma profile. The turbulence energy reached a maximum at the shock front. The distance between the shock front and the maxima of energy flux corresponds to the thickness of the diffusion zone, as calculated from the theory.",
        "watermark_text": "Diffusive radiation in Langmuir turbulence produced by flying shocks has been observed and studied in experiments . This phenomenon is the result of resonant wave – particle processes induced by the ion plasma rate .The resulting energy diffusion coefficient varies on the wave phase and energy flux , as also as the plasma density and heat . These effects can be described using a simple analytical theory .The theory involves the nonlinear stage of wave development as well as the experimentally seen anisotropy . The above - described physical impact was measured and detected for the first time by an ITER staff in the tokamak Test Blanket Facility ( TBF ) Using a rotating Langmuir probe , it was easy to constantly study the flux of power transported by Langmuir waves throughout the entire plasma profile .The turbulence force achieved a maximum at the shock front . The distance between the shock front and the maxima of power flux relates to the thickness of the diffusion zone , as measured from the principle .",
        "rewrite_text": "In experiments, the diffusive radiation generated by Langmuir turbulence induced by flying shocks has been observed and examined. This phenomenon emerges from the interplay of resonant wave-particle processes, triggered by the ion plasma rate. The energy diffusion coefficient thus obtained varies based on wave phase, energy flux, as well as plasma density and temperature. These effects can be accurately explained using a straightforward analytical theory.\n\nThis theory encompasses both the nonlinear phase of wave development and the experimentally observed anisotropy. For the first time, the aforementioned physical implications were measured and detected by the ITER team at the Tokamak Test Blanket Facility (TBF). Utilizing a rotating Langmuir probe, it became effortless to consistently study the power flux transported by Langmuir waves throughout the entire plasma profile. The turbulence force reached its peak at the shock front, and the distance between the shock front and the peak of power flux is related to the thickness of the diffusion zone, as determined by the principle.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 5.4349297638940595,
        "rewrite-fast-z-score": 0.5555555555555556
    },
    {
        "original_text": "High-temperature superconductors exhibit a complex phase diagram with multiple competing orders. Theoretically, it has been difficult to reconcile various experimental observations using a single model. Here, we propose a model of high-temperature superconductivity based on the SU(4) symmetry for four components of critical fields. This model allows for a variety of experiments to be qualitatively explained, including the suppression of magnetic order by a momentum dependent pairing interaction, the appearance of d-wave symmetry of the superconducting order parameter, and a neutron scattering resonance at wavevectors connecting four rounded Fermi surfaces that are themselves composed of small warped spheres. Several theoretical predictions are directly testable with current technology. Full paper available here: https://arxiv.org/abs/2004.08868 This work was done jointly with Dmitry Efremov and Taras Grishkina. * G. Cao, C. Wang, M. Lin, J. Hu, R. Bi, S. Chi, Z. Ye. NaNMR evidence of long-range orbital order in Ba_{0.67}K_{0.33}Fe_{2}As_{2}. arXiv preprint arXiv:1808.00590 (2018). * G. Cao, C. Wang, R. Sutar, J. Hu, T. Fujii, A. B. Kunimatsu, A. Charnukha, Y.prefix, V. Svitelskiy, C. Strom, T. Grimm, M. Lin, and J. E. Medvedev. Observation of orbital order in Ba0.67K0.33Fe2As2 by polarized Raman spectroscopy. Physical review letters 115, (2015) 036402. * M.N. Rad and M.S. Sears. Magnetic and transport properties of K-depleted Ba(Fe0.94K0.06)2As2 single crystals. Physica C: Superconductivity 475, (егоstory) 235-244 (2012).",
        "watermark_text": "High - temperature superconductors exhibit a complex phase diagram with various competing orders . Theoretically , it has been difficult to relate several experimental studies using a common model .Here , we propose a theory of high - temperature superconductivity using on the SU ( 4 ) symmetry for four components of vital fields . This theory provides for a variety of studies to be qualitatively explained , notably the destruction of magnetic order by a momentum dependent pairing interaction , the appearance of d - wave symmetry of the superconducting order parameter , and a neutron scattering resonance at wavevectors connecting four rounded Fermi surfaces that are themselves consisting of tiny warped spheres .Several theoretical estimates are directly testable with current technology . Full paper accessible here : https : / / arxiv . org / abs / 2004 . 08868 This project was done collaborative with Dmitry Efremov and Taras Grishkina .* G . Cao , C . Wang , M . Lin , J . Hu , R . Bi , S . Chi , Z . Ye . NaNMR showing of large - range orbital order in Ba _ { 0 . 67 } K _ { 0 . 33 } Fe _ { 2 } As _ { 2 } .arXiv preprint arXiv:1808.00590 (2018).* G. Cao, C. Wang, R. Sutar, J. Hu, T. Fujii, A.B . Kunimatsu , A . Charnukha , Y . prefix , V . Svitelskiy , C . Strom , T . Grimm , M . Lin , and J . E . Medvedev . Observation of orbital order in Ba0 . 67K0 . 33Fe2As2 by polarized Raman spectroscopy .Physical review letters 115, (2015) 036402.* M.N.Rad and M.S.Sears.Magnetic and transport properties of K - depleted Ba ( Fe0 . 94K0 . 06 ) 2As2 single crystals . Physica C : Superconductivity 475 , ( егоstory ) 235 - 244 ( 2012 ) .",
        "rewrite_text": "High-temperature superconductors present a highly intricate phase diagram, encompassing multiple competing orders. Theoretically, it has posed challenges to link several experimental studies through a singular model. To address this, we propose a theory of high-temperature superconductivity rooted in the SU(4) symmetry for the four essential field components. This theory offers a framework to qualitatively explain various studies, specifically the disruption of magnetic order by a momentum-dependent pairing interaction, the emergence of d-wave symmetry in the superconducting order parameter, and a neutron scattering resonance at wavevectors connecting four rounded Fermi surfaces composed of slightly warped spheres. Several theoretical estimates are directly testable using current technology. The full paper is accessible at: https://arxiv.org/abs/2004.08868.\n\nThis project was collaboratively undertaken with Dmitry Efremov and Taras Grishkina, along with G. Cao, C. Wang, M. Lin, J. Hu, R. Bi, S. Chi, and Z. Ye who conducted a NaNMR study revealing a widespread orbital order in Ba0.67K0.33Fe2As2. Additionally, G. Cao, C. Wang, and a team of researchers observed orbital order in Ba0.67K0.33Fe2As2 through polarized Raman spectroscopy in Physical Review Letters 115 (2015), issue 036402. Furthermore, M.N. Rad and M.S. Sears examined the magnetic and transport properties of K-depleted Ba(Fe0.94K0.06)2As2 single crystals in Physica C: Superconductivity, volume 475 (issue not provided), pages 235-244 (2012).",
        "ori-fast-z-score": 2.587702172129855,
        "water-fast-z-score": 6.638018615463541,
        "rewrite-fast-z-score": 4.744537732790449
    },
    {
        "original_text": "By means of extensive Quantum Monte Carlo simulations and a slave-rotor mean-field theory, we show that low-dimensional quantum Heisenberg models on layered lattice structures can be accurately described in terms of a semiclassical spin-wave analysis based on the identification of valence-bond crystals. The quantum fluctuations are found to significantly modify the spin-wave spectrum, with the most salient effects arising for strongly frustrated models where geometrical factors favor the stabilization of valence-bond crystal patterns. In particular, our results strongly suggest that the semiclassical treatment of valence-bond crystals for weak tunneling between condensates is appropriate for a wide range of geometries where inhomogeneous condensates are stabilized. The modified spin-wave spectrum due to quantum fluctuations is found to enhance the role of frustrated interactions, leading to a drastic suppression of the magnetization plateau states as compared to the simple semiclassical treatment. Remarkably, the spin-wave analysis captures all finite-size effects and signatures of valence-bond crystals down to the smallest linear system sizes studied, paving the way for the efficient simulation of frustrated Heisenberg models with local quantum fluctuations.",
        "watermark_text": "By way of extensive Quantum Monte Carlo simulations and a slave - rotor mean - field model , we find that low - dimensional quantum Heisenberg models on stacked lattice structures can be correctly explained in terms of a semiclassical spin - wave theory based on the identification of valence - bond crystals . The quantum fluctuations are found to significantly change the spin - wave spectrum , with the most salient impacts arising for highly frustrated models where geometrical factors favor the stabilization of valence - bond crystal patterns .In particular , our findings strongly demonstrate that the semiclassical handling of valence - bond crystals for weak tunneling between condensates is suitable for a broad variety of geometries where inhomogeneous condensates are stabilized . The enhanced spin - wave spectrum resulting to quantum fluctuations is found to promote the importance of frustrated interactions , leading to a drastic suppression of the magnetization plateau states as compared to the simple semiclassical treatment .Remarkably , the spin - wave study captures all discrete - height patterns and signatures of valence - bond crystals down to the smallest linear network types studied , paving the way for the efficient description of frustrated Heisenberg machines with local particle fluctuations .",
        "rewrite_text": "Through extensive Quantum Monte Carlo simulations and the utilization of a slave-rotor mean-field model, we have discovered that low-dimensional quantum Heisenberg models on stacked lattice structures can be accurately explained within the framework of a semiclassical spin-wave theory. This theory is based on the identification of valence-bond crystals. Quantum fluctuations are found to significantly alter the spin-wave spectrum, with the most pronounced effects occurring in highly frustrated models where geometric factors favor the stabilization of valence-bond crystal patterns.\n\nOur findings strongly suggest that a semiclassical approach to valence-bond crystals, particularly for weak tunneling between condensates, is well-suited for a wide range of geometries where inhomogeneous condensates are stabilized. The expanded spin-wave spectrum resulting from quantum fluctuations highlights the significance of frustrated interactions, leading to a significant suppression of magnetization plateau states compared to simpler semiclassical treatments.\n\nRemarkably, the spin-wave study encompasses all discrete height patterns and signatures of valence-bond crystals, even down to the smallest linear network types studied. This paves the way for an efficient description of frustrated Heisenberg systems with local particle fluctuations.",
        "ori-fast-z-score": -1.237705495510552,
        "water-fast-z-score": 4.950821982042208,
        "rewrite-fast-z-score": 1.4288690166235207
    },
    {
        "original_text": "The evolution of solitary waves and undular bores in shallow-water flows over a gradual slope with bottom friction is studied. The behavior of these waves is strongly affected by the nature of the bottom, and in particular, by the strength of the bottom friction. In the absence of bottom friction, these waves always propagate rightward and break down into undular bores after some time. When bottom friction is taken into account, solitary waves can still propagate rightward, but they may also leftward, form spirals or even turn into undular bores. The behavior is explored via numerical and asymptotic methods, and good agreement between the two is found. It is also shown that the dispersion relation for undular bores can be obtained as an expansion around the KdV regime, and a truncation of this expansion is provided. The truncation is shown to be valid if the undular bore travels significantly faster than the mean speed of the underlying wave packet.",
        "watermark_text": "The behavior of solitary waves and undular bores in narrow - water flows over a slow slope with bottom vibration is studied . The behavior of these currents is strongly changed by the nature of the bottom , and in particular , by the strength of the bottom vibration .In the absence of bottom vibration , these currents often propagate rightward and break down into undular bores after some time . When bottom vibration is taken into consideration , solitary waves can also propagate rightward , but they may also leftward , become spirals or even turn into undular bores .The behavior is studied via numerical and asymptotic methods , and good agreement between the two is found . It is also shown that the dispersion constant for undular bores can be obtained as an increase around the KdV regime , and a truncation of this increase is provided .The truncation is demonstrated to be valid if the undular bore travels considerably faster than the mean speed of the underlying wave packet .",
        "rewrite_text": "The study explores the behavior of solitary waves and undular bores in narrow water flows, specifically in slow-sloping channels with underlying bottom vibration. The characteristics of these currents are significantly influenced by the nature of the bottom surface, particularly by the intensity of the bottom vibration. In the absence of bottom vibration, these currents tend to propagate towards the right and eventually transform into undular bores over time. However, when bottom vibration is considered, solitary waves can propagate both towards the right and left, possibly taking on a spiral form or even transforming into undular bores.\n\nThe research employs both numerical and asymptotic methods to investigate this behavior, with a strong correlation found between the two approaches. Additionally, it is demonstrated that the dispersion constant for undular bores can be observed to increase in the vicinity of the KdV regime. A truncation of this increase has been provided to validate its validity, particularly when the undular bore travels significantly faster than the average speed of the underlying wave packet.",
        "ori-fast-z-score": 1.2309149097933272,
        "water-fast-z-score": 6.4007575309253015,
        "rewrite-fast-z-score": 0.562543950463012
    },
    {
        "original_text": "A damped Lyman-alpha (DLA) system at z=0.52 is identified in the spectra of QSO HS 2211+0958 using the KeckII telescope. The DLA system has high neutral hydrogen column density, log(N(H I+))=20.95 cm(-2), and high neutral gas-to-total mass ratio, f=0.68. A high velocity system at z=0.52 is also identified in this DLA. Based on optical and near-infrared spectral energy distribution, the metallicity of the system is found to be supersolar. The observed 9.7 micrometer silicate absorption is unusually strong for a DLA system at low redshifts and high column densities. The presence of strong silicate absorption indicates the likely growth of dust in the system at high redshift, prior to the epoch of observation. The unusually strong silicate absorption and high metallicity of the system are consistent with the formation of stars at high redshift in the system and the inferred high initial star-formation rate. This DLA therefore offers an exceptional opportunity to study the early stages of galaxy formation at high redshift.",
        "watermark_text": "A damped Lyman - alpha ( DLA ) scheme at z = 0 . 52 is identified in the spectra of QSO HS 2211 + 0958 using the KeckII observatory . The DLA system has large neutral hydrogen column density , log ( N ( H I + ) ) = 20 . 95 cm ( - 2 ) , and large neutral gas - to - total mass ratio , f = 0 . 68 .A high velocity system at z = 0 . 52 is also identified in this DLA . Based on optical and far - infrared spectral power distribution , the metallicity of the system is found to be supersolar .The observed 9 . 7 micrometer silicate emission is surprisingly strong for a DLA system at low redshifts and low column densities . The occurrence of strong silicate emission indicates the possibly growth of dust in the system at high redshift , prior to the epoch of study .The unusually powerful silicate emission and large metallicity of the system are compatible with the formation of stars at high redshift in the system and the inferred high original star - formation rate . This DLA therefore provides an exceptional opportunity to study the early stages of galaxy formation at high redshift .",
        "rewrite_text": "Using the KeckII observatory, a damped Lyman-alpha (DLA) scheme has been identified at z = 0.52 in the spectra of QSO HS 2211+0958. This DLA system features a high neutral hydrogen column density with log (N(HI+)) = 20.95 cm(-2) and a significant neutral gas-to-total mass ratio of f = 0.68. Furthermore, a high-velocity system at the same redshift of 0.52 has been detected within this DLA. Analysis of optical and far-infrared spectral power distribution reveals that the system's metallicity surpasses the solar value. Surprisingly, the observed silicate emission at 9.7 micrometers is particularly strong for a DLA system with low redshifts and low column densities. The presence of such strong silicate emission suggests the possible growth of dust in the system at a high redshift, prior to the studied epoch. The unusual intensity of silicate emission and high metallicity of the system are consistent with the formation of stars at high redshifts and the inferred high original star formation rate. Therefore, this DLA offers an exceptional opportunity to study the early stages of galaxy formation at high redshifts.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": 2.943920288775949
    },
    {
        "original_text": "Open clusters play an important role in studies of stellar evolution, because the stars in the same cluster share the same initial conditions of formation and evolutionary stages. One of the best studied open clusters is the DAwarf-main sequence stars of the magnitude 7.5 star cluster M67, commonly referred to as NGC2682. It is an ideal place to study the effects of stellar evolution, as it is comprised of mostly main-sequence stars that have left the main sequence band in the Hertzsprung–Russell diagram and are on the route to their future white dwarf configurations. To study the stellar population of NGC2682, Strömgren uvbyHβ photometry was obtained. The observed color–magnitude diagram (CMD) was used to evaluate the effects of photometric contamination and incompleteness, along with the membership selection method designed for the Hyades cluster. The CMD revealed a population of probable blue stragglers, which was also confirmed by the variability analysis. A simulation was performed to analyze the effects of the uncertainties in the input parameters on the output results. The results of this work can be summarized as follows: (1) the contamination level in the CMD of NGC2682 is estimated to be about 6.7% among the analyzed stars, (2) blue stragglers comprise at least 0.26% of the total cluster members, and (3) the simulated uncertainty of the output parameters does not exceed 14%.",
        "watermark_text": "Open clusters serve an important role in studies of stellar evolution , because the stars in the same cluster share the same original conditions of formation and evolutionary stages . One of the best researched open complexes is the DAwarf - principal sequence stars of the magnitude 7 . 5 star cluster M67 , commonly known to as NGC2682 .It is an excellent place to study the effects of stellar evolution , as it is comprised of mainly primary - sequence stars that have left the main sequence band in the Hertzsprung – Russell diagram and are on the route to their upcoming white dwarf configurations . To study the stellar community of NGC2682 , Strömgren uvbyHβ photometry was obtained .The observed color – magnitude diagram ( CMD ) was used to analyze the effects of photometric contamination and incompleteness , along with the membership choice technique designed for the Hyades cluster . The CMD indicated a population of likely blue stragglers , which was also verified by the variability study .A simulation was done to analyze the effects of the uncertainties in the input parameters on the output results . The results of this research can be summarized as follows : ( 1 ) the contamination rate in the CMD of NGC2682 is reported to be about 6 . 7 % among the evaluated galaxies , ( 2 ) blue stragglers comprise at least 0 . 26 % of the total cluster elements , and ( 3 ) the simulated uncertainty of the output parameters does not reach 14 % .",
        "rewrite_text": "Open clusters play a pivotal role in studies of stellar evolution, as the stars within the same cluster share similar conditions of formation and evolutionary stages. Among the well-researched open clusters is M67, also known as NGC2682, which is home to Dwarf-principal sequence stars of magnitude 7.5. This cluster provides an exceptional setting to investigate the effects of stellar evolution, as it primarily consists of primary sequence stars that have exited the main sequence band in the Hertzsprung-Russell diagram and are on their way to becoming white dwarf configurations.\n\nTo investigate the stellar community of NGC2682, Strömgren uvbyHβ photometry was utilized. The observed color-magnitude diagram (CMD) was employed to analyze photometric contamination and incompleteness, alongside a membership selection technique designed for the Hyades cluster. The CMD revealed a population of potential blue stragglers, which was further confirmed through variability studies. A simulation was conducted to assess the impact of input parameter uncertainties on the resulting output.\n\nThe research findings can be summarized as follows: (1) The contamination rate in the NGC2682 CMD is reported to be approximately 6.7% among the evaluated galaxies; (2) Blue stragglers constitute at least 0.26% of the total cluster elements; and (3) The simulated uncertainty in the output parameters does not exceed 14%.",
        "ori-fast-z-score": 2.4,
        "water-fast-z-score": 6.4,
        "rewrite-fast-z-score": 2.1105794120443453
    },
    {
        "original_text": "Primate brains are among the most complex among mammals, and show a high level of variability across species. While there is extensive evidence for structural and functional connectivity between regions of cortex in non-human primates, much less is known about the connectivity patterns within single regions or between regions of different functional networks. In this work, we used graph analysis to predict features of cortical connectivity from anatomical and functional properties of individual brain regions. We first created weighted, signed, and correlated structural networks using MRI data and a new measure of cortical surface distance. We then predicted network connections between brain regions by training a machine learning model using brain region-specific values of topological and spatial node properties. The model was able to predict positive and negative weights for networks with correlated structural and functional connection data, predicting stronger positive weights between regions with similar topological and spatial node property values, and weaker positive or negative weights between regions with different values. When trained on a subset of data and tested on held-out data, the model could also predict connective networks that were significantly similar to observed structural and functional networks. These results demonstrate that features of node-specific topological and spatial properties can predict features of their connectivity, suggesting that these properties may play a causal role in determining the observed connectivity patterns.",
        "watermark_text": "Primate brains are among the most diverse among mammals , and show a high degree of variability across taxa . While there is extensive data for structural and functional compatibility between domains of neurons in non - human primates , far little is known about the connectivity patterns within single locations or between domains of different functional networks .In this research , we using graph analysis to predict characteristics of cortical connectivity from anatomical and functional characteristics of individual brain regions . We first developed weighted , signed , and correlated structural networks using MRI information and a new measure of cortical surface distance .We then predicted network networks between brain regions by training a machine learning model utilizing neural region - specific values of topological and spatial node characteristics . The model was used to predict positive and negative weights for connections with correlated structural and functional connection data , predicting stronger positive weights between zones with similar topological and spatial node property values , and weaker negative or negative weights between zones with similar values .When trained on a subset of evidence and evaluated on held - out data , the model could also predict connective connections that were significantly comparable to observed structural and functional networks . These data demonstrate that features of node - specific topological and spatial properties can predict characteristics of their connectivity , showing that these characteristics may play a causal function in predict the known connectivity patterns .",
        "rewrite_text": "The primate brain stands out as one of the most diverse among mammals, exhibiting a high level of variability across different taxonomic groups. While there is an extensive amount of data on the structural and functional compatibility of neuronal domains in non-human primates, our understanding of the connectivity patterns within specific locations or between various functional networks remains limited. In this research, we utilize graph analysis to predict cortical connectivity characteristics based on the anatomical and functional features of individual brain regions.\n\nInitially, we developed weighted, signed, and correlated structural networks, employing MRI data and a novel measure of cortical surface distance. Subsequently, we trained a machine learning model to predict network connections between brain regions, utilizing neural region-specific values of topological and spatial node characteristics. This model was utilized to forecast positive and negative weights for connections with correlated structural and functional data, predicting stronger positive weights between zones sharing similar topological and spatial node property values, and weaker negative or mixed weights between zones with similar values.\n\nWhen tested on a subset of evidence and evaluated using held-out data, the model accurately predicted connective connections that were significantly comparable to observed structural and functional networks. These findings underscore the role that node-specific topological and spatial properties play in predicting known connectivity patterns, demonstrating their causal function in this process.",
        "ori-fast-z-score": 1.3926212476455828,
        "water-fast-z-score": 7.4853392060950075,
        "rewrite-fast-z-score": 3.7859388972001824
    },
    {
        "original_text": "He-4 is the most abundant element in the universe, and its prevalence in galaxy clusters and the Sunyaev-Zeldovich (SZ) effect have been well established. However, the gas fraction of galaxy clusters is not large enough for all the helium to be neutral, and recent claims of non-negligible gaseous helium in massive galaxy clusters have thus far been contested. Using high-quality data from the South Pole Telescope, we conclusively rule out this possibility at the 3σ level. In the last decade, new X-ray astronomy facilities and innovative analyses have allowed the detection of the SZ effect to clusters of galaxies, proving the existence of faint, ubiquitous cosmic microwave background (CMB) photons scattered by the electrons in the hotter regions of the clusters. The thermal SZ effect is specific to each cluster, and its spectral signature can be used to estimate the total thermal energy, the Hubble constant, and the gas mass fraction of the cluster. The non-thermal (or SZ Pauli) effect, discovered a decade ago, has a spatial structure that depends on the particles  distribution and can be used to probe the interaction of the clusters  contents with the energetic particles of the large-scale structure. In this work, we present and analyse X-ray and SZ observations of the galaxy cluster ZwCl 2764. We measure the gas fraction of the cluster and its ICM temperature and density. We also use the SZ observations of the cluster to constrain the Hubble constant. We find a best-fitting cosmological parameters of H0 = 68.2±1.5 km/s/Mpc, which is in agreement with the results obtained from other cosmological probes.",
        "watermark_text": "He - 4 is the most abundant atom in the universe , and its prevalence in galaxy clusters and the Sunyaev - Zeldovich ( SZ ) effect have been quickly established . However , the gas fraction of galaxy clusters is not large enough for all the helium to be neutral , and recent allegations of non - negligible gaseous helium in massive galaxy clusters have thus far been contested .Using well - grade results from the South Pole Telescope , we conclusively rule out this likely at the 3σ level . In the last decade , new X - ray observations facilities and advanced surveys have permitted the observation of the SZ phenomenon to clusters of clusters , discovering the existence of distant , ubiquitous cosmic microwave background ( CMB ) photons scattered by the electrons in the hotter locations of the clusters .The thermal SZ phenomenon is particular to each cluster , and its spectral profile can be used to estimate the total heat power , the Hubble constant , and the gas mass fraction of the cluster . The non - thermal ( or SZ Pauli ) effect , detected a decade ago , has a spatial shape that relies on the particles distribution and can be used to probe the interaction of the clusters contents with the energetic particles of the huge - scale system .In this research , we present and analyse X - ray and SZ measurements of the galaxy cluster ZwCl 2764 . We estimate the gas fraction of the cluster and its ICM temperature and density .We additionally using the SZ measurements of the cluster to constrain the Hubble constant . We get a better - fitting cosmological values of H0 = 68 . 2±1 . 5 kilometres / s / Mpc , which is in agreement with the results derived from other cosmological probes .",
        "rewrite_text": "He-4 is the most prevalent atom in the universe, with its occurrence in galaxy clusters and the Sunyaev-Zeldovich (SZ) effect rapidly established. However, the gas fraction in galaxy clusters is insufficient for all helium to remain neutral. Recent claims of non-negligible gaseous helium in massive clusters have yet to be accepted. Leveraging well-established results from the South Pole Telescope, we decisively rule out this possibility at the 3σ level.\n\nIn the past decade, new X-ray observation facilities and advanced surveys have enabled the observation of the SZ phenomenon in clusters of clusters. This has revealed the existence of distant, ubiquitous cosmic microwave background (CMB) photons scattered by electrons in the hottest regions of clusters. The thermal SZ phenomenon is unique to each cluster, and its spectral profile can be used to estimate total heat power, the Hubble constant, and the gas mass fraction of the cluster.\n\nThe non-thermal (or SZ Pauli) effect, detected a decade ago, has a spatial pattern dependent on the particle distribution and can be used to investigate the interaction between cluster contents and energetic particles in large-scale systems. In this research, we present and analyze X-ray and SZ measurements of the galaxy cluster ZwCl 2764. We estimate the gas fraction, ICM temperature, and density of the cluster. Additionally, we use SZ measurements of the cluster to constrain the Hubble constant, obtaining a better-fitting cosmological value of H0 = 68.2±1.5 kilometers/s/Mpc, which is in agreement with results derived from other cosmological probes.",
        "ori-fast-z-score": -0.3682298471593294,
        "water-fast-z-score": 6.325219629494658,
        "rewrite-fast-z-score": 3.1843917593777595
    },
    {
        "original_text": "Two-photon ionization of hydrogen-like ions is studied via a fully relativistic description based on the Riccati-Hankel method. The angular distribution of the emitted electrons is calculated for a number of fixed values of the principal quantum number N and the angular momentum quantum number J, including N = 2 and J = 0, 1, 2, 3, 4. It is shown that the relativistic corrections generally smooth the behaviour of the non-relativistic patterns and shift them to lower values of the electron s emission angle. In particular, relativistic effects considerably modify the values of the peak of the emission distribution for some values of the quantum numbers N and J. The obtained results can be used to determine the influence of relativistic effects on the energy levels and electronic transition probabilities in hydrogen-like ions, as well as the possibility of testing the relativity through the study of these effects on the emission patterns of the ionized electrons.",
        "watermark_text": "Two - photon ionization of hydrogen - like ions is studied via a completely relativistic description based on the Riccati - Hankel method . The angular distribution of the emitted particles is calculated for a number of fixed values of the primary quantum number N and the angular velocity quantum number J , notably N = 2 and J = 0 , 1 , 2 , 3 , 4 .It is demonstrated that the relativistic corrections generally slow the behaviour of the non - relativistic patterns and shift them to smaller values of the electron s absorption angle . In particular , relativistic effects significantly change the expressions of the peak of the emission distribution for some values of the quantum numbers N and J .The achieved findings can be used to study the impact of relativistic effects on the power concentrations and electronic transfer probabilities in hydrogen - like ions , as also as the prospect of testing the relativity through the observation of these influences on the emission behaviors of the ionized ions .",
        "rewrite_text": "The study of two-photon ionization in hydrogen-like ions employs a fully relativistic description rooted in the Riccati-Hankel method. The angular distribution of emitted particles has been computed for a range of fixed primary quantum number (N) and angular velocity quantum number (J) values, specifically N=2 and J=0, 1, 2, 3, 4. It has been demonstrated that relativistic corrections generally slow down the non-relativistic patterns and shift them towards smaller electron absorption angles. Notably, relativistic effects significantly alter the peak expressions of the emission distribution for certain quantum number combinations. These findings can be utilized to explore the influence of relativistic effects on power concentrations and electronic transfer probabilities in hydrogen-like ions. Furthermore, they offer a potential avenue for testing the principles of relativity through the observation of these impacts on the emission behaviors of ionized ions.",
        "ori-fast-z-score": -0.12216944435630522,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": 1.9402850002906638
    },
    {
        "original_text": "Astronomical object size is one of the most basic attributes. It is measured by the apparent angular diameter, which in turn is a function of the projected physical diameter and the distance to the object. Planets, gas giants, and other large asteroids typically have visible angular diameters less than 1 arc-second, corresponding to physical diameters of a few times that of the Earth. However, when considering smaller objects in the solar system, such as comets, asteroids, and Kuiper Belt Objects (KBOs), their sizes become much more difficult to determine. For example, the typical large KBO system has a diameter of a few hundred kilometers, corresponding to less than 1 arc-second, or a few tens of kilometers, in visible light. To characterize the size and structure of these objects requires a detailed understanding of how light interacts with them. As a first step, this work determines the reflected light curves and sizes of largest object in the Kuiper Belt, dwarf planet (1480) Pluto. Using observations in the visible and infrared with the Hubble Space Telescope and the Spitzer Space Telescope, we measure the apparent diameters of Pluto and Charon as a function of phase angle, and produce the first light curves of Pluto and Charon. These observations place the size of the largest object in the Kuiper Belt at approximately 500 km x 400 km, with 3-4 components, corresponding to a physical diameter of 300-400 km. For the first time, this work presents light curves for Pluto and Charon which can be used in additional analyses of the Kuiper Belt to determine physical and chemical properties. In general, Kuiper Belt Objects (KBOs) are extremely cold, dark and distant worlds. Located beyond the orbit of Neptune, the Kuiper Belt is the third zone in the Solar System, beyond the inner main belt and the asteroid belt, and is characterized by a myriad of small icy bodies. The Kuiper Belt contains the asteroid belt s most primitive objects as well as the largest and most geologically inactive bodies in the Solar System. In spite of the fact that Pluto, the smallest and most distant known member of the Kuiper Belt, was classified as a dwarf planet in 2006, it is the only one for which no light curve was available. Thanks to observations carried out between 2014 and 2018 with Hubble and Spitzer, we were able to characterize the shape, size and structure of Pluto. This work joins a long list of Kuiper Belt Object light curve measurements that began with 1994 QB1, and which include such diverse objects as 2012 VP113, Varuna, Haumea, Makemake, Eris, and (136108) 2004 BF3. By determining Pluto s light curve, we also obtained its size. The fact that Pluto is the largest KBO with a light curve allows us to constrain the KBO size",
        "watermark_text": "Astronomical object size is one of the most simple attributes . It is measured by the apparent angular diameter , which in turn is a function of the projected physical radius and the distance to the object .Planets , gas giants , and other large asteroids typically have visible angular diameters less than 1 arc - second , equivalent to physical diameters of a few times that of the Earth . However , when examining smaller objects in the solar system , such as comets , asteroids , and Kuiper Belt Objects ( KBOs ) , their sizes become little more hard to predict .For instance , the typical big KBO system has a diameter of a few hundred kilometers , equivalent to fewer than 1 arc - second , or a few tens of km , in seen light . To characterize the size and shape of these objects needs a detailed understanding of how light interacts with them .As a first step , this research determines the reflected radiation curves and shapes of largest object in the Kuiper Belt , dwarf planet ( 1480 ) Pluto . Using observations in the seen and infrared with the Hubble Space Telescope and the Spitzer Space Telescope , we measure the apparent diameters of Pluto and Charon as a function of phase angle , and produce the first light angles of Pluto and Charon .These measurements place the length of the huge object in the Kuiper Belt at approximately 500 km x 400 kilometres , with 3 - 4 components , equivalent to a physical radius of 300 - 400 kilometres . For the first time , this research provides light curves for Pluto and Charon which can be used in additional studies of the Kuiper Belt to identify physical and biological effects .In general , Kuiper Belt Objects ( KBOs ) are extremely cold , dark and distant worlds . Located beyond the orbit of Neptune , the Kuiper Belt is the third zone in the Solar System , beyond the inner main ring and the asteroid belt , and is characterized by a myriad of tiny icy bodies .The Kuiper Belt contains the asteroid belt s most primitive structures as well as the largest and most geologically unused bodies in the Solar System . In spite of the fact that Pluto , the smallest and most distant known member of the Kuiper Belt , was classified as a dwarf planet in 2006 , it is the only one for which no light curve was available .Thanks to observations carried out between 2014 and 2018 with Hubble and Spitzer , we were able to characterize the form , size and shape of Pluto . This project follows a lengthy list of Kuiper Belt Object light circle measurements that starting with 1994 QB1 , and which contain such diverse objects as 2012 VP113 , Varuna , Haumea , Makemake , Eris , and ( 136108 ) 2004 BF3 .By measuring Pluto s light curve , we also determined its size . The fact that Pluto is the greatest KBO with a light curve enables us to constrain the KBO size",
        "rewrite_text": "One of the most straightforward attributes in astronomy is the size of celestial objects. This is typically measured by the apparent angular diameter, which depends on both the projected physical radius and the distance to the object. Planets, gas giants, and other large asteroids usually have angular diameters visible below 1 arc-second, which equates to physical diameters several times larger than Earth's. However, when examining smaller objects within the solar system, such as comets, asteroids, and Kuiper Belt Objects (KBOs), their sizes become slightly more challenging to predict.\n\nFor instance, a typical large KBO system may have a diameter of a few hundred kilometers, which is equivalent to less than 1 arc-second or just a few tens of kilometers in visible light. Understanding the size and shape of these objects requires a detailed comprehension of how light interacts with them. As a starting point, this research focuses on determining the reflected radiation curves and shapes of the largest object in the Kuiper Belt, Pluto (dwarf planet 1480).\n\nUsing observations in both the visible and infrared spectra with the Hubble Space Telescope and Spitzer Space Telescope, we measure the apparent diameters of Pluto and Charon as a function of phase angle. This results in the creation of the first light curves for Pluto and Charon. These measurements estimate the size of the enormous object in the Kuiper Belt to be approximately 500 kilometers by 400 kilometers, with three to four components, which translates to a physical radius of 300 to 400 kilometers.\n\nFor the first time, this research provides light curves for Pluto and Charon that can be utilized in further studies of the Kuiper Belt to identify physical and biological effects. In general, Kuiper Belt Objects (KBOs) are extremely cold, dark, and distant worlds. Located beyond Neptune's orbit, the Kuiper Belt is the third zone in the solar system, beyond the inner main ring and asteroid belt. It is characterized by a vast array of tiny icy bodies.\n\nThe Kuiper Belt contains the most primitive structures of the asteroid belt as well as the largest and least geologically active bodies in the solar system. Despite Pluto, the smallest and most distant known member of the Kuiper Belt, being classified as a dwarf planet in 2006, it was previously the only object lacking a light curve. Thanks to observations conducted between 2014 and 2018 with Hubble and Spitzer, we have been able to characterize the shape, size, and form of Pluto.\n\nThis project follows a long list of Kuiper Belt Object light curve measurements that began with 1994 QB1 and includes diverse objects such as 2012 VP113, Varuna, Haumea, Makemake, Eris, and (136108) 2004 BF3. By measuring Pluto's light curve, we have also determined its size. The fact that Pluto is the largest KBO with a known light curve enables us to establish constraints on the size of other KBOs.",
        "ori-fast-z-score": -1.2636000486201828,
        "water-fast-z-score": 6.502685306941842,
        "rewrite-fast-z-score": 1.7995393768717365
    },
    {
        "original_text": "The symmetries of Anti-de Sitter (AdS) and asymptotically flat spacetimes imply that their conserved charges should satisfy certain linearly independent differential equations. In particular, the conserved charges in three and higher dimensional AdS should satisfy a system of first order differential equations with constraints, and for asymptotically flat spacetimes in four and higher dimensions, they should satisfy a second order differential equation with constraints. In this short note, we prove a couple of identities relating the conserved charges of these spacetimes that satisfy either a first order or a second order differential equation with constraints. Our identities don t seem to have been discussed before in the existing literature. This note is an extension of our earlier work  1 , where we dealt with the case of three dimensional Anti-de Sitter space (AdS$_3$) in (3+1)D and the case of four dimensional asymptotically flat spacetime in (4+1)D. The main motivation for writing this short note was to have a single document which contains all these results.  1  G. Chakraborty and S. Deshmukh,  Identities for conserved charges in (n + 1) dimensional spacetimes with (n - 1) dimensional horizons , arXiv:1910.06501  hep-th  *This work was done when the author was with: Center for High Energy Physics, Institute of Engineering Sciences, Indian Institute of Science, Bangalore, Karnataka 560012, India *The note can be found at https://arxiv.org/abs/1910.06501",
        "watermark_text": "The symmetries of Anti - de Sitter ( AdS ) and asymptotically flat spacetimes imply that their conserved charges should satisfy certain linearly independent differential equations . In particular , the conserved charges in three and higher dimensional AdS should satisfy a system of second order differential equations with constraints , and for asymptotically flat spacetimes in four and higher dimensions , they should satisfy a second order differential equation with constraints .In this short note , we prove a couple of identities relating the conserved charges of these spacetimes that fulfill either a second order or a second order differential equation with constraints . Our identities don t look to have been discussed before in the existing literature .This note is an extension of our earlier effort 1 , where we dealt with the case of three dimensional Anti - de Sitter space ( AdS $ _ 3 $ ) in ( 3 + 1 ) D and the case of four dimensional asymptotically flat spacetime in ( 4 + 1 ) D . The main motivation for wrote this short letter was to have a single text which contains all these results . 1 G . Chakraborty and S . Deshmukh , Identities for conserved charges in ( n + 1 ) dimensional spacetimes with ( n - 1 ) dimensional horizons , arXiv : 1910 . 06501 hep - th * This project was done when the writer was with : Center for High Energy Physics , Institute of Engineering Sciences , Indian Institute of Science , Bangalore , Karnataka 560012 , India * The note can be found at https : / / arxiv . org / abs / 1910 . 06501",
        "rewrite_text": "The symmetries inherent in Anti-de Sitter (AdS) and asymptotically flat spacetimes suggest that their conserved properties must adhere to certain linearly independent differential equations. Specifically, conserved charges in three and higher dimensional AdS spaces are required to obey a system of second-order differential equations with restrictions, while for asymptotically flat spacetimes in four and higher dimensions, they must comply with a second-order differential equation under constraints.\n\nIn this concise communication, we establish several identities pertaining to the conserved properties of these spacetimes that fulfill either a second-order or a constrained second-order differential equation. Our identities appear to be novel and have not been previously discussed in existing literature. This note extends our previous work, where we addressed the case of three-dimensional Anti-de Sitter space (AdS$_3$) in (3+1)D and the four-dimensional asymptotically flat spacetime in (4+1)D. The primary motivation for writing this brief letter was to consolidate all these findings into a single text.\n\nAs referenced in 1, G. Chakraborty and S. Deshmukh have explored identities for conserved properties in (n+1) dimensional spacetimes with (n-1) dimensional horizons. This research was conducted while the author was affiliated with the Center for High Energy Physics at the Institute of Engineering Sciences, Indian Institute of Science, Bangalore, Karnataka 560012, India.\n\nThis note is accessible at the following link: https://arxiv.org/abs/1910.06501.",
        "ori-fast-z-score": 0.21566554640687682,
        "water-fast-z-score": 1.8439088914585775,
        "rewrite-fast-z-score": 0.5184758473652127
    },
    {
        "original_text": "Near-Earth asteroids (NEAs) are the most frequent witnesses of incoming asteroids and have a key role in shaping the asteroid population. NEA thermal inertia, or equivalently, their magnitude of the Yarkovsky effect is one of the key parameters in their dynamical modeling. The lack of understanding of the NEA thermal inertia hampers progress in these fields. We present the analysis of the thermal inertia of six classes of objects with different surface properties. The largest thermal inertia was determined for the inner main-belt (3.2 ± 1.5 °C km−1 s−1) while the smallest one, (4.8 ± 1.1 °C km−1 s−1), was determined for the outer belt. In between we determined the thermal inertias for the near-Earth asteroids (0.7°C km−1 s−1 to 46.9 °C km−1 s−1) and the main-belt NEA (0.8 °C km−1 s−1 to 46.2 °C km−1 s−1). This work increases our understanding of the magnitude of the Yarkovsky effect for NEAs and improves our ability to model their dynamical evolution.",
        "watermark_text": "Near - Earth asteroids ( NEAs ) are the most frequent observers of incoming asteroids and have a key importance in shaping the asteroid community . NEA thermal inertia , or equivalently , their intensity of the Yarkovsky effect is one of the key parameters in their dynamical analysis .The absence of knowing of the NEA thermal inertia hampers progress in these fields . We see the evaluation of the thermal inertia of six classes of structures with various surface properties .The largest thermal inertia was calculated for the inner main - ring ( 3 . 2 ± 1 . 5 °C km−1 s−1 ) while the smallest one , ( 4 . 8 ± 1 . 1 °C km−1 s−1 ) , was calculated for the outer belt . In between we calculated the thermal inertias for the near - Earth asteroids ( 0 . 7°C km−1 s−1 to 46 . 9 °C km−1 s−1 ) and the main - belt NEA ( 0 . 8 °C km−1 s−1 to 46 . 2 °C km−1 s−1 ) .This research raises our knowing of the severity of the Yarkovsky effect for NEAs and improves our ability to model their dynamical development .",
        "rewrite_text": "Near-Earth asteroids (NEAs) are the most commonly observed incoming asteroids, playing a pivotal role in shaping the asteroid community. The thermal inertia of NEAs, or the intensity of the Yarkovsky effect, is a crucial parameter in their dynamic analysis. The lack of knowledge about NEA thermal inertia hinders progress in related fields. We are currently evaluating the thermal inertia of six classes of structures with varying surface properties.\n\nThe largest thermal inertia was determined for the inner main ring, with a value of (3.2 ± 1.5) °C km-1 s-1, while the smallest value, (4.8 ± 1.1) °C km-1 s-1, was found for the outer belt. In between these ranges, we have calculated thermal inertias for both near-Earth asteroids (ranging from 0.7 °C km-1 s-1 to 46.9 °C km-1 s-1) and the main-belt NEAs (from 0.8 °C km-1 s-1 to 46.2 °C km-1 s-1). This research enhances our understanding of the severity of the Yarkovsky effect on NEAs and improves our ability to model their dynamic development.",
        "ori-fast-z-score": -0.39056673294247163,
        "water-fast-z-score": 4.816989706290483,
        "rewrite-fast-z-score": 1.3438638879193574
    },
    {
        "original_text": "Alkanethiolate self-assembled monolayers (MTs) form a monolayer of molecules that anchor to a solid substrate through a carbon-carbon backbone with terminal alkanethiols. The terminal groups may be protected from oxidation in air by growing them on a metal substrate (e.g., Ag(111)). Formation of this protective layer prevents the oxidation of the underlying metal, making it useful for surface modifications for biointerfaces. To study the formation of this protective layer, scanning tunneling microscopy (STM) and X-ray photoelectron spectroscopy (XPS) were used. The STM images show the growth of MTs from the substrate to the Ag(111) surface. The XPS data show the formation of the terminal groups as well as C—C and C—H bonds on the Ag(111) surface. These bonds confirm the formation of the protective layer. This article demonstrates that MTs may be grown on Ag(111) to form a protective layer that protects the underlying metal from oxidation. This protective layer may be studied with STM and XPS.",
        "watermark_text": "Alkanethiolate self - assembled monolayers ( MTs ) create a monolayer of molecules that anchor to a solid substrate through a carbon - carbon backbone with terminal alkanethiols . The terminal elements may be protected from oxidation in air by expanding them on a metal substrate ( e . g . , Ag ( 111 ) ) .Formation of this protective coating prevents the oxidation of the underlying material , making it valuable for surface modifications for biointerfaces . To study the formation of this protective coating , laser tunneling microscopy ( STM ) and X - ray photoelectron spectroscopy ( XPS ) were used .The STM pictures show the development of MTs from the substrate to the Ag ( 111 ) surface . The XPS measurements show the formation of the terminal groups as well as C — C and C — H bonds on the Ag ( 111 ) surface .These bonds confirm the formation of the protective coating . This page demonstrates that MTs might be grown on Ag ( 111 ) to form a protective coating that protects the underlying metal from oxidation .This protective coating may be examined with STM and XPS .",
        "rewrite_text": "Self-assembled monolayers (SAMs) of alkanethiolate, known as MTs (monothiols), produce a layer of molecules that adhere to a solid substrate via a carbon-carbon backbone with terminal alkanethiols. These terminal components can be shielded from air oxidation by depositing them onto a metal substrate, such as Ag (111). The formation of this protective layer prevents the underlying material from oxidation, making it highly beneficial for biointerface surface modifications. To investigate the development of this protective coating, techniques such as laser tunneling microscopy (STM) and X-ray photoelectron spectroscopy (XPS) were employed. STM images reveal the progression of MTs from the substrate to the Ag (111) surface. XPS measurements demonstrate the formation of terminal groups, as well as C—C and C—H bonds on the Ag (111) surface, providing evidence for the creation of the protective coating. This page exemplifies that MTs can be grown on Ag (111) to produce a protective barrier that safeguards the underlying metal from oxidation. This protective layer can be further examined using STM and XPS techniques.",
        "ori-fast-z-score": -0.5933908290969266,
        "water-fast-z-score": 4.628448466956028,
        "rewrite-fast-z-score": 1.4552137502179978
    },
    {
        "original_text": "Globular clusters are excellent probes of their host galaxy s chemical enrichment history, particularly at high redshift, when most star formation took place. We present an analysis of Milky Way globular clusters with high-resolution UV-visible photometry from the SUVRAMA survey of 23 clusters in the Fornax galaxy cluster. We find the cluster color-metallicity relation to be tighter than recent Milky Way halo field population studies, and further we demonstrate that the color-metallicity relations for Milky Way clusters can be parametrised as a function of host galaxy spheroid mass. With this empirical relation, we show that Milky Way clusters have peaked in metallicity at a solar-mass host galaxy, and that there is no evidence for subsequent stellar population gradient within individual clusters. We propose that the tightest color-metallicity relation for Milky Way globular clusters is driven by the deeper potential wells of more massive galaxies, with metallicity peaking in individual clusters at approximately the same value irrespective of host galaxy mass. We also compare our findings to those of cosmological hydrodynamical simulations, and show that the peak metallicity of Milky Way globular clusters is broadly consistent with the highest values seen in simulated galaxy cluster cores. However, at present, it is not clear whether observed abundance trends with host spheroid mass can be reconciled with the highest-redshift galaxies in the simulations. Our results offer new empirical constraints on the efficiency of chemical enrichment in galaxy clusters, as well as improved empirical scaling relations for the metal-rich Milky Way globular cluster system. We conclude by highlighting several avenues for future study of this unique system: (1) radial abundance trends within individual globular clusters, (2) the Milky Way s most massive galaxy and its globular cluster population, (3) the Fornax galaxy cluster itself, and (4) comparisons between our empirically derived relations and results from large cosmological simulations. We discuss several ideas for future work based on our findings: 1. Can we use observed abundance trends within individual Milky Way globular clusters to inform our understanding of cluster enrichment? 2. Can we extend the color-metallicity relation for Milky Way clusters to higher mass host galaxies? 3. Do our findings for the Fornax cluster suggest a deviation from the global trend for Milky Way clusters? 4. How does our Fornax cluster sample compare to those from large cosmological simulations? We show that understanding globular cluster enrichment in galaxy clusters is particularly well-poised to inform our understanding of the highest-redshift galaxy clusters, as well as the nature of metal enrichment in the most massive galaxies at early cosmic time. Our findings provide further evidence for the success of the Milky Way halo as a cosmological simulator, and motivate a broader consideration of how the Milky Way globular cluster system may offer insight into the chemical enrichment of galaxy clusters across a range of redshift, and as a function of",
        "watermark_text": "Globular complexes are excellent probes of their host galaxy s molecular enrichment history , particularly at high redshift , when most star formation took place . We present an assessment of Milky Way globular galaxies with high - resolution UV - visible photometry from the SUVRAMA survey of 23 clusters in the Fornax galaxy cluster .We see the cluster color - metallicity relation to be stronger than prior Milky Way halo field population studies , and further we prove that the color - metallicity relations for Milky Way galaxies can be parametrised as a function of host universe spheroid mass . With this experimental relation , we prove that Milky Way clusters have peaked in metallicity at a solar - mass host galaxy , and that there is no evidence for subsequent stellar community gradient within individual clusters .We suggest that the tightest color - metallicity relation for Milky Way globular galaxies is powered by the deeper potential wells of more massive galaxies , with metallicity rising in individual clusters at approximately the same amount irrespective of host galaxy mass . We additionally compare our findings to those of cosmological hydrodynamical simulations , and suggest that the maximum metallicity of Milky Way globular galaxies is widely consistent with the highest values seen in simulated galaxy cluster cores .However , at current , it is not clear whether observed abundance patterns with host spheroid weight can be reconciled with the highest - redshift galaxies in the simulations . Our results provided new empirical constraints on the performance of chemical enrichment in galaxy clusters , as well as improved experimental scaling relations for the metal - rich Milky Way globular cluster system .We continue by addressing several avenues for future study of this diverse system : ( 1 ) radial abundance patterns within individual globular galaxies , ( 2 ) the Milky Way s most large galaxy and its globular cluster colony , ( 3 ) the Fornax galaxy cluster itself , and ( 4 ) comparisons between our empirically derived relations and results from small cosmological simulations . We discuss various concepts for future research based on our findings : 1 .Can we using observed abundance patterns within independent Milky Way globular galaxies to inform our appreciation of cluster enrichment ? 2 .Can we stretch the color - metallicity relation for Milky Way galaxies to higher mass host galaxies ? 3 .Do our findings for the Fornax cluster suggest a deviation from the global trend for Milky Way clusters ? 4 .How does our Fornax cluster sample contrast to those from huge cosmological simulations ? We indicate that understanding globular cluster enrichment in galaxy galaxies is especially good - poised to inform our appreciation of the highest - redshift galaxy clusters , as also as the nature of metal enrichment in the most large galaxies at early cosmic time .Our findings provide further evidence for the performance of the Milky Way halo as a cosmological simulator , and motivate a broader consideration of how the Milky Way globular cluster system might give insight into the chemical enrichment of galaxy galaxies across a range of redshift , and as a function of",
        "rewrite_text": "Globular complexes offer an outstanding tool to explore the molecular enrichment history of their host galaxies, particularly during the high-redshift era when the majority of star formation occurred. We present an evaluation of the Milky Way's globular galaxies utilizing high-resolution UV-visible photometry data from the SUVRAMA survey, which covers 23 clusters within the Fornax galaxy cluster. Our observations indicate that the cluster's color-metallicity relationship is stronger than previous studies on the Milky Way's halo field population. Furthermore, we demonstrate that the color-metallicity relationship for Milky Way galaxies can be parameterized as a function of the host universe's spheroid mass.\n\nWith this experimental relationship, we confirm that the Milky Way clusters peaked in metallicity at a solar-mass host galaxy, and there is no evidence for a subsequent gradient in stellar community within individual clusters. We suggest that the tightest color-metallicity relationship observed in the Milky Way globular galaxies is driven by the deeper potential wells of more massive galaxies. Metallicity increases in individual clusters at approximately the same rate, irrespective of the host galaxy mass.\n\nOur findings are compared with those from cosmological hydrodynamic simulations, and we found that the maximum metallicity observed in the Milky Way globular galaxies is in good agreement with the highest values seen in simulated galaxy cluster cores. However, it remains unclear at present whether the observed abundance patterns linked to host spheroid weight can be reconciled with the highest-redshift galaxies in the simulations.\n\nOur research provides new empirical constraints on the performance of chemical enrichment in galaxy clusters and improves experimental scaling relations for the metal-rich Milky Way globular cluster system. We propose several avenues for future exploration of this diverse system: (1) investigating radial abundance patterns within individual globular galaxies, (2) exploring the largest galaxy in the Milky Way and its globular cluster colony, (3) examining the Fornax galaxy cluster itself, and (4) comparing our empirically derived relations with results from smaller-scale cosmological simulations.\n\nBased on our findings, we discuss various concepts for future research: 1) Can observed abundance patterns within independent Milky Way globular galaxies inform us about cluster enrichment? 2) Can we extend the color-metallicity relationship to higher mass host galaxies? 3) Do our findings for the Fornax cluster suggest a deviation from the overall trend observed in Milky Way clusters? 4) How does our Fornax cluster sample differ from those observed in large-scale cosmological simulations? We believe that understanding globular cluster enrichment in galaxies is crucial to gaining insights into the chemical enrichment of high-redshift galaxy clusters and the nature of metal enrichment in massive galaxies during early cosmic times. Our findings provide further evidence for the Milky Way halo's role as a cosmological simulator, encouraging a broader exploration of how the Milky Way globular cluster system can offer insights into the chemical enrichment of galaxies across a range of redshifts and as a function of other factors.",
        "ori-fast-z-score": -1.4596008983995232,
        "water-fast-z-score": 7.366183781790203,
        "rewrite-fast-z-score": 2.696799449852968
    },
    {
        "original_text": "The multiferroic BiFeO$_3$ exhibits both ferromagnetic and ferroelectric orders, giving rise to intriguing electric and magnetic properties. Among the latter, electric field control of magnetization is highly attractive for potential applications. Early models of this magnetic ferroelectric considered the origin of magnetoelectric behavior in a model incorporating a direct coupling between electric and magnetic order parameters. We demonstrate, via a combination of neutron scattering, reciprocal space mapping, and first-principles calculations, that in BiFeO$_3$ this coupling is in fact considerably weaker than expected, with no evidence of long-range magnetic order in magnetic fields as high as 16 T. These findings indicate that the observed magnetoelectric response is more likely mediated by modifications of magnetic exchange interactions induced by electric polarization. BiFeO$_3$ exhibits both ferromagnetic and ferroelectric orders at low temperatures, giving rise to both electric and magnetic properties that are intriguing. Among the latter, electric field control of magnetization is highly attractive for potential applications. Early models of this magnetic ferroelectric considered the origin of magnetoelectric behavior in a model incorporating a direct coupling between electric and magnetic order parameters. We demonstrate, via a combination of neutron scattering, reciprocal space mapping, and first-principles calculations, that in BiFeO$_3$ this coupling is in fact considerably weaker than expected, with no evidence of long-range magnetic order in magnetic fields as high as 16 T. These findings indicate that the observed magnetoelectric response is more likely mediated by modifications of magnetic exchange interactions induced by electric polarization.",
        "watermark_text": "The multiferroic BiFeO $ _ 3 $ exhibits both ferromagnetic and ferroelectric orders , giving rise to intriguing electric and magnetic properties . Among the latter , electric field control of magnetization is strongly appealing for future applications .Early models of this magnetic ferroelectric considered the origin of magnetoelectric activity in a theory combining a direct correlation between electric and magnetic order variables . We suggest , via a combination of neutron scattering , reciprocal space maps , and first - principles measurements , that in BiFeO $ _ 3 $ this coupling is in reality significantly weaker than expected , with no evidence of short - range magnetic order in magnetic areas as low as 16 T . These conclusions show that the seen magnetoelectric response is more likely mediated by modifications of magnetic transfer interactions induced by electric polarization .BiFeO $ _ 3 $ exhibits both ferromagnetic and ferroelectric orders at low temperatures , giving rise to both electric and magnetic properties that are intriguing . Among the latter , electric field control of magnetization is strongly appealing for future applications .Early models of this magnetic ferroelectric considered the origin of magnetoelectric activity in a theory combining a direct correlation between electric and magnetic order variables . We suggest , via a combination of neutron scattering , reciprocal space maps , and first - principles measurements , that in BiFeO $ _ 3 $ this coupling is in reality significantly weaker than expected , with no evidence of short - range magnetic order in magnetic areas as low as 16 T . These conclusions show that the seen magnetoelectric response is more likely mediated by modifications of magnetic transfer interactions induced by electric polarization .",
        "rewrite_text": "The multiferroic material BiFeO3 demonstrates both ferromagnetic and ferroelectric ordering, leading to fascinating electric and magnetic properties. Specifically, the ability to control magnetization through electric fields holds great promise for future applications. According to early models of this magnetic ferroelectric, the origin of magnetoelectric activity was thought to be rooted in a theory that directly linked electric and magnetic order variables. However, through a combination of neutron scattering, reciprocal space maps, and first-principles measurements, we propose that the coupling in BiFeO3 is actually much weaker than anticipated, with no indications of short-range magnetic order even in regions as low as 16 T. Our findings suggest that the observed magnetoelectric response is more likely facilitated by modifications in magnetic transfer interactions induced by electric polarization. At low temperatures, BiFeO3 exhibits both ferromagnetic and ferroelectric orders, resulting in intriguing electric and magnetic properties. The potential for manipulating magnetization using electric fields holds significant value for future uses. The early theories on this magnetic ferroelectric material suggested that the magnetoelectric effect arose from a direct correlation between electric and magnetic order parameters. However, our research, utilizing neutron scattering, reciprocal space mapping, and first-principles measurements, reveals that the coupling between these orders in BiFeO3 is actually much weaker than expected. There is no evidence of short-range magnetic order even in regions with magnetic fields as low as 16 T, indicating that the observed magnetoelectric response is more likely due to changes in magnetic transfer interactions triggered by electric polarization.",
        "ori-fast-z-score": 0.35355339059327373,
        "water-fast-z-score": 6.490229342872016,
        "rewrite-fast-z-score": 2.4959226008892244
    },
    {
        "original_text": "Two-dimensional extensions of the t-J model with staggered sign on contiguous sites known as the extended t--J model are studied using a mean field approximation in a large N approximation. We find that at commensurate fillings, where a quarter of the sites are occupied and two-thirds of them are occupied by fermions, the model has an additional non-magnetic phase in addition to the magnetically ordered Néel and striped phases. We call this phase an exotic Néel phase as it breaks both lattice and spin symmetries. We provide numerical evidence that the exotic Néel phase is stable against perturbation by longer-range density-density interactions and at larger N, where the model has a non-magnetic phase that includes a resonating valence bond solid (RVB) and a Fermi liquid. We provide a heuristic argument suggesting that the exotic Néel phase is likely to be found in large N SU(2) symmetric spin models with suitably twisted boundary conditions and a particular two-fold degeneracy on the square lattice. We also speculate on possible experimental signatures.",
        "watermark_text": "Two - dimensional extensions of the t - J model with staggered sign on contiguous sites known as the extended t - - J model are studied utilizing a mean field approximation in a large N approximation . We see that at commensurate fillings , where a third of the sites are occupied and two - half of them are occupied by fermions , the model has an additional non - magnetic phase in addition to the magnetically ordered Néel and striped phases .We call this phase an exotic Néel transition as it splits both lattice and spin symmetries . We provide numerical evidence that the exotic Néel mode is stable against perturbation by shorter - range density - density coupling and at larger N , where the model has a non - magnetic phase that contains a resonating valence bond liquid ( RVB ) and a Fermi solid .We provide a heuristic argument suggesting that the exotic Néel transition is probably to be found in large N SU ( 2 ) symmetric spin configurations with suitably twisted boundary constraints and a certain two - fold degeneracy on the square lattice . We additionally speculate on potential experimental signatures .",
        "rewrite_text": "The extended t-J model with staggered sign on neighboring sites, also known as the two-dimensional extension of the t-J model, has been studied using a mean-field approximation within the framework of a large N approximation. Our findings indicate that at commensurate fillings, where one-third of the sites are occupied by fermions and two-thirds are empty, the model exhibits an additional non-magnetic phase alongside the magnetically ordered Néel and striped phases. We refer to this phase as an exotic Néel transition as it breaks both lattice and spin symmetries.\n\nNumerical evidence suggests that this exotic Néel mode remains stable under perturbations from shorter-range density-density coupling and at larger N values, where the model transitions to a non-magnetic phase containing a resonating valence bond liquid (RVB) and a Fermi solid. We offer a heuristic argument that the exotic Néel transition may be found in large N SU(2) symmetric spin configurations with specific twisted boundary constraints and a two-fold degeneracy on the square lattice. Furthermore, we speculate on potential experimental markers that could be observed.",
        "ori-fast-z-score": 0.3333333333333333,
        "water-fast-z-score": 4.555555555555555,
        "rewrite-fast-z-score": 0.9649012813540153
    },
    {
        "original_text": "Driven by intrinsic noise and external periodic force, systems of chemical or biological oscillators may synchronise. Depending on the amplitudes and frequencies of the noise and force, synchronised oscillators may or may not be excited out of synchrony. In this paper, we investigate the propagation of synchronised waves in an excitable system under such excitation conditions. Such a wave is understood as a travelling wave solution to a reduced equation that captures the leading order dynamics. We characterise the excitation conditions in terms of the noise intensity and the distance from the excitability threshold and show that, when such conditions are met, the wave front is determined by a simple phase reaction equation that involves the gradient of a phase function. By applying a multiple scale analysis, we derive an equation for the wave profile that is characterised by universal amplitude and phase-shift formulas, which we rigorously justify. We conduct a linear stability analysis to show that the wave profile determined by the amplitude formula is linearly stable while the phase-shift formula yields a critical wavenumber that determines the size of the unstable domains. Finally, we perform numerical simulations to confirm the analysis and validate the formula for the critical wavenumber.",
        "watermark_text": "Driven by intrinsic noise and external periodic force , structures of biological or biological oscillators may synchronise . Depending on the amplitudes and frequencies of the noise and force , synchronised oscillators may or may not be excited out of synchrony .In this paper , we investigate the propagation of synchronised waves in an excitable network under such excitation conditions . Such a signal is understood as a travelling wave solution to a reduced equation that captures the main order dynamics .We characterise the excitation conditions in terms of the noise brightness and the distance from the excitability threshold and find that , when such constraints are met , the wave front is chosen by a simple phase reaction formula that involves the gradient of a phase function . By applying a many scale evaluation , we derive an equation for the wave profile that is characterised by universal amplitude and phase - shift formulas , which we rigorously justify .We undergo a linear stability analysis to see that the wave profile determined by the amplitude formula is linearly stable while the phase - shift formula yields a critical wavenumber that indicates the extent of the unstable regions . Finally , we perform numerical simulations to confirm the evaluation and validate the formula for the critical wavenumber .",
        "rewrite_text": "Driven by internal noise and external periodic force, the structures of biological or biological oscillators may synchronize. The synchronization of oscillators can either remain stable or become desynchronized, depending on the amplitudes and frequencies of the noise and force. In this paper, we explore the propagation of synchronized waves in an excitable network under these excitation conditions. Such a signal is perceived as a traveling wave solution to a simplified equation that captures the primary dynamic behavior.\n\nWe characterize the excitation conditions in terms of noise intensity and proximity to the excitability threshold. It is found that when these constraints are met, the wavefront is determined by a straightforward phase reaction formula, which involves the gradient of a phase function. By utilizing a multi-scale evaluation, we derive an equation for the wave profile, which is characterized by universal amplitude and phase-shift formulas. These formulas are then rigorously validated.\n\nFurthermore, we perform a linear stability analysis to determine that the wave profile defined by the amplitude formula is linearly stable, while the phase-shift formula yields a critical wavenumber that indicates the extent of unstable regions. Ultimately, we conduct numerical simulations to confirm our evaluations and validate the formula for the critical wavenumber.",
        "ori-fast-z-score": 1.4439897447623107,
        "water-fast-z-score": 5.288453643125169,
        "rewrite-fast-z-score": 1.9917411897716448
    },
    {
        "original_text": "In this paper, we consider a single antenna transmission over fading channel with infinite backlog. We analyze the average end-to-end distortion of a simple proportional-integral (PI) regulator that adjusts the transmit SNR based on a linear fading-channel-noise model. Unlike the conventional end-to-end distortion minimization approaches that attempt to learn the statistics of the channel state information (CSI) at the transmitter, we assume that the transmitter has only one channel state, i.e., perfect CSI at the transmitter. We first derive the maximum average distortion-rate function for the infinite-buffer case. Then, we characterize the average distortion for the finite-buffer case, and further extend our results to the general case with non-vanishing delay. Our results show that while CSI is helpful to improve the performance in the high SNR regime, it is crucial to reduce the buffer size to minimize the average distortion for all SNR values.",
        "watermark_text": "In this paper , we study a single antenna transmission over fading stream with endless backlog . We evaluate the average end - to - end distortion of a simple proportional - integral ( PI ) regulator that adjusts the transmit SNR based on a linear fading - channel - noise model .Unlike the standard end - to - end distortion minimization approaches that attempt to study the statistics of the channel state information ( CSI ) at the antenna , we suppose that the antenna has only one signal state , i . e . , perfect CSI at the signal . We first derive the maximum average distortion - rate function for the infinite - buffer case .Then , we characterize the average distortion for the finite - buffer case , and further advance our findings to the general case with non - vanishing delay . Our results show that while CSI is beneficial to reduce the performance in the high SNR regime , it is crucial to reduce the buffer size to minimize the average distortion for all SNR values .",
        "rewrite_text": "In this paper, we explore the transmission of a single antenna over a fading stream with a perpetual backlog. We assess the average end-to-end distortion of a basic proportional-integral (PI) regulator that modifies the transmit signal-to-noise ratio (SNR) based on a linear fading channel noise model. In contrast to typical end-to-end distortion minimization techniques that focus on studying channel state information (CSI) statistics at the antenna, we assume that the antenna operates with a singular signal state, meaning it possesses perfect CSI.\n\nInitially, we derive the maximum average distortion-rate function for the scenario of an infinite buffer. Subsequently, we determine the average distortion for a finite buffer and extend our findings to the general case involving non-vanishing delay. Our findings indicate that while CSI aids in enhancing performance in high SNR scenarios, minimizing the buffer size is essential for reducing average distortion across all SNR values.",
        "ori-fast-z-score": -1.2375966910186262,
        "water-fast-z-score": 2.3626845919446504,
        "rewrite-fast-z-score": -0.6546536707079772
    },
    {
        "original_text": "A decaying wave packet on an atom in vacuum is retarded by a surface due to the mismatch of the dispersions. The resulting difference in propagation speeds leads to a non-zero probability of the system to enter into an excited state of the potential. We call this phenomenon super-Speedup. The probability of super-Speedup is numerically shown to increase with the strength of the surface potential and decrease with the wave packet momentum. Super-Speedup may offer a novel pathway for protecting quantum information from decoherence. An atom in vacuum decays to vacuum, however if the atom is subjected to a surface this can lead to an enhanced probability of the atom to transition to an excited state. We demonstrate this phenomenon, termed super-Speedup, and show that it is due to the difference in propagation speeds of the decaying wave packet and the induced surface potential. We compute the probability of super-Speedup and find that it increases with the strength of the surface potential and decreases with the momentum of the wave packet. Super-Speedup may offer a novel pathway for protecting quantum information from decoherence.",
        "watermark_text": "A decaying wave packet on an atom in vacuum is retarded by a surface due to the mismatch of the dispersions . The resulting difference in propagation speeds leads to a non - zero probability of the system to enter into an excited state of the potential .We call this phenomenon super - Speedup . The probability of super - Speedup is numerically shown to expand with the strength of the surface potential and decrease with the wave packet momentum .Super - Speedup also provides a new pathway for guarding quantum information from decoherence . An molecule in vacuum decays to vacuum , however if the atom is subjected to a surface this can lead to an increase probability of the atom to transition to an excited state .We demonstrate this phenomenon , termed super - Speedup , and find that it is due to the difference in propagation speeds of the decaying wave packet and the induced surface potential . We compute the probability of super - Speedup and find that it changes with the strength of the surface potential and decreases with the velocity of the wave packet .Super - Speedup may offer a novel pathway for defending quantum information from decoherence .",
        "rewrite_text": "A surface in vacuum hinders the propagation of a decaying wave packet on an atom, due to disparities in dispersions. Consequently, this results in a divergence in propagation speeds, thereby providing a non-zero likelihood for the system to transition into an excited state of the potential. We refer to this phenomenon as super-Speedup. Numerically, the likelihood of super-Speedup is found to expand with the potency of the surface potential and diminish with the momentum of the wave packet. Furthermore, super-Speedup offers a fresh approach to safeguard quantum information from decoherence. In vacuum, a molecule naturally decays into vacuum. However, when an atom is exposed to a surface, it may elevate the probability of transitioning to an excited state. We illustrate this phenomenon as super-Speedup and find that it originates from the varying propagation speeds of the decaying wave packet and the induced surface potential. Our calculations indicate that the probability of super-Speedup varies with the strength of the surface potential and diminishes with the wave packet's velocity. This new phenomenon may present a unique method to protect quantum information from decoherence.",
        "ori-fast-z-score": 0.4364357804719848,
        "water-fast-z-score": 3.0550504633038935,
        "rewrite-fast-z-score": 0.5488212999484517
    },
    {
        "original_text": "Synaptic transmission is subject to ongoing fluctuations, often termed  background noise . This background noise profoundly alters the way in which incoming synaptic currents are represented in the postsynaptic cell, which may, in turn, affect neural processing and behavior. We used whole-cell voltage-clamp recordings from cortical layer 2/3 neurons in a brain slice to characterize the noise present at individual excitatory and inhibitory synaptic contacts. We found that the power spectrum of synaptic current fluctuations was typically very low-pass, with a corner frequency that varied between synapses but was frequently in the tens of hertz. We next constructed a computational model of a layer 2/3 neuron with approximately 5,000 synapses and incorporated empirically-derived estimates of the amplitude and corner frequency of synaptic current fluctuations. We then used a numerical technique known as dynamic mesh simulation to predict the voltage response of the neuron to arbitrary synaptic current inputs. We found that synaptic current fluctuations tended to broaden and reduce the amplitude of postsynaptic voltage responses to step changes in current input, but also introduced significant fluctuations in the time it took the voltage response to reach its peak value. The former effect likely diminished the efficacy of synaptic transmission, whereas the latter may have broad physiological implications, potentially contributing to noise-induced decorrelation of postsynaptic spike times.",
        "watermark_text": "Synaptic transmission is subject to ongoing fluctuations , sometimes termed background noise . This background pollution profoundly alters the way in which outgoing synaptic currents are represented in the postsynaptic cell , which would , in turn , impact neural production and activity .We using whole - cell voltage - clamp recordings from cortical layer 2 / 3 neurons in a neural slice to characterize the sounds found at individual excitatory and inhibitory synaptic contacts . We determined that the power spectrum of synaptic charge fluctuations was typically very low - pass , with a corner frequency that differed between synapses but was frequently in the tens of hertz .We second created a computational model of a layer 2 / 3 neuron with nearly 5 , 000 synapses and utilized empirically - derived estimates of the frequency and corner frequency of synaptic charge fluctuations . We then utilized a numerical technique referred as dynamic mesh simulation to predict the voltage resistance of the neuron to arbitrary synaptic current inputs .We identified that synaptic current fluctuations tended to broaden and reduce the frequency of postsynaptic voltage reactions to step changes in current input , but also introduced significant fluctuations in the period it taking the voltage response to reach its peak value . The former phenomenon possibly diminished the efficacy of synaptic transmission , whereas the former may have broad clinical implications , possibly leading to noise - caused decorrelation of postsynaptic surge times .",
        "rewrite_text": "Synaptic transmission is constantly subject to fluctuations, often referred to as background noise, which significantly alters the representation of outgoing synaptic currents in the postsynaptic cell. This, in turn, affects neural production and activity. To characterize the sounds at individual excitatory and inhibitory synaptic contacts, we employ whole-cell voltage-clamp recordings from neurons in cortical layer 2/3 of a neural slice. Our findings indicate that the power spectrum of synaptic charge fluctuations typically exhibits low-pass characteristics, with a corner frequency that varies between synapses but frequently falls within the range of tens of hertz.\n\nFurthermore, we developed a computational model of a layer 2/3 neuron with nearly 5,000 synapses and utilized empirically derived estimates of synaptic charge fluctuation frequency and corner frequency. Utilizing a technique known as dynamic mesh simulation, we predicted the voltage resistance of the neuron to arbitrary synaptic current inputs. Our observations reveal that synaptic current fluctuations tend to broaden and reduce the frequency of postsynaptic voltage responses to changes in current input. Additionally, these fluctuations significantly affect the time it takes for the voltage response to reach its peak value. While the former phenomenon may diminish the efficiency of synaptic transmission, the latter may have broad clinical implications, potentially leading to noise-induced decorrelation of postsynaptic spiking times.",
        "ori-fast-z-score": -0.6704783996548059,
        "water-fast-z-score": 6.034305596893254,
        "rewrite-fast-z-score": 2.2445701677816263
    },
    {
        "original_text": "Intervening metal systems in the line of sight to Gamma-Ray Bursts and Quasi-Stellar Objects (QSOs) offer the opportunity to measure the distribution of heavy elements in the early universe. Using observations from the Keck I telescope and Hubble Space Telescope, I examine the Carbon, Oxygen, and Magnesium distributions in the Universe back to z > 6.5. While Carbon and Oxygen remain relatively uniformly distributed, Magnesium is depleted at a 4.2σ significance level in the highest redshift QSO host, at z = 6.5. If the abundance pattern of the Milky Way (MW) interstellar medium (ISM) can be applied to these distant galaxies, the implied redshift of formation for the majority of this Mg depletion is z = 9.1 ± 2.3. If instead we apply the abundance pattern of the local group gas, the implied formation redshift is z = 6.5 ± 0.7. In the Gamma-Ray Burst host, a possible local group interloper at z = 0.48 is depleted at the 2.6σ level at z = 6.5, consistent with the formation at z = 9.1 ± 2.3. No depletion is observed in the highest redshift QSO and Gamma-Ray Burst sight-lines, at z = 6.5 ± 0.7 and 9.1 ± 2.3, respectively, suggesting that the first galaxies may not have undergone the same processes of chemical enrichment as local group gas. This study represents the first measurement of the chemical evolution of the early universe beyond the range of the EUCLID satellite, and will likely remain the most precise measurement until galaxies can be observed to significantly higher redshifts using the James Webb Space Telescope or other future telescopes.",
        "watermark_text": "Intervening metal systems in the line of sight to Gamma - Ray Bursts and Quasi - Stellar Objects ( QSOs ) offer the option to measure the distribution of heavy metals in the early universe . Using observations from the Keck I telescope and Hubble Space Telescope , I examine the Carbon , Oxygen , and Magnesium distributions in the Universe back to z > 6 . 5 .While Carbon and Oxygen remain remarkably uniformly scattered , Magnesium is exhausted at a 4 . 2σ significance level in the highest redshift QSO host , at z = 6 . 5 . If the abundance behavior of the Milky Way ( MW ) interstellar medium ( ISM ) can be applied to these distant galaxies , the implied redshift of formation for the majority of this Mg depletion is z = 9 . 1 ± 2 . 3 .If instead we apply the abundance behavior of the local group gas , the implied structure redshift is z = 6 . 5 ± 0 . 7 . In the Gamma - Ray Burst host , a possible local group interloper at z = 0 . 48 is exhausted at the 2 . 6σ level at z = 6 . 5 , consistent with the formation at z = 9 . 1 ± 2 . 3 .No depletion is observed in the highest redshift QSO and Gamma - Ray Burst seeing - lines , at z = 6 . 5 ± 0 . 7 and 9 . 1 ± 2 . 3 , respectively , showing that the first galaxies must not have undergone the same processes of biological enrichment as local group gas . This study constitutes the first measurement of the chemical evolution of the early universe beyond the range of the EUCLID satellite , and will probably remain the most accurate calculation until galaxies can be observed to significantly greater redshifts using the James Webb Space Telescope or other upcoming telescopes .",
        "rewrite_text": "The intervening metal systems along the line of sight to gamma-ray bursts and quasi-stellar objects (QSOs) provide an opportunity to measure the distribution of heavy metals in the early universe. By utilizing observations from the Keck I telescope and the Hubble Space Telescope, I examine the distributions of carbon, oxygen, and magnesium in the universe back to a redshift of z > 6.5. Carbon and oxygen are found to be remarkably uniformly distributed, while magnesium is depleted at a significance level of 4.2σ in the highest redshift QSO host at z = 6.5. If the abundance behavior of the Milky Way's interstellar medium (ISM) can be applied to these distant galaxies, the implied redshift of formation for the majority of this magnesium depletion is estimated to be z = 9.1 ± 2.3. If, on the other hand, we apply the abundance behavior of the local group gas, the inferred structural redshift is z = 6.5 ± 0.7.\n\nIn the Gamma-Ray Burst host, a possible local group interloper at z = 0.48 is found to be depleted at the 2.6σ level at z = 6.5, which is consistent with a formation redshift of z = 9.1 ± 2.3. No depletion is observed in the highest redshift QSO and Gamma-Ray Burst sightlines at z = 6.5 ± 0.7 and z = 9.1 ± 2.3 respectively, indicating that the first galaxies did not undergo the same processes of biological enrichment as local group gas. This study represents the first measurement of the chemical evolution of the early universe beyond the reach of the EUCLID satellite, and is likely to remain the most accurate calculation until galaxies can be observed to significantly greater redshifts using the James Webb Space Telescope or other upcoming telescopes.",
        "ori-fast-z-score": -0.30779350562554625,
        "water-fast-z-score": 3.3857285618810087,
        "rewrite-fast-z-score": 1.7822655773580138
    },
    {
        "original_text": "This paper uses a multilevel statistical analysis to examine the role of behavioural and demographic factors in determining the size of firms. We find that differences in firm sizes among agents are significantly related to the behaviour, i.e. strategies, that these agents use in their interactions, but not to demographics factors such as gender, age or position. We also find that the same behavioural factors are significantly related to the size of groups of agents, but only to a smaller degree. These findings have significant implications for the understanding of why firms differ in size and provide a new perspective on the discussion of strategy and organizational behaviour. This paper uses a multilevel statistical analysis to examine the role of behavioural and demographic factors in determining the size of firms. We find that differences in firm sizes among agents are significantly related to the behaviour, i.e. strategies, that these agents use in their interactions, but not to demographics factors such as gender, age or position. We also find that the same behavioural factors are significantly related to the size of groups of agents, but only to a smaller degree. These findings have significant implications for the understanding of why firms differ in size and provide a new perspective on the discussion of strategy and organizational behaviour. We study the Firm Size Effect (FSE), which is the observation that large firms tend to be larger than small ones of equal measures in an economic system. This size variation, commonly observed in the data, has provoked decades of debate about the causes. We analyse large data from Poland and show that the FSE is mostly explained by variation in behaviour. We partition the sample in groups of agents using hierarchical clustering based on similarity in their strategies. The analysis reveals that groups of similar agents with similar strategies account for most of the FSE. Within each strategy group, however, the FSE disappears. These findings have significant implications for the understanding of the FSE and provide a new perspective on the discussion of strategy and organizational behaviour.",
        "watermark_text": "This paper using a multilevel statistical analysis to examine the importance of behavioural and demographic factors in measuring the size of firms . We see that differences in company numbers among agents are greatly related to the behaviour , i . e .strategies , that these agents use in their interactions , but not to demographics parameters such as gender , age or rank . We additionally find that the same behavioural factors are greatly related to the size of groups of agents , but only to a smaller degree .These studies have important implications for the knowledge of why firms differ in size and bring a new insight on the debate of strategy and management behaviour . This paper using a multilevel statistical analysis to examine the importance of behavioural and demographic factors in measuring the size of firms .We see that differences in company numbers among agents are greatly related to the actions , i . e . strategies , that these agents use in their interactions , but not to demographics considerations such as gender , age or rank .We additionally find that the same behavioural factors are greatly related to the size of groups of agents , but only to a smaller degree . These studies have important implications for the knowledge of why firms differ in size and bring a new insight on the debate of strategy and management habits .We research the Firm Size Effect ( FSE ) , which is the observation that large firms tend to be larger than large ones of equal measures in an economic system . This size variation , commonly observed in the information , has provoked decades of dispute about the causes .We analyse large data from Poland and suggest that the FSE is mainly explained by variation in behaviour . We partition the sample in teams of agents using hierarchical clustering based on similarity in their strategies .The data reveals that groups of related agents with similar strategies account for most of the FSE . Within each strategy group , however , the FSE disappears .These studies have important implications for the knowledge of the FSE and bring a new insight on the debate of strategy and management behaviour .",
        "rewrite_text": "This study employs a multilevel statistical analysis to explore the significance of behavioral and demographic factors in determining the size of businesses. We observe that the disparities in company counts among various agents are strongly linked to their interactive strategies rather than demographic parameters such as gender, age, or rank. Furthermore, we discover that these behavioral factors are highly correlated with the size of groups of agents, albeit to a lesser extent.\n\nThese findings hold crucial implications for understanding why firms differ in size and offer fresh perspectives on the debate surrounding strategy and management behavior. We are also investigating the Firm Size Effect (FSE), which refers to the observation that larger firms tend to be larger than their peers in an economic system when measured equally. This size variation, commonly observed in various industries, has sparked decades of debate over its causes.\n\nBy analyzing extensive data from Poland, we propose that the FSE is primarily explained by variations in behavior. We segment the sample into teams of agents using hierarchical clustering based on their strategy similarity. The data reveals that groups of related agents with similar strategies account for the majority of the FSE. However, within each strategy group, the FSE becomes insignificant.\n\nThese studies hold significant implications for understanding the FSE and provide fresh insights into the discourse on strategy and management habits.",
        "ori-fast-z-score": -1.5713484026367723,
        "water-fast-z-score": 7.014182615527996,
        "rewrite-fast-z-score": 1.6570343122169822
    },
    {
        "original_text": "Researchers around the world are making large, ever more complex simulations of the universe. These simulations, called “models,” are used to make predictions about the universe, and they can be tested against observations of the universe. One such model, which has been very successful at explaining a lot of data about the early universe, is the “Lambda Cold Dark Matter” model, or ΛCDM for short. It assumes that the universe is composed of about 72% dark energy, 23% dark matter, and just 5% ordinary matter, including light particles such as photons, electrons and neutrinos. The neutrinos are particularly important as they are extremely difficult to directly observe, but their presence is essential to keeping the universe flat and stopping it from collapse back onto itself. The model neutrinos are called “neutrino masses,” and the phenomenon of neutrino mass is one of the most mysterious in all of science. In the early 2000s, the Laser Interferometer Space Antenna (LISA) detected a significant decrease in the speed of neutrinos moving through space-time, which suggested that neutrinos might have a tiny but non-zero mass. Many observations since then, however, have failed to find this decrease in speed, which would be expected if the neutrino mass was a tiny 0.1eV. On the other hand, various extensions of the standard model of particle physics suggest that neutrinos might have a non-zero mass. These include the see-saw mechanism, which postulates the existence of right-handed neutrinos. By adding a symmetry which forces right-handed neutrinos to have a non-zero, but tiny, mass, the standard model of particle physics can be extended to the see-saw model. Additional experimental evidence in favor of neutrino mass includes the observation of neutrino oscillations, which show that at least two of the neutrinos have non-zero, but differing, masses. Additionally, cosmology shows that if neutrinos have non-zero masses, then they must have a “normal” mass of approximately 0.05eV or less. If neutrino masses are confirmed, then they will be the first elementary particles shown to have this property. While their interactions are so far unsolved mysteries, neutrinos are the only known particles that can interact via the weak force, which interacts in this way over such large distances. The observation of neutrino masses would be a major breakthrough in our understanding of the universe.",
        "watermark_text": "Researchers around the world are making large , ever more sophisticated simulations of the universe . These simulations , known “ models , ” are applied to make predictions about the universe , and they can be evaluated against measurements of the universe .One such model , which has been very effective at explaining a lot of evidence about the early universe , is the “ Lambda Cold Dark Matter ” model , or ΛCDM for short . It assumes that the universe is composed of about 72 % bright energy , 23 % light matter , and just 5 % everyday matter , mostly light particles such as photons , electrons and neutrinos .The neutrinos are particularly important as they are extremely difficult to fully observe , but their presence is crucial to maintaining the universe flat and stop it from collapse back onto itself . The model neutrinos are called “ neutrino masses , ” and the phenomenon of neutrino mass is one of the most bizarre in all of science .In the early 2000s , the Laser Interferometer Space Antenna ( LISA ) detected a substantial drop in the speed of neutrinos moving through space - time , which suggested that neutrinos might have a small but non - zero mass . Many experiments since then , however , have failed to find this reduction in speed , which would be anticipated if the neutrino mass was a small 0 . 1eV .On the other hand , various extensions of the standard theory of particle theory suggest that neutrinos might have a non - zero mass . These include the saw - saw mechanism , which postulates the existence of right - handed neutrinos .By adding a symmetry which forces right - handed neutrinos to have a non - zero , but tiny , mass , the standard theory of particle science can be enlarged to the saw - saw model . Additional experimental evidence in favor of neutrino mass encompasses the observation of neutrino oscillations , which show that at least two of the neutrinos have non - zero , but varying , masses .Additionally , cosmology shows that if neutrinos have non - zero masses , then they must have a “ normal ” mass of approximately 0 . 05eV or smaller . If neutrino masses are confirmed , then they will be the first elementary particles shown to have this property .While their interactions are so far unsolved mysteries , neutrinos are the only known ions that can connect via the weak force , which interacts in this way over such great distances . The observation of neutrino masses might be a major advancement in our studying of the universe .",
        "rewrite_text": "Global researchers are carrying out extensive and increasingly sophisticated simulations of the universe. These simulations, commonly known as \"models,\" are utilized to make predictions about the universe and can be evaluated based on measurements of the same. One such model, particularly effective in explaining a considerable amount of evidence regarding the early universe, is the \"Lambda Cold Dark Matter\" model, or ΛCDM for short.\n\nThis model posits that the universe is composed of approximately 72% bright energy, 23% light matter, and just 5% regular matter, predominantly made up of light particles such as photons, electrons, and neutrinos. Neutrinos are particularly significant as they are notoriously difficult to fully observe, yet their presence is crucial for maintaining the flatness of the universe and preventing it from collapsing back onto itself. The model's neutrinos are referred to as \"neutrino masses,\" and the phenomenon of neutrino mass is one of the most enigmatic in all of science.\n\nIn the early 2000s, the Laser Interferometer Space Antenna (LISA) detected a notable decrease in the speed of neutrinos moving through space-time, suggesting that neutrinos might have a small but non-zero mass. However, numerous subsequent experiments have failed to confirm this speed reduction as expected if the neutrino mass were a mere 0.1eV.\n\nOn the other hand, various extensions of the standard theory of particle physics suggest that neutrinos may indeed have a non-zero mass. This includes the saw-saw mechanism, which proposes the existence of right-handed neutrinos. By introducing a symmetry that forces right-handed neutrinos to have a non-zero but minuscule mass, the standard theory of particle science can be expanded to the saw-saw model. Additional experimental evidence supporting the existence of neutrino mass includes observations of neutrino oscillations, which indicate that at least two of the known neutrinos have non-zero but varying masses.\n\nFurthermore, cosmology indicates that if neutrinos have non-zero masses, they must have a \"normal\" mass of approximately 0.05eV or less. If confirmed, neutrino masses would be the first elementary particles known to possess this property. Despite the mysteries surrounding their interactions, neutrinos are the only known particles that can interact via the weak force over vast distances. The discovery of neutrino masses could be a significant breakthrough in our understanding of the universe.",
        "ori-fast-z-score": 2.3846153846153846,
        "water-fast-z-score": 7.769230769230769,
        "rewrite-fast-z-score": 2.6610007244439693
    },
    {
        "original_text": "Cosmic rays with energies greater than 10^{15} eV are believed to interact with the atmosphere of our planet and their fluxes are significantly diminished as compared to the fluxes at lower energies. The so-called  knee  in the energy spectrum of these cosmic rays is defined as the energy at which the differential flux becomes approximately constant. There are two conflicting hypotheses for the nature of this  knee . One hypothesis attributes the knee to a suppression of the spectral index of the cosmic ray energies observed by the experiments flying at lower altitudes, while the other argues that the knee is an actual change of the primary cosmic ray energy spectrum. To settle this dispute, it is important to have a method to determine the energy of primary cosmic rays with an adequate precision to either detect a change or determine if a suppression of the flux is present. The GAMMA (Gamma-ray Astronomy in the Initiative Era) experiment, designed for the detection of gamma-rays produced by cosmic ray interactions in the atmosphere, can contribute to this problem because, in contrast to most other cosmic ray experiments, it is able to measure the energy of primary cosmic rays with an absolute accuracy of approximately 30%. In this work, we present spectra of primary cosmic rays measured by the GAMMA experiment for four different rigidities of the experimental setup. We found that the differential flux of primary cosmic rays decreases with energy approximately according to a power law with an index of −2.59 at rigidities greater than 10^{18.5} eV/(γ-1), where γ is the Lorentz factor of the primary cosmic ray. For rigidities between 10^{18.5} and 10^{17.5} eV/(γ-1), the energy spectrum of cosmic rays starts to slowly flatten, indicating a possible change in the energy spectrum. However, the available statistics does not allow us to determine if the change is statistically significant.",
        "watermark_text": "Cosmic rays with energies higher than 10 ^ { 15 } eV are said to interact with the atmosphere of our planet and their fluxes are greatly diminished as compared to the fluxes at lower energies . The so - called toe in the power spectrum of these cosmic rays is characterized as the energy at which the differential flux becomes roughly continuous .There are two conflicting hypotheses for the nature of this leg . One hypothesis relates the knee to a suppression of the spectral index of the cosmic ray energies recorded by the experiments flew at lower altitudes , while the other asserts that the knee is an actual shift of the primary cosmic ray energy spectrum .To settle this dispute , it is important to have a technique to estimate the power of primary cosmic rays with an ample accuracy to either detect a change or assess if a suppression of the flux is present . The GAMMA ( Gamma - ray Astronomy in the Initiative Era ) experiment , built for the observation of gamma - particles generated by gamma radiation interactions in the atmosphere , can help to this challenge because , in comparison to most other cosmic ray studies , it is able to measure the power of primary cosmic rays with an absolute accuracy of almost 30 % .In this research , we present spectra of primary cosmic rays calculated by the GAMMA experiment for four different rigidities of the experimental setup . We showed that the differential flux of primary cosmic rays decreases with energy approximately according to a power law with an index of −2 . 59 at rigidities greater than 10 ^ { 18 . 5 } eV / ( γ - 1 ) , where γ is the Lorentz factor of the primary cosmic ray .For rigidities between 10 ^ { 18 . 5 } and 10 ^ { 17 . 5 } eV / ( γ - 1 ) , the power spectrum of cosmic rays starts to slowly flatten , showing a possible change in the power spectrum . However , the provided statistics does not enable us to predict if the shift is statistically substantial .",
        "rewrite_text": "Cosmic rays with energies exceeding 10^15 eV are known to interact with the Earth's atmosphere, and their fluxes are significantly reduced in comparison to those at lower energies. The so-called \"knee\" in the power spectrum of these cosmic rays is defined as the energy where the differential flux becomes nearly continuous. There exist two contrasting hypotheses regarding the nature of this phenomenon. One hypothesis links the knee to a suppression of the spectral index of cosmic ray energies recorded by experiments conducted at lower altitudes. The other hypothesis posits that the knee represents a genuine shift in the primary cosmic ray energy spectrum.\n\nTo resolve this debate, it is essential to develop a technique that accurately estimates the power of primary cosmic rays, enabling the detection of changes or assessing the presence of flux suppression. The GAMMA (Gamma-ray Astronomy in the Initiative Era) experiment, designed to observe gamma particles generated by gamma radiation interactions in the atmosphere, can aid in this challenge. In contrast to many other cosmic ray studies, GAMMA is capable of measuring the power of primary cosmic rays with an absolute accuracy of nearly 30%.\n\nIn this research, we present spectra of primary cosmic rays calculated by the GAMMA experiment for four different setup rigidities. Our findings indicate that the differential flux of primary cosmic rays decreases with energy, approximately following a power law with an index of -2.59 at rigidities exceeding 10^18.5 eV/(γ-1), where γ represents the Lorentz factor of the primary cosmic ray. For rigidities ranging between 10^18.5 and 10^17.5 eV/(γ-1), the power spectrum of cosmic rays begins to flatten slightly, suggesting a potential change in the power spectrum. However, the provided statistics do not permit us to determine whether this shift is statistically significant.",
        "ori-fast-z-score": -0.0842151921066519,
        "water-fast-z-score": 7.832012865918626,
        "rewrite-fast-z-score": 3.546580225021987
    },
    {
        "original_text": "A superbimonte discovered in 1703 by a Swedish amateur astronomer, Georgius Barwick, is known today as the Nobeyama molecular cloud. The molecular cloud is located in the southern celestial hemisphere around 5.5 kiloyears away, which makes it the most remote object for which a spatial mapping of thedistribution of molecular gas has been performed so far. Using the Nobeyama radio observatory, we have mapped the carbon monoxidedistribution in a sample of 25 nearby spiral galaxies and found that barred spiral galaxies have a larger quantityof molecular gas than non-barred spiral galaxies at the same optical luminosity. We propose that the strongerbars, which are associated with bigger bulges, stabilize the potential, and this in turn promotes theformation of more molecular gas. It is now well known that barred spirals are more abundant in the universe than non-barred spirals, which indicates that bars are important mechanisms in the evolution of galaxies. Our finding supports the view that bars promote the evolution of the host galaxies, probably by promoting the transfer of gas to the central region1,2. The gas there forms stable circumnuclear regions and through SF produces new generations of stars, leading to the formation of bigger and bigger bulges. 1 Athanassoula, E.; Parijs, G. *Origin of Bars in Disk Galaxies*. Astronomy and Astrophysics 270,angle= 270 ,Author= E. Athanassoula & G. Parijs  abstract= Athanassoula, E.; Parijs, G. *Origin of Bars in Disk Galaxies*. Astronomy and Astrophysics 270,angle= 270 ,Author= E. Athanassoula & G. Parijs  author_role= author  datetime= 2017-12-09T14:00:00.000Z  url= https://ui.adsabs.harvard.edu/abs/2017A&A...540L...A.. Subramaniam, A.; Magdis, G. A.; Rigopoulou, D.; Hsu, N.-Y. *A Two-Pronged Approach to Understanding the Origin of Bars*. Astronomy and Astrophysics 558,angle= 558 ,Author= A. Subramaniam & G. A. Magdis  author_role= author  datetime= 2019-02-24T15:30:00.000Z  url= https://ui.adsabs.harvard.edu/abs/2019A&A...610A..43S  abstract= Athanassoula, E.; Parijs, G. *Origin of Bars in Disk Galaxies*. Astronomy and Astrophysics 270,angle= 270 ,Author= Athanassoula, E.; Parijs, G. > <frontmatter> <author_name>A",
        "watermark_text": "A superbimonte found in 1703 by a Swedish amateur observatory , Georgius Barwick , is known nowadays as the Nobeyama molecular cloud . The chemical cloud is situated in the southern heavenly hemisphere around 5 . 5 kiloyears distant , which makes it the most remote body for which a spatial mapping of thedistribution of molecular vapor has been performed so far .Using the Nobeyama radio observatory , we have charted the carbon monoxidedistribution in a sample of 25 nearby spiral galaxies and found that barred spiral clusters have a greater quantityof molecular vapor than non - barred spiral clusters at the same visual luminosity . We suggest that the strongerbars , which are identified with bigger bulges , stabilize the potential , and this in turn encourages theformation of more molecular gas .It is now well believed that barred spirals are more rich in the universe than non - barred spirals , which implies that bars are important processes in the evolution of galaxies . Our found confirms the view that barred promote the evolution of the host galaxies , probably by encouraging the transfer of gas to the main region1 , 2 .The gas there creates stable circumnuclear regions and through SF produces novel generations of stars , leading to the formation of bigger and larger bulges . 1 Athanassoula , E . ; Parijs , G . * Origin of Bars in Disk Galaxies * .Astronomy and Astrophysics 270 , angle = 270 , Author = E . Athanassoula & G . Parijs abstract = Athanassoula , E . ; Parijs , G . * Origin of Bars in Disk Galaxies * . Astronomy and Astrophysics 270 , angle = 270 , Author = E . Athanassoula & G . Parijs author _ role = writer datetime = 2017 - 12 - 09T14 : 00 : 00 . 000Z url = https : / / ui . adsabs . harvard . edu / abs / 2017A & A . . . 540L . . . A . . Subramaniam , A . ; Magdis , G . A . ; Rigopoulou , D . ; Hsu , N . - Y .* A Two - Pronged Approach to Understanding the Origin of Bars * . Astronomy and Astrophysics 558 , angle = 558 , Author = A . Subramaniam & G . A . Magdis writer _ role = writer datetime = 2019 - 02 - 24T15 : 30 : 00 . 000Z url = https : / / ui . adsabs . harvard . edu / abs / 2019A & A . . . 610A . . 43S abstract = Athanassoula , E . ; Parijs , G . * Origin of Bars in Disk Galaxies * .Astronomy and Astrophysics 270 , angle = 270 , Author = Athanassoula , E . ; Parijs , G . > < frontmatter > < writer _ title > A",
        "rewrite_text": "The Nobeyama molecular cloud, discovered in 1703 by the Swedish amateur observer Georgius Barwick, is now a well-known feature in the sky. This chemical cloud is located in the southern hemisphere of the heavens, approximately 5.5 kiloyears away, making it the most distant body where a spatial mapping of molecular vapor distribution has been achieved so far. Using the Nobeyama radio observatory, we have mapped the distribution of carbon monoxide in a sample of 25 nearby spiral galaxies and found that barred spiral clusters tend to have a higher quantity of molecular vapor than non-barred spiral clusters with the same visual luminosity. We propose that the stronger bars, which are associated with larger bulges, stabilize the potential, thereby encouraging the formation of more molecular gas. It is widely believed that barred spirals are more prevalent in the universe than non-barred spirals, indicating that bars play a crucial role in the evolution of galaxies. Our findings reinforce the notion that bars promote the evolution of host galaxies, potentially by encouraging the transfer of gas to the main region. The gas then creates stable circumnuclear regions and, through star formation, produces new generations of stars, leading to the formation of larger and more prominent bulges.\n\nReferences:\n\n1. Athanassoula, E., & Parijs, G. (2017). Origin of Bars in Disk Galaxies. Astronomy and Astrophysics, 270(angle=270).\n2. Subramaniam, A., Magdis, G. A., Rigopoulou, D., & Hsu, N. - Y. (2019). A Two-Pronged Approach to Understanding the Origin of Bars. Astronomy and Astrophysics, 558(angle=558).\n\nThese references provide further information on the evolution of galaxies and the role of bars in them.",
        "ori-fast-z-score": -0.5773502691896257,
        "water-fast-z-score": 6.292853089020909,
        "rewrite-fast-z-score": 1.671258043593467
    },
    {
        "original_text": "Atomistic simulations based on density functional theory (DFT) calculations are presented for the dissociation of oxygen molecules on the Al(111) surface. Calculations were performed using the method of rapid adiabaticfollowing, which permits the description of non-adiabatic dynamics by solving the time-dependent Schrödinger equation on a mixed normal-Wigner representation Hamiltonian. The simulation method is first validated against experimental reaction coordinates for both the associative and dissociative pathways. The results are then presented for the first truly non-adiabatic dynamics simulations of the oxygen molecule dissociation on Al(111). It is found that the non-adiabatic effects can play an important role in the associative, as well as in the dissociative, pathway, notably by affecting the transition state ensemble. The influence of the substrate in the dissociation process is finally discussed. In particular, these simulations permit the first description of the non-adiabatic dissociation process of O2 molecules on the Al(111) surface. The simulations show that the dissociative pathway is non-adiabatic, whereas the associative one is adiabatic up to the transition state. In the dissociative path, non-adiabatic coupling is very strong at the entrance of the transition state, whereas it is negligible in the associative path. Finally, the simulations evidence that the substrate plays a significant role in the dissociation process. The Al(111) surface hosts two stable configurations for the O2 molecule: 1x2 and 2x2. The 1x2 structure corresponds to the linear geometry with the oxygen atom in the top position and the two hydrogen atoms in the bridge position, whereas the 2x2 structure corresponds to the bent geometry with the oxygen atom in the second position. These two structures differ by the orientation of the molecular axis. In the following, we will refer to the 2x2 O2 structure as “bent” and to the 1x2 O2 structure as “linear”. Initially, the O2 molecule is located on top of the surface in the linear geometry, either in the top or bridge position. A sufficiently high energy excitation could then lead to the rearrangement of the molecular axis with a 1x2 O2 structure, in which the molecular axis points towards the substrate. The nuclear wavepacket corresponding to this excitation has a large amplitude on the top position of the linear structure, and essentially vanishes on the bridge position. These simulations show that the molecular axis is located above the bridge position of the Al surface, as observed in experiments. This excitation could also lead to the dissociation of the O2 molecule into two atomic oxygen atoms. However, only a small portion of the wavepacket localised on top of the linear structure is initially involved in the dissociation process, in good agreement with available experimental data. This means that the associative pathway is adiabatic. In this work, we go one step further and consider the non-adiabatic dynamics of both pathways. For that purpose",
        "watermark_text": "Atomistic simulations based on density functional theory ( DFT ) calculations are presented for the dissociation of oxygen molecules on the Al ( 111 ) surface . Calculations were performed using the method of rapid adiabaticfollowing , which allows the description of non - adiabatic dynamics by solving the period - dependent Schrödinger equation on a mixed normal - Wigner model Hamiltonian .The model approach is initially validated against empirical reaction coordinates for both the associative and dissociative pathways . The results are then presented for the first truly non - adiabatic dynamics simulations of the oxygen molecule dissociation on Al ( 111 ) .It is found that the non - adiabatic effects can play an important role in the associative , as well as in the dissociative , mechanism , notably by involving the transition state ensemble . The impact of the substrate in the dissociation process is next considered .In particular , these simulations permit the first explanation of the non - adiabatic dissociation process of O2 compounds on the Al ( 111 ) surface . The simulations prove that the dissociative pathway is non - adiabatic , whereas the associative one is adiabatic up to the transition state .In the dissociative path , non - adiabatic coupling is very strong at the entrance of the transition state , whereas it is negligible in the associative path . Finally , the simulations evidence that the substrate plays a substantial importance in the dissociation process .The Al ( 111 ) surface hosts two stable arrangements for the O2 molecule : 1x2 and 2x2 . The 1x2 form corresponds to the linear geometry with the oxygen atom in the top position and the two hydrogen atoms in the bridge position , whereas the 2x2 configuration relates to the curved structure with the oxygen atom in the second placement .These two structures differ by the orientation of the molecular axis . In the following , we will refer to the 2x2 O2 structure as “ twisted ” and to the 1x2 O2 form as “ linear ” .Initially , the O2 molecule is situated on top of the surface in the linear geometry , either in the top or bridge position . A sufficiently high energy excitation might then result to the rearrangement of the molecular axis with a 1x2 O2 configuration , in which the molecular axis points towards the substrate .The atomic wavepacket associated to this excitation has a large frequency on the top position of the linear structure , and essentially vanishes on the bridge position . These simulations reveal that the molecular axis is situated above the bridge position of the Al surface , as demonstrated in experiments .This excitation might additionally lead to the dissociation of the O2 molecule into two atomic oxygen atoms . However , only a small portion of the wavepacket localised on top of the linear structure is initially engaged in the dissociation process , in good agreement with existing experimental evidence .This implies that the associative pathway is adiabatic . In this research , we went one step further and consider the non - adiabatic dynamics of both pathways .For that purpose",
        "rewrite_text": "Atomistic simulations utilizing density functional theory (DFT) calculations are presented to explore the dissociation of oxygen molecules on the Al (111) surface. These calculations employ the method of rapid adiabatic following, which enables the description of non-adiabatic dynamics by solving the period-dependent Schrödinger equation on a mixed normal-Wigner model Hamiltonian. The model's validity is initially confirmed through comparison with empirical reaction coordinates for both associative and dissociative pathways.\n\nThe results are then presented for the initial non-adiabatic dynamics simulations of oxygen molecule dissociation on Al (111). It is found that non-adiabatic effects play a significant role in both the associative and dissociative mechanisms, notably involving the transition state ensemble. The influence of the substrate in the dissociation process is subsequently examined.\n\nThese simulations provide the first explanation of the non-adiabatic dissociation process of O2 compounds on the Al (111) surface. The simulations demonstrate that the dissociative pathway is non-adiabatic, while the associative pathway remains adiabatic until the transition state. In the dissociative path, non-adiabatic coupling is notably strong at the onset of the transition state, while it is negligible in the associative path.\n\nFurthermore, the simulations highlight the crucial role of the substrate in the dissociation process. The Al (111) surface hosts two stable arrangements for the O2 molecule: a 1x2 arrangement and a 2x2 arrangement. The 1x2 form corresponds to a linear geometry with the oxygen atom in the top position and two hydrogen atoms in the bridge position, while the 2x2 configuration refers to a curved structure with the oxygen atom in a second placement. These two structures differ in the orientation of the molecular axis.\n\nIn these simulations, the 2x2 O2 structure will be referred to as \"twisted,\" while the 1x2 O2 form will be referred to as \"linear.\" Initially, the O2 molecule is positioned on top of the surface in a linear geometry, either in a top or bridge position. A sufficiently high energy excitation can lead to a rearrangement of the molecular axis, resulting in a 1x2 O2 configuration where the molecular axis points towards the substrate. The associated atomic wavepacket has a high frequency on the top position of the linear structure and essentially disappears at the bridge position.\n\nThese simulations confirm that the molecular axis is positioned above the bridge position of the Al surface, as demonstrated in experiments. This excitation may further lead to the dissociation of the O2 molecule into two atomic oxygen atoms. However, only a small portion of the wavepacket localized on top of the linear structure is initially involved in the dissociation process, which aligns well with existing experimental evidence. This suggests that the associative pathway is adiabatic. In this research, we have taken a step further to investigate the non-adiabatic dynamics of both pathways.",
        "ori-fast-z-score": 1.9335101599064688,
        "water-fast-z-score": 7.938566201357355,
        "rewrite-fast-z-score": 4.0818547820247675
    },
    {
        "original_text": "The long-wavelength (low-frequency) dynamics of liquids, such as water, appear to be well described by the Stokes equation. In this limit, the viscosity conveys a local divergence of velocity field. Incompressibility of the liquid implies a divergence of stress tensor as well. In turn, the force balance between stress and elastic forces leaves an intrinsic shear wave whose amplitude decays as the inverse square of the distance from the forcing point. These few simple assumptions are enough to account for most of the complex non-equilibrium phenomena in the low-frequency rheology of Newtonian fluids. However, in practice most real-world liquids have non-Newtonian properties. This is particularly evident in the limit of long wavelengths, such that the elastic forces become negligible as compared to the viscous forces. In this limit, the only mechanism of forcing is viscosity. It remains to be seen if this simple model is capable of accounting for the low-frequency rheology of such fluids.",
        "watermark_text": "The long - wavelength ( low - frequency ) dynamics of liquids , such as water , appear to be well described by the Stokes equation . In this limit , the viscosity conveys a local divergence of velocity field .Incompressibility of the liquid suggests a divergence of stress tensor as also . In turn , the force balance between stress and elastic forces leaves an intrinsic shear wave whose intensity decays as the inverse square of the distance from the forcing point .These few basic statements are enough to account for most of the complex non - equilibrium phenomena in the small - frequency rheology of Newtonian fluids . However , in practice most real - time liquids have non - Newtonian properties .This is especially obvious in the limit of extended wavelengths , such that the elastic forces become negligible as compared to the viscous forces . In this limit , the only principle of forcing is viscosity .It remains to be shown if this straightforward model is capable of accounting for the small - frequency rheology of such fluids .",
        "rewrite_text": "The dynamics of liquids with long wavelengths (low frequencies), such as water, seem to be accurately described by the Stokes equation. Within this context, viscosity is observed to induce a local velocity field divergence. The incompressibility of the liquid implies a similar divergence in the stress tensor. Consequently, the equilibrium of force between stress and elastic forces results in an intrinsic shear wave, whose intensity diminishes as the inverse square of the distance from the forcing point. These fundamental statements are sufficient to explain most of the complex non-equilibrium phenomena in the low-frequency rheology of Newtonian fluids.\n\nHowever, in practical applications, most real-time liquids exhibit non-Newtonian properties. This becomes especially evident in the realm of extended wavelengths, where elastic forces become negligible in comparison to viscous forces. In this scenario, the sole principle of force is viscosity. It remains to be determined whether this simplified model is capable of accounting for the low-frequency rheology of such fluids.",
        "ori-fast-z-score": 2.9260286799032644,
        "water-fast-z-score": 6.203180801394921,
        "rewrite-fast-z-score": 2.393172105652397
    },
    {
        "original_text": "A spectropolarimetric observation of the Ca II 8498 A and 8542 A lines in the quiet Sun is presented. The data were obtained using the Dutch Open Telescope with the new Waveslicer on the Meteor satellite. The spectropolarimetric data cubes cover a field of view of 30 by 30 solar radii and a resolution of 0.33 by 0.33 arcseconds. The observed area was close to the solar disc centre and included a granule of size 4 by 4 Mm in resolution. Polarization signals of both line profiles and the continuum were detected, but no circular polarization signals above the detection limit of 0.1 percent were observed. The line-of-sight component of the magnetic field strengths was derived using the forward integration technique and was found to be around 100 G. The average values of the field strength in the centre of the granules were found to be in the range 50-200 G.",
        "watermark_text": "A spectropolarimetric measurement of the Ca II 8498 A and 8542 A lines in the quiet Sun is displayed . The data were obtained using the Dutch Open Telescope with the new Waveslicer on the Meteor spacecraft .The spectropolarimetric data cubes cover a field of view of 30 by 30 solar radii and a resolution of 0 . 33 by 0 . 33 arcseconds . The observed area was close to the solar disc centre and included a granule of diameter 4 by 4 Mm in resolution .Polarization signals of both line profiles and the continuum were detected , but no linear polarization signals above the detection limit of 0 . 1 percent were detected . The line - of - seeing component of the magnetic field strengths was derived using the back analysis technique and was obtained to be around 100 G . The estimated values of the field intensity in the centre of the granules were found to be in the range 50 - 200 G .",
        "rewrite_text": "A spectropolarimetric measurement of the Ca II 8498 A and 8542 A lines in the undisturbed Sun is presented. The data were acquired using the Dutch Open Telescope equipped with the latest Waveslicer technology on the Meteor spacecraft. The spectropolarimetric datasets encompass a 30x30 solar radius field of view, with a resolution of 0.33x0.33 arcseconds. The observed area was situated near the center of the solar disk and included a granule with a diameter and resolution of 4x4 Mm. Both line profiles and continuum polarization signals were detected, but no linear polarization signals exceeding the detection limit of 0.1% were found. The magnetic field strengths' line-of-sight component was derived using back analysis techniques, resulting in an estimate of approximately 100 G. The estimated field intensity values at the center of the granules were found to range between 50 and 200 G.",
        "ori-fast-z-score": 0.13483997249264842,
        "water-fast-z-score": 3.640679257301507,
        "rewrite-fast-z-score": 1.8708286933869707
    },
    {
        "original_text": "Recent cosmological simulations of galaxy evolution have reproduced the morphology-density relationship and star formation rate - density relationship observed in local galaxies. These simulations, however, cannot reproduce the full distribution of galaxy properties at both low and high redshift. It has been suggested that feedback from active galactic nuclei (AGN) and quasars may play a role in regulating star formation in their host galaxies. In this paper, we present high resolution, cosmological, hydrodynamical simulations of a sub-set of galaxies in a single volume of a large, modern simulation, in which a simple model for radio-mode feedback is included. The model is characterized by a constant Bondi-Hoyle-Lyttleton accretion rate onto the supermassive black hole, which is adjusted so that the AGN contributes a specified amount of radio heating to the interstellar medium. We have selected a simulation which is volume-limited and resolves all galaxies above a given mass, and which includes the effects of stellar feedback, AGN feedback and the evolving gas properties from the larger simulation. We find that even without AGN triggered starbursts, the momentum input from radio mode feedback is enough to significantly disrupt the cold streams which form the core of cooled galaxy clusters, and thus suppress the formation of bright ellipticals. At the same time, this feedback is not sufficient to disrupt the clusters completely. In lower mass galaxies, below the critical cluster mass, we do not see significant suppression of star formation, although we do find a decrease in the concentration of stellar metallicity and stellar density. We conclude that while radio-mode feedback is an effective regulator of star formation in the early universe, it alone is not sufficient to reproduce the full distribution of galaxies across the Hubble sequence.",
        "watermark_text": "Recent cosmological simulations of galaxy evolution have reproduced the morphology - density correlation and galaxy formation rate - density correlation observed in nearby galaxies . These simulations , however , unable reproduce the full pattern of galaxy structures at both high and low redshift .It has been proposed that feedback from active galactic nuclei ( AGN ) and quasars might play a role in controlling galaxy formation in their host galaxies . In this paper , we present high resolution , cosmological , hydrodynamical simulations of a sub - set of stars in a single volume of a large , modern experiment , in which a simple simulation for television - mode feedback is included .The model is characterized by a steady Bondi - Hoyle - Lyttleton accretion rate onto the supermassive black hole , which is adjusted so that the AGN adds a specified amount of radio heating to the interstellar medium . We have picked a simulation which is volume - limited and resolves all galaxies above a given mass , and which includes the effects of stellar feedback , AGN feedback and the evolving gas characteristics from the smaller simulation .We see that even without AGN triggered starbursts , the velocity input from radio mode feedback is enough to significantly disrupt the cool streams which form the core of cooled galaxy galaxies , and therefore suppress the formation of bright ellipticals . At the same time , this input is not sufficient to destroy the clusters completely .In smaller density clusters , below the key cluster mass , we do not see considerable suppression of galaxy formation , although we do discover a reduction in the density of stellar metallicity and stellar concentration . We suggest that while broadcast - mode feedback is an efficient regulator of galaxy formation in the early universe , it alone is not sufficient to reproduce the full distribution of stars across the Hubble sequence .",
        "rewrite_text": "Recent studies utilizing advanced cosmological simulations have accurately replicated the morphology-density correlation and galaxy formation rate-density correlation observed in nearby galaxies. However, these simulations have failed to entirely replicate the complex patterns of galaxy structures at both high and low redshifts. It has been proposed that the feedback from active galactic nuclei (AGN) and quasars may play a pivotal role in regulating galaxy formation within their host galaxies.\n\nIn this research paper, we present high-resolution, cosmological, hydrodynamic simulations focusing on a subset of stars within a single volume of a large, modern experimental setup. This model incorporates a simplified simulation of television-mode feedback. It is characterized by a consistent Bondi-Hoyle-Lyttleton accretion rate onto the supermassive black hole, which is adjusted to ensure that the AGN contributes a specific amount of radio heating to the interstellar medium.\n\nWe have selected a simulation that is volume-limited and resolves all galaxies above a specified mass. This simulation incorporates the effects of stellar feedback, AGN feedback, and the evolving gas characteristics from smaller simulations. Our findings indicate that even without AGN-triggered starbursts, the velocity input from radio mode feedback is sufficient to significantly disrupt the cool streams that form the core of cooling galaxies, thereby suppressing the formation of bright elliptical galaxies. At the same time, this input is not enough to completely destroy the clusters.\n\nIn clusters of lower density, below a critical cluster mass, we observe no significant suppression of galaxy formation. However, we do detect a reduction in the density of stellar metallicity and stellar concentration. We suggest that while feedback in broadcast mode is an efficient regulator of galaxy formation in the early universe, it alone is insufficient to reproduce the complete distribution of stars across the Hubble sequence.",
        "ori-fast-z-score": -0.2491364395612199,
        "water-fast-z-score": 7.333333333333333,
        "rewrite-fast-z-score": 3.8248057562244493
    },
    {
        "original_text": "Fourier analysis on homogeneous spaces has a long tradition in analysis. Convolution on locally compact groups is one of the most important and widely studied operation. In contrast, Fourier analysis on semisimple Lie groups and their homogeneous spaces has been less developed, although they appear naturally in the representation theory of Lie groups. In this paper, we establish convolution results on these groups, by imitating the locally compact case. In particular, we show that if G is a semisimple Lie group and K a compact subgroup, then the natural analogue of the convolution map on the unitary dual of G, is also a associative algebra product on the algebra of functions on the coset space G/K. We also discuss some related results and examples. This work is part of a series, in which we explore the structure and representation theory of semisimple Lie groups, and their homogeneous spaces. This work was completed during the summer of 2022 as part of the MIUR summer program SFAE at Northeastern University.",
        "watermark_text": "Fourier analysis on homogeneous maps has a long tradition in analysis . Convolution on locally compact groups is one of the most important and well understood procedure .In comparison , Fourier analysis on semisimple Lie groups and their homogeneous spaces has been less developed , although they appear naturally in the representation theory of Lie groups . In this paper , we obtain convolution results on these groups , by imitating the locally compact situation .In particular , we prove that if G is a semisimple Lie group and K a compact subgroup , then the natural analogue of the convolution map on the unitary dual of G , is also a associative algebra sum on the algebra of functions on the coset space G / K . We also discuss some related results and examples .This project is part of a trilogy , in which we investigate the composition and representation theory of semisimple Lie groups , and their homogeneous maps . This project was done during the summer of 2022 as part of the MIUR summer program SFAE at Northeastern University .",
        "rewrite_text": "Fourier analysis on homogeneous maps has a rich history in mathematical analysis. Convolution on locally compact groups stands out as a highly significant and well-understood procedure. In contrast, Fourier analysis on semisimple Lie groups and their homogeneous spaces has not been as extensively explored, yet they play a natural role in the representation theory of Lie groups. This paper presents convolution results achieved by drawing parallels from the locally compact scenario. Specifically, we demonstrate that when G represents a semisimple Lie group and K a compact subgroup, the convolution map's natural counterpart on the unitary dual of G emerges as an associative algebra sum within the function algebra of the coset space G/K. Additionally, we delve into related findings and illustrative examples.\n\nThis project is part of a larger series of investigations where we explore the composition and representation theory of semisimple Lie groups and their homogeneous maps. This work was carried out during the summer of 2022 as a part of the MIUR summer program SFAE at Northeastern University.",
        "ori-fast-z-score": 0.4923659639173309,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "Recently, the existence of localized waves in binary solids with equal concentrations of the two components, called binary Laves phases, has been established. These waves, called gyration waves, were observed experimentally for the first time in a eutectic alloy by Xia et al.  1 . They are characterized by a compact support in the transverse direction and a (non-trivial) crystal structure in the longitudinal direction. The existence of these waves was theoretically predicted by Nesterenko  2  and later confirmed by various authors using different modeling approaches  3-7 . This Letter presents an analysis of gyration waves in the framework of the Landau-Lifshitz equation for nonlinear dispersive waves. The existence of two branches of wave dispersion is established: a heavy branch corresponding to the density wave and a light branch corresponding to the spin wave. Moreover, the linearization spectrum includes two eigenvalues corresponding to the in-phase and out-of-phase oscillations of the heavy and light wave branches. The spin wave has a finite amplitude at the threshold of instability. Thus, gyration waves can be regarded as localized modes consisting of the superposition of a heavy and a light waves with the corresponding wavelengths (Goldstone modes). This picture is confirmed by the numerical solution of the full Landau-Lifshitz equation.",
        "watermark_text": "Recently , the existence of localized waves in binary solids with equal levels of the two parts , called binary Laves phases , has been known . These waves , known gyration currents , were detected experimentally for the first time in a eutectic alloy by Xia et al .1 . They are characterized by a compact support in the transverse direction and a ( non - trivial ) crystal formation in the longitudinal direction .The occurrence of these currents was theoretically confirmed by Nesterenko 2 and later confirmed by various papers using separate modeling strategies 3 - 7 . This Letter offers an discussion of gyration currents in the framework of the Landau - Lifshitz formula for nonlinear dispersive currents .The existence of two branches of wave dispersion is established : a heavy branch corresponding to the density signal and a light branch corresponding to the spin wave . Moreover , the linearization spectrum contains two eigenvalues corresponding to the in - phase and out - of - phase oscillations of the heavy and light wave branches .The spin wave has a finite amplitude at the threshold of instability . Thus , gyration currents can be regarded as confined modes formed of the superposition of a heavy and a light beams with the equivalent wavelengths ( Goldstone modes ) .This picture is confirmed by the numerical solving of the full Landau - Lifshitz formula .",
        "rewrite_text": "Recently, the existence of localized waves in binary solids with equal parts, referred to as binary Laves phases, has been widely recognized. These waves, known as gyration currents, were experimentally detected for the first time in a eutectic alloy by Xia et al. (1). They are characterized by a compact support in the transverse direction and a non-trivial crystal formation in the longitudinal direction. The occurrence of these currents has been theoretically confirmed by Nesterenko (2), and subsequently verified by various research papers employing distinct modeling strategies (3-7).\n\nThis letter delves into a discussion of gyration currents within the framework of the Landau-Lifshitz formula for nonlinear dispersive currents. The existence of two branches of wave dispersion is established: a heavy branch associated with the density signal and a light branch corresponding to the spin wave. Furthermore, the linearization spectrum encompasses two eigenvalues, corresponding to in-phase and out-of-phase oscillations of the heavy and light wave branches. The spin wave exhibits a finite amplitude at the threshold of instability, indicating that gyration currents can be viewed as confined modes formed by the superposition of heavy and light beams with equivalent wavelengths (Goldstone modes). This notion is supported by numerical solutions obtained from the complete Landau-Lifshitz formula.",
        "ori-fast-z-score": 0.20628424925175867,
        "water-fast-z-score": 5.703234321017339,
        "rewrite-fast-z-score": 3.147573111914219
    },
    {
        "original_text": "In this paper, we study the effect of confining the hard-sphere fluid between two parallel hard walls on its average properties. First, we calculate the perturbation correction to the equation of state of the ideal hard-sphere fluid. This correction is exponentially small when the wall-fluid density ratio is small and the separation between the walls is much larger than the particle diameter. Using this perturbation result, we show that the change in average density of the fluid due to confinement scales as the volume of the space enclosed by the confining walls. Next, we examine the effect of confinement on the structure factor and show that this also scales with the volume of the space. We also calculate the effect of confinement on the speed of sound and the inter-diffusion coefficient and show that they are independent of the volume of the enclosed space. Our results imply that confinement has little or no effect on the average entropy, enthalpy, heat capacity, viscosity, and self-diffusivity of the hard-sphere fluid.",
        "watermark_text": "In this paper , we study the impact of confining the hard - sphere fluid between two parallel hard walls on its average characteristics . First , we determine the perturbation correction to the equation of state of the ideal tough - sphere fluid .This correction is exponentially small when the wall - fluid density factor is tiny and the separation between the walls is much larger than the particle radius . Using this perturbation result , we find that the shift in average density of the liquid caused to confinement scales as the quantity of the space enclosed by the confining walls .Next , we investigate the impact of confinement on the formation factor and find that this also scales with the volume of the space . We additionally calculate the impact of confinement on the speed of noise and the inter - diffusion coefficient and know that they are independent of the volume of the enclosed space .Our results assume that confinement has little or no effect on the average entropy , enthalpy , temperature ability , viscosity , and self - diffusivity of the hard - sphere fluid .",
        "rewrite_text": "In this research, we examine the effects of enclosing a hard-sphere fluid between two parallel hard walls on its average characteristics. Firstly, we determine the perturbation correction to the equation of state for the ideal tough-sphere fluid. This correction is exponentially small when the density ratio between the wall and fluid is minimal and when the distance between the walls greatly exceeds the particle radius. Utilizing this perturbation result, we discover that the average density shift of the liquid due to confinement scales with the volume enclosed by the limiting walls.\n\nFurthermore, we investigate the influence of confinement on the formation factor and observe that it also scales with the space volume. Additionally, we calculate how confinement affects the speed of sound and the inter-diffusion coefficient, realizing that they are independent of the volume of the enclosed space. Our findings suggest that confinement has minimal or no impact on the average entropy, enthalpy, temperature capacity, viscosity, and self-diffusivity of the hard-sphere fluid.",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 5.666666666666667,
        "rewrite-fast-z-score": 2.4596747752497685
    },
    {
        "original_text": "Large spirals are found to possess statistically significant regular magnetic fields. These fields range from a few G to a few tens of mG, and are poloidal in structure, with essentially field-free regions near the galaxy s edge. The fields are composed of large-scale poloidal components, with small-scale toroidal components; i.e. the fields look roughly like the product of a large-scale poloidal vector with a small-scale toroidal scalar. The magnetic energy is typically a few times 10−18 to a few times 10−16 J, or a few times 1044 to a few times 10−15 ergs. The mean gas pressures in the galaxies are high enough (a few times 10−13 to a few times 10−12 Pa) that the observed field strengths can be produced by the equipartition between the gas pressures and the field pressures.",
        "watermark_text": "Large spirals are found to contain statistically substantial regular magnetic waves . These fields range from a few G to a few hundred of mG , and are poloidal in structure , with apparently field - free regions near the universe s boundary .The fields are composed of large - scale poloidal components , with little - scale toroidal components ; i . e . the fields resemble essentially like the product of a large - scale poloidal vector with a small - scale toroidal scalar .The magnetic energy is typically a few times 10−18 to a few times 10−16 J , or a few times 1044 to a few times 10−15 ergs . The mean gas pressures in the galaxies are high enough ( a few times 10−13 to a few times 10−12 Pa ) that the known field strengths can be obtained by the equipartition between the gas pressures and the field pressures .",
        "rewrite_text": "Large spiral galaxies are found to contain statistically significant regular magnetic waves with a range of field strengths from a few Gauss to several hundred milliGauss. These fields exhibit a poloidal structure, with apparent field-free regions close to the boundary of the universe. The fields are composed primarily of large-scale poloidal components, accompanied by smaller-scale toroidal components, essentially resembling the product of a large-scale poloidal vector and a small-scale toroidal scalar. The magnetic energy generally varies between a few times 10^-18 and a few times 10^-16 joules, or equivalently, between a few times 10^44 and a few times 10^-15 ergs. The mean gas pressures within these galaxies are sufficiently high (ranging from a few times 10^-13 to a few times 10^-12 Pascals), allowing for the equilibrium of known field strengths through the equipartition of gas and field pressures.",
        "ori-fast-z-score": 0.39735970711951313,
        "water-fast-z-score": 3.910359202286804,
        "rewrite-fast-z-score": 2.3566599571949607
    },
    {
        "original_text": "In this paper we consider a system of particles interacting via binary collisions and which also interact by means of a pairwise interaction potential which is degenerate in some sense. We show that the dynamics of this particle system can be approximated, in the appropriate scaling, by a coupled system of non-linear conservation laws. The approximation is in the sense of weak convergence of probability measures on appropriate function spaces. This hydrodynamic limit is then used to study the large time asymptotic behavior of the particle system. We consider the case when the interaction potential degenerate in a Kramers sense. In this case the hard-core interaction force and the inter-particle interaction force are not opposite, and they have the same scaling with the particle density. This degenerate dynamics do not satisfy the hyperbolicity condition. We show that if the initial data are regular enough and the time interval is small enough, then the particle system approach the hydrodynamic solution as time evolve. The proof is based on the theory of viscosity solutions and the compactness method.",
        "watermark_text": "In this paper we imagine a system of particles interacting via binary collisions and which also react by means of a pairwise coupling potential which is degenerate in some sense . We see that the dynamics of this particle system can be approximated , in the appropriate scaling , by a coupled system of non - linear conservation laws .The approximation is in the sense of weak convergence of likelihood measures on suitable function spaces . This hydrodynamic limit is then utilized to study the huge time asymptotic behavior of the particle system .We consider the case when the interaction potential degenerate in a Kramers sense . In this instance the hard - core interaction field and the inter - particle interaction force are not identical , and they have the same scaling with the particle density .This degenerate dynamics do not satisfy the hyperbolicity condition . We see that if the first data are regular enough and the period interval is tiny enough , then the particle system approach the hydrodynamic solution as time evolve .The proof is based on the principle of viscosity solutions and the compactness method .",
        "rewrite_text": "In this study, we envision a system of particles that engage in binary collisions and react through a pairwise coupling potential that is degenerate in some ways. We observe that the dynamics of this particle system, under appropriate scaling, can be approximated by a set of coupled non-linear conservation laws. This approximation is achieved through weak convergence of likelihood measures within suitable function spaces. This hydrodynamic limit is employed to investigate the long-term asymptotic behavior of the particle system.\n\nWe explore the situation where the interaction potential degenerates in a Kramers-like sense. In this scenario, the hard-core interaction field and the inter-particle force are not identical and maintain the same scaling with particle density. This degenerate dynamics does not meet the hyperbolicity criterion. We find that if the initial data are sufficiently regular and the period interval is sufficiently small, the particle system converges towards the hydrodynamic solution as time progresses. The proof relies on the principle of viscosity solutions and the compactness method.",
        "ori-fast-z-score": -1.3416407864998738,
        "water-fast-z-score": 2.9068883707497264,
        "rewrite-fast-z-score": 0.808290376865476
    },
    {
        "original_text": "In this paper, we present a linear reformulation of the Kuramoto model of self-synchronizing oscillators. In particular, we consider the network where each node is endowed with a dimensional variable and all the nodes are connected via undirected networks, the coupling function is a function of the difference of the dimensional variables of the connected nodes and all nodes are identical. We show that this model can be written as a coupling of diagonal and scalar ODEs. We then use the Schur complement to reduce the dynamics of the original model to this diagonal plus scalar system. We analyze the equilibria of the reduced model and show that in certain cases, the reduced model can exhibit oscillatory and chaotic behavior that is not present in the original model. We also show that, when some conditions on the coupling and the dimensional variables are met, the dynamics of the original system are lossless propagated to the reduced system.",
        "watermark_text": "In this paper , we present a linear reformulation of the Kuramoto model of self - synchronizing oscillators . In particular , we study the network where each node is endowed with a dimensional variable and all the nodes are connected via undirected networks , the coupling function is a function of the difference of the dimensional parameters of the linked nodes and all nodes are identical .We see that this model can be written as a coupling of diagonal and scalar ODEs . We then use the Schur complement to reduce the dynamics of the previous model to this diagonal plus scalar scheme .We evaluate the equilibria of the reduced model and find that in certain cases , the reduced model can exhibit oscillatory and chaotic activity that is not present in the previous model . We additionally find that , when some conditions on the interaction and the dimensional parameters are reached , the dynamics of the original system are lossless propagated to the reduced system .",
        "rewrite_text": "In this study, we introduce a linear re-formulation of the Kuramoto model for self-synchronizing oscillators. Specifically, we explore a network where each node is assigned a multi-dimensional variable and all nodes are interconnected through undirected networks. The coupling function relies on the difference in dimensional parameters between linked nodes, and all nodes are identical. We observe that this model can be expressed as a combination of diagonal and scalar ordinary differential equations (ODEs). Utilizing the Schur complement, we simplify the dynamics of the original model into this diagonal plus scalar framework. We assess the equilibrium points of the simplified model and discover that, in certain scenarios, it exhibits oscillatory and chaotic behaviors not present in the initial model. Furthermore, we find that when certain conditions regarding interactions and dimensional parameters are met, the dynamics of the original system seamlessly propagate to the reduced system.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 3.0,
        "rewrite-fast-z-score": -0.6108472217815261
    },
    {
        "original_text": "Massive young stellar objects (MYSOs) are among the most luminous and turbulent objects in the interstellar medium, yet their complex structures and transition to rich stellar clusters makes them challenging to study. In this letter, we present a multi-epoch high-frequency radio continuum study of a sample of 38 MYSOs. These sources were chosen to be strong water masers, potential signposts of high-mass star formation. We find a high incidence of positive spectral index gradients (approximately half the sample), which are often large and exhibit variability on timescales of weeks. We interpret this as variability of the free-free emission primarily arising from shocks excited by protostellar outflows and jets, in agreement with recent findings at lower frequencies. We also find a large population of sources with negative spectral index gradients, likely arising from free-free emission from ionized accreting material. The infrared luminosity, inferred mass accretion rate, and spectral gradient characteristics of this sample suggest that many of these sources may be in the process of transitioning from MYSO to either evolved star with an ionized inner disk, or to a less massive star with a circumstellar disk.",
        "watermark_text": "Massive young stellar bodies ( MYSOs ) are among the most luminous and turbulent elements in the interstellar medium , yet their complex shapes and shift to rich stellar clusters makes them complicated to study . In this letter , we present a multi - epoch high - frequency radio continuum study of a sample of 38 MYSOs .These sources were chosen to be powerful water masers , likely signposts of high - mass star formation . We get a high prevalence of favorable spectral index gradients ( nearly half the sample ) , which are often large and undergo variability on timescales of weeks .We interpret this as variability of the free - free emission principally originating from shocks excited by protostellar outflows and jets , in agreement with recent results at lower frequencies . We additionally find a large colony of sources with negative spectral index gradients , likely arising from free - free emission from ionized accreting matter .The infrared luminosity , inferred mass accretion rate , and spectral gradient qualities of this specimen suggest that several of these sources might be in the process of transitioning from MYSO to either mature star with an ionized inner disk , or to a smaller massive star with a circumstellar disk .",
        "rewrite_text": "Massive Young Stellar Objects (MYSOs) are among the most brilliant and unpredictable components in the interstellar medium. However, their intricate shapes and transitions into abundant stellar clusters make them challenging to study. In this communication, we present a multi-epoch high-frequency radio continuum investigation of a sample consisting of 38 MYSOs. These sources were carefully selected as powerful water masers, likely indicators of high-mass star formation.\n\nOur findings reveal a high frequency of favorable spectral index gradients (almost half of the sample), often significant and subject to changes on weekly timescales. We interpret this as variability in free-free emission primarily stemming from shocks stimulated by protostellar outflows and jets, aligning with recent findings at lower frequencies. Furthermore, we discover a significant cluster of sources with negative spectral index gradients, likely arising from free-free emission resulting from ionized accreting matter.\n\nThe infrared luminosity, inferred mass accretion rate, and spectral gradient characteristics of these sources suggest that several of them may be in the process of transitioning from MYSOs to either mature stars with an ionized inner disk or to smaller, massive stars with a circumstellar disk.",
        "ori-fast-z-score": 0.9434563530497265,
        "water-fast-z-score": 5.7655666019705505,
        "rewrite-fast-z-score": 0.8340576562282991
    },
    {
        "original_text": "An incoming droplet bouncing on a smooth horizontal surface may appear to a viewer as a ball of steady size. How much does it actually change in size? The obvious answer would be that it bounces as a unit, spreading its lift over the whole drop. This simple model, known as the unit bounce, was proposed in the 17th century and found to match experiments until the late 1800s. Since then, droplets bouncing on a solid surface with a high enough RH (relative humidity) demonstrate non-uniform size change: they are found to shrink in some places and to grow in others. This effect, known as non-uniform bouncing, was first observed in 1908 and is now well documented in videos. This phenomenon is counterintuitive and does not fit the classical bouncing model. We present a simple model predicting the lifetime of a bouncing droplet, showing that it is finite. After presenting a short qualitative description of the model, we describe a more detailed one, involving a partial differential equation, solved numerically. The solution reproduces the key features of the observed non-uniform bouncing: the shrinked and growed regions and their lifetime. We then discuss various physical mechanisms that could explain the observed effects, both in the framework of the proposed model and in the more standard framework of droplet bouncing.",
        "watermark_text": "An incoming droplet bouncing on a solid horizontal surface may appear to a reader as a ball of steady shape . How much does it really change in size ?The obvious answer would be that it bounces as a unit , spreading its lift over the whole drop . This basic model , known as the unit bounce , was suggested in the seventeenth century and found to match experiments until the mid 1800s .Since then , droplets scattering on a solid surface with a high enough RH ( relative humidity ) show non - uniform size difference : they are found to decline in some places and to expand in others . This phenomenon , known as non - uniform bouncing , was first observed in 1908 and is now well documented in videos .This phenomenon is counterintuitive and does not fit the classical bouncing theory . We present a simple simulation predicting the life of a spinning droplet , showing that it is finite .After presenting a brief qualitative summary of the model , we define a more thorough one , involving a partial differential equation , solved numerically . The solution reproduces the key features of the seen non - regular bouncing : the shrinked and growed regions and their lifetime .We then discuss various mechanical pathways that might explain the seen phenomena , both in the framework of the suggested model and in the more conventional framework of droplet bouncing .",
        "rewrite_text": "To a reader, an incoming droplet bouncing on a horizontal, solid surface may appear as a consistently shaped ball. However, how much does its size actually change? The apparent answer is that it bounces as a cohesive unit, distributing its lift across the entire droplet. This fundamental model, referred to as the unit bounce, was proposed in the 17th century and was found to align with experiments until the mid-1800s.\n\nSince then, droplets scattering on a solid surface, especially in environments with a high relative humidity (RH), exhibit non-uniform size variations. In certain areas, they are observed to diminish while expanding in others. This phenomenon, known as non-uniform bouncing, was first documented in 1908 and is now frequently captured in video recordings. This observation is counterintuitive and does not align with classical bouncing theories.\n\nWe present a straightforward simulation that forecasts the lifespan of a spinning droplet, revealing that it is finite. Following a brief qualitative overview of the model, we introduce a more comprehensive version, incorporating a partial differential equation solved numerically. This solution replicates the key characteristics of the observed irregular bouncing, including the shrunken and expanded regions and their lifespan.\n\nWe then explore various mechanical explanations for the observed phenomena, both within the context of the suggested model and within the more traditional framework of droplet bouncing.",
        "ori-fast-z-score": -1.1547005383792515,
        "water-fast-z-score": 5.703745285369415,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Recent progress in treating dynamical mean-field theory (DMFT) as a constrained variational procedure has led to its efficient numerical implementation in the linear muffin-tin approximation (LMTA). We report the first such implementation that allows for the calculation of both fermionic self-energies and related response functions. In this implementation the Hartree-Fock-like condition of no-double counting of interactions and response functions is ensured by combining a straightforward partial derivation of the LMTA equations with a constraint on the charge-density. Using the LDA+DMFT approximation to the photoemission spectrum of the half-filled three-dimensional Bethe lattice as a test case, we show that the resulting scheme yields charge-density profiles that are in excellent agreement with those found from full solution of the LDA+DMFT equations, but at a considerably lower computational cost. The linear muffin-tin approximation to DMFT has until now been limited to computing fermionic self-energies and related response functions. We report the first such implementation that allows for the calculation of both fermionic self-energies and related response functions. In this implementation the Hartree-Fock-like condition of no-double counting of interactions and response functions is ensured by combining a straightforward partial derivation of the LMTA equations with a constraint on the charge-density. Using the LDA+DMFT approximation to the photoemission spectrum of the half-filled three-dimensional Bethe lattice as a test case, we show that the resulting scheme yields charge-density profiles that are in excellent agreement with those found from full solution of the LDA+DMFT equations, but at a considerably lower computational cost. Theory Recently, a scheme for performing DMFT in the linear muffin-tin approximation (LMTA) was proposed. The resulting method is both efficient and accurate, and has been widely used to study electronic structure in metallic and strongly correlated systems. In this implementation, the Hartree-Fock-like condition of no-double counting of interactions and response functions is ensured by combining a straightforward partial derivation of the LMTA equations with a constraint on the charge-density. In this work, we report the first such calculation of dynamical response functions within the LMTA and show that this approach yields charge-density profiles that are in excellent agreement with those found from full solution of the LDA+DMFT equations, but at a considerably lower computational cost. Methods LDA+DMFT Linear muffin-tin approximation to DMFT LDA+DMFT self-consistency Theory of photoelectron spectroscopy Photoemission spectra Dynamical mean-field theory Linear muffin-tin approximation Dynamical mean-field theory within the LDA Dynamical mean-field theory within the LDA+DMFT Limitations of LDA+DMFT Finite-",
        "watermark_text": "Recent developments in treating dynamical mean - field theory ( DMFT ) as a constrained variational procedure has led to its efficient numerical realization in the linear muffin - tin approximation ( LMTA ) . We report the first such implementation that enables for the determination of both fermionic self - energies and related reaction functions .In this implementation the Hartree - Fock - like condition of no - double counting of molecules and response functions is achieved by combining a straightforward partial derivation of the LMTA equations with a constraint on the charge - density . Using the LDA + DMFT approximation to the photoemission spectrum of the half - covered three - dimensional Bethe structure as a demonstration case , we prove that the resulting scheme produces charge - density profiles that are in good agreement with those observed from full solving of the LDA + DMFT equations , but at a considerably lower computational expensive .The linear muffin - tin approximation to DMFT has until now been limited to computing fermionic self - energies and related reaction functions . We report the first such implementation that enables for the computing of both fermionic self - energies and related reaction functions .In this implementation the Hartree - Fock - like condition of no - double counting of molecules and response functions is achieved by combining a straightforward partial derivation of the LMTA equations with a constraint on the charge - density . Using the LDA + DMFT approximation to the photoemission spectrum of the half - covered three - dimensional Bethe structure as a demonstration case , we prove that the resulting scheme produces charge - density profiles that are in good agreement with those observed from full solving of the LDA + DMFT equations , but at a considerably lower computational expensive .Theory Recently , a scheme for performing DMFT in the linear muffin - tin approximation ( LMTA ) was suggested . The resulting method is both efficient and precise , and has been widely using to study electronic structure in metallic and strongly interacting systems .In this implementation , the Hartree - Fock - like condition of no - double counting of interactions and response functions is achieved by combining a straightforward partial derivation of the LMTA equations with a constraint on the charge - density . In this research , we publish the first such calculation of dynamical response functions within the LMTA and find that this methodology produces charge - density profiles that are in good agreement with those observed from full solving of the LDA + DMFT equations , but at a considerably lower computational expensive .Methods LDA + DMFT Linear muffin - tin approximation to DMFT LDA + DMFT self - stability Theory of photoelectron spectroscopy Photoemission spectra Dynamical mean - field model Linear muffin - tin approximation Dynamical mean - field model within the LDA Dynamical mean - field model within the LDA + DMFT Limitations of LDA + DMFT Finite -",
        "rewrite_text": "Recent advancements in the application of dynamical mean-field theory (DMFT) as a constrained variational procedure have resulted in efficient numerical implementations, particularly in the linear muffin-tin approximation (LMTA). We present the initial realization that enables the determination of both fermionic self-energies and associated reaction functions. In this approach, the Hartree-Fock-like condition of avoiding double counting of molecules and response functions is achieved through a combination of a straightforward partial derivation of the LMTA equations and a constraint on charge density.\n\nAs a demonstration, we employ the LDA+DMFT approximation to the photoemission spectrum of a half-covered three-dimensional Bethe structure. This confirms that our method produces charge-density profiles in good agreement with those obtained from a complete solution of the LDA+DMFT equations, but with significantly reduced computational cost.\n\nIn the realm of theory, a scheme for performing DMFT in the LMTA has been proposed recently. This method is both effective and accurate, and has become widely utilized in studying electronic structures in both metallic and strongly interacting systems. Within this implementation, the Hartree-Fock-like principle of avoiding double counting of interactions and response functions is achieved by combining a straightforward partial differentiation of the LMTA equations with a charge-density constraint.\n\nIn our research, we introduce the first calculation of dynamical response functions within the LMTA framework. Our findings indicate that this methodology produces charge-density profiles that are in good agreement with those derived from a comprehensive solution of the LDA+DMFT equations, yet with a notably reduced computational burden.\n\nFurthermore, we discuss methods such as LDA+DMFT, the linear muffin-tin approximation to DMFT, the self-stability theory of photoelectron spectroscopy, photoemission spectra, the dynamical mean-field model, the linear muffin-tin approximation within the LDA, and the application of the dynamical mean-field model within the LDA+DMFT framework. We also explore the limitations of LDA+DMFT and its potential for further finite... (remaining text has been truncated due to character limit)",
        "ori-fast-z-score": 0.1466471150213533,
        "water-fast-z-score": 8.25674954467424,
        "rewrite-fast-z-score": 1.2288478807785608
    },
    {
        "original_text": "In Dissipative Particle Dynamics (DPD), interactions between different particles are modeled by short-range, collision-driven forces that are approximately mean-zero and have a specified auto-correlation function. This approach, which treats short-range interactions as Gaussian white noise, is accurate for particle separations much larger than the dissipative lengthscale. At smaller separations, however, the separation-dependent correlations between the force and its underlying velocity cause the noise to become non-Gaussian. We derive an effective stochastic interaction by measuring the correlation between force and velocity at several different separations and using a minimum-squares fit to determine a two-point covariance function. This covariance function enables the stochastic interactions at short distances to better approximate the non-Gaussian force correlations at intermediate separations, while still enabling effective coupling at large distances where the noise approaches Gaussianity. The resulting DPD equations of motion are integrated with a stochastic velocity-Verlet scheme, and the accuracy of the effective interaction is tested by comparison with simulations of thermal equilibrium behavior.",
        "watermark_text": "In Dissipative Particle Dynamics ( DPD ) , behaviors between various particles are modeled by short - range , collision - triggered forces that are approximately mean - zero and have a specified auto - correlation function . This method , which presents short - range interactions as Gaussian white interference , is accurate for electron separations much larger than the dissipative lengthscale .At smaller separations , however , the separation - dependent correlations between the force and its underlying velocity cause the noise to become non - Gaussian . We derive an efficient stochastic interaction by assessing the relationship between pressure and speed at several different separations and using a limit - squares fit to find a two - point covariance matrix .This covariance matrix allows the stochastic interactions at short distances to easier predict the non - Gaussian force correlations at intermediate separations , while nevertheless allowing appropriate coupling at large distances where the noise approaches Gaussianity . The resulting DPD equations of movement are coupled with a stochastic velocity - Verlet scheme , and the accuracy of the effective interaction is tested by analogy with simulations of thermal equilibrium dynamics .",
        "rewrite_text": "In Dissipative Particle Dynamics (DPD), the behaviors among diverse particles are represented by short-range, collision-triggered forces that approximately maintain a mean-zero value and possess a designated auto-correlation function. This approach accurately reflects short-range interactions as Gaussian white noise interference for electron separations greater than the dissipative lengthscale. However, at smaller separations, the force's velocity-dependent correlations can lead to non-Gaussian noise.\n\nTo derive an efficient stochastic interaction, we analyze the relationship between pressure and speed at various distances and employ a least-squares fit to generate a two-point covariance matrix. This matrix facilitates the prediction of non-Gaussian force correlations at intermediate distances for short-range stochastic interactions. It also allows for appropriate coupling at greater distances where the noise tends towards Gaussianity.\n\nThe resulting DPD equations of motion are integrated with a stochastic velocity Verlet algorithm, and the effectiveness of the interactions is tested through comparisons with simulations of thermal equilibrium dynamics.",
        "ori-fast-z-score": -0.43133109281375365,
        "water-fast-z-score": 5.314796216557077,
        "rewrite-fast-z-score": 1.5275252316519468
    },
    {
        "original_text": "We present numerical results for three-nucleon observables in nuclear matter as functions of the pion mass. The predictions are obtained within the framework of a coupled-cluster expansion for the nuclear wave function, using two interaction kernels: a long-range one, corresponding to the leading order of a systematic chiral perturbation theory (ChPT) expansion, and a short-range one, corresponding to two-nucleon contact interactions. We show that, for observables related to nuclear matter saturation, the convergence of the results towards the physical point depends on the range of the interaction, with faster convergence for the short-range kernel. In addition, for a given range of the interaction, the dependence on the pion mass of the predictions increases as the density of the system under consideration decreases, i.e., as the range of the interaction increases. Finally, we argue that, in order to properly describe the evolution of the nuclearmany-body system with the QCD scale, an appropriate renormalization group analysis of ChPT should be performed.",
        "watermark_text": "We present numerical findings for three - nucleon observables in nuclear material as functions of the pion mass . The predictions are derived within the framework of a coupled - cluster expansion for the atomic wave function , using two coupling kernels : a long - range one , corresponding to the main order of a systematic chiral perturbation theory ( ChPT ) expansion , and a short - range one , equivalent to two - nucleon connection interactions .We see that , for observables associated to nuclear material saturation , the convergence of the results towards the physical point varies on the range of the interaction , with higher convergence for the short - range kernel . In addition , for a given range of the interaction , the dependence on the pion mass of the predictions increases as the density of the system under consideration decreases , i . e . , as the range of the interaction grows .Finally , we argue that , in order to properly describe the evolution of the nuclearmany - bodies scheme with the QCD scale , an appropriate renormalization group study of ChPT should be performed .",
        "rewrite_text": "We present numerical results for three-nucleon observables in nuclear matter, exploring their relationship with the pion mass. These predictions are derived within the context of a coupled-cluster expansion for the atomic wave function, utilizing two coupling kernels: one with a long-range corresponding to the primary order of a systematic chiral perturbation theory (ChPT) expansion, and another with a short-range akin to two-nucleon connection interactions. Our findings indicate that for observables linked to nuclear material saturation, the convergence of results towards the physical point varies depending on the interaction range, with a higher convergence observed for the short-range kernel. Furthermore, for a specific interaction range, the dependence of predictions on the pion mass increases as the density of the system decreases, i.e., as the interaction range expands. Ultimately, we suggest that to accurately describe the evolution of nuclear many-body systems with the QCD scale, a thorough renormalization group study of ChPT is essential.",
        "ori-fast-z-score": -0.808290376865476,
        "water-fast-z-score": 3.670651741928988,
        "rewrite-fast-z-score": 1.462614271203831
    },
    {
        "original_text": "Using observations from the Fermi Gamma Ray Space Telescope, we have detected gamma-rays from 37 supernova remnants (SNRs). The gamma-rays are dominated by hadrons, with a smaller component from bremsstrahlung. The observed gamma-ray spectra are well fitted by a single power-law in kinetic energy, with indices between 2.0 and 2.7. The gamma-ray efficiency, the fraction of supernova explosion energy deposited in the form of gamma-rays, is about 1% for young SNRs and increases with age as the amount of shocked gas increases. Hadronic gamma-ray production rates are a few percent of the historical yields, suggesting that cosmic-ray particles propagate through a dense medium with relatively mild interactions. We conclude that the observed gamma-rays are consistent with production in nuclei interactions with background protons, although uncertainties in the local cosmic-ray density and composition are large enough to allow significant contributions from electrons and Dark Matter particles. The detection of gamma-rays from young and middle-aged SNRs without corresponding radio synchrotron emission challenges existing models of gamma-ray production in SNRs.",
        "watermark_text": "Using observations from the Fermi Gamma Ray Space Telescope , we have discovered γ - radiation from 37 supernova remnants ( SNRs ) . The gamma - rays are dominated by hadrons , with a smaller component from bremsstrahlung .The observed gamma - ray spectra are better fitted by a single power - law in kinetic power , with indices between 2 . 0 and 2 . 7 . The gamma - ray efficiency , the fraction of supernova explosion power deposited in the form of gamma - rays , is about 1 % for young SNRs and increases with age as the proportion of excited gas tends .Hadronic gamma - ray production rates are a few third of the historical yields , showing that cosmic - ray waves propagate through a dense medium with relatively mild interactions . We suggest that the seen alpha - radiation are compatible with production in nuclei interactions with background protons , although uncertainties in the local cosmic - ray density and composition are small enough to allow significant contributions from electrons and Dark Matter particles .The observation of gamma - radiation from young and mid - old SNRs without corresponding radio synchrotron emission questions previous models of gamma - ray transmission in SNRs .",
        "rewrite_text": "Utilizing observations from the Fermi Gamma Ray Space Telescope, we have detected gamma-radiation originating from 37 supernova remnants (SNRs). The gamma-rays are predominantly composed of hadrons, with a lesser contribution from bremsstrahlung radiation. The observed gamma-ray spectra are more accurately fitted by a single power-law in kinetic power, with indices ranging between 2.0 and 2.7. The gamma-ray efficiency, which represents the fraction of supernova explosion energy deposited as gamma-rays, is approximately 1% for young SNRs and increases with age as the proportion of excited gas increases. \n\nHadronic gamma-ray production rates are approximately one-third of historical yields, indicating that cosmic ray waves propagate through a dense medium with relatively minimal interactions. We propose that the observed alpha-radiation is compatible with production through nucleus interactions with background protons. However, uncertainties in local cosmic ray density and composition are sufficiently small to allow for significant contributions from electrons and dark matter particles. The detection of gamma-radiation from young and mid-aged SNRs without corresponding radio synchrotron emission challenges previous models of gamma-ray transmission in SNRs.",
        "ori-fast-z-score": 0.953998092005724,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": 1.5230192477004287
    },
    {
        "original_text": "Using the Mileura Widefield Array low frequency demonstrator field prototype system, we report on observations of the crab giant pulses (GCTs). The Mileura field is located at ~1260 km distance from the mooncenter and exhibits highFaraday rotation. We recorded 20 hrs of observations in 2016, during which we detected 5 GCTs and determined Faraday rotation measures (RM) following each pulse. We also triggered simultaneosly 1.2 MHz of recording modes at two adjacent north-south locations in the array. In this band we detected two GCTs but only with sub-meter accuracy due to the long field of view of the array. We estimate the distance to the GCTs based on the dispersion measure (DM) and the RM, and constrain their sizes to be ~1 mas. We detect DM and RM vectors typically aligned with the center of the GCT peaks, consistent with the extreme magneto-ionic conditions in the crab pulsar magnetosphere. The results presented here demonstrate that the Mileura Widefield Array has the sensitivity to detect the GCTs with DMs up to ~15 pc/cm^3 and demonstrate its ability to perform multi-frequency searches for DM and RM variations across GCT pulses. Such observations can potentially detect the earth rotation relative to the rotating neutron star magnetosphere.",
        "watermark_text": "Using the Mileura Widefield Array short wavelength demonstrator field prototype system , we publish on observations of the crab giant pulses ( GCTs ) . The Mileura field is situated at ~ 1260 km distance from the mooncenter and exhibits highFaraday rotation .We recorded 20 hrs of measurements in 2016 , during which we spotted 5 GCTs and determined Faraday rotation measures ( RM ) following each pulse . We additionally generated simultaneosly 1 . 2 MHz of recording modes at two adjacent north - south sites in the array .In this band we spotted two GCTs but only with sub - meter accuracy thanks to the long field of view of the array . We estimate the distance to the GCTs based on the dispersion measure ( DM ) and the RM , and constrain their height to be ~ 1 mas .We detect DM and RM vectors typically aligned with the center of the GCT peaks , consistent with the severe magneto - ionic conditions in the crab pulsar magnetosphere . The results presented here demonstrate that the Mileura Widefield Array has the sensitivity to identify the GCTs with DMs up to ~ 15 pc / cm ^ 3 and suggest its able to conduct multi - frequency investigations for DM and RM changes across GCT signals .Such observations can possibly locate the earth rotation relative to the rotating neutron star magnetosphere .",
        "rewrite_text": "Using the Mileura Widefield Array's short-wavelength demonstrator field prototype system, we publish our observations of the Crab Giant Pulses (GCTs). The Mileura field is located approximately 1260 kilometers from the center of the Moon and displays a high Faraday rotation. In 2016, we recorded measurements for 20 hours during which we detected five GCTs and determined Faraday rotation measures (RM) following each pulse.\n\nFurthermore, we simultaneously generated 1.2 MHz of recording modes at two adjacent north-south sites within the array. Within this frequency band, we observed two GCTs but with sub-meter accuracy due to the wide field of view offered by the array. We estimate the distance to the GCTs based on dispersion measures (DM) and RM, and constrain their height to be approximately 1 milli-arcsecond. Our observations reveal that DM and RM vectors are typically aligned with the centers of GCT peak intensities, consistent with the severe magneto-ionic conditions within the Crab Pulsar magnetosphere.\n\nThe results presented here demonstrate that the Mileura Widefield Array has the sensitivity to identify GCTs with DMs up to approximately 15 pc/cm^3, suggesting its potential for conducting multi-frequency investigations into changes in DM and RM across GCT signals. Such observations may help to locate the Earth's rotation relative to the rotating neutron star magnetosphere.",
        "ori-fast-z-score": 0.8944271909999159,
        "water-fast-z-score": 5.444444444444445,
        "rewrite-fast-z-score": 3.2349831961031525
    },
    {
        "original_text": "The aim of this paper is to illustrate a methodology based on Principal Component Analysis (PCA) and Automatic Relevance Determination (ARD) for rapid data acquisition in geoscience applications. The presented approach is implemented for hydrogeological applications, where the acquired data consist of Ground-Penetrating Radar (GPR) signals, which are recorded over the same region where pressure changes are to be estimated. The proposed methodology consists of two steps. First, several signals are recorded under different conditions, from which a set of data will be selected for the learning process. The selection criteria are based on the prediction error computed from a model, assuming a Kalman Filter (KF) structure, using the recorded data to predict the pressure changes. Second, the Principal Component Analysis is applied to the selected data in order to reduce the dimensionality and to obtain a low-cost model. The proposed methodology is tested with artificial data and compared with the ARD technique, showing the effectiveness of the proposed approach.",
        "watermark_text": "The goal of this paper is to illustrate a technique based on Principal Component Analysis ( PCA ) and Automatic Relevance Determination ( ARD ) for rapid data acquisition in geoscience applications . The published method is implemented for hydrogeological applications , where the acquired data consist of Ground - Penetrating Radar ( GPR ) transmissions , which are recorded over the same region where pressure changes are to be assessed .The proposed methodology consists of two stages . First , various signals are recorded under various circumstances , from which a set of evidence will be selected for the learning task .The selection criteria are based on the prediction error computed from a model , assuming a Kalman Filter ( KF ) structure , using the captured data to predict the pressure changes . Second , the Principal Component Analysis is applied to the selected data in order to reduce the dimensionality and to obtain a small - cost model .The proposed methodology is tested with artificial data and compared with the ARD method , showing the ability of the suggested approach .",
        "rewrite_text": "The objective of this paper is to present a technique that utilizes Principal Component Analysis (PCA) and Automatic Relevance Determination (ARD) for expedited data acquisition in geoscience applications. This method has been implemented for hydrogeological applications, where the collected data encompasses Ground-Penetrating Radar (GPR) transmissions recorded over a specific region where pressure variations are to be evaluated.\n\nThe proposed methodology is structured into two phases. Initially, a range of signals are recorded under various conditions, from which a subset of data is selected as evidence for the learning process. The selection criteria are determined by the prediction error calculated from a model employing a Kalman Filter (KF) structure, utilizing the captured data to forecast pressure changes.\n\nIn the second phase, the selected data is subjected to Principal Component Analysis to reduce its dimensionality and produce a cost-effective model. The proposed methodology has been tested using artificial data and compared to the ARD method, demonstrating its effectiveness and reliability.",
        "ori-fast-z-score": 1.9123657749350298,
        "water-fast-z-score": 5.737097324805089,
        "rewrite-fast-z-score": 0.9299811099505543
    },
    {
        "original_text": "We report the detection of six magnetically-driven explosions of rapidly-rotating white dwarfs (WDs) following accretion-induced collapse. The survey was conducted using the ASAS-SN virtual telescope, which monitors the entire visible sky every few hours. Such explosions had not been observed from these objects before as all previously-known examples were from low-mass WDs. These explosions are characterized by a rapidly- rising brightness in the days leading up to the explosion, followed by a plateau, and ultimately a decay in magnitude. The plateau is caused by the ongoing collapse of the rotating WD to a neutron star or black hole. The explosion is confirmed by the presence of an underlying plateau in the subsequent light curve, which is not seen in cases of stable nuclear burning on the WD surface. The plateau duration and luminosity are consistent with the expected range for the amount of ejected material. We compare the observations to 1D hydrodynamical simulations of accretion-induced collapse with and without the inclusion of powerful magnetic fields, and find that the emission can best reproduced by simulations that include magnetic fields. We calculate the magnetic fields required to drive the explosions are approximately 100MG, which is within the capability of future megnetore facilities such as the Exawatt center for Plasma research in the EU and NASA s Long duration balloon program. Here we report on the detection of six exploding WDs, all of which had been observed by ASAS-SN previously. Of the six, four showed plateau-like light curves with distinct decreases in magnitude in the days leading up to the explosion, consistent with previous reports of magnetically-driven explosions from low-mass WDs. Two additional objects, namely SDSS J0804+2052 and SDSS J2315+1855, showed distinct plateau-like decreases in magnitude without the preceding increase in brightness, consistent with the scenario for magnetically-driven explosions from rapidly-rotating WDs for the first time. We compared the observed plateau durations and luminosities to simulations that included and excluded magnetic fields, and found that the emission from both simulations was best reproduced by simulations that included magnetic fields. Using our measured plateau durations and luminosities, we calculated the corresponding ejected material masses for each event, and found that these were within the expected range for material ejected in such explosions. We also determined the probable locations of the progenitor stars by cross-referencing our sample to Gaia Data Release 2 and found that four of the exploding WDs had most likely been observed in the field of view of their respective host galaxies. This is consistent with the idea that explosions from rapidly-rotating WDs are more likely to be detectable from their host galaxies than explosions from low-mass WDs. Our findings demonstrate that magnetic fields have a critical impact on the explosion mechanisms and outcomes from this astrophysical scenario and that such explosions are a promising avenue for the retention of mass in rapidly-rotating WDs. We report the detection of six exploding white",
        "watermark_text": "We report the observation of six magnetically - triggered explosions of quickly - spinning white dwarfs ( WDs ) following accretion - caused instability . The survey was done utilizing the ASAS - SN virtual observatory , which monitors the entire visible sky every few hours .Such explosions had not been observed from these objects before as all earlier - recorded examples were from small - density WDs . These explosions are marked by a rapidly - rising brightness in the days led up to the explosion , followed by a peak , and eventually a decay in intensity .The plateau is caused by the ongoing fall of the rotating WD to a neutron star or black hole . The explosion is predicted by the presence of an underlying plateau in the subsequent light curve , which is not seen in cases of stable nuclear burning on the WD surface .The plateau duration and luminosity are compatible with the expected range for the extent of expelled rock . We compare the observations to 1D hydrodynamical simulations of accretion - caused collapse with and without the introduction of powerful magnetic fields , and find that the emission can best reconstructed by simulations that include magnetic fields .We calculate the magnetic fields demanded to drive the explosions are approximately 100MG , which is within the capability of potential megnetore systems such as the Exawatt center for Plasma research in the EU and NASA s Long duration balloon program . Here we report on the discovery of six exploding WDs , all of which had been observed by ASAS - SN previously .Of the six , four displayed plateau - like light curves with distinct decreases in intensity in the days led up to the explosion , consistent with previous accounts of magnetically - driven explosions from small - mass WDs . Two additional objects , comprising SDSS J0804 + 2052 and SDSS J2315 + 1855 , exhibited distinct plateau - like decreases in magnitude without the preceding change in intensity , consistent with the scenario for magnetically - driven explosions from rapidly - spinning WDs for the first time .We compared the reported plateau durations and luminosities to simulations that included and excluded magnetic fields , and found that the emission from both simulations was well reproduced by simulations that included magnetic fields . Using our measured plateau durations and luminosities , we calculated the associated ejected substance masses for each event , and found that these were within the expected range for material expelled in such explosions .We additionally identified the probable sites of the progenitor stars by inter - referencing our sample to Gaia Data Release 2 and found that four of the exploding WDs had most likely been observed in the field of view of their respective host galaxies . This is consistent with the idea that explosions from swiftly - spinning WDs are more likely to be detectable from their host galaxies than incidents from high - mass WDs .Our findings show that magnetic fields have a critical effect on the explosion mechanisms and outcomes from this astrophysical situation and that such explosions are a likely avenue for the retaining of mass in rapidly - spinning WDs . We report the observation of six exploding white",
        "rewrite_text": "We present observations of six rapidly-spinning white dwarfs (WDs) that experienced magnetically-triggered explosions following accretion-induced instability. These observations were conducted using the ASAS-SN virtual observatory, which monitors the entire visible sky every few hours. These explosions have not been previously recorded from these objects, as all previous examples were observed in low-density WDs. The explosions are characterized by a rapid increase in brightness in the days leading up to the explosion, followed by a peak intensity and subsequent decay.\n\nThe plateau phase is attributed to the continuous collapse of the rotating WD towards a neutron star or black hole. The presence of an underlying plateau in the subsequent light curve predicts the explosion, which is not observed in cases of stable nuclear burning on the WD surface. The duration and luminosity of the plateau are consistent with the expected range for the amount of ejected material.\n\nWe compared our observations to 1D hydrodynamic simulations of accretion-induced collapse with and without the introduction of strong magnetic fields. We found that simulations including magnetic fields best reconstruct the observed emission. The required magnetic fields to drive these explosions are estimated to be approximately 100MG, which is within the capability of potential systems such as the Exawatt center for Plasma research in the EU and NASA's Long Duration Balloon program.\n\nIn this report, we detail the discovery of six exploding WDs, all of which were previously observed by ASAS-SN. Of these six objects, four exhibited plateau-like light curves with distinct decreases in intensity in the days leading up to the explosion, consistent with previous accounts of magnetically-driven explosions from low-mass WDs. Additionally, two other objects, SDSS J0804+2052 and SDSS J2315+1855, demonstrated distinct plateau-like decreases in magnitude without a preceding change in intensity, a scenario previously unobserved for rapidly-spinning WD-driven explosions.\n\nBy comparing reported plateau durations and luminosities to simulations with and without magnetic fields, we found that simulations including magnetic fields reproduce the observed emission well. Using our measured plateau durations and luminosities, we calculated the associated ejected mass for each event, which was found to be within the expected range for material ejected in such explosions. Furthermore, we identified the probable sites of the progenitor stars by referencing our sample to Gaia Data Release 2 and found that four of the exploding WDs were likely observed within the field of view of their respective host galaxies. This suggests that explosions from rapidly-spinning WDs are more likely to be detectable from their host galaxies than incidents involving high-mass WDs.\n\nOur findings indicate that magnetic fields play a critical role in the explosion mechanisms and outcomes in this astrophysical scenario, and that such explosions may be a significant pathway for maintaining mass in rapidly-spinning WDs. We present the observation of six exploding white dwarfs that provide valuable insights into this fascinating astrophysical process.",
        "ori-fast-z-score": -1.348399724926484,
        "water-fast-z-score": 6.524926101764231,
        "rewrite-fast-z-score": 1.6107745325136584
    },
    {
        "original_text": "Francium (Fr), radon (Ra), noble gases (He, Ne, Ar, Kr, and Xe) and halogen atoms (F, Cl, Br, and I) are isoelectronic sequences. Here we present electronic structure calculations to predict the electron affinity (EA), excitation energies (EX), excitation spectra, polarizabilities (PA), and lifetime (τ) of the francium isoelectronic sequence. We predict EA = 4.15 (4) eV and EX = 0.73 (15) eV for the anion of FrH-. The 0_00-1_00 SO2+ ionization potential (IP) of Fr+ is located at IP = 9.23 (3) eV. The 0_11-1_11 IP is 7.44 (3) eV. The predicted 0_00-1_00 IP-IP separation of 1.33 (9) eV is in excellent agreement with experiment. The polarizability of Fr+ is 155 (16) Å3, in excellent agreement with the experimental measurement of 160 (15) Å3. The 0_00-1_00 τ of Fr+ is 1.48 (16) ps. This is in good agreement with the experimentally measured 1.59 (2) ps.",
        "watermark_text": "Francium ( Fr ) , radon ( Ra ) , noble metals ( He , Ne , Ar , Kr , and Xe ) and halogen atoms ( F , Cl , Br , and I ) are isoelectronic sequences . Here we present electronic structure estimates to predict the electron affinity ( EA ) , excitation energies ( EX ) , excitation spectra , polarizabilities ( PA ) , and lifetime ( τ ) of the francium isoelectronic sequence .We predict EA = 4 . 15 ( 4 ) eV and EX = 0 . 73 ( 15 ) eV for the anion of FrH - . The 0 _ 00 - 1 _ 00 SO2 + ionization potential ( IP ) of Fr + is situated at IP = 9 . 23 ( 3 ) eV .The 0 _ 11 - 1 _ 11 IP is 7 . 44 ( 3 ) eV . The predicted 0 _ 00 - 1 _ 00 IP - IP separation of 1 . 33 ( 9 ) eV is in good agreement with research .The polarizability of Fr + is 155 ( 16 ) Å3 , in good agreement with the empirical observation of 160 ( 15 ) Å3 . The 0 _ 00 - 1 _ 00 τ of Fr + is 1 . 48 ( 16 ) ps .This is in good agreement with the experimentally recorded 1 . 59 ( 2 ) ps .",
        "rewrite_text": "Francium (Fr), radon (Ra), noble metals (including He, Ne, Ar, Kr, and Xe), and halogen atoms (F, Cl, Br, and I) constitute isoelectronic sequences. We present estimates of electronic structure to predict the electron affinity (EA), excitation energies (EX), excitation spectra, polarizabilities (PA), and lifetime (τ) for the francium isoelectronic sequence. We predict an EA value of 4.15(4) eV and an EX value of 0.73(15) eV for the anion of FrH⁻. The 0_00 - 1_00 SO2+ ionization potential (IP) of Fr+ is situated at IP = 9.23(3) eV. The 0_11 - 1_11 IP is 7.44(3) eV. The predicted 0_00 - 1_00 IP-IP separation of 1.33(9) eV aligns well with research findings. The polarizability of Fr+ is estimated to be 155(16) Å3, which is in good agreement with the empirical observation of 160(15) Å3. Furthermore, the 0_00 - 1_00 τ of Fr+ is calculated to be 1.48(16) ps, which is in good agreement with the experimentally recorded value of 1.59(2) ps.",
        "ori-fast-z-score": 0.5222329678670935,
        "water-fast-z-score": 4.003786086981051,
        "rewrite-fast-z-score": 3.2796489996607274
    },
    {
        "original_text": "WZ Sge was first identified as a strongly interacting binary system consisting of a K-type donor and a white dwarf secondary. Using very recent data, WZ Sge still stands as an unique example of a dwarf nova with extremely fast outbursts. Its giant outbursts reached a magnitude of 5.5 in 1957, 1957, 1964, 1966, 1967, 1969, 1970, 1971, 1973, 1975, 1976, 1977, 1978, 1981, 1982, 1983, 1984, 1985, 1986, 1988, 1990, 1991, 1992, 1994, 1995, 1996, 1997, 1998, 1999, 2000, 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2012, 2013, 2014, 2015, and 2016. The 2017 outburst was only recently caught by ASAS-SN. WZ Sge is the longest recognized outburst interval of a dwarf nova. Here we report the results of a long-term photometric and spectroscopic monitoring of WZ Sge since 2010. Our data set covers 19 outbursts of WZ Sge, one normal outburst and several normal non-outburst intervals. We describe the method and data analysis in the Appendix. Our results show that the WZ Sge system has undergone a long-term behaviour evolution that can be described by a variation of the mass ratio q=K3secondary/Kdonor=0.068±0.002 ranging from 0.068 to 0.078.",
        "watermark_text": "WZ Sge was first identified as a strongly interacting binary system consisting of a K - class donor and a white dwarf secondary . Using very recent results , WZ Sge currently stands as an unique example of a dwarf nova with incredibly fast outbursts .Its giant outbursts achieved a magnitude of 5 . 5 in 1957 , 1957 , 1964 , 1966 , 1967 , 1969 , 1970 , 1971 , 1973 , 1975 , 1976 , 1977 , 1978 , 1981 , 1982 , 1983 , 1984 , 1985 , 1986 , 1988 , 1990 , 1991 , 1992 , 1994 , 1995 , 1996 , 1997 , 1998 , 1999 , 2000 , 2001 , 2002 , 2003 , 2004 , 2005 , 2006 , 2007 , 2008 , 2009 , 2010 , 2012 , 2013 , 2014 , 2015 , and 2016 . The 2017 outburst was only lately caught by ASAS - SN .WZ Sge is the longest recognized outburst period of a dwarf nova . Here we publish the conclusion of a long - term photometric and spectroscopic monitoring of WZ Sge since 2010 .Our data set covers 19 outbursts of WZ Sge , one regular outburst and many normal non - outburst ranges . We define the method and information study in the Appendix .Our results show that the WZ Sge system has undergone a many - term behaviour development that can be described by a variation of the mass ratio q = K3secondary / Kdonor = 0 . 068±0 . 002 ranging from 0 . 068 to 0 . 078 .",
        "rewrite_text": "WZ Sge has been identified as a binary system with strong interactions, featuring a K-class donor and a white dwarf secondary. Utilizing recent research findings, WZ Sge stands out as an exceptional case of a dwarf nova with astonishingly swift outbursts. Its significant outbursts reached a magnitude level of 5.5 across multiple years, spanning from 1957 to 2016. The outburst observed in 2017 was only recently captured by ASAS-SN. WZ Sge holds the longest-recognized outburst period among dwarf novae.\n\nIn this study, we present the conclusion of a comprehensive long-term photometric and spectroscopic monitoring of WZ Sge that has been conducted since 2010. Our dataset encompasses 19 outbursts of WZ Sge, including one regular outburst and various normal non-outburst phases. Details on the method and information studied are provided in the Appendix. Our research findings reveal that the WZ Sge system has undergone complex behavioral developments that can be described by a change in the mass ratio, q = K3secondary / Kdonor, ranging from 0.068 to 0.078 with an average of 0.068±0.002.",
        "ori-fast-z-score": 0.762000762001143,
        "water-fast-z-score": 6.017733356846111,
        "rewrite-fast-z-score": 0.8551861104941365
    },
    {
        "original_text": "Particle physics is an exciting field of science, with the discovery of the Higgs Boson completing the foundational work necessary for us to complete our understanding of the universe. The open access publishing model, in which the author pays a publication fee rather than reads fees, is resulting in a large increase in the amount of research being published. This increase in publishing has the potential to drastically increase our collective understanding of the universe, however it can be difficult for researchers not involved in the field to understand these papers and the datasets they reference. In this paper we provide a brief introduction to the open access publishing model for the non-expert, with an eye towards highlighting some of the more interesting articles published in this emerging field of research. We hope this will act as an introductory resource for people interested in this emerging area of research and encourage research into more in-depth articles about specific fields within particle physics.",
        "watermark_text": "Particle science is an exciting area of science , with the discovery of the Higgs Boson completing the foundational work necessary for us to complete our knowing of the universe . The open source publishing approach , in which the writer pays a publication fee rather than reads fees , is leading in a large rise in the quantity of research being published .This increase in publishing has the potential to drastically increase our collective understanding of the universe , however it can be challenging for researchers not involved in the field to comprehend these articles and the datasets they quote . In this paper we provide a brief introduction to the open source publishing approach for the non - expert , with an eye towards highlighting some of the more exciting publications published in this new area of research .We believe this will act as an introductory resource for people interested in this new area of research and encourage research into more in - depth topics about different fields within physics physics .",
        "rewrite_text": "Particle science stands as an intriguing domain of science, where the discovery of the Higgs Boson has paved the way for a comprehensive understanding of the universe. The adoption of an open-source publishing model, where writers pay a publication fee instead of reading fees, has contributed to a significant surge in the number of research publications. This surge in publications holds the potential to significantly enhance our collective knowledge of the cosmos. However, it can pose a challenge for non-experts to grasp the content and datasets mentioned in these articles.\n\nIn this paper, we offer a concise introduction to the open-source publishing approach for those unfamiliar with it, focusing on some of the most intriguing publications emerging from this new research frontier. We believe this will serve as a valuable introductory resource for individuals interested in this emerging field of research, encouraging further exploration of more in-depth topics across various physics disciplines.",
        "ori-fast-z-score": 1.462614271203831,
        "water-fast-z-score": 7.313071356019155,
        "rewrite-fast-z-score": 0.7977240352174656
    },
    {
        "original_text": "The 2D Ising model with strong square coupling in the limit of zero aspect ratio of the system exhibits a continuous transition at temperature Tc between a low temperature phase in which the interfaces are pinned by the Gaussian disorder and a high temperature phase in which they are unpinned. Using Monte Carlo simulations we estimate the critical exponent associated with the order of the transition as o = 1.23(2) which is consistent with the theoretical prediction of o = 4/11 = 0.3622.... We propose an efficient dynamic renormalization group (DRG) approximation to the problem. The DRG flows for the effective interface tension and Gaussian disorder strength are shown to be inconsistent with the two-sided free field fixed point. This indicates that the transition is indeed continuous. The predicted critical exponents are o = 1.25(5) and y = 0.38(1). Our DRG results were checked against the strong coupling expansion of  1  and excellent quantitative agreement was found for T < Tc. Beyond Tc our DRG flows fail to converge and a new fixed point with nontrivial field content must be sought for.  1  B. Nienhuis, J. Phys. A: Math. Gen. 15 (1982) 4943 https://arxiv.org/pdf/1702.05862.pdf PS: Please feel free to comment/ask for any clarifications --- The 2D Ising model with strong square coupling in the limit of zero aspect ratio of the system exhibits a continuous transition at temperature Tc between a low temperature phase in which the interfaces are pinned by the Gaussian disorder and a high temperature phase in which they are unpinned. Using Monte Carlo simulations we estimate the critical exponent associated with the order of the transition as o = 1.23(2). We propose an efficient dynamic renormalization group (DRG) approximation to the problem. The DRG flows for the effective interface tension and Gaussian disorder strength are shown to be inconsistent with the two-sided free field fixed point. This indicates that the transition is indeed continuous. The predicted critical exponents are o = 1.25(5) Our DRG results were checked against the strong coupling expansion of  1  and excellent quantitative agreement was found for T < Tc. Beyond Tc our DRG flows fail to converge and a new fixed point with nontrivial field content must be sought for.  1  B. Nienhuis, J. Phys. A: Math. Gen. 15 (1982). We thank the researchers of this paper for submitting it to arXiv.",
        "watermark_text": "The 2D Ising model with powerful square coupling in the limit of zero aspect ratio of the system displays a consistent change at temperature Tc between a high heat phase in which the connections are pinned by the Gaussian instability and a high heat phase in which they are unpinned . Using Monte Carlo simulations we estimate the important exponent identified with the order of the shift as o = 1 . 23 ( 2 ) which is compatible with the theoretical forecast of o = 4 / 11 = 0 . 3622 . . . . We suggest an efficient dynamic renormalization group ( DRG ) algorithm to the issue .The DRG flows for the effective interface pressure and Gaussian disorder strength are shown to be inconsistent with the two - sided free field fixed point . This implies that the transfer is indeed continuous .The predicted critical exponents are o = 1 . 25 ( 5 ) and y = 0 . 38 ( 1 ) . Our DRG results were tested against the strong coupling expansion of 1 and good quantitative agreement was obtained for T < Tc .Beyond Tc our DRG flows refuse to converge and a new fixed point with nontrivial field content must be seek for . 1 B . Nienhuis , J . Phys .A : Math . Gen . 15 ( 1982 ) 4943 https : / / arxiv . org / pdf / 1702 . 05862 . pdf PS : Please feel safe to comment / ask for any clarifications - - - The 2D Ising model with powerful square coupling in the limit of zero aspect ratio of the system demonstrates a consistent change at temperature Tc between a low temperature phase in which the interfaces are pinned by the Gaussian disorder and a high temperature phase in which they are unpinned .Using Monte Carlo simulations we estimate the significant exponent identified with the order of the shift as o = 1 . 23 ( 2 ) . We suggest an efficient dynamic renormalization group ( DRG ) algorithm to the issue .The DRG flows for the effective interface pressure and Gaussian disorder strength are shown to be inconsistent with the two - sided free field fixed point . This implies that the transfer is indeed continuous .The predicted critical exponents are o = 1 . 25 ( 5 ) Our DRG results were tested against the strong coupling expansion of 1 and good quantitative agreement was shown for T < Tc . Beyond Tc our DRG flows refuse to converge and a new fixed point with nontrivial field content must be seek for .1 B . Nienhuis , J . Phys . A : Math .Gen . 15 ( 1982 ) . We thank the researchers of this paper for submitting it to arXiv .",
        "rewrite_text": "The 2D Ising model exhibits a consistent temperature-induced transition, marked by strong square coupling and zero aspect ratio of the system. Specifically, this transition takes place between a low heat phase where the interfaces are pinned due to Gaussian disorder and a high heat phase where they become unpinned. Leveraging Monte Carlo simulations, we estimate the critical exponent linked to the order of the transition as o = 1.23 (2), which aligns with the theoretical prediction of o = 4/11 = 0.3622... We propose an efficient dynamic renormalization group (DRG) algorithm to address this issue. Our analysis shows that DRG flows for effective interface pressure and Gaussian disorder strength are inconsistent with the two-sided free field fixed point, indicating a continuous transition. Predicted critical exponents are o = 1.25 (5) and y = 0.38 (1). Our DRG findings have been tested against the strong coupling expansion of 1, and we've achieved good quantitative agreement for temperatures below Tc. However, above Tc, our DRG flows fail to converge, suggesting the need to seek a new fixed point with nontrivial field content. We appreciate the contributions made by B. Nienhuis et al. in their 1982 paper on J. Phys. A: Math. Gen. 15 and are grateful for the submission to arXiv at https://arxiv.org/pdf/1702.05862.pdf. Feel free to comment or seek clarifications.",
        "ori-fast-z-score": -1.697749375254331,
        "water-fast-z-score": 5.479596155372586,
        "rewrite-fast-z-score": -0.9332565252573828
    },
    {
        "original_text": "In quantum mechanics, a density operator is a projection valued, Hermitian operator that represents the state of a quantum system with uncertainity. The density operator is a fundamental and important mathematical object in quantum mechanics, which encapsulates the statistical information of the quantum system. In this paper, we study the geometry of the set of all density operators on a complex Hilbert space, i.e., the quantum state space. First, we present a generalized von Neumann entropy and characterize the entropy landscape of the set of all quantum states. We then study the extreme points, the convolution structure, and provide alternative characterizations of the set of physically allowed quantum states. Next, we show that a quantum state is a density state if and only if it can be expressed as a tomographic probability distribution. As an application of these results, we derive a unified geometric framework for the construction of positive maps and use it to completely classify super-operator positive maps on trace-class operators. As an example, we provide an explicit formula for the operator Jordan map, which can be considered as a quantum analog of the complex equator of the Riemann sphere.",
        "watermark_text": "In quantum mechanics , a density operator is a projection valued , Hermitian operator that represents the state of a quantum system with uncertainity . The density operator is a crucial and influential mathematical device in quantum mechanics , which encapsulates the statistical information of the quantum system .In this paper , we study the topology of the set of all density operators on a complex Hilbert space , i . e . , the quantum state space . First , we present a generalized von Neumann entropy and characterize the entropy landscape of the set of all quantum states .We then research the extreme points , the convolution structure , and get alternative characterizations of the set of physically permitted quantum states . Next , we prove that a quantum state is a density state if and only if it can be stated as a tomographic likelihood distribution .As an implementation of these results , we derive a consolidated geometric structure for the creation of positive mapping and use it to totally classify super - operator positive mapping on trace - class operators . As an instance , we provide an explicit formula for the operator Jordan map , which can be regarded as a quantum analog of the complex equator of the Riemann sphere .",
        "rewrite_text": "In quantum mechanics, a density operator serves as a projection-valued, Hermitian operator that signifies the state of a quantum system with uncertainty. This mathematical tool is paramount and influential in quantum mechanics, as it encapsulates the statistical information of the quantum system. This paper explores the topology of the entire set of density operators within a complex Hilbert space, also known as the quantum state space.\n\nInitially, we introduce a generalized von Neumann entropy and delineate the entropy landscape of the set encompassing all quantum states. Subsequently, we delve into the extreme points, the convolution structure, and obtain alternative characterizations of the set of physically allowed quantum states.\n\nFurthermore, we establish that a quantum state is a density state if and only if it can be expressed as a tomographic likelihood distribution. As an application of these findings, we develop a consolidated geometric structure for creating positive mappings and use it to comprehensively classify super-operator positive mappings on trace-class operators.\n\nAs an example, we provide an explicit formula for the operator Jordan map, which can be regarded as the quantum counterpart of the complex equator on the Riemann sphere.",
        "ori-fast-z-score": 0.5241424183609592,
        "water-fast-z-score": 4.216370213557839,
        "rewrite-fast-z-score": 2.225995548013356
    },
    {
        "original_text": "For a family of operators related to the one-dimensional Schrödinger operator with periodic potential, the asymptotics of eigenfunctions in the semiclassical limit is studied. Using the classical shooting argument, it is shown that the spectral problem is equivalent to one for a new family of operators with parameter dependent on the derivative of the periodic potential. The new operators are defined in a “homogenized” domain, where the spectrum is assumed to be uniformly separated from the essential spectrum. In the semiclassical limit, the operator family turns into a Schrodinger operator with constant coefficient. The asymptotics of the eigenfunctions is studied in the spirit of Floquet theory, replacing the fixed dispersion relation by the uniform separation of the spectrum. It is shown that the eigenfunctions are real-valued and trigonometric polynomials. The semiclassical quantization condition for the period of the potential is derived, and it is shown to be equivalent to the Gutzwiller’s trace formula.",
        "watermark_text": "For a family of operators similar to the one - dimensional Schrödinger operator with periodic potential , the asymptotics of eigenfunctions in the semiclassical limit is studied . Using the classical firing argument , it is demonstrated that the spectral problem is analogous to one for a new family of operators with parameter dependent on the derivative of the periodic potential .The new functions are formulated in a “ homogenized ” domain , where the spectrum is expected to be uniformly split from the necessary spectrum . In the semiclassical limit , the operator family turns into a Schrodinger operator with constant coefficient .The asymptotics of the eigenfunctions is studied in the spirit of Floquet theory , replacing the fixed dispersion relation by the uniform splitting of the spectrum . It is demonstrated that the eigenfunctions are real - valued and trigonometric polynomials .The semiclassical quantization condition for the period of the potential is calculated , and it is demonstrated to be analogous to the Gutzwiller ’ s trace formula .",
        "rewrite_text": "The study examines the asymptotics of eigenfunctions for a family of operators resembling the one-dimensional Schrödinger operator with a periodic potential in the semiclassical limit. By utilizing the classical firing argument, it is shown that the spectral problem bears resemblance to that of a fresh family of operators, where the parameter is dependent on the derivative of the periodic potential. These new functions are formulated within a \"homogenized\" domain, where the spectrum is anticipated to be uniformly separated from the essential spectrum. In the semiclassical limit, the operator family transforms into a Schrödinger operator with constant coefficients.\n\nThe investigation of these eigenfunctions' asymptotics follows the principles of Floquet theory, replacing the fixed dispersion relation with a uniform spectrum splitting. It is demonstrated that the eigenfunctions are composed of real-valued trigonometric polynomials. Additionally, the semiclassical quantization condition for the potential period is calculated, which is found to be analogous to Gutzwiller's trace formula.",
        "ori-fast-z-score": -0.762000762001143,
        "water-fast-z-score": 2.794002794004191,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "Realizable Hamiltonians are proposed for universal adiabatic quantum computers. In contrast to earlier proposals, our Hamiltonians can be implemented with near-term quantum devices. We consider a general setup with a target unitary to realize and an adiabatic evolution over a time period T. The generated Hamiltonian is allowed to have any term supported on the eigenstates of the initial Hamiltonian, and thus in general cannot be expressed as a sum of bilinears of the form {|,i}H{i,}{,}. We present a general framework to prove that the final Hamiltonian is realizable for the given target unitary, subject to certain regularity conditions. For a special case when the final Hamiltonian is also a sum of bilinears of the above form, we show that the realization can be obtained via gadgets composed of qubit transverters and clusters of surface gates. For a more general case, we provide explicit constructions based on Kitaev’s quantum double model and its adaption for surface codes. We analyze the performance of the resulting Hamiltonians via the Taylor series expansion. In particular, we show that the gap of the resulting Hamiltonian closes at the order of 1/T2, which means the gap closes exponentially at the best rate T−2. This observation implies that for a universal T, the corresponding quantum computer still requires an infinite power of quantum seed state in the starting Hamiltonian to compensate for the error accumulation. We conclude by discussing possible ways forward, which may potentially address the error accumulation and improve the performance of the resulting adiabatic quantum computers.",
        "watermark_text": "Realizable Hamiltonians are proposed for global adiabatic quantum computers . In comparison to earlier proposals , our Hamiltonians can be executed with close - term quantum devices .We consider a general setup with a target unitary to realize and an adiabatic progression over a time time T . The generated Hamiltonian is allowed to have any term supported on the eigenstates of the first Hamiltonian , and therefore in general cannot be stated as a sum of bilinears of the form { | , i } H { i , } { , } . We present a general framework to prove that the finished Hamiltonian is realizable for the particular target unitary , subject to certain regularity rules .For a special case when the last Hamiltonian is also a sum of bilinears of the above form , we show that the realization can be obtained via gadgets consisting of qubit transverters and clusters of surface doors . For a more general case , we provide explicit constructions using on Kitaev ’ s quantum double theory and its adaption for surface codes .We evaluate the performance of the resulting Hamiltonians via the Taylor series expansion . In particular , we find that the gap of the resulting Hamiltonian closes at the order of 1 / T2 , which means the gap closes exponentially at the best rate T−2 .This observation assumes that for a universal T , the associated quantum computer still needs an endless power of quantum seed state in the starting Hamiltonian to compensate for the error accumulation . We follow by examining potential ways forward , which would potentially overcome the error accumulation and improve the performance of the resulting adiabatic quantum computers .",
        "rewrite_text": "Proposed Realizable Hamiltonians for Global Adiabatic Quantum Computers\n\nIn contrast to previous proposals, our Hamiltonians are designed for execution with near-term quantum devices. We consider a general setup where a target unitary is to be realized through an adiabatic progression over a duration of time T. The generated Hamiltonians can include any terms supported by the eigenstates of the initial Hamiltonian, and therefore cannot generally be expressed as a sum of bilinear forms { | , i } H { i , } { , }.\n\nWe present a comprehensive framework to demonstrate the realizability of the final Hamiltonian for a specific target unitary, subject to certain regularities. For a special case where the final Hamiltonian is also a sum of bilinear forms, we show that its realization can be achieved using gadgets consisting of qubit transverters and clusters of surface gates.\n\nFor a more general scenario, we provide explicit constructions utilizing Kitaev's quantum double theory and its adaptation for surface codes. We assess the performance of these Hamiltonians through Taylor series expansion. Specifically, we observe that the gap in the resulting Hamiltonian closes at an order of 1/T^2, indicating an exponential closure rate of T^-2 at best. This observation suggests that, for a universal T, the associated quantum computer still requires an extensive quantum seed state in the initial Hamiltonian to compensate for error accumulation.\n\nWe proceed to explore potential avenues forward that could potentially overcome error accumulation and improve the performance of adiabatic quantum computers.",
        "ori-fast-z-score": -0.4662524041201569,
        "water-fast-z-score": 4.7557745220256,
        "rewrite-fast-z-score": -0.4833682445228318
    },
    {
        "original_text": "Fossil groups are large clusters of galaxies captured by the gravity of a larger central halo. The galaxies in the groups are early-type, similar to the way in which the central galaxy is described as cuspy. We examine the dynamics of galaxy groups and compare to a subset of the Millennium Simulation. While the average groups masses in the simulation agree with observational values, the fraction of groups with multiple luminous members is less than that in observed groups. We investigate the causes and conclude that the low multi-occupancy of the groups in the Millennium Simulation can be reproduced by standard treatments of galaxy-galaxy interactions and mergers, without the need for modification to the mass or dynamics of the groups. The Millennium Simulation is a $N$-body simulation of our universe, run by the Virgo Consortium and funded by the American Astronomical Society. The resolution of the simulation is approximately 108 times better than that of the Las Campanas survey. I compare the dynamics of galaxy groups in the Millennium Simulation to that in the Las Campanas survey. The fraction of groups with multiple luminous members in the simulation is less than that in the survey. I investigate the causes and conclude that the low multi-occupancy of the groups in the simulation can be reproduced by standard treatments of galaxy-galaxy interactions and mergers, without the need for modification to the mass or dynamics of the groups. I confirm this by comparing the dynamics of groups in the Millennium Simulation to that in the Las Campanas survey. In the simulation, the average groups masses agree with observational values. However, the fraction of groups with multiple luminous members is less than that in the survey. I investigate the causes and conclude that the low multi-occupancy of the groups in the simulation can be reproduced by standard treatments of galaxy-galaxy interactions and mergers, without the need for modification to the mass or dynamics of the groups.",
        "watermark_text": "Fossil groups are big bands of clusters caught by the gravity of a greater central halo . The galaxies in the groups are early - class , analogous to the way in which the main galaxy is characterized as cuspy .We assess the dynamics of galaxy groups and compare to a subset of the Millennium Simulation . While the average groups masses in the simulation agree with observational values , the fraction of groups with many luminous members is less than that in observed bands .We explore the causes and suggest that the poor multi - occupancy of the groups in the Millennium Simulation can be reproduced by typical treatments of galaxy - galaxy dynamics and mergers , without the necessity for modification to the mass or dynamics of the groups . The Millennium Simulation is a $ N $ - body model of our universe , run by the Virgo Consortium and funded by the American Astronomical Society .The resolution of the model is approximately 108 times higher than that of the Las Campanas sample . I relate the dynamics of galaxy populations in the Millennium Simulation to that in the Las Campanas survey .The percentage of groups with many luminous members in the model is less than that in the census . I examine the causes and suggest that the poor multi - occupancy of the groups in the model can be reproduced by traditional treatments of galaxy - star interactions and mergers , without the necessity for modification to the mass or dynamics of the groups .I support this by comparing the dynamics of groups in the Millennium Simulation to that in the Las Campanas sample . In the simulation , the average groups masses agree with observational values .However , the fraction of groups with many luminous members is less than that in the poll . I examine the causes and suggest that the poor multi - occupancy of the groups in the model can be reproduced by traditional treatments of galaxy - star interactions and mergers , without the necessity for modification to the mass or dynamics of the groups .",
        "rewrite_text": "Fossil groups are large clusters of galaxies that are bound together by the gravity of a central, larger halo. These galaxies belong to the early-class category, resembling the cusp-like characteristics of the main galaxy. We assess the dynamics of these galaxy groups and compare them to a subset of the Millennium Simulation. While the average group masses in the simulation align with observed values, the frequency of groups with numerous luminous members is lower than that observed in actual groups. We explore potential reasons and suggest that this underrepresentation of multiple galaxies in the Millennium Simulation can be explained by standard treatments of galaxy-galaxy dynamics and mergers, without requiring modifications to the group's mass or dynamics.\n\nThe Millennium Simulation is an N-body model of our universe, run by the Virgo Consortium and funded by the American Astronomical Society. Its resolution is approximately 108 times greater than that of the Las Campanas sample. I compare the dynamics of galaxy populations in the Millennium Simulation with those observed in the Las Campanas survey. The percentage of groups with numerous bright members in the model is less than that reported in surveys. I investigate the reasons and propose that this discrepancy could be due to traditional treatments of galaxy-star interactions and mergers, without requiring changes to the group's mass or dynamics. This is supported by my comparison of group dynamics in the Millennium Simulation to those in the Las Campanas sample. In the simulation, average group masses align with observed values; however, the frequency of groups with many luminous members remains lower than that observed in surveys. I further explore potential explanations and suggest that traditional approaches to modeling galaxy-star interactions and mergers can account for this difference, without necessitating modifications to group mass or dynamics.",
        "ori-fast-z-score": -1.9295276424754644,
        "water-fast-z-score": 6.363961030678928,
        "rewrite-fast-z-score": 2.263009527424072
    },
    {
        "original_text": "General relativity, as formulated by Albert Einstein, is a cornerstone of modern physics. It describes gravity as the warping of space and time, resulting from the asymmetric warping of space and time caused by matter and energy. One of the most well-tested theories in all of physics, general relativity has withstood the test of time, having passed all experimental tests with high accuracy. However, there are some that speculate that a deeper understanding of gravity may require the introduction of new concepts and ideas. As part of this trend, there has been recent interest in the process by which we describe the gravitational interaction using Einstein’s theory; that is, renormalization. Renormalization refers to the process by which infinities inherent in the theory are eliminated by redefining the parameters of the theory. In the process, a more general framework for gravitational theory—one that includes various alternative theories such as f(R) gravity—is created. In this paper, we examine this framework for renormalization. We provide an extensive account of the process and provide a step-by-step guide to performing calculations in this framework. We illustrate this process through several examples, including calculating the effective action for a system of gravitons. We end with a discussion of our findings and suggestions for future work.",
        "watermark_text": "General relativity , as formulated by Albert Einstein , is a cornerstone of modern physics . It treats gravitational as the warping of space and time , arising from the asymmetric warping of space and time induced by matter and energy .One of the most well - tested theories in all of science , universal relativity has withstood the test of time , having completed all experimental tests with high clarity . However , there are some that speculate that a deeper understanding of gravitational might require the introduction of new concepts and ideas .As part of this growth , there has been current attention in the process by which we explain the gravitational interaction using Einstein ’ s concept ; that is , renormalization . Renormalization refers to the process by which infinities inherent in the model are eliminated by redefining the variables of the model .In the process , a more general basis for gravitational theory — one that contains various alternative theories such as f ( R ) gravity — is created . In this paper , we investigate this framework for renormalization .We take an extensive description of the process and include a step - by - step guide to conducting analyses in this framework . We illustrate this process through several examples , including calculating the effective action for a system of gravitons .We end with a debate of our findings and suggestions for future research .",
        "rewrite_text": "General relativity, as formulated by Albert Einstein, serves as a pivotal pillar in modern physics. It conceptualizes gravity as the distortion of space and time, arising from the asymmetric curvature induced by matter and energy. Among the most extensively tested scientific theories, universal relativity has proven resilient over time, successfully passing all experimental tests with clarity. Nevertheless, some speculate that a deeper comprehension of gravity may necessitate the introduction of novel concepts and ideas.\n\nIn this growth, there has been a spotlight on how we explain gravitational interactions using Einstein's renormalization concept. Renormalization pertains to the process of eliminating infinite values inherent in the model by redefining its variables. Through this process, a more comprehensive basis for gravitational theory emerges, encompassing various alternative theories such as f(R) gravity.\n\nIn this paper, we delve into the framework of renormalization. We provide a comprehensive description of the process and include a step-by-step guide for conducting analyses within this framework. We illustrate this framework through several examples, including the calculation of the effective action for a system of gravitons.\n\nFinally, we present a discussion of our findings and suggest potential avenues for future research.",
        "ori-fast-z-score": 1.9291577137538762,
        "water-fast-z-score": 7.3484692283495345,
        "rewrite-fast-z-score": 2.9514591494904874
    },
    {
        "original_text": "The MINOS Experiment at FNAL has deployed the first long-baseline neutrino oscillation appearance experiment in the NuMI beamline and has recently published its first-year results. The experiment uses a 3-detector setup, with one upstream detector at the Northern Initial Data taking site and two downstream detectors at the Soudan Underground Mine in Minnesota (NE detector) and Saints, Louisiana (SW detector). The data taking period was from 2011 to 2012, and the total exposure in the far detector was 2.2 ton-years. In this period MINOS measured muon neutrino disappearance with a significance of 5.2σ, and presented first evidence for electron neutrino appearance, with 3.6σ of significance. This first MINOS results paper presents the new data and preliminary global fit results. MINOS was developed to study muon neutrino disappearance and pion- created electron neutrinos appearance. The new dataset includes neutrino interactions in the detector during 2011 and 2012, and the global fit incorporates the full dataset, with the preliminary results presented in this paper updated to include the full dataset. The main update in the new results is the re-weighting of the event rates with the improved cross section calculations by the NNPDF collaboration. This allows for a more precise estimate of the experiment’s background and sensitivity. The experiment finds no evidence for muon neutrino disappearance and presents 3.6σ of evidence for electron neutrino appearance. The result has an element of uncertainty from our knowledge of the neutrino flux and cross sections, which is reduced with this update.",
        "watermark_text": "The MINOS Experiment at FNAL has deployed the first long - baseline neutrino oscillation appearance experiment in the NuMI beamline and has recently published its first - year results . The project utilizes a 3 - detector setup , with one upstream detector at the Northern Initial Data taking location and two downstream detectors at the Soudan Underground Mine in Minnesota ( NE detector ) and Saints , Louisiana ( SW detector ) .The data taking date was from 2011 to 2012 , and the total exposure in the far detector was 2 . 2 ton - years . In this time MINOS measured muon neutrino disappearance with a significance of 5 . 2σ , and offered first data for electron neutrino presence , with 3 . 6σ of importance .This first MINOS results paper offers the new data and preliminary global fit results . MINOS was developed to study muon neutrino disappearance and pion - produced electron neutrinos shape .The revised dataset includes neutrino interactions in the detector during 2011 and 2012 , and the global fit incorporates the full dataset , with the detailed conclusions presented in this paper updated to contain the full dataset . The main revision in the new data is the re - weighting of the event frequencies with the improved cross area measurements by the NNPDF consortium .This enables for a more precise estimate of the experiment ’ s background and sensitivity . The observation discovers no evidence for muon neutrino disappearance and provides 3 . 6σ of evidence for electron neutrino presence .The result has an element of uncertainty from our experience of the neutrino flux and cross sections , which is decreased with this revision .",
        "rewrite_text": "The MINOS Experiment conducted at FNAL has implemented the inaugural long-baseline neutrino oscillation experiment within the NuMI beamline, recently unveiling its first-year findings. This project employs a three-detector setup, with one upstream detector situated at the Northern Initial Data acquisition site and two downstream detectors positioned at the Soudan Underground Mine in Minnesota (NE detector) and Saints, Louisiana (SW detector). Data collection spanned from 2011 to 2012, with a total exposure of 2.2 ton-years in the far detector. Throughout this period, MINOS measured the significant muon neutrino disappearance with a 5.2σ significance, and provided pioneering data on electron neutrino presence with a 3.6σ significance.\n\nThis initial MINOS research paper presents novel data and preliminary global fit results. MINOS was designed to investigate muon neutrino disappearance and the shape of pion-produced electron neutrinos. The updated dataset encompasses neutrino interactions within the detector during 2011 and 2012, while the global fit incorporates the complete dataset, with detailed conclusions presented in this paper now updated to include the full dataset. The primary revision in the new data involves re-weighting event frequencies with improved cross-sectional area measurements from the NNPDF consortium, enabling a more precise estimation of the experiment's background and sensitivity.\n\nThe observations yield no evidence for muon neutrino disappearance and offer 3.6σ evidence for the presence of electron neutrinos. There is a degree of uncertainty in the result due to our understanding of neutrino flux and cross-sections, which is mitigated with this revision.",
        "ori-fast-z-score": 1.0734900802433864,
        "water-fast-z-score": 6.601706163700764,
        "rewrite-fast-z-score": 1.2686700948330931
    },
    {
        "original_text": "Dewetting is an important micro-machining technique for nanometer-scale feature fabrication in a wide range of materials, including metals, semiconductors and dielectrics. The process relies on the removal of a thin layer of material from one substrate to produce a thin film of the material on another substrate. Dewetting is initiated by selectively reducing the temperature of one or more regions of the substrate material, which can be thermal, chemical or mechanical patterning. One typical dewetting scenario is the removal of a thin layer of material from a bulk semi-conductor to produce a thin film of the material on another substrate. This generally results in the formation of one or more holes, which can have sharp edges and possess a high energy surface in the form of a free-air interface. A significant problem in this scenario is the ability of the edge of the hole to relax towards equilibrium, which requires the provision of sufficient edge energy to overcome surface tension forces. In this work, we perform a full-scale hydrodynamic calculation of the relaxation of an initially circular dewetting hole, which is constrained at its edges and is free to move in two dimensions. We compare the evolution of the hole shape and energy with both circular and elliptic equilibrium solutions, and examine the importance of various physical mechanisms for relaxation, including bulk diffusion, convection and surface tension.",
        "watermark_text": "Dewetting is an important micro - machining methodology for nanometer - scale feature fabrication in a broad variety of materials , notably metals , semiconductors and dielectrics . The method relies on the removal of a thin layer of material from one substrate to produce a thin film of the metal on another substrate .Dewetting is initiated by selectively decreasing the temperature of one or more regions of the substrate material , which can be thermal , chemical or mechanical patterning . One common dewetting scenario is the removal of a thin layer of material from a bulk semi - conductor to produce a thin film of the metal on another substrate .This generally leads in the formation of one or more holes , which can have blunt margins and possess a high energy edge in the form of a free - air contact . A critical difficulty in this situation is the ability of the edge of the hole to relax towards equilibrium , which requires the availability of sufficient edge energy to overcome surface friction tension .In this research , we perform a complete - scale hydrodynamic calculation of the relaxation of an initially circular dewetting hole , which is constrained at its edges and is free to move in two dimensions . We relate the evolution of the hole position and energy with both rectangular and elliptic equilibrium solutions , and consider the importance of several physical mechanisms for relaxation , notably bulk diffusion , convection and surface friction .",
        "rewrite_text": "Dewetting is a crucial micro-machining technique for creating nanometer-scale features in a wide range of materials, particularly metals, semiconductors, and dielectrics. This method involves the removal of a thin layer of material from one substrate to produce a metal thin film on another. Dewetting is initiated by selectively reducing the temperature of one or more regions of the substrate material, which can be achieved through thermal, chemical, or mechanical patterning.\n\nA common dewetting scenario involves removing a thin layer of material from a bulk semiconductor to create a metal thin film on another substrate. This often results in the formation of one or more holes with blunt edges and a high-energy edge in the form of a free-air contact. A key challenge in this process is the ability of the hole edge to relax towards equilibrium, which requires sufficient edge energy to overcome surface friction tension.\n\nIn this research, we conduct a comprehensive hydrodynamic calculation of the relaxation process of an initially circular dewetting hole that is constrained at its edges and free to move in two dimensions. We correlate the evolution of the hole's position and energy with both rectangular and elliptic equilibrium solutions. We also consider the significance of several physical mechanisms for relaxation, notably bulk diffusion, convection, and surface friction.",
        "ori-fast-z-score": -0.09578262852211514,
        "water-fast-z-score": 5.651175082804793,
        "rewrite-fast-z-score": 2.5018511664883785
    },
    {
        "original_text": "In recent years, the notion of emergent spacetime has been introduced to try to unify gravity with other physical forces, most notably quantum mechanics. These emergent spacetimes have been shown to arise in a bottom-up fashion from consistent theories in a higher dimensional spacetime, general relativity in d dimensions plus a scalar field. In this paper, we show that for a particular choice of coupling between the scalar and gravitational fields, this higher dimensional spacetime admits a higher symmetry, and the theory appears in one higher dimension as a dual gravitational theory with a doubled number of spacetime dimensions. Specifically, we consider the theory of a scalar field with a non-linear self-interaction on d+1 dimensional Minkowski space. The corresponding equations of motion have a critical point corresponding to empty spacetime. We show that by introducing a new scalar field conformally coupled to the first one, the system admits a higher symmetry, and reduces to a theory in d dimensions with an emergent ((d-1)+1) dimensional Minkowski spacetime. This theory allows for non-trivial solutions describing black holes with multiple horizons and with a length scale set by the charge of the black holes. In addition, in the classical solution, there is a smooth lower dimensional region behind the black hole horizon, in which the original (d+1) dimensional spacetime is smoothly extended, with the extension encoded in the structure of a solitonic string connecting the black hole horizon with the smooth region. We analyze the solutions with a single black hole and a cosmological horizon, finding that for large enough charge the black hole and cosmological horizons coincide, giving rise to a tri-horizon solution. We compute the entropy of the black holes in the dual gravitational theory, and find that it matches the Bekenstein-Hawking entropy of the black holes in the tri-horizon solution. Finally, we argue that the general solution of the theory includes the Reissner-Nordström black holes in higher dimensions, for which the gravitational theory reduces to five dimensions. We end with a discussion of the results and possible extensions. Date: 30 January 2023 Author: Jose Antonio Burroni Title: Dual Field Theories In (d-1)+1 Emergent Spacetimes From A Unifying Field Theory In d+2 Spacetime. Abstract: In recent years, the notion of emergent spacetime has been introduced to try to unify gravity with other physical forces, most notably quantum mechanics. These emergent spacetimes have been shown to arise in a bottom-up fashion from consistent theories in a higher dimensional spacetime, general relativity in d dimensions plus a scalar field. In this paper, we show that for a particular choice of coupling between the scalar and gravitational fields, this higher dimensional spacetime admits a higher symmetry, and the theory appears in one higher dimension as a dual gravitational theory with a doubled number of spacetime dimensions. Specifically,",
        "watermark_text": "In recent years , the notion of emergent spacetime has been used to try to unify gravity with other physical forces , most including quantum mechanics . These emergent spacetimes have been shown to arise in a bottom - up fashion from consistent ideas in a higher dimensional spacetime , special relativity in d dimensions plus a scalar field .In this paper , we prove that for a certain choice of coupling between the scalar and gravity fields , this higher dimensional spacetime admits a higher symmetry , and the theory presents in one greater dimension as a dual gravitational theory with a doubled number of spacetime dimensions . Specifically , we define the theory of a scalar field with a non - linear self - interaction on d + 1 dimensional Minkowski space .The equivalent equations of movement have a critical position equivalent to empty spacetime . We see that by bringing a new scalar field conformally linked to the first one , the scheme assumes a higher symmetry , and transforms to a theory in d dimensions with an emergent ( ( d - 1 ) + 1 ) dimensional Minkowski spacetime .This theory provides for non - simple solutions involving grey holes with many horizons and with a length range set by the charge of the dark holes . In addition , in the classical solution , there is a smooth lower dimensional region behind the dark hole horizon , in which the previous ( d + 1 ) dimensional spacetime is smoothly stretched , with the extension encoded in the structure of a solitonic string linking the dark hole horizon with the smooth zone .We evaluate the systems with a single black hole and a cosmological horizon , finding that for large enough charge the dark hole and cosmological horizons coincide , giving rise to a tri - horizon solution . We compute the entropy of the dark holes in the dual gravitational theory , and find that it matches the Bekenstein - Hawking entropy of the dark holes in the tri - horizon solution .Finally , we claim that the general solution of the principle contains the Reissner - Nordström black holes in greater dimensions , for which the gravitational theory reduces to five dimensions . We end with a debate of the results and possible extensions .Date : 30 January 2023 Author : Jose Antonio Burroni Title : Dual Field Theories In ( d - 1 ) + 1 Emergent Spacetimes From A Unifying Field Theory In d + 2 Spacetime . Abstract : In recent years , the notion of emergent spacetime has been used to try to unify gravity with other physical forces , most including quantum mechanics .These emergent spacetimes have been shown to arise in a bottom - up fashion from consistent ideas in a higher dimensional spacetime , special relativity in d dimensions plus a scalar field . In this paper , we prove that for a certain choosing of coupling between the scalar and gravity fields , this low dimensional spacetime implies a higher symmetry , and the theory presents in one greater dimension as a dual gravitational concept with a doubled number of spacetime dimensions .Specifically,",
        "rewrite_text": "In recent years, the concept of emergent spacetime has been employed to integrate gravity with other physical forces, primarily including quantum mechanics. These emerging spacetimes have been demonstrated to emerge in a bottom-up manner from coherent ideas within a higher dimensional spacetime, incorporating special relativity in d dimensions with a scalar field.\n\nIn this paper, we establish that for a specific selection of coupling between the scalar and gravitational fields, this lower dimensional spacetime exhibits a greater symmetry. The theory emerges in one additional dimension as a dual gravitational theory with a doubled number of spacetime dimensions. Specifically, we define the theory of a scalar field with a nonlinear self-interaction within d + 1 dimensional Minkowski space. The corresponding equations of motion share a critical position equivalent to empty spacetime.\n\nBy introducing a new scalar field conformally linked to the first, the framework assumes a higher level of symmetry and transforms into a theory in d dimensions with an emerging ((d - 1) + 1) dimensional Minkowski spacetime. This theory offers non-trivial solutions involving grey holes with multiple horizons and a length range determined by the charge of dark holes.\n\nAdditionally, in the classical solution, there is a smoothly transitioning lower dimensional region behind the dark hole horizon, where the previous (d + 1) dimensional spacetime is smoothly extended. This extension is encoded in the structure of a solitonic string connecting the dark hole horizon with the smooth zone.\n\nWe evaluate systems with a single black hole and a cosmological horizon, finding that for sufficiently large charge, the dark hole and cosmological horizons converge, resulting in a tri-horizon solution. We calculate the entropy of dark holes in the dual gravitational theory and find it to match the Bekenstein-Hawking entropy of dark holes in the tri-horizon solution.\n\nFinally, we assert that the general solution of this principle encompasses higher dimensional Reissner-Nordström black holes, where the gravitational theory reduces to five dimensions. We conclude with a discussion on the results and potential extensions.\n\nDate: January 30th, 2023\nAuthor: Jose Antonio Burroni\nTitle: Dual Field Theories in (d - 1) + 1 Emergent Spacetimes Derived from a Unifying Field Theory in d + 2 Spacetime\nAbstract: In recent years, the idea of emergent spacetime has been utilized to unify gravity with other physical forces, primarily quantum mechanics. These emerging spacetimes have been observed to arise from consistent ideas in higher dimensional spacetimes through a bottom-up approach, incorporating special relativity in d dimensions with a scalar field. This paper demonstrates that for a specific choice of coupling between scalar and gravitational fields, this lower dimensional spacetime exhibits greater symmetry. The theory emerges in an additional dimension as a dual gravitational concept with doubled spacetime dimensions. Specifically, we outline...",
        "ori-fast-z-score": -3.05085107923876,
        "water-fast-z-score": 5.520524474738834,
        "rewrite-fast-z-score": 1.1094003924504583
    },
    {
        "original_text": "A formal definition of quantum zero-knowledge proof (QKZP) has been given by Watrous  19 . The definition uses two interactive probabilistic protocols. The first, the witness generation, simulates a particular probabilistic polynomial-time (PPT) procedure. The second, the simulator, accepts any PPT procedure as an adversary. The verifier chooses a random witness, sends it to the simulator, and receives a response. The response must contain a statement that is a polynomial-time verification condition for the original procedure. The proof is valid if the simulator accepts the witness with probability at least 2/3, and the witness generation stage simulates the procedure with advantage at least 1/3. In this paper, we examine general properties of QKZP. First, we show that the definition is expressible in the language of complexity theory. Using this fact, we show that the PSPACE assumption is necessary in the definition. Then we show that it is not possible to construct a valid QKZP with advantage better than 2/3 given only black-box access to the original procedure. These results imply the triviality of QKZP. We also present a variant of QKZP with an enhanced soundness property. The proof of the strengthened soundness is challenging and interesting in its own right.",
        "watermark_text": "A rigorous concept of quantum zero - knowledge proof ( QKZP ) has been given by Watrous 19 . The definition utilizes two interactive probabilistic protocols .The first , the witness generation , simulates a certain probabilistic polynomial - time ( PPT ) method . The second , the simulator , accepts any PPT method as an adversary .The verifier picks a random witness , sends it to the simulator , and gets a reaction . The answer must include a expression that is a polynomial - time verification condition for the original procedure .The proof is valid if the simulator accepts the witness with probability at least 2 / 3 , and the witness generation phase simulates the method with advantage at least 1 / 3 . In this paper , we investigate general properties of QKZP .First , we prove that the notion is expressible in the language of complexity theory . Using this fact , we show that the PSPACE assumption is required in the definition .Then we prove that it is not practical to build a valid QKZP with advantage better than 2 / 3 considering only black - box access to the previous method . These conclusions imply the triviality of QKZP .We additionally offer a version of QKZP with an augmented soundness property . The confirmation of the enhanced soundness is challenging and exciting in its own right .",
        "rewrite_text": "A comprehensive definition of the quantum zero-knowledge proof (QKZP) has been provided by Watrous in his 19th work. This definition incorporates two interactive probabilistic protocols. The first protocol, known as witness generation, mimics a specific probabilistic polynomial-time (PPT) method. The second, the simulator, accepts any PPT method as a potential adversary.\n\nIn this framework, the verifier randomly selects a witness, sends it to the simulator, and receives a response. This response must encompass an expression that serves as a polynomial-time verification condition for the original process. The proof is considered valid if the simulator accepts the witness with a probability of at least 2/3, and the witness generation phase simulates the method with an advantage of at least 1/3.\n\nIn this paper, we explore the general properties of QKZP. Firstly, we establish that this concept can be expressed in the language of complexity theory. Utilizing this fact, we demonstrate the necessity of the PSPACE assumption in its definition. Secondly, we prove that it is impractical to construct a valid QKZP with an advantage exceeding 2/3, solely relying on black-box access to previous methods. These conclusions highlight the fundamental nature of QKZP.\n\nFurthermore, we present a version of QKZP with an enhanced soundness property. The validation of this improved soundness is both challenging and intriguing in its own right.",
        "ori-fast-z-score": -1.5096588248481377,
        "water-fast-z-score": 4.824506406770077,
        "rewrite-fast-z-score": 0.9434563530497265
    },
    {
        "original_text": "The title of the paper describes the main topic of the abstract. The abstract is very long because it also includes a background on the topic and general comments on the paper. neutron star is the term used to describe a celestial object made almost entirely of neutron, the subatomic particles with the atomic number of indivdual nuclei. Neutron stars are incredibly dense objects, with more mass than that of the Sun packed into a volume of about a cubic kilometer or more. Neutron stars are formed as the result of the supernova explosion that occurs when approximately 99% of a star s mass has been condensed into its core. Such extreme densities are reached because neutrons are so much more dense than normal atoms, and the strong force that binds nuclei together weakens as you get closer to the core. When the core has reached approximately 2-3 solar masses, there is not enough mass to support the core against its own gravity. The excess energy is released as a supernova explosion, also known as a neutron star formation. Most neutron stars are found in close binary systems with another neutron star or, more rarely, a black hole. These systems are known as low-mass X-ray binaries (LMXBs). LMXBs are interesting systems to study because they allow us to study neutron stars from two perspectives. The first is   quiescent   (also known as  inactive  or  non-pulsing ) in which the neutron star orbits a more compact companion and is not emitting X-rays. The second is when the neutron star  transientsly  (for a time) increases its X-ray emission, called an  outburst . When an LMXB transits from one to the other state, it is interesting to study the differences between the two states. For example, we have recently shown that transientsly brighter outbursts are associated with an increase of the neutron star s spin period. Conversely, when the neutron star is quiescent, but its X-ray emission is somewhat brighter than normal, its spin period is found to be larger than when the source is significantly fainter. These differences in behavior have suggested a  two-states  model for the behavior of LMXBs. In this model, there are two types of accretion regimes onto the neutron star, with the differences between them explained by different ways in which the captured material is deposited onto the neutron star surface. The first is called  Spoon-fed  in which the capture material, mostly protons, sticks to the neutron star surface, forming a  spoon  shaped structure. Such transients are found to be brighter when the neutron star has a higher spin rate. The second is  Indirect  in which the capture material, mostly electrons and protons, forms a  cloud  around the neutron star. These transients are found to be brighter when the neutron star is observed to be in a state",
        "watermark_text": "The title of the paper represents the main theme of the abstract . The abstract is very long because it also contains a background on the subject and general comments on the paper .neutron star is the term utilized to mean a celestial object making almost completely of neutron , the subatomic particles with the atomic number of indivdual nuclei . Neutron stars are incredibly heavy items , with more mass than that of the Sun packed into a volume of about a cubic kilometer or more .Neutron stars are created as the result of the supernova explosion that happens when approximately 99 % of a star s mass has been condensed into its core . Such intense densities are reached because neutrons are so much more thick than usual atoms , and the strong pull that binds nuclei apart weakens as you go closer to the core .When the core has reached approximately 2 - 3 solar masses , there is not sufficiently mass to support the core against its own gravity . The excess energy is released as a supernova explosion , sometimes called as a neutron star formation .Most neutron stars are found in close binary systems with another neutron star or , more frequently , a black hole . These systems are known as low - weight X - ray binaries ( LMXBs ) .LMXBs are interesting systems to study because they allow us to study neutron stars from two perspectives . The first is quiescent ( also known as inactive or non - pulsing ) in which the neutron star orbits a more compact companion and is not emitting X - radiation .The second is when the neutron star transientsly ( for a time ) changes its X - ray radiation , known an outburst . When an LMXB transits from one to the other state , it is curious to study the differences between the two states .For instance , we have notably shown that transientsly brighter outbursts are related with an increase of the neutron star s spin time . Conversely , when the neutron star is quiescent , but its X - ray radiation is significantly brighter than usual , its spin time is found to be larger than when the source is significantly fainter .These changes in behavior have suggested a two - states theory for the behavior of LMXBs . In this model , there are two forms of accretion regimes onto the neutron star , with the differences between them described by various ways in which the captured material is dumped onto the neutron star surface .The first is dubbed Spoon - fed in which the escape substance , mostly protons , sticks to the neutron star surface , forming a spoon shaped structure . Such transients are found to be brighter when the neutron star has a higher spin speed .The second is Indirect in which the capture material , mostly electrons and protons , forms a mist around the neutron star . These transients are found to be brighter when the neutron star is observed to be in a state",
        "rewrite_text": "The main theme of the paper's abstract is encapsulated in its title. The abstract extends beyond the main theme, providing a comprehensive background on the subject and general comments on the paper. The term 'neutron star' refers to a celestial object predominantly composed of neutrons, the subatomic particles with individual nuclear atomic numbers. These stars are incredibly massive, packing more mass than the Sun into a volume of approximately a cubic kilometer or more.\n\nNeutron stars are formed as a result of supernova explosions that occur when approximately 99% of a star's mass is condensed into its core. Such intense densities are achieved because neutrons are much more dense than typical atoms, and as you approach the core, the force that binds nuclei together weakens. When the core reaches approximately 2-3 solar masses, it no longer has enough mass to counteract its own gravitational pull, leading to the release of excess energy as a supernova explosion, sometimes referred to as neutron star formation.\n\nMost neutron stars are found in close binary systems with other neutron stars or, more frequently, black holes. These systems are known as low-weight X-ray binaries (LMXBs), making them interesting subjects for study. LMXBs allow us to observe neutron stars from two distinct perspectives. The first is the quiescent state, in which the neutron star orbits a more compact companion and does not emit X-rays. The second is when the neutron star temporarily changes its X-ray radiation, known as an outburst.\n\nThe transition between these two states in an LMXB is fascinating to study, as it reveals differences in behavior. For instance, we have found that brighter outbursts during transients are associated with an increase in the neutron star's spin time. Conversely, when the neutron star is in a quiescent state but its X-ray radiation is significantly brighter than usual, its spin time is found to be longer when compared to fainter sources. These changes in behavior have led to the proposal of a two-state theory for LMXBs' behavior.\n\nIn this model, there are two forms of accretion regimes onto the neutron star. The differences between them are described by various ways in which the captured material is deposited onto the neutron star surface. The first is called Spoon-fed accretion, where the escaping substance, mostly protons, adheres to the neutron star surface, forming a spoon-shaped structure. These transients are found to be brighter when the neutron star has a higher spin speed. The second is Indirect accretion, where the captured material, mostly electrons and protons, forms a mist around the neutron star. These transients are observed to be brighter when the neutron star is in a particular state.",
        "ori-fast-z-score": 1.9321835661585918,
        "water-fast-z-score": 8.28078671210825,
        "rewrite-fast-z-score": 3.8044295512634103
    },
    {
        "original_text": "The cluster Blanco 1, in the direction of the Chamaeleon star-forming region at a distance of 120-150 pc, was identified by Béjar et al. (2004) as a young open cluster with an estimated age of about 10-30 million years. The masses of its low-mass members have not been directly measured. In this work we estimate the masses of the 10-3 Mjup substellar members with the combination of adaptive optics imaging in the infrared and high-precision radial velocities. We derive a lower mass function of n(m) ~ m^ - 1.5, with a median value of 0.088 Mjup, i.e., 7% of the cluster mass. This is the deepest exploration to date of the substellar regime in a young cluster. The cluster Blanco 1 contains low-mass members from 30 Mjup down to the substellar domain, with a median value of 0.088 Mjup. The lower mass function is n(m) ~ m^ - 1.5.",
        "watermark_text": "The cluster Blanco 1 , in the direction of the Chamaeleon star - creating area at a distance of 120 - 150 pc , was described by Béjar et al . ( 2004 ) as a young open cluster with an estimated age of about 10 - 30 million years .The masses of its low - mass members have not been directly measured . In this research we estimate the masses of the 10 - 3 Mjup substellar members with the combination of adaptive optics imaging in the infrared and large - precision radial velocities .We derive a lower mass function of n ( m ) ~ m ^ - 1 . 5 , with a median value of 0 . 088 Mjup , i . e . , 7 % of the cluster mass . This is the deepest exploration to date of the substellar regime in a young cluster .The cluster Blanco 1 contains low - mass members from 30 Mjup down to the substellar domain , with a median value of 0 . 088 Mjup . The lower mass function is n ( m ) ~ m ^ - 1 . 5 .",
        "rewrite_text": "The Blanco 1 cluster, situated in the Chamaeleon star-forming region at a distance of approximately 120 to 150 parsecs, has been characterized by Béjar et al. (2004) as a young open cluster with an estimated age range of 10 to 30 million years. Direct measurements of the masses of its low-mass members have not been conducted. In this study, we estimate the masses of the substellar members weighing between 10 and 3 Mjup, utilizing a combination of adaptive optics imaging in the infrared and high-precision radial velocity measurements.\n\nWe have derived a lower mass function of the form n(m) ~ m^-1.5, with a median value of 0.088 Mjup, which accounts for approximately 7% of the cluster's total mass. This represents the most extensive exploration of the substellar population in a young cluster so far. The Blanco 1 cluster comprises members with masses ranging from 30 Mjup down to the substellar limit, with a median mass of 0.088 Mjup. The lower mass function follows the trend of n(m) ~ m^-1.5.",
        "ori-fast-z-score": -2.213211486674006,
        "water-fast-z-score": -1.171700198827415,
        "rewrite-fast-z-score": -1.3054598240132387
    },
    {
        "original_text": "The MiniBooNE experiment at Fermilab announced new data in March 2017 constraining the possiblenum of sterile neutrinos mixing with ordinary neutrinos. These results were based on data taken between 2008 and 2011. The new data supported the hypothesis that there are at least two sterile neutrinos with square-mass difference of (0.15 - 2.29) eV2 and at least one with a non-zero mass. Several neutrino mass schemes were tested with no evidence of flavon fields driving mass to a large value, as might be expected in an underlying theories such as the See-Saw mechanism. The oscillation parameters determined in this analysis are also consistent with recent results from the Daya Bay and RENO experiments. This paper provides a critical evaluation of recent results from the MiniBooNE and other experiments in light of possible sterile neutrino species. We summarize three common hypotheses for the allowed sterile neutrino masses and mixing, including the minimal, mixed, and two-zero schemes. We present the formalism for each hypothesis and show the associated range of oscillation parameters for three representative sets of light neutrino mass splittings. We discuss potential models for sterile neutrino generation that predict distinct behaviors in each of the three sterile neutrino mass schemes. We conclude by summarizing the implications of recent sterile neutrino results in light of the possible schemes. Authors: M.Maltoni et al. E-mail: mmaltoni@fnal.gov Fermi National Accelerator Laboratory Batavia, IL 60510",
        "watermark_text": "The MiniBooNE experiment at Fermilab announced additional data in March 2017 constraining the possiblenum of sterile neutrinos mix with normal neutrinos . These conclusions were based on statistics taken between 2008 and 2011 .The revised data advanced the notion that there are at least two sterile neutrinos with square - mass difference of ( 0 . 15 - 2 . 29 ) eV2 and at least one with a non - zero mass . Several neutrino mass systems were tested with no evidence of flavon fields driving mass to a large value , as could be anticipated in an underlying theories such as the See - Saw mechanism .The oscillation parameters calculated in this analysis are also consistent with recent results from the Daya Bay and RENO experiments . This paper provides a critical assessment of recent results from the MiniBooNE and other experiments in light of possible sterile neutrino species .We summarize three common hypotheses for the allowed sterile neutrino masses and mixing , including the reduced , blended , and two - zero schemes . We introduce the formalism for each theory and get the associated range of oscillation parameters for three representative sets of light neutrino mass splittings .We discuss possible models for sterile neutrino production that forecast distinct behaviors in each of the three sterile neutrino mass schemes . We follow by summarizing the implications of recent sterile neutrino results in light of the suggested schemes .Authors : M . Maltoni et al . E - mail : mmaltoni @ fnal . gov Fermi National Accelerator Laboratory Batavia , IL 60510",
        "rewrite_text": "At the Fermilab, the MiniBooNE experiment announced additional data in March 2017 that further constrained the potential number of sterile neutrinos mixing with regular neutrinos. These conclusions were derived from statistical analyses conducted between 2008 and 2011. The revised data strengthened the notion that there exist at least two sterile neutrinos with a square-mass difference range of (0.15 - 2.29) eV² and at least one with a non-zero mass. Several neutrino mass systems were tested, revealing no evidence of flavon fields driving mass to excessive values, as anticipated in underlying theories such as the See-Saw mechanism. The oscillation parameters calculated in this analysis are also in agreement with recent findings from the Daya Bay and RENO experiments.\n\nThis paper provides a critical evaluation of recent results from the MiniBooNE and other experiments in relation to possible sterile neutrino species. We summarize three common hypotheses for the allowed sterile neutrino masses and mixing, including the reduced, blended, and two-zero schemes. For each theory, we introduce the relevant formalism and derive the associated range of oscillation parameters for three representative sets of light neutrino mass splittings.\n\nWe also discuss potential models for the production of sterile neutrinos, which predict distinct behaviors in each of the three sterile neutrino mass schemes. Finally, we summarize the implications of recent sterile neutrino findings in the context of the suggested schemes.\n\nAuthors: M. Maltoni et al. E-mail: mmaltoni@fnal.gov Fermi National Accelerator Laboratory, Batavia, IL 60510.",
        "ori-fast-z-score": 0.5184758473652127,
        "water-fast-z-score": 5.838403593598094,
        "rewrite-fast-z-score": 3.2
    },
    {
        "original_text": "The weighted complexity and the determinant functions of graphs are defined. These two new graph invariants generalizes several existing graph invariants such as vertexcover, edgecover, matching, determinant, branching number, independence number and total graph domination number. Weights are assigned to the vertices according to their importance in the sense that vertices with higher weights are preferred. We show that the weighted complexity of a graph is equal to the matching complexity of the line graph of the graph, and the determinant of a graph is equal to the branch-width of the line graph of the graph. We give several relationships between these invariants. In particular, we show that the weighted complexity and the determinant are both lower bounds for the matching complexity, branching number and independence number. On the other hand, we give two graphs with different weights for which the weighted complexity and the determinant are the same but their matching complexity and branch-width are different. We conclude by observing that the problem of computing the weighted complexity and the determinant is -complete even when the weights are all one.",
        "watermark_text": "The weighted complexity and the determinant functions of graphs are established . These two modern graph invariants generalizes numerous established graph invariants such as vertexcover , edgecover , matching , determinant , branching number , independence number and total graph domination number .Weights are assigned to the vertices according to their importance in the sense that vertices with higher weights are preferred . We see that the weighted complexity of a graph is equal to the matching complexity of the line graph of the graph , and the determinant of a graph is equal to the branch - length of the line graph of the graph .We get several relationships between these invariants . In particular , we tell that the weighted complexity and the determinant are both lower bounds for the matching complexity , branching number and freedom number .On the other hand , we give two graphs with different weights for which the weighted complexity and the determinant are the same but their matching complexity and branch - length are different . We follow by observing that the question of computing the weighted complexity and the determinant is - full even when the weights are all one .",
        "rewrite_text": "The weighted complexity and determinant functions for graphs have been established. These contemporary graph invariants generalize numerous established concepts, including vertex cover, edge cover, matching, determinant, branching number, independence number, and total graph domination number. Vertices are assigned weights based on their significance, prioritizing those with higher weights. It is observed that the weighted complexity of a graph is equivalent to the matching complexity of its line graph, while the determinant of a graph aligns with the branch length of its line graph.\n\nSeveral relationships exist between these invariants. Specifically, we find that both the weighted complexity and determinant serve as lower bounds for matching complexity, branching number, and freedom number. On the other hand, we present two graphs with differing weight assignments where their weighted complexity and determinant are identical, yet their matching complexity and branch length vary. Finally, we note that determining the weighted complexity and determinant remains challenging even when all weights are set to one.",
        "ori-fast-z-score": -1.6666666666666667,
        "water-fast-z-score": 1.8888888888888888,
        "rewrite-fast-z-score": 0.9878783399072131
    },
    {
        "original_text": "Interaction between proteins is crucial for the stability and function of a protein. Networks provide a valuable tool to understand the interrelationships between proteins and how these interactions impact the function of the protein. Clustering coefficient, which measures the degree to which a node is a part of a local network of interconnected clusters, has been found to be a key property of complex networks with applications in network robustness, dynamics and heterogeneity. In this study, we have calculated clustering coefficients for a number of protein-protein interaction (PPI) networks. Results show that clustering coefficients of PPI networks are generally low compared with those of corresponding random networks. Furthermore, we have observed that the degree distribution of PPI networks follows a power law, which may explain low clustering coefficients of the networks. Our findings could be helpful for a better understanding of the architecture of PPI networks and related biological implications. Zhang, W., Chen, M., Zhang, L. & Wang, J. (2017). Clustering Coefficients of Protein-Protein Interaction Networks. arXiv preprint arXiv:1705.04288.",
        "watermark_text": "Interaction between proteins is crucial for the stability and activity of a protein . Networks provide a crucial tool to explain the interrelationships between proteins and how these interactions impact the function of the protein .Clustering parameter , which calculated the degree to which a node is a component of a local system of interconnected clusters , has been shown to be a key property of complex networks with applications in system robustness , dynamics and heterogeneity . In this study , we have calculated clustering coefficients for a number of gene - gene interaction ( PPI ) networks .Results show that clustering coefficients of PPI networks are typically poor compared with those of corresponding random networks . Furthermore , we have noted that the degree distribution of PPI networks follows a power law , which may reason small clustering coefficients of the connections .Our findings may be beneficial for a better study of the structures of PPI networks and related biological implications . Zhang , W . , Chen , M . , Zhang , L . & Wang , J .(2017).Clustering Coefficients of Protein-Protein Interaction Networks.arXiv preprint arXiv:1705.04288.",
        "rewrite_text": "The significance of protein interactions lies at the core of maintaining protein stability and activity. Networks offer an indispensable means to elucidate the interdependencies between proteins and how these interactions influence protein function. The clustering parameter, which quantifies the extent to which a node is integrated into a local system of interconnected clusters, has been identified as a crucial attribute of intricate networks, playing a role in system resilience, dynamics, and diversity.\n\nIn this study, we computed clustering coefficients for various gene-gene interaction (PPI) networks. Our findings indicate that these PPI networks typically exhibit lower clustering coefficients compared to corresponding random networks. Additionally, we observed that the degree distribution of PPI networks follows a power law, which may explain the relatively low clustering coefficients of the connections. Our research could potentially aid in a deeper understanding of the structures of PPI networks and their relevant biological implications.\n\n(Zhang et al., 2017). Clustering Coefficients of Protein-Protein Interaction Networks. arXiv preprint arXiv:1705.04288.",
        "ori-fast-z-score": 0.47809144373375745,
        "water-fast-z-score": 5.737097324805089,
        "rewrite-fast-z-score": 1.3587324409735149
    },
    {
        "original_text": "In this note we point out a connection between Leonard triples and hypercubes. Leonard triples are a family of vectors in Euclidean space that arise from combinations of fundamental bases for algebra derived from the symplectic root system. We show that the entries of a Leonard triple in the first coordinate correspond to the centers of the edges of a hypercube. adjudicate In this note we point out a connection between Leonard triples and hypercubes. Leonard triples are a family of vectors in Euclidean space that arise from combinations of fundamental bases for algebra derived from the symplectic root system. We show that the entries of a Leonard triple in the first coordinate correspond to the centers of the edges of a hypercube. panel secondly We show that the entries of a Leonard triple in the first coordinate correspond to the centers of the edges of a hypercube. Leonard triples are a family of vectors in Euclidean space that arise from combinations of fundamental bases for algebra derived from the symplectic root system. The first member of this family was found by Nicholas Shaheen",
        "watermark_text": "In this note we note out a connection between Leonard triples and hypercubes . Leonard triples are a family of vectors in Euclidean space that arise from combinations of fundamental bases for algebra constructed from the symplectic root system .We see that the entries of a Leonard triple in the first coordinate correspond to the centers of the edges of a hypercube . adjudicate In this note we point out a connection between Leonard triples and hypercubes .Leonard triples are a family of vectors in Euclidean space that arise from combinations of fundamental bases for algebra constructed from the symplectic root structure . We see that the entries of a Leonard triple in the first coordinate correspond to the centers of the edges of a hypercube .panel secondly We see that the entries of a Leonard triple in the first coordinate correspond to the centers of the edges of a hypercube . Leonard triples are a family of vectors in Euclidean space that arise from combinations of fundamental bases for algebra constructed from the symplectic root structure .The first member of this family was seen by Nicholas Shaheen",
        "rewrite_text": "In this report, we establish a relationship between Leonard triples and hypercubes. Leonard triples form a family of vectors in the Euclidean space that are derived from the combination of fundamental algebraic bases built upon the symplectic root system. It is evident that the elements of a Leonard triple, particularly in the first coordinate, correspond precisely to the centers of the hypercube's edges. Moreover, this connection is further exemplified by the observation that the first member of this family was noted by Nicholas Shaheen.",
        "ori-fast-z-score": 2.182820625326997,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": 1.0606601717798212
    },
    {
        "original_text": "HD 73526 is a spectroscopic binary system consisting of two stars with similar mass which orbit around their common center of gravity with a period of approximately 20.87 days. This short period make it possible for the two stars to interact with each other strongly through angular momentum exchange and subsequent formation of a common envelope. This process is expected to dramatically change the configurations of the stars and their orbital parameters. In this work, we study this system through full numerical integration of the corresponding equations of motion, in order to characterize the present configurations and orbital parameters as well as their variations with time. We find that after an initial stage during which the system loses a large amount of orbital energy and angular momentum, the system reaches a stage of resonant lock in which the period of the inner orbit is equal to the period of the outer orbit. The two stars are then observed to be in a stable configuration with constant period, mass, and distance.",
        "watermark_text": "HD 73526 is a spectroscopic binary system consisting of two stars with similar mass which orbit around their common center of gravity with a period of approximately 20 . 87 days . This short period making it able for the two stars to interact with each other significantly through angular velocity exchange and subsequent formation of a common envelope .This process is expected to dramatically alter the configurations of the stars and their orbital variables . In this research , we study this scheme through full statistical integration of the associated equations of movement , in order to characterize the present arrangements and orbital variables as well as their variations with time .We see that after an initial stage during which the system loses a large amount of orbital energy and angular velocity , the system reaches a stage of resonant lock in which the period of the inner orbit is equal to the period of the inner orbit . The two stars are then discovered to be in a steady arrangement with constant period , mass , and distance .",
        "rewrite_text": "HD 73526 is a binary system that is spectroscopic in nature, comprising of two stars with comparable masses that revolve around their shared center of gravity with a periodicity of approximately 20.87 days. This brief period enables the two stars to engage in significant interactions via the exchange of angular velocity, ultimately leading to the formation of a shared envelope. This process is anticipated to significantly alter the configurations of the stars and their orbital parameters. In this research, we employ comprehensive statistical integration of the associated equations of motion to characterize the current configurations and orbital parameters, as well as their temporal variations. We observe that, after an initial phase where the system loses a considerable amount of orbital energy and angular velocity, it settles into a state of resonant lock where the period of the inner orbit matches that of the outer orbit. Consequently, the two stars are found to maintain a stable arrangement with consistent period, mass, and distance.",
        "ori-fast-z-score": 0.22086305214969307,
        "water-fast-z-score": 4.417261042993862,
        "rewrite-fast-z-score": 0.1125087900926024
    },
    {
        "original_text": "Large extra dimensions (LEDs) provide a natural mechanism by which conservation laws may be localised on a four dimensional brane embedded in a higher dimensional space-time. The LEDs must be compact, in order to solve the hierarchy problem. The apparent violation of the conservation of energy in our universe, implied by the observed dilution of the cosmic microwave background (CMB), is explained by the creation of our universe on a brane embedded in an anti-de Sitter (AdS) LED with negative cosmological constant. In this scenario the universe appears four dimensional due to a fundamental conflict between the conservation of energy in our brane world and the continuous creation of AdSLEDs. In standard models of cosmology, the observed CMB temperature is generated by photons which are created after the decoupling of the free electrons which comprise the CMB. The photons travel across the observable universe before reaching us. Therefore, the observed temperature of the CMB is the result of the sum of the energies of all the particles after they were created, before they were able to interact. If the universe is strictly four dimensional, with no additional space-time dimensions, then this conservation of energy is maintained, as all energy remains in the universe. However, if our universe is embedded in an AdSLED with negative cosmological constant, then this conservation is violated, as AdSLEDs are created by the brane world dynamics.",
        "watermark_text": "Large extra dimensions ( LEDs ) offer a natural mechanism by which conservation laws may be localised on a four dimensional brane embedded in a higher dimensional space - time . The LEDs must be compact , in order to solve the hierarchy problem .The alleged violation of the conservation of power in our universe , assumed by the observed dilution of the cosmic microwave background ( CMB ) , is explained by the creation of our universe on a brane embedded in an anti - de Sitter ( AdS ) LED with negative cosmological constant . In this situation the universe appears four dimensional due to a basic conflict between the conservation of power in our brane world and the ongoing creation of AdSLEDs .In standard models of cosmology , the observed CMB heat is generated by photons which are created after the decoupling of the free electrons which contain the CMB . The photons travel across the observable universe before reaching us .Therefore , the seen temperature of the CMB is the result of the sum of the energies of all the particles after they were created , before they were allowed to interact . If the universe is strictly four dimensional , with no additional space - time dimensions , then this conservation of power is maintained , as all electricity stays in the universe .However , if our universe is embedded in an AdSLED with negative cosmological function , then this conservation is violated , as AdSLEDs are created by the brane world dynamics .",
        "rewrite_text": "Large extra dimensions (LEDs) provide a natural mechanism that allows conservation laws to be localized on a four-dimensional brane embedded within a higher-dimensional space-time continuum. These LEDs must be compact in order to resolve the hierarchy issue. The alleged violation of power conservation in our universe, inferred from the observed dilution of the cosmic microwave background (CMB), is explained by the creation of our universe on a brane that is embedded in an anti-de Sitter (AdS) LED with a negative cosmological constant. In this scenario, the universe appears four-dimensional due to a fundamental conflict between power conservation in our brane-based reality and the continuous creation of AdSLEDs.\n\nIn conventional cosmology models, the observed heat of the CMB is generated by photons that are created after the decoupling of free electrons containing the CMB. These photons traverse the observable universe before reaching us. Therefore, the observed temperature of the CMB is a result of the cumulative energy of all particles after their creation, before they are allowed to interact. If the universe were strictly four-dimensional, with no additional space-time dimensions, power conservation would be maintained as all electricity remains within the universe. However, if our universe is embedded within an AdSLED with a negative cosmological function, this conservation of power is violated due to the creation of AdSLEDs by the dynamics of the brane world.",
        "ori-fast-z-score": 1.876629726513673,
        "water-fast-z-score": 4.744537732790449,
        "rewrite-fast-z-score": 2.154554539378824
    },
    {
        "original_text": "Tidal fields reconstructed from the distribution of dark matter contain a wealth of information about the growth and merger history of galaxies, as well as their current dynamical state. We present strong and weak-lensing mass reconstructions of the Corullo-IATA1638+29 cluster, and show that the two correlated structures comprise a triple merger with the central galaxy’s associated tidal field revealing the orbital plane of the cluster and its projected separation. We additionally show that the orientations of the projected spins of the galaxies cluster around this same plane, and quantify the alignment using two-dimensional Kuiper test significance, testing for spatial correlation with the orientation of the cluster’s tidal field. We additionally test the hypothesis that the galaxies’ spins are uniformly distributed, finding a probability of < 10-4 that the orientation of the spins are random. We conclude that the galaxies in this structure have aligned their spins with the orbital plane of their merger.",
        "watermark_text": "Tidal fields reconstructed from the distribution of dark matter contain a rich of evidence about the development and merger history of clusters , as well as their current dynamical state . We create strong and weak - lensing mass reconstructions of the Corullo - IATA1638 + 29 cluster , and suggest that the two correlated systems comprise a triple amalgamation with the main galaxy ’ s associated tidal field revealing the orbital plane of the cluster and its projected separation .We additionally prove that the orientations of the projected spins of the galaxies cluster around this same plane , and quantify the alignment using two - dimensional Kuiper test significance , screening for spatial correlation with the orientation of the cluster ’ s tidal field . We additionally check the assumption that the galaxies ’ spins are uniformly distributed , finding a probability of < 10 - 4 that the orientation of the spins are random .We assume that the galaxies in this formation have aligned their spins with the orbital plane of their merger .",
        "rewrite_text": "The reconstructed tidal fields from the distribution of dark matter provide an abundance of evidence regarding the evolution and merger histories of clusters, as well as their current dynamic state. We have conducted strong and weak-lensing mass reconstructions of the Corullo-IATA1638+29 cluster, and propose that the two interrelated systems constitute a triple merger, with the main galaxy's associated tidal field unveiling the orbital plane and its projected separation. Furthermore, we have verified that the projected spin orientations of galaxies cluster around the same plane, quantifying the alignment using a two-dimensional Kuiper test for spatial correlation with the cluster's tidal field orientation. We also tested the assumption that galaxy spins are uniformly distributed, finding a probability less than 10-4 that the spin orientations are random. We believe that the galaxies in this formation have aligned their spins with the orbital plane of their merging process.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 3.2118202741878643,
        "rewrite-fast-z-score": 0.3511234415883917
    },
    {
        "original_text": "Three-dimensional space is filled with a thicket of 3-dimensional curves. All of these can be classified by their topological type, their algebraical type, or a combination of these. The most interesting 3-dimensional curves are singular ones: they are generically n-dimensional and transversal to n-dimensional space. Singular curves are traditionally classified by their genera, i.e. by their codimensions in the parameter space. A comprehensive theory of singular curves was developed in the late 1980s and early 1990s, see in particular the works of Harris and points of view of Coppo and Garrone. These works prove a deformation theoretic framework for understanding the birational classification of such curves, i.e. their ability to be infinitesimally deformed into another one. This framework implies also that for a generic choice of a curve, there is a plane in the nine-dimensional projective space of quadric hypersurfaces that the curve doesn’t intersect, and such a plane is called an exceptional plane. Curves with exceptional planes are called decomposable. This paper studies the case when the generic curve is not decomposable: it has a unique (up to projective equivalence) exceptional plane. Such curves are called non-decomposable. The most common case is when the exceptional plane is a plane of linear dependency for the general curve: such a plane is called an exceptional secant plane. The present paper develops the theory of such curves, and among other results, gives a birational classification (the so-called Wahl map) and describes how the variety of non-decomposable curves of a given genus (which is open of infinite type) fibers over the space of curves with a given exceptional secant plane (which is of finite type).",
        "watermark_text": "Three - dimensional space is filled with a thicket of 3 - dimensional shapes . All of these can be categorized by their topological type , their algebraical type , or a combination of these .The most important 3 - dimensional shapes are singular ones : they are generically n - dimensional and transversal to n - dimensional space . Singular curves are traditionally grouped by their genera , i . e .by their codimensions in the parameter space . A rigorous theory of singular curves was developed in the mid 1980s and early 1990s , see in instance the works of Harris and points of view of Coppo and Garrone .These works demonstrate a deformation theoretic framework for studying the birational subdivision of such curves , i . e . their potential to be infinitesimally deformed into another one .This framework asserts also that for a generic choice of a curve , there is a plane in the nine - dimensional projective space of quadric hypersurfaces that the curve doesn ’ t intersect , and such a plane is dubbed an exceptional plane . Curves with exceptional planes are called decomposable .This paper studies the case when the generic curve is not decomposable : it has a unique ( up to projective equivalence ) special plane . Such surfaces are called non - decomposable .The most common example is when the extraordinary plane is a plane of linear dependency for the general curve : such a plane is called an exceptional secant plane . The present paper develops the notion of such curves , and among other results , offers a birational classification ( the so - called Wahl map ) and explains how the range of non - decomposable curves of a given genus ( which is closed of infinite type ) fibers over the space of curves with a given exceptional secant plane ( which is of finite type ) .",
        "rewrite_text": "The three-dimensional space is teeming with a dense network of three-dimensional shapes, all of which can be categorized based on their topological type, algebraic type, or a combination of both. Among these, the most significant three-dimensional shapes stand out as singular: they are generally n-dimensional and traverse the n-dimensional space. These singular curves are traditionally grouped by their genera, which refers to their codimensions in the parameter space.\n\nA rigorous theory of singular curves was established in the mid-1980s and early 1990s, exemplified by the works of Harris and the perspectives of Coppo and Garrone. These works present a deformation-theoretic framework for studying the birational subdivision of such curves, i.e., their potential to be infinitely deformed into another. This framework posits that for a generically chosen curve, there exists a plane within the nine-dimensional projective space of quadric hypersurfaces that the curve does not intersect. Such a plane is referred to as an exceptional plane. Curves with exceptional planes are deemed decomposable.\n\nThis paper delves into the scenario where the generic curve is non-decomposable, possessing a unique (up to projective equivalence) special plane. These surfaces are termed non-decomposable. A common instance arises when this exceptional plane represents a plane of linear dependency for the general curve, known as an exceptional secant plane. This paper develops the concept of these curves and, among other findings, offers a birational classification (known as the Wahl map) while elucidating how the range of non-decomposable curves of a given genus (which is closed and of infinite type) is fibered over the space of curves with a specific exceptional secant plane (which is of finite type). This process enables a deeper understanding of the intricacies and interconnections between these shapes within the three-dimensional space.",
        "ori-fast-z-score": 1.5852581740085334,
        "water-fast-z-score": 6.18146635643918,
        "rewrite-fast-z-score": 3.391784439004382
    },
    {
        "original_text": "Post-CCSD(T) molecular atomization energies (i.e., atomization energies after correction for single and double excitations, but before frozen pair.) are typically computed on a smaller basis set than coupled cluster doubles (CCD) energies, though there is no rigorous rationale for this. Here, we demonstrate that basis set convergence for atomization energies can be rationalized with the pair natural orbital partial correction (PNOC), which captures the importance of dynamic correlation in the post-CCSD energy. Using explicit correlation as a metric, we show that PNOC energies converge more rapidly than CCD energies with basis set size. This rationalization allows us to predict that atomization energies computed with the CBS-QB3 method, which combines dynamic correlation through the CBS connection cost with a reasonably accurate semi-local exchange-correlation functional, will converge substantially more rapidly than CCSD(T) atomization energies on a basis set consistent with the quality of the CBS-QB3 geometry. We test this hypothesis for ten systems with experimental atomization energies and CCSD(T) geometries, and find that CBS-QB3 atomization energies converge to within 1 kcal/mol on a 6-31G** basis, in good agreement with the CBS-QB3 CBS (6-31G(d,p) + CBS-QB3) equilibrium geometries. These results demonstrate that CBS-QB3 can be an efficient method for predicting atomization energies and that post-CCSD(T) energies are sufficient to rationalize basis set convergence.",
        "watermark_text": "Post - CCSD ( T ) molecular atomization values ( i . e . , atomization values after adjustment for single and double excitations , but before frozen pair . ) are typically computed on a smaller basis set than linked cluster doubles ( CCD ) energies , though there is no rigorous rationale for this .Here , we prove that basis set convergence for atomization energies can be rationalized with the pair natural orbital partial correction ( PNOC ) , which demonstrates the importance of dynamic coupling in the post - CCSD energy . Using explicit correlation as a metric , we prove that PNOC energies converge more swiftly than CCD energies with basis set size .This rationalization allows us to predict that atomization energies computed with the CBS - QB3 technique , which mixes dynamic coupling through the CBS link cost with a reasonably accurate semi - regional exchange - correlation functional , will converge substantially more swiftly than CCSD ( T ) atomization energies on a basis set consistent with the performance of the CBS - QB3 geometry . We test this hypothesis for ten systems with experimental atomization energies and CCSD ( T ) geometries , and find that CBS - QB3 atomization values converge to within 1 kcal / mol on a 6 - 31G * * basis , in good agreement with the CBS - QB3 CBS ( 6 - 31G ( d , r ) + CBS - QB3 ) equilibrium geometries .These data demonstrate that CBS - QB3 can be an efficient method for predicting atomization intensity and that post - CCSD ( T ) energies are adequate to rationalize basis set convergence .",
        "rewrite_text": "The post-CCSD(T) molecular atomization values, which are adjusted for single and double excitations but before the frozen pair, are typically computed on a smaller basis set compared to the linked cluster doubles (CCD) energies. However, there is no strict rationale behind this comparison. In this study, we demonstrate that the convergence of atomization energies with the basis set can be rationalized using the Pair Natural Orbital Partial Correction (PNOC). This approach highlights the significance of dynamic coupling in the post-CCSD energy calculations. By utilizing explicit correlation as a metric, we prove that PNOC energies converge faster than CCD energies with an increasing basis set size.\n\nThis rationalization enables us to predict that atomization energies computed using the CBS-QB3 technique, which combines dynamic coupling through the CBS link cost with a semi-regional exchange-correlation functional for accuracy, will converge significantly faster than CCSD(T) atomization energies on a basis set compatible with the performance of the CBS-QB3 geometry. To test this hypothesis, we examined ten systems with experimental atomization energies and CCSD(T) geometries and found that CBS-QB3 atomization values converge to within 1 kcal/mol on a 6-31G** basis, in good agreement with the CBS-QB3 CBS (6-31G(d,r) + CBS-QB3) equilibrium geometries. These data demonstrate that CBS-QB3 can be an efficient method for predicting atomization intensities, and that post-CCSD(T) energies are suitable for rationalizing basis set convergence.",
        "ori-fast-z-score": -0.9332565252573828,
        "water-fast-z-score": 3.4219405926104036,
        "rewrite-fast-z-score": 1.5389675281277313
    },
    {
        "original_text": "TRUFAS (TRANSIT RECOGNITION USING FAST ALGORITHMS), a wavelet based algorithm for the rapid detection of planetary transits, is presented. TRUFAS consists of a detection stage and a validation stage. In the detection stage, TRUFAS computes a series of detection statistics for a given light curve. A detection is declared when the detection statistics reach a specified threshold. A periodogram is computed for each transit detection, and the transit parameters are determined by parabolic fitting to the highest peak in the periodogram. TRUFAS is sensitive to planetary transit depths as small as a few percent of the light curve root mean square (RMS) noise. TRUFAS is efficient, automating the transit search process. The validation stage validates the transit detections using simulated planet systems, and False Alarm Probability (FAP) statistics are computed for each validated transit. The TRUFAS website contains example Light curves for each of the detected TRAPPIST-1 transit candidates, as well as detection statistics and validated planet system information.",
        "watermark_text": "TRUFAS ( TRANSIT RECOGNITION USING FAST ALGORITHMS ) , a wavelet based algorithm for the quick detection of planetary transits , is displayed . TRUFAS consists of a detection phase and a validation phase .In the detection phase , TRUFAS computes a sequence of detection data for a given light curve . A detection is announced when the detection data achieve a specified threshold .A periodogram is computed for each transit detection , and the transit parameters are decided by parabolic fitting to the highest peak in the periodogram . TRUFAS is sensitive to planetary transit depths as low as a few third of the light curve root mean square ( RMS ) pollution .TRUFAS is efficient , automating the transportation search process . The validation phase validates the transit detections using modeled planet systems , and False Alarm Probability ( FAP ) statistics are computed for each validated transit .The TRUFAS web contains example Light curves for each of the detected TRAPPIST - 1 transit candidates , as well as discovery data and validated planet system data .",
        "rewrite_text": "TRUFAS (TRANSIT RECOGNITION USING FAST ALGORITHMS) is a wavelet-based algorithm presented for swift detection of planetary transits. It comprises two phases: a detection phase and a validation phase.\n\nDuring the detection phase, TRUFAS calculates a sequence of detection data for a given light curve. A detection is declared when the obtained data meet a predefined threshold. A periodogram is calculated for each transit detection, and the transit parameters are determined through parabolic fitting to the highest peak in the periodogram. TRUFAS demonstrates sensitivity to planetary transit depths as low as a certain fraction of the light curve's root mean square (RMS) pollution. This algorithm proves efficient and automates the process of transportation search.\n\nIn the validation phase, the transit detections are verified using modeled planet systems, and False Alarm Probability (FAP) statistics are computed for each validated transit. The TRUFAS website provides example light curves for each detected TRAPPIST-1 transit candidate, along with discovery data and validated planet system information.",
        "ori-fast-z-score": -2.629502940535666,
        "water-fast-z-score": 2.390457218668787,
        "rewrite-fast-z-score": 0.3464101615137754
    },
    {
        "original_text": "Network communities, or clusters of nodes with a higher density of connections between nodes in the same community than between nodes in different communities, are important structure in networks. Most current algorithms only detect large groups, or clusters, ignoring the detailed structure of communities. Here we introduce a resolution limit for network clustering, and develop a hierarchical method that detects both large and small communities in networks. The proposed method works by first identifying super-communities at many resolutions, and then clustering each super-community at the given resolution. We validate our method on random and real-world networks, and show that our method outperforms other state-of-the-art community detection algorithms. The full paper, along with technical details and data, is available here: https://arxiv.org/abs/1711.09755 Ian O. Tolstoy, Han Wang, Paul J. Mucha, and Mihaela Van con, “Network Communities with a High Resolution,” arXiv:1711.09755 (December 13, 2017). There has been a recent resurgence of interest in network clustering, sparked by the development of a wide variety of network data, as well as the ubiquity of network datasets in other fields, such as social media and biology. The study of network communities has traditionally focused on detecting large groups or clusters of nodes with high density within the network, or networks. Community detection algorithms have been shown to be sensitive to a resolution parameter, which determines the granularity of clusters to be detected. Higher resolution generally leads to higher quality clusters, although the tradeoff is that more computationally expensive algorithms are needed to detect lower resolution communities. In this paper, we introduce a hierarchical method for community detection that identifies both large and small communities at many resolution levels. The method identifies super-communities, or large groups of nodes that are highly dense and well connected within but low density with respect to other super-communities. The method then clusters the identified super-communities, refining the cluster structure at each resolution. This hierarchical approach allows us to detect both large and small communities at many resolutions. We demonstrate the utility of our approach on both synthetic and real-world networks, showing that our method is highly accurate in detecting clusters and outperforms state-of-the-art community detection algorithms.",
        "watermark_text": "Network networks , or communities of nodes with a higher density of links between nodes in the same community than between nodes in different populations , are important structure in networks . Most recent algorithms only identify huge groups , or communities , ignoring the detailed composition of groups .Here we provide a resolution limit for network clustering , and develop a hierarchical method that detects both large and tiny populations in networks . The proposed system works by first recognizing super - communities at many resolutions , and then clustering each super - village at the particular resolution .We validate our technique on random and actual - time systems , and suggest that our technique outperforms other state - of - the - art community screening schemes . The full paper , along with technical details and information , is accessible here : https : / / arxiv . org / abs / 1711 . 09755 Ian O . Tolstoy , Han Wang , Paul J . Mucha , and Mihaela Van con , “ Network Communities with a High Resolution , ” arXiv : 1711 . 09755 ( December 13 , 2017 ) .There has been a recent resurgence of focus in system clustering , prompted by the development of a broad variety of system data , as also as the ubiquity of system datasets in other fields , such as social media and biology . The investigation of system populations has generally focused on detecting large clusters or clusters of nodes with high density within the organization , or networks .Community detection methods have been shown to be sensitive to a resolution parameter , which determines the granularity of clusters to be spotted . Higher resolution generally leads to higher quality clusters , although the tradeoff is that more computationally expensive algorithms are needed to identify lower resolution communities .In this paper , we provide a hierarchical method for community diagnosis that identifies both large and tiny populations at many resolution levels . The method identifies super - communities , or large clusters of nodes that are extremely dense and well connected within but little density with regard to other super - communities .The method then clusters the established super - communities , refining the cluster structure at each resolution . This hierarchical method enables us to identify both large and tiny populations at many resolutions .We showed the utility of our approach on both synthetic and actual - global networks , showing that our technique is remarkably accurate in detecting clusters and outperforms state - of - the - art community screening schemes .",
        "rewrite_text": "Networks are composed of interconnected communities, known as 'networks of networks' or 'communities of nodes', in which nodes within the same community have a higher density of links than nodes across different communities. Such communities are considered to be essential structural elements of networks. However, modern algorithms often only identify large communities, overlooking the intricate details of smaller groups. To address this, we introduce a resolution limit for network clustering and develop a hierarchical approach that can detect both large and small populations in networks.\n\nOur system operates by first recognizing multiple levels of 'super-communities' and then clustering each of these super-communities at a specific resolution. We have validated our technique using both synthetic and real-world systems, and our results suggest that it outperforms other leading community detection methods.\n\nThe complete paper, along with technical details and additional information, can be accessed at: https://arxiv.org/abs/1711.09755. This study by Ian O. Tolstoy, Han Wang, Paul J. Mucha, and Mihaela Van Con is titled \"Network Communities with a High Resolution\" and was published on December 13th, 2017.\n\nRecently, there has been a renewed focus on system clustering due to the development of a wide range of system data and the prevalence of system datasets in various fields such as social media and biology. Research in this area has primarily focused on detecting large clusters or groups of nodes with high internal connectivity within the network structure.\n\nCommunity detection methods are sensitive to a resolution parameter that determines the level of detail in the clusters being identified. While higher resolutions generally lead to higher-quality clusters, they also require more computationally intensive algorithms to identify communities at lower resolutions. In our paper, we present a hierarchical method for community analysis that can identify both large and small populations at various resolution levels. This method first identifies super-communities, which are large clusters of highly connected nodes that are sparsely connected to other super-communities. It then further clusters these super-communities to refine the cluster structure at each resolution level. This approach enables us to detect both large and small populations across multiple resolution levels.\n\nWe have demonstrated the effectiveness of our approach using both synthetic and real-world global networks, showing that our technique is highly accurate in identifying clusters and surpasses state-of-the-art community detection methods.",
        "ori-fast-z-score": -1.4744195615489712,
        "water-fast-z-score": 8.648888870168502,
        "rewrite-fast-z-score": 0.9949366763261821
    },
    {
        "original_text": "Noether’s theorem is one of the fundamental principles of physics, stating that in any symmetry of the laws of physics, there is a conserved quantity. In this Letter, we show that it is possible to obtain conservation laws in modified gravity theories by invoking a hidden symmetries. We demonstrate this for the class of f(R) theories of gravity and show that the standard Einstein-Hilbert term of General Relativity gives way to a modification that has no conventional Newtonian description. Despite the non-linearity of the field equations, the curvature scalar is not an appropriate variable to describe the dynamics of our universe, and a reduced description based on the development of a new scalar degree of freedom is needed. We explore this theory quantitatively through the analysis of solutions to the field equations and show that this leads to modifications to the Poisson equation and the form of the matter density in the universe, which can lead to new physical effects in the large scale structure of the universe. We present a simple and general procedure for finding exact solutions to the field equations of these theories, and demonstrate its use with some specific examples. We show that these solutions give way to inhomogeneous stellar distributions of stars with compact cores, which have not been identified in previous solutions to the field equations of modified gravity theories.",
        "watermark_text": "Noether ’ s theorem is one of the fundamental principles of physics , stating that in any symmetry of the rules of physics , there is a conserved quantity . In this Letter , we prove that it is possible to obtain conservation laws in modified gravity theories by invoking a hidden symmetries .We suggest this for the class of f ( R ) theories of gravitational and suggest that the standard Einstein - Hilbert term of General Relativity gives way to a modification that has no standard Newtonian description . Despite the non - linearity of the field equations , the curvature scalar is not an appropriate variable to explain the dynamics of our universe , and a reduced explanation based on the development of a new scalar degree of liberty is required .We explore this theory quantitatively through the evaluation of solutions to the field equations and find that this contributes to modifications to the Poisson equation and the form of the mind concentration in the universe , which can lead to novel physical effects in the huge scale system of the universe . We introduce a simple and general technique for finding exact solutions to the field equations of these theories , and suggest its use with some specific examples .We see that these solutions give way to inhomogeneous stellar distributions of stars with compact cores , which have not been described in earlier solutions to the field equations of modified gravity physics .",
        "rewrite_text": "The Noether's theorem stands as a fundamental principle in physics, stating that every symmetry in the laws of physics is accompanied by a conserved property. In this letter, we demonstrate that hidden symmetries can be leveraged to derive conservation laws in altered gravity theories. We propose this approach for the class of f(R) gravitational theories, suggesting that the conventional Einstein-Hilbert term in General Relativity may be superseded by a modification lacking a standard Newtonian description. Despite the non-linearity of the field equations, the curvature scalar is inadequate to fully explain the dynamics of our universe. Instead, a simplified explanation rooted in the development of a novel scalar degree of freedom is required.\n\nWe quantitatively explore this theory by evaluating solutions to the field equations and find that it contributes to modifications in the Poisson equation and the form of matter concentration in the universe, potentially leading to novel physical effects on a large scale. We introduce a straightforward and versatile technique for finding exact solutions to the field equations of these theories, offering its application with specific examples. Our findings indicate that these solutions yield inhomogeneous stellar distributions with compact cores, which have not been previously observed in solutions to the field equations of modified gravity physics.",
        "ori-fast-z-score": 0.6,
        "water-fast-z-score": 5.4,
        "rewrite-fast-z-score": 1.1881770515720091
    },
    {
        "original_text": "In this paper, we study the (0,2) linear sigma model (LSM) on supermanifold. The (0,2) LSM is a useful tool to study various aspects of string theory compactified on non-trivial Ricci-flat backgrounds such as (p,q) 7-branes, etc... The (0,2) LSM is also expected to provide an effective description of the low-energy dynamics of some complete intersections in super Calabi-Yau manifolds. We generalize the (0,2) LSM to the (0,2) gauged linear sigma model (GLSM) on supermanifold, and show that the GLSM on supermanifold is renormalizable when the supermanifold is non-compact and admits a (restricted) gauge symmetry. We also show that the on-shell (0,2) superfield gauge transformation is also a local symmetry of the action. Then we discuss the vacua and some of their symmetries of the (0,2) GLSM on supermanifold, and give some typical examples of its applications.",
        "watermark_text": "In this paper , we study the ( 0 , 2 ) linear sigma system ( LSM ) on supermanifold . The ( 0 , 2 ) LSM is a helpful resource to study various parts of string theory compactified on non - simple Ricci - flat backgrounds such as ( p , q ) 7 - branes , etc . . .The ( 0 , 2 ) LSM is also expected to provide an useful characterization of the small - energy dynamics of some complete intersections in super Calabi - Yau manifolds . We generalize the ( 0 , 2 ) LSM to the ( 0 , 2 ) gauged linear sigma model ( GLSM ) on supermanifold , and find that the GLSM on supermanifold is renormalizable when the supermanifold is non - compact and admits a ( restricted ) gauge symmetry .We also prove that the on - shell ( 0 , 2 ) superfield gauge transformation is also a local symmetry of the operation . Then we explain the vacua and some of their symmetries of the ( 0 , 2 ) GLSM on supermanifold , and take some common examples of its applications .",
        "rewrite_text": "In this research, we explore the (0, 2) linear sigma system (LSM) on a supermanifold. The (0, 2) LSM serves as a valuable tool for studying various aspects of string theory compactified on non-simple Ricci-flat backgrounds, such as (p, q) 7-branes. Additionally, it is anticipated to offer a useful characterization of the small-energy dynamics within certain complete intersections in super Calabi-Yau manifolds.\n\nWe extend the (0, 2) LSM to the (0, 2) gauged linear sigma model (GLSM) on supermanifolds. We find that the GLSM on a non-compact supermanifold is renormalizable when it adheres to a (restricted) gauge symmetry. We also demonstrate that the on-shell (0, 2) superfield gauge transformation is a local symmetry of the operation.\n\nFurthermore, we elucidate the vacua and their associated symmetries in the (0, 2) GLSM on supermanifolds, providing common examples of its applications.",
        "ori-fast-z-score": 0.14002800840280097,
        "water-fast-z-score": 4.060812243681228,
        "rewrite-fast-z-score": 1.131370849898476
    },
    {
        "original_text": "Soft Constraint Logic Programming is used to implement Unicast and Multicast Qos routing. This allows achieving the desired QoS level without the need of intermediate routers, thus reducing the overall network footprint and the required infrastructure. Numerical simulations show the effectiveness of the approach in terms of QoS delivered to the desired traffic, reduction of the overall network footprint and energy consumption.  This work extends our previous paper  1  by also supporting multicast sessions. First, a background on Soft Constraint Programming is presented to provide the reader with the basic concepts on which Soft Constraint Logic Programming is based. Next, the approach used in  1  is extended to support Unicast as well as Multicast Qos traffic. A numerical example is used to show the effectiveness of the approach in terms of QoS delivered to the desired traffic, reduction of the overall network footprint and energy consumption.  This work was partially supported by Spanish project TEC2014-51847-R, and Mexican SEP projects with awards CONACYT 248735 and SNI 74598.",
        "watermark_text": "Soft Constraint Logic Programming is utilized to execute Unicast and Multicast Qos routing . This enables obtaining the desired QoS level without the require of intermediate routers , thus reducing the overall network footprint and the necessary equipment .Numerical simulations see the performance of the approach in terms of QoS delivered to the desired traffic , removal of the overall network footprint and energy consumption . This project extends our previous report 1 by already including multicast meetings .First , a background on Soft Constraint Programming is provided to provide the viewer with the fundamental concepts on which Soft Constraint Logic Programming is based . Next , the approach used in 1 is extended to support Unicast as well as Multicast Qos traffic .A numerical example is taken to indicate the performance of the approach in terms of QoS delivered to the desired traffic , removal of the overall network footprint and energy consumption . This effort was partially backed by Spanish program TEC2014 - 51847 - R , and Mexican SEP programs with grants CONACYT 248735 and SNI 74598 .",
        "rewrite_text": "Soft Constraint Logic Programming is employed for the execution of Unicast and Multicast QoS routing. This enables the achievement of the desired QoS level without the need for intermediate routers, thereby reducing the overall network footprint and necessary equipment. Numerical simulations have assessed the performance of this approach in terms of the QoS provided to targeted traffic, the reduction of network footprint, and energy consumption.\n\nThis project builds on our previous report, expanding it to include multicast sessions. Initially, a background on Soft Constraint Programming is provided to establish the fundamental concepts upon which Soft Constraint Logic Programming is founded. Subsequently, the approach from the previous report is extended to support both Unicast and Multicast QoS traffic. A numerical example illustrates the effectiveness of this approach in delivering QoS to desired traffic, minimizing network footprint, and optimizing energy usage.\n\nThis effort was partially supported by the Spanish TEC2014 program (grant number 51847-R) and Mexican SEP programs with grants CONACYT 248735 and SNI 74598.",
        "ori-fast-z-score": 0.9428090415820635,
        "water-fast-z-score": 6.83536555146996,
        "rewrite-fast-z-score": 1.8325416653445783
    },
    {
        "original_text": "Relativistic fluctuation theorems are a set of equalities relating the probabilities of different outcomes for systems in thermal equilibrium. They were conjectured by Robert equilibrium in the late 1990s and proved for the first time in 2003 by Euc59lin Wang and Keiji Saito. Since then, a multitude of variants and generalisations have been proven. The original versions are only valid in the special case of thermodynamic processes in which the dynamics are Markov and the system has only first-order time-dependent dependencies. In this case the corresponding theorems are known as the classical or non-covariant fluctuation theorems. More recently it has been shown that, under less restrictive conditions, similar theorems also hold with a covariance factor which depends on the dynamics of the system. In this article we present a complete derivation of these general theorems, including all covariances. As a check, several previously proven special cases are shown to be recovered as limiting cases of the general theorem. This article is an extended and updated version of a review published in 2014.",
        "watermark_text": "Relativistic fluctuation theorems are a setting of equalities concerning the probabilities of different outcomes for processes in heat equilibrium . They were conjectured by Robert equilibrium in the mid 1990s and demonstrated for the first time in 2003 by Euc59lin Wang and Keiji Saito .Since then , a variety of variants and generalisations have been proven . The original versions are only valid in the particular instance of thermodynamic systems in which the dynamics are Markov and the process has only first - order time - based dependencies .In this instance the equivalent theorems are known as the classical or non - covariant fluctuation theorems . More recently it has been shown that , under fewer restrictive conditions , related theorems still hold with a covariance parameter which depends on the dynamics of the process .In this page we present a complete derivation of these general theorems , including all covariances . As a check , various previously proven particular instances are shown to be recovered as limiting cases of the general theorem .This section is an extended and updated edition of a review published in 2014 .",
        "rewrite_text": "The relativistic fluctuation theorems constitute a set of equalities that govern the probabilities of various outcomes in processes maintaining heat equilibrium. These theorems were first conjectured by Robert in the mid-1990s and subsequently first demonstrated in 2003 by Euc59lin Wang and Keiji Saito. Since then, numerous variants and generalizations have been proven. The original formulations are only applicable specifically to thermodynamic systems with Markov dynamics and first-order time dependencies. In such cases, the equivalent theorems are referred to as classical or non-covariant fluctuation theorems.\n\nMore recently, it has been found that, under less restrictive conditions, related theorems still hold with a covariance parameter that is dependent on the process dynamics. On this page, we provide a comprehensive derivation of these general theorems, encompassing all covariances. As a verification, we show that various previously proven special cases can be recovered as limiting scenarios of the general theorem. This section is an expanded and updated version of a review published in 2014.",
        "ori-fast-z-score": -0.9701425001453319,
        "water-fast-z-score": 5.497624996033735,
        "rewrite-fast-z-score": 1.5
    },
    {
        "original_text": "Atomic data from the Iron Project (IP) have been made available via the internet for use in stellar and plasma physics, quantum chemistry, nuclear physics, materials science, and atomic science. The IP has been a long-term collaborative effort among many institutions to collect and organize accurate, robust, and well-vetted atomic data for all elements with masses between He and copper. The data has been organized in easily browsable databases with custom HTTP and HTTPS endpoints. We provide here data for radiative transition rates and collision strengths for Ca II. The data have been checked for reliability and consistency against other sources and our previous data for similar elements. Where data were not available elsewhere we have provided our own calculations and estimates of the uncertainty to allow for modeling and analysis by others. The full details of the data and its organization, including references to the original sources, can be found at https://doi.org/10.26208/data.0000002329.",
        "watermark_text": "Atomic information from the Iron Project ( IP ) have been made accessible via the online for use in stellar and plasma physics , quantum science , nuclear science , materials science , and atomic physics . The IP has been a many - term cooperation effort among various institutions to collect and arrange precise , robust , and well - vetted nuclear information for all atoms with masses between He and copper .The data has been organized in easily browsable libraries with custom HTTP and HTTPS endpoints . We provide here statistics for radiative transfer rates and collision strengths for Ca II .The data have been checked for accuracy and accuracy against other sources and our previous data for related components . Where information were not accessible elsewhere we have provided our own calculations and accounts of the uncertainty to allow for modeling and assessment by others .The full information of the information and its organization , including references to the original sources , can be found at https : / / doi . org / 10 . 26208 / data . 0000002329 .",
        "rewrite_text": "The Iron Project (IP) atomic data is now available online, facilitating research in various fields such as stellar and plasma physics, quantum science, nuclear science, materials science, and atomic physics. This project represents a long-term collaborative effort between multiple institutions aimed at gathering and arranging precise, reliable, and thoroughly vetted nuclear information for all atoms with masses ranging from helium to copper.\n\nThe organized data is accessible in user-friendly libraries with custom HTTP and HTTPS endpoints. Here, we provide statistics on radiative transfer rates and collision strengths for Ca II. These data have been rigorously verified for accuracy against other sources and our previous data for related components. In cases where information was not readily available elsewhere, we have provided our own calculations and accounted for uncertainties to enable modeling and assessment by others.\n\nThe complete information and its organization, including references to the original sources, can be found at the following link: https://doi.org/10.26208/data.0000002329.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 5.658032638058332,
        "rewrite-fast-z-score": 3.0193176496962755
    },
    {
        "original_text": "The NUGA galaxy survey is an integral field spectroscopy survey of 32 early-type galaxies, undertaken using the SSO Telescope. Here we present the data and analysis for the sole spiral galaxy in the sample, NGC4569. We find that the majority of the molecular gas in NGC4569 resides in a large-scale bar. This is shown to be driving a two-sided slow-rotating wave in the potential, funneling gas inwards. The molecular gas in this bar is compact, having a deconvolved size of approximately 1.1 kpc. Using a simple rotating bar model, we show that this observed size is consistent with that expected for the bar strength. We detect molecular hydrogen in two secondary resonances, providing further evidence for the existence of a massive bar in the centre of NGC4569. This is the first molecular hydrogen observations of a large scale bar in a spiral galaxy. The presence of a bar in the centre of NGC4569 and its relation to the molecular gas observed are discussed.",
        "watermark_text": "The NUGA galaxy survey is an integral field spectroscopy survey of 32 early - class stars , undertaken using the SSO Telescope . Here we present the information and analysis for the sole spiral galaxy in the sample , NGC4569 .We see that the majority of the molecular gas in NGC4569 exists in a large - scale bar . This is demonstrated to be driving a two - sided slow - spinning wave in the potential , funneling energy inwards .The molecular gas in this bar is compact , having a deconvolved diameter of approximately 1 . 1 kpc . Using a simple rotating bar model , we prove that this measured diameter is compatible with that expected for the bar power .We detect molecular hydrogen in two secondary resonances , providing further evidence for the existence of a huge bar in the centre of NGC4569 . This is the first molecular hydrogen observations of a large scale bar in a spiral galaxy .The presence of a bar in the centre of NGC4569 and its connection to the molecular gas observed are discussed .",
        "rewrite_text": "The NUGA Galaxy Survey is a comprehensive integral field spectroscopy survey that focuses on 32 early-class stars, employing the SSO Telescope for observations. In this study, we present details and analysis of the sole spiral galaxy in the sample, namely NGC 4569. Our findings reveal that the majority of the molecular gas in NGC 4569 is concentrated in a large-scale bar. This bar is observed to be driving a two-sided, slow-rotating wave in the galaxy's potential, efficiently channeling energy inward. The molecular gas within this bar is compact, with a deconvolved diameter of approximately 1.1 kpc. By utilizing a simplified rotating bar model, we confirm that this measured diameter aligns with the expected size for the bar's power. Furthermore, we have detected molecular hydrogen in two secondary resonances, providing additional evidence for the presence of a massive bar at the center of NGC 4569. This is a pioneering observation of molecular hydrogen in a large-scale bar of a spiral galaxy. The existence of a bar at the center of NGC 4569 and its relationship with the observed molecular gas are thoroughly discussed in this study.",
        "ori-fast-z-score": 1.0125791108334214,
        "water-fast-z-score": 4.076197322920544,
        "rewrite-fast-z-score": 1.2792042981336627
    },
    {
        "original_text": "Quarks are assumed to be elementary particles that interact through the strong force. Experimentally, only two of the six quarks, called up and down quarks, occur in stable particles (protons and neutrons) inside ordinary matter. The other four, called strange, up, down and bottom, occur in forms called hadrons, which combine to form matter only through the strong force. The strong force binds the quarks within protons and neutrons in hadrons. The binding energy of a hadron is called its mass. Quarks are believed to be particles that behave like tiny magnets and each come in six different  colors  (representations of the symmetry group SU(3) in quantum chromodynamics, or QCD). The six different  colors  of a quark correspond to the six pieces of a mathematical object called an octet. Each octet flavor has a characteristic fraction of charge and mass, which makes it possible to define the names up and down quarks (theCharge is 2/3 and 1/3 their mass). The remaining flavors, strange, up, down and bottom, each have a corresponding named hadron. These six hadron names can be combined to formlarger hadrons. For example, the bottom quark carries a small amount of the bottom hadron s charge, so a bottom hadron is a weakly interacting particle containing a bottom quark. Within the standard model of particle physics, quarks are assumed to be point particles, and the strong force between them is described by quantum chromodynamics (QCD). The current theory of the strong interaction, quantum chromodynamics (QCD), successfully describes many features of hadrons, including their spectra and decays. One important consequence of the QCD is that the up and down quarks will never appear alone but always appear in combinations, called hadrons. This fundamental interaction between quarks is called  confinement . According to the phenomenological quark model, the most fundamental particles in the universe are the up, down and strange quarks and their corresponding hadrons, called hadrons. The strong force that binds the quarks together forms hadrons. Hadronic matter is typical matter, made of hadrons. It is found throughout the universe in planets, stars and galaxies. However, in order to study it, nuclear physicists study particles that are either fragments of hadrons, such as beta particles and pi mesons, or full hadrons, such as protons and neutrons. These nuclei of hydrogen and helium are called ordinary matter. Quarks are also found inside other particles, but their behavior is not well understood. Because they are so much more massive than the quarks that make up ordinary matter, these  sea quarks  are mainly found inside hadrons, which bind them together. The current theory for the strong interaction between them is called quantum chromodynamics (QCD), but despite numerous experimental tests of its predictions, its exact nature remains a mystery. One prediction of QCD is that when heavy enough particles, such as aPb",
        "watermark_text": "Quarks are assumed to be elementary particles that interact through the strong force . Experimentally , only two of the six quarks , called up and down quarks , occur in stable objects ( protons and neutrons ) inside ordinary matter .The other four , called bizarre , up , down and bottom , occur in forms called hadrons , which combine to form matter only through the strong force . The strong pull binds the quarks within protons and neutrons in hadrons .The binding energy of a hadron is known its mass . Quarks are considered to be particles that behave like tiny magnets and each come in six various shades ( representations of the symmetry class SU ( 3 ) in quantum chromodynamics , or QCD ) .The six various shades of a quark relate to the six pieces of a mathematical instrument called an octet . Each octet flavor has a peculiar proportion of charge and mass , which makes it convenient to define the names up and down quarks ( theCharge is 2 / 3 and 1 / 3 their mass ) .The remaining flavors , strange , up , down and bottom , each have a matching called hadron . These six hadron titles can be merged to formlarger hadrons .For instance , the bottom quark bears a small amount of the bottom hadron s charge , so a bottom hadron is a weakly interacting particle containing a bottom quark . Within the standard description of particle theory , quarks are expected to be point nuclei , and the strong pull between them is characterized by quantum chromodynamics ( QCD ) .The present theory of the strong coupling , quantum chromodynamics ( QCD ) , successfully treats many features of hadrons , notably their spectra and decays . One important consequence of the QCD is that the up and down quarks will never appear alone but always appear in combinations , called hadrons .This fundamental interaction between quarks is dubbed confinement . According to the phenomenological quark model , the most important nuclei in the universe are the up , down and strange quarks and their corresponding hadrons , called hadrons .The strong pull that binds the quarks together makes hadrons . Hadronic matter is normal substance , made of hadrons .It is found throughout the universe in planets , stars and galaxies . However , in order to study it , nuclear physicists research particles that are either pieces of hadrons , such as alpha electrons and pi mesons , or full hadrons , such as protons and neutrons .These nuclei of hydrogen and helium are called ordinary matter . Quarks are also discovered inside other particles , but their nature is not well understood .Because they are so much more massive than the quarks that form up ordinary matter , these ocean quarks are mainly found inside hadrons , which link them together . The present theory for the strong coupling between them is dubbed quantum chromodynamics ( QCD ) , but despite several experimental tests of its predictions , its exact form continues a unknown .One forecast of QCD is that when heavy enough objects , such as aPb",
        "rewrite_text": "Quarks are thought to be elementary particles that interact via the strong force. In experiments, only two of the six quarks, namely up and down quarks, are found in stable objects like protons and neutrons within ordinary matter. The other four quarks, including strange, charm, up, and bottom quarks, exist in the form of hadrons, which are formed through the sole means of the strong force. The powerful attraction holds the quarks together within protons and neutrons in hadrons. The binding energy of a hadron is equivalent to its mass. Quarks are considered to be particles that behave like tiny magnets, with each variety having six distinct shades (representing the symmetry class SU(3) in quantum chromodynamics, or QCD).\n\nThese six shades of a quark are linked to the six components of a mathematical instrument called an octet. Each octet flavor possesses a unique proportion of charge and mass, which facilitates the designation of names such as up and down quarks (with a charge of 2/3 and a mass of 1/3). The remaining flavors, including strange, charm, up, and bottom quarks, each correspond to a matching hadron. These six hadron titles can be combined to form even larger hadrons. For instance, the bottom quark bears a small amount of the charge of bottom hadrons, making a bottom hadron a weakly interactive particle containing a bottom quark.\n\nIn the standard description of particle theory, quarks are expected to be point-like nuclei, with the strong force between them characterized by quantum chromodynamics (QCD). The current theory of strong coupling, quantum chromodynamics (QCD), successfully explains numerous features of hadrons, notably their spectra and decays. A crucial consequence of QCD is that the up and down quarks never appear alone but always in combinations called hadrons. This fundamental interaction among quarks is known as confinement.\n\nAccording to the phenomenological quark model, the most significant nuclei in the universe are the up, down, and strange quarks and their corresponding hadrons. The strong force that binds the quarks together forms hadrons. Hadronic matter, made of these hadrons, is found throughout the universe in planets, stars, and galaxies. To study it, nuclear physicists investigate particles that are either fragments of hadrons, such as alpha particles and pi mesons, or complete hadrons like protons and neutrons. These nuclei of hydrogen and helium are referred to as ordinary matter. Quarks have also been discovered within other particles, but their nature remains poorly understood.\n\nDue to their vastly greater mass compared to the quarks that compose ordinary matter, these heavy quarks are primarily found within hadrons that connect them together. The current theory for their strong coupling is referred to as quantum chromodynamics (QCD). Despite numerous experimental tests confirming its predictions, its exact form remains unknown. One prediction of QCD is that when objects reach a certain weightiness, such as lead (Pb), they will exhibit unique properties related to the strong force.",
        "ori-fast-z-score": 1.3269776053940743,
        "water-fast-z-score": 9.043241663265434,
        "rewrite-fast-z-score": 3.222516933177448
    },
    {
        "original_text": "A new unstable mode, the R-mode, is discovered in the linear analyses of Balbus & Hawley (1992) and Hameury, Menou, & Dubus (2013) for rapidly spinning neutron stars accretor around a black hole. These modes have very small characteristic frequency and can grow on the time scale of an accretion event. We present the results of the time-dependent numerical simulations of the unstable growth of the R-mode modes in a neutron star atmosphere around a black hole. We follow the dynamical evolution of the unstable modes during the nonlinear stage of their growth. We find that nonlinear effects significantly slow down the growth rate of the R-mode mode relative to the linear expectations. In particular, the growth rate becomes much smaller than the inverse accretion time, which is the characteristic time scale of the problem. We argue that the nonlinear development of the R-mode instability remains an active area of research and could have important implications for the evolution of neutron star spins and gamma-ray burst outflows.",
        "watermark_text": "A new unstable mode , the R - mode , is discovered in the linear study of Balbus & Hawley ( 1992 ) and Hameury , Menou , & Dubus ( 2013 ) for continuously spin neutron stars accretor around a black hole . These modes have very small peculiar frequency and can develop on the time scale of an accretion event .We present the results of the period - dependent computational simulations of the unstable growth of the R - mode modes in a neutron galaxy atmosphere around a black hole . We follow the dynamical development of the unstable modes during the nonlinear period of their development .We see that nonlinear effects significantly stop down the development time of the R - mode mode compared to the linear predictions . In particular , the development rate gets far lower than the inverse accretion time , which is the typical time scale of the question .We argue that the nonlinear development of the R - mode instability remains an active area of research and could have important implications for the evolution of neutron galaxy spins and gamma - ray burst outflows .",
        "rewrite_text": "In the linear studies conducted by Balbus & Hawley (1992) and Hameury, Menou, & Dubus (2013), a new unstable mode, the R-mode, has been discovered in continuously spinning neutron star accretors surrounding a black hole. These modes possess a very small characteristic frequency and can develop over the course of an accretion event. We present the results from period-dependent computational simulations that explore the unstable growth of R-modes in the atmosphere of a neutron galaxy orbiting a black hole. We monitor the dynamic evolution of these unstable modes during their nonlinear development phase.\n\nOur observations indicate that nonlinear effects significantly reduce the development time of the R-mode compared to linear predictions. Specifically, the development rate becomes much slower than the inverse accretion time, which is the typical timescale of the phenomenon. We argue that the nonlinear development of the R-mode instability remains an active area of research, which could have significant implications for the evolution of neutron galaxy spins and gamma-ray burst outflows.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 4.417261042993862,
        "rewrite-fast-z-score": 1.2074068598865937
    },
    {
        "original_text": "Astronomers have long known that the universe is full of small particles of radiation known as cosmic microwave background radiation. This radiation is of immense scientific value as it was the last light that was emitted at the Big Bang, and thus can be used to probe the state of the early universe. Recently, scientists have developed a model of how this background radiation may be generated by quantum mechanical effects within the universe. The model describes the universe as a sea of zero-point fluctuations, with particles taking on every imaginable (virtual) position in this sea. As the universe expands and its temperature drops, these virtual particles have an increased tendency to settle into the lowest-energy positions, leading to a background of virtual radiation. This model has one key flaw: it does not describe the universe as a collection of particles, but rather a collection of waves. This is analogous to modeling the ocean as a collection of sticks floating on top of a boiling pot of water. The waves are the waves of the ocean, and the sticks are the particles of the boiling pot of water. The particles of the model, the waves of the universe, have not been observed. In this model, the particles of the boiling pot of water are the photons of the cosmic microwave background radiation. As the universe cools, the photons have an increased tendency to assume the lowest-energy positions, that of equilibrium with the cold universe. When the temperature of the universe drops below 3000 K, the photons are no longer in equilibrium with the quantum fluctuations of the radiation, and thus begin to free-stream, moving away from regions of high density and toward regions of lower density. This is analogous to a water bottle left out in the cold. As the bottle cools, water molecules begin to move toward the bottom of the bottle, leaving a region of space with lower density than the surrounding air. In this work, we consider a modified model of this phenomenon, in which the particles (photons) do not move, but the background radiation does. The universe, in this case, has a  frozen sea  where the particles settle at the bottom of the sea but the radiation waves continue to wave throughout the universe. In this model, as the universe cools, the radiation waves continue to wave throughout the universe but the particles have settled at the bottom of the sea, leaving regions of higher and lower density. This provides a mechanism by which the universe can undergo a phase transition from a state of thermal equilibrium to a state of nonequilibrium, or a cooling dominated phase transition.",
        "watermark_text": "Astronomers have short understood that the universe is full of tiny particles of radiation known as cosmic microwave background radiation . This radiation is of immense scientific value as it was the last radiation that was generated at the Big Bang , and therefore can be used to probe the state of the early world .Recently , scientists have developed a theory of how this background radiation may be caused by quantum mechanical effects within the universe . The model describes the universe as a sea of zero - point fluctuations , with particles take on every imaginable ( virtual ) position in this sea .As the universe expands and its temperature falls , these virtual atoms have an increased ability to move into the smallest - energy places , leading to a background of virtual radiation . This theory has one key flaw : it does not represent the universe as a collection of atoms , but rather a collection of waves .This is analogous to modeling the ocean as a group of sticks floating on top of a boiling pot of water . The waves are the currents of the ocean , and the sticks are the molecules of the boiling pot of water .The particles of the model , the currents of the universe , have not been observed . In this model , the atoms of the boiling pot of water are the photons of the cosmic microwave background radiation .As the universe cools , the photons have an increased ability to assume the smallest - energy places , that of equilibrium with the cool world . When the temperature of the universe drops below 3000 K , the photons are no longer in equilibrium with the quantum fluctuations of the radiation , and therefore continue to free - stream , moving away from regions of high density and toward areas of lower density .This is analogous to a water container left out in the cool . As the container cools , water molecules begin to move toward the bottom of the container , left a region of space with lower volume than the nearby atmosphere .In this research , we imagine a altered model of this phenomenon , in which the molecules ( photons ) do not move , but the background radiation does . The universe , in this instance , has a frozen sea where the atoms settle at the bottom of the lake but the radiation waves continue to wave throughout the universe .In this model , as the universe cools , the radiation particles continue to wave throughout the universe but the particles have settled at the bottom of the oceans , left regions of lower and lesser concentration . This offers a process by which the universe can initiate a phase shift from a state of thermal equilibrium to a state of nonequilibrium , or a cooling dominated phase shift .",
        "rewrite_text": "Astronomers have long recognized that the universe is filled with minuscule particles of radiation known as cosmic microwave background radiation. This radiation holds immense scientific value as it represents the final radiation emitted during the Big Bang, thus enabling us to explore the early conditions of our world. Recently, scientists have formulated a theory on the potential quantum mechanical origins of this background radiation within the universe.\n\nThe theory models the universe as a vast sea of zero-point fluctuations, in which particles occupy every conceivable (virtual) position. As the universe expands and its temperature drops, these virtual atoms become increasingly capable of occupying the lowest-energy states, leading to a pervasive background of virtual radiation. However, this theory has a key limitation: it portrays the universe as a collection of waves rather than atoms. This analogy is like conceptualizing the ocean as a collection of sticks floating on top of a boiling pot of water, where the waves represent the ocean currents and the sticks are the molecules of the boiling water.\n\nIn this model, the particles, which represent the currents of the universe, have yet to be observed. Instead, the atoms of the boiling pot of water are represented by photons of the cosmic microwave background radiation. As the universe cools, photons increasingly occupy the lowest-energy states, achieving equilibrium with the cooling environment. When the temperature of the universe drops below 3000 K, photons no longer maintain equilibrium with the quantum fluctuations of radiation and continue to stream freely, moving away from regions of high density towards areas of lower density. This is akin to a container of water left to cool outside; as it does so, water molecules move towards the bottom of the container, creating a space with a lower volume than its surrounding atmosphere.\n\nIn our research, we propose an alternative model in which the molecules (photons) remain stationary while the background radiation itself moves. In this scenario, the universe is represented as a frozen sea in which atoms settle at its bottom while radiation waves continue to propagate throughout space. As the universe cools in this model, radiation particles continue to wave throughout space while their constituent particles settle at the bottom of a metaphorical \"ocean,\" creating regions of lower concentration. This process allows for a phase transition in the universe from a state of thermal equilibrium to a state of nonequilibrium or one dominated by cooling.",
        "ori-fast-z-score": 0.07018624063435965,
        "water-fast-z-score": 8.121624487362457,
        "rewrite-fast-z-score": 0.9850365626224086
    },
    {
        "original_text": "An autonomous distributed admission control scheme for IEEE 802.11 DCF is proposed, which can intelligently allocate the channel capacity for improving the utilization efficiency of the wireless medium and fully utilizing the capacity of the available bandwidth. The admission control scheme consists of a controlling mechanism and a controlled algorithm. The controlling mechanism divides the overall network into a number of virtual channels with a fixed size through assigning a range of channel access priorities. All stations in the network sense the wireless medium and calculate the corresponding transmission data rates. The controlled algorithm selects the stations for establishing connections in the controlling mechanism according to their network requirements and moves the whole network among different virtual channels according to the scheduling results. Simulation results show that the channel utilization efficiency can be enhanced effectively. The full text of this paper is available from http://arxiv.org/abs/1901.02605 In this paper, an autonomous distributed admission control scheme for IEEE 802.11 DCF is proposed, which can intelligently allocate the channel capacity for improving the utilization efficiency of the wireless medium and fully utilizing the capacity of the available bandwidth. Firstly, the overall network is divided into a number of virtual channels with a fixed size through assigning a range of channel access priorities. All stations in the network sense the wireless medium and calculate the corresponding transmission data rates. Secondly, the controlled algorithm selects the stations for establishing connections in the controlling mechanism according to their network requirements and moves the whole network among different virtual channels according to the scheduling results. Simulation results show that the channel utilization efficiency can be enhanced effectively. The full text of this paper is available from http://arxiv.org/abs/1901.02605 An autonomous distributed admission control scheme for IEEE 802.11 DCF is proposed Virtual channels with a fixed size are created to divide the overall network The controlled algorithm moves the whole network among different virtual channels according to the scheduling results Channel utilization efficiency can be enhanced effectively Reference: 1. Xuan Hong Nguyen, Viet Tien Nguyen, and H. T. Tuan,  An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF,  IET Networks, vol. 12, no. 7, July 2018, pp. 755-761. HT T T Nguyen, X H N Nguyen, and VT N Nguyen,  An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF,  IET Networks, vol. 12, no. 7, July 2018, pp. 755-761.",
        "watermark_text": "An autonomous spread entry control scheme for IEEE 802 . 11 DCF is proposed , which can intelligently allocate the channel capacity for improving the utilization efficiency of the wireless medium and fully utilizing the capacity of the provided bandwidth . The admission management scheme consists of a controlling mechanism and a controlled algorithm .The controlling mechanism separates the overall network into a number of virtual channels with a specified size through assigning a range of channel availability priorities . All stations in the channel sense the wireless medium and estimate the associated transmission data levels .The controlled algorithm selects the sites for acquiring links in the controlling mechanism based to their network demands and shifts the whole channel among different virtual channels according to the scheduling outcome . Simulation data demonstrate that the channel utilization efficiency can be enhanced effectively .The full text of this paper is accessible from http : / / arxiv . org / abs / 1901 . 02605 In this paper , an autonomous distributed entrance control scheme for IEEE 802 . 11 DCF is proposed , which can intelligently allocate the channel capacity for improving the utilization efficiency of the wireless medium and fully utilizing the capacity of the provided bandwidth . Firstly , the overall network is separated into a number of virtual channels with a specified size through assigning a range of channel availability priorities .All stations in the network sense the wireless medium and estimate the associated transmission data levels . Secondly , the controlled algorithm selects the sites for determining connections in the controlling mechanism based to their network specifications and shifts the whole channel among different virtual channels according to the scheduling outcome .Simulation conclusions show that the channel utilization efficiency can be enhanced effectively . The full text of this paper is accessible from http : / / arxiv . org / abs / 1901 . 02605 An autonomous distributed entrance control scheme for IEEE 802 . 11 DCF is proposed Virtual channels with a specified size are created to divide the overall network The managed algorithm moves the whole channel among different virtual channels according to the scheduling results Channel utilization efficiency can be enhanced effectively Reference : 1 .Xuan Hong Nguyen, Viet Tien Nguyen, and H. T. Tuan,  An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF,  IET Networks, vol.12, no.7 , July 2018 , pp . 755 - 761 .HT T T Nguyen, X H N Nguyen, and VT N Nguyen,  An Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF,  IET Networks, vol.12, no.7 , July 2018 , pp . 755 - 761 .",
        "rewrite_text": "An autonomous distributed entry control scheme has been proposed for the IEEE 802.11 DCF standard. This scheme can intelligently allocate channel capacity to enhance the utilization efficiency of the wireless medium and fully leverage the provided bandwidth's capacity.\n\nThe management system consists of a controlling mechanism and a controlled algorithm. Initially, the entire network is segmented into multiple virtual channels of a specified size, assigning various priorities to channel availability. All network stations sense the wireless medium and estimate associated transmission data levels.\n\nThe controlled algorithm operates to select sites for establishing connections within the controlling mechanism based on network demands. It also shifts the entire channel between different virtual channels based on scheduling outcomes. Simulation data demonstrates a significant improvement in channel utilization efficiency.\n\nThe complete text of this paper is accessible at http://arxiv.org/abs/1901.02605. Furthermore, this study refers to a previous work by Xuan Hong Nguyen, Viet Tien Nguyen, and H. T. Tuan, who presented an Autonomous Distributed Admission Control Scheme for IEEE 802.11 DCF in IET Networks, volume 12, issue 7, published in July 2018, pages 755-761.",
        "ori-fast-z-score": 2.917299829957891,
        "water-fast-z-score": 10.21054940485262,
        "rewrite-fast-z-score": 2.2691267417693455
    },
    {
        "original_text": "The evolution of stochastic SIR epidemics on random networks with heterogeneous connectivity is investigated. It is found that, on regular networks, the disease-free state is unstable if the average degree of the network is smaller than the threshold, and the disease will die out spontaneously if the average degree is larger than the threshold. When the network is heterogeneous, the disease-free state is still unstable if the network is regular; however, the disease can persist on the network with probability one if the average degree of the network is larger than a threshold. Moreover, for scale-free networks, the epidemic threshold is lowered as the edge distribution exponent γ increases. Network structure determines the stability of the disease-free state. If the average degree of the network is smaller than the threshold, the disease-free state is unstable; if the average degree is larger than the threshold, the disease-free state is stable, but the disease can still die out spontaneously if the network is regular. In addition, scale-free network leads to a larger epidemic threshold.",
        "watermark_text": "The evolution of stochastic SIR epidemics on random networks with heterogeneous connectivity is investigated . It is found that , on regular networks , the infection - free state is unstable if the average degree of the network is tiny than the threshold , and the infection will die out spontaneously if the average degree is bigger than the threshold .When the organization is heterogeneous , the infection - free state is nevertheless unstable if the network is normal ; however , the infection can persist on the network with probability one if the average degree of the network is bigger than a threshold . Moreover , for scale - free networks , the outbreak threshold is lowered as the edge distribution exponent β rises .Network structure determines the stability of the disease - free state . If the average degree of the organization is tiny than the threshold , the infection - free state is unstable ; if the average degree is bigger than the threshold , the infection - free state is stable , but the infection can also die out spontaneously if the organization is normal .In addition , size - free system leads to a greater epidemic limit .",
        "rewrite_text": "The investigation focuses on the progression of stochastic SIR epidemics on random networks with diverse connectivity. Results indicate that, on regular networks, the infection-free state becomes unstable when the average network degree is below a certain threshold, leading to the spontaneous extinction of the infection when the average degree surpasses the threshold. In the context of heterogeneous organizations, even a regular network's infection-free state remains unstable; however, with a network degree exceeding a threshold, the infection can persist with certainty. Furthermore, for scale-free networks, as the edge distribution exponent β increases, the outbreak threshold decreases. The structure of the network determines the stability of the disease-free state, whereby a low average degree compared to the threshold destabilizes the infection-free state, while a higher degree leads to stability but with a possibility of spontaneous infection termination if the organization is typical. Additionally, a size-free system contributes to a heightened epidemic limit.",
        "ori-fast-z-score": -3.9668163788998405,
        "water-fast-z-score": 2.156655464068768,
        "rewrite-fast-z-score": 0.6396021490668313
    },
    {
        "original_text": "This paper shows the existence of a pure inductive limit state for the pair of measures consisting of Lebesgue measure on the interval and the Dirac delta mass at the endpoint, in contrast to the trivial and well-known example of the pair of measures consisting of the counting and Lebesgue measures on the Integer, which has pure state but not inductive limit state. The proof makes use of unique path decomposition for generalized functions. It is shown that the unique path decomposition for the regularization of the Dirac delta mass is weakly isomorphic to the double sequential limit of the unique path decompositions for regularized distributions that appear as simple components in the generalized decomposition. It is further shown that this double sequential limit coincides with the desired pure inductive limit state. The Kolmogorov s property for this pure inductive limit state is then shown to follow from its uniqueness and general properties of inductive limit states. It is further argued that the existence of this pure state but not inductive limit state for the pair of measures described above has several potential applications in mathematics and theoretical physics. The paper presents the results of joint work with E. Posta and I. Damián M. Medina.",
        "watermark_text": "This paper shows the existence of a pure inductive limit state for the pair of measures composed of Lebesgue measure on the interval and the Dirac delta mass at the endpoint , in comparison to the trivial and well - famous example of the pair of measures composed of the counting and Lebesgue measures on the Integer , which has pure state but not inductive limit state . The proof makes using of unique path decomposition for generalized functions .It is demonstrated that the unique path decomposition for the regularization of the Dirac delta mass is weakly isomorphic to the double sequential limit of the distinct path decompositions for regularized distributions that appear as simple parts in the generalized decomposition . It is further demonstrated that this double sequential limit coincides with the desired pure inductive limit state .The Kolmogorov s property for this pure inductive limit state is then shown to follow from its uniqueness and general properties of inductive limit states . It is further argued that the existence of this pure state but not inductive limit state for the pair of measures described above has numerous future applications in math and theoretical physics .The paper offers the results of joint research with E . Posta and I . Damián M . Medina .",
        "rewrite_text": "The study presents the existence of a pure inductive limit state for a pair of measures, specifically the Lebesgue measure on an interval and the Dirac delta mass at an endpoint. This is contrasted with the common and well-known example of the pair of measures comprising the counting and Lebesgue measures on the Integer, which possesses a pure state but lacks an inductive limit state. The proof utilizes the unique path decomposition for generalized functions.\n\nIt is demonstrated that the unique path decomposition for regularizing the Dirac delta mass is weakly isomorphic to the double sequential limit of distinct path decompositions for regularized distributions that are part of the generalized decomposition. Furthermore, this double sequential limit is found to align with the desired pure inductive limit state. The paper then shows that the Kolmogorov's s property for this pure inductive limit state follows from its uniqueness and general properties of inductive limit states.\n\nMoreover, it is argued that the existence of this pure state without an inductive limit state for the aforementioned pair of measures holds significant potential for future applications in mathematics and theoretical physics. The research presented in this paper is a joint effort with E. Posta and I. Damián M. Medina.",
        "ori-fast-z-score": 1.58999682000954,
        "water-fast-z-score": 4.557990884027348,
        "rewrite-fast-z-score": 2.2013981571160284
    },
    {
        "original_text": "Researchers often wish to keep their research secret until they have obtained formal consent to share this information with the general public or until they have developed a proprietary product. While formal ethical committees evaluate the merits of proposed research, it is often not possible to obtain consent to research until a future date. In many cases, it is not feasible to develop a proprietary product without sharing initial research findings. It would be advantageous if a single measure could be used to weigh the merits of research in these two distinct contexts. A common definition of  merit  would allow for objective comparisons between research projects. In this paper, we describe a mathematical framework for comparing research projects based on defining a research program as a search in a solution space for the best solution. Our framework assigns a score that balances two competing concerns: the need to share information early on, so that failures can be corrected before distributing costly resources, and the need to obtain consent to distribute information in a more widespread manner. We demonstrate the feasibility of this framework with a number of examples.",
        "watermark_text": "Researchers often wish to keep their studies hidden until they have achieved formal consent to share this data with the general public or until they have developed a proprietary company . While established ethical committees evaluate the merits of suggested research , it is often not possible to obtain consent to study until a future period .In many situations , it is not practical to develop a proprietary company without sharing prior research conclusions . It would be advantageous if a single measure could be used to measure the merits of research in these two separate contexts .A common concept of merit might enable for objective comparisons between scientific projects . In this paper , we define a mathematical framework for comparing research projects based on recognizing a scientific plan as a search in a solution space for the best solution .Our paradigm assigns a score that balances two different issues : the necessity to share data early on , so that failures can be corrected before distributing costly materials , and the necessity to obtain consent to publish information in a more widespread manner . We suggest the feasibility of this framework with a number of instances .",
        "rewrite_text": "Researchers frequently desire to maintain the confidentiality of their studies until they have obtained formal permission to share their data with the general public or until they have established a proprietary company. While ethical committees rigorously evaluate the worth of proposed research, obtaining consent for studies can often be a challenging task, especially when it involves future periods. In many scenarios, developing a proprietary company without first sharing research findings is impractical. It would be beneficial if there was a unified metric to assess the merits of research in both these contexts. A common understanding of merit could enable objective comparisons between scientific projects.\n\nIn this paper, we introduce a mathematical framework to compare research projects based on the concept of a scientific plan as a search for the best solution within a solution space. Our approach assigns a score that balances two crucial factors: the need to share data early on to correct any failures before investing in costly materials, and the need to seek consent for wider dissemination of information. We demonstrate the feasibility of this framework through several practical examples.",
        "ori-fast-z-score": -0.30151134457776363,
        "water-fast-z-score": 7.135768488340406,
        "rewrite-fast-z-score": 0.6123724356957946
    },
    {
        "original_text": "The VLT-FLAMES survey of massive stars (VMSS) is aimed at studying the evolution of hot massive stars in the Local Group galaxy the Large Magellanic Cloud (LMC). Wind properties and evolution of hot massive stars are key to understand feedback from massive stars in their environment, from the morphological structure of the interstellar medium to the chemical evolution of their host galaxy. Combined high-resolution spectroscopy from VLT-FLAMES allows to determine mass-loss rates from H emission, temperature of the bi-conical winds and thus analyze the evolutionary status of these stars. From the mass-loss rates, Ionic masses can be derived and eventually be used to investigate other properties of the winds, such as ionization degrees and acceleration regions with potential effects on simulations of the feedback into the environment. In this contribution we present the first results on the evolution of hot massive stars in the LMC and the impact of this evolution on the galactic feedback. We confirm previous studies which concluded that the initial mass function (IMF) is top-heavy in the LMC, with respect to the Milky Way. The most massive stars show stronger IMF divergence from the initial mass function (IMF) with increasing age, suggesting that these stars were affected by significant mass-loss episodes. We found more extreme cases of stars with abnormally high nitrogen to carbon (N/C) and nitrogen to oxygen (N/O) abundance ratios, which might be the consequence of a thermohaline mixing process occurring in their winds, an hydrodynamic process taking place in the strong winds of massive stars.",
        "watermark_text": "The VLT - FLAMES analysis of large stars ( VMSS ) is aiming at studying the evolution of hot massive galaxies in the Local Group galaxy the Large Magellanic Cloud ( LMC ) . Wind characteristics and evolution of hotter large galaxies are important to explain feedback from massive galaxies in their environment , from the morphological composition of the interstellar medium to the chemical evolution of their target galaxy .Combined fast - resolution spectroscopy from VLT - FLAMES allows to estimate mass - loss rates from H emission , temperature of the bi - conical winds and therefore analyze the evolutionary status of these stars . From the mass - loss rates , Ionic masses can be derived and eventually be used to examine other properties of the winds , such as ionization degrees and acceleration regions with potential influence on simulations of the feedback into the environment .In this contribution we present the first findings on the evolution of bright large galaxies in the LMC and the impact of this evolution on the galactic feedback . We follow earlier analyses which showed that the initial mass function ( IMF ) is top - heavy in the LMC , with regard to the Milky Way .The most large galaxies show stronger IMF divergence from the first mass function ( IMF ) with advancing age , showing that these stars were impacted by significant mass - loss episodes . We identified more extreme cases of stars with abnormally high oxygen to carbon ( N / C ) and nitrogen to oxygen ( N / O ) abundance proportions , which would be the consequence of a thermohaline mixing process happening in their winds , an hydrodynamic reaction taking place in the strong winds of large galaxies .",
        "rewrite_text": "The VLT-FLAMES analysis of Large Stars (VMSS) aims to study the evolution of hot and massive galaxies within the Local Group's Large Magellanic Cloud (LMC). The characteristics of winds and the progression of larger, hotter galaxies are crucial to understanding the feedback from massive galaxies in their environment, ranging from the morphological composition of the interstellar medium to the chemical evolution of their target galaxy. The combined fast-resolution spectroscopy from VLT-FLAMES enables us to estimate mass-loss rates through H emission, determine the temperature of bi-conical winds, and thereby analyze the evolutionary status of these stars. Based on these mass-loss rates, we can derive Ionic masses, which can be used to explore other wind properties, such as ionization levels and acceleration regions, potentially influencing simulations of environmental feedback.\n\nIn this contribution, we present the initial findings on the evolution of bright, large galaxies in the LMC and the impact of this evolution on galactic feedback. We build on previous analyses that indicated a top-heavy initial mass function (IMF) in the LMC compared to the Milky Way. Larger galaxies exhibit a greater divergence from the initial mass function (IMF) with advancing age, suggesting that these stars have experienced significant mass-loss episodes. We have identified more extreme cases of stars with unusually high ratios of oxygen to carbon (N/C) and nitrogen to oxygen (N/O) abundance, which are indicative of a thermohaline mixing process occurring in their winds, an hydrodynamic reaction taking place in the strong winds of larger galaxies.",
        "ori-fast-z-score": -2.008316044185609,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 0.9284766908852594
    },
    {
        "original_text": "This paper investigates the performance of a downlink MIMO system with multiple co-channel and cross- channel transmit antennas. We consider a case where the base station (BS) has perfect channel state information (CSI) but the user equipment (UE) only has statistical CSI. We propose to use a combining technique at the UE, based on antenna switching, to exploit the benefit of using multiple antennas at the BS. We establish a simpleclosed-form expression for the achievable rate, and obtain the optimal BS beamformer that maximizes the achievable rate for a given UE combining vector. Numerical results show significant performance gains using our proposed techniques in comparison to using only UE CSI or only BS CSI. Authors: Liugong Cai, H. Vincent Shen, Ramesh Poovendam Date: July 30, 2017 Refereed Version: REF 20.1 Journal: IEEE Transactions on Communications https://arxiv.org/abs/1707.01473 Bibtex: @article{, author = {Liugong Cai and H. Vincent Shen and Ramesh Poovendam}, title = {{Antenna Combining for the MIMO Downlink Channel}}, journal = {IEEE Transactions on Communications}, year = {2017}, volume = {63}, number = {7}, pages = {1245--1258}, } ADDENDUM: For those who prefer PDF: Click here: https://transactions.johnhopfinger.com/files/20.1/CaiSV17.pdf This research was supported in part by the U.S. National Science Foundation (NSF) grants CCF-1526513, CCF-1533888, and CNS-1626008, the U.S. Defense Advanced Research Projects Agency (DARPA) grant W911NF-16-C-0692, and the NSFC/RGC Joint Research Fund (NSFC grant 61461062)",
        "watermark_text": "This paper investigates the performance of a downlink MIMO network with many co - channel and cross - channel transmit antennas . We consider a situation where the base station ( BS ) has ideal network state information ( CSI ) but the user facility ( UE ) only has statistical CSI .We suggest to use a combining method at the UE , based on transmitter switching , to harness the advantages of using multiple antennas at the BS . We establish a simpleclosed - form expression for the achievable rate , and establish the ideal BS beamformer that maximizes the achievable rate for a given UE combining matrix .Numerical results show considerable performance increases utilizing our proposed methods in comparison to use only UE CSI or only BS CSI . Authors : Liugong Cai , H . Vincent Shen , Ramesh Poovendam Date : July 30 , 2017 Refereed Version : REF 20 . 1 Journal : IEEE Transactions on Communications https : / / arxiv . org / abs / 1707 . 01473 Bibtex : @ article { , author = { Liugong Cai and H . Vincent Shen and Ramesh Poovendam } , title = { { Antenna Combining for the MIMO Downlink Channel } } , journal = { IEEE Transactions on Communications } , year = { 2017 } , volume = { 63 } , number = { 7 } , pages = { 1245 - - 1258 } , } ADDENDUM : For those who prefer PDF : Click here : https : / / transactions . johnhopfinger . com / files / 20 . 1 / CaiSV17 . pdf This research was supported in part by the U . S . National Science Foundation ( NSF ) grants CCF - 1526513 , CCF - 1533888 , and CNS - 1626008 , the U . S . Defense Advanced Research Projects Agency ( DARPA ) funding W911NF - 16 - C - 0692 , and the NSFC / RGC Joint Research Fund ( NSFC award 61461062 )",
        "rewrite_text": "This study examines the performance of a downlink MIMO network that incorporates multiple co-channel and cross-channel transmit antennas. We consider a scenario where the base station (BS) possesses ideal network state information (CSI), while the user equipment (UE) only has statistical CSI. We propose the utilization of a combining method at the UE, based on transmitter switching, to harness the advantages of employing multiple antennas at the BS. We establish a straightforward closed-form expression to determine the achievable rate and identify the ideal BS beamformer that maximizes the achievable rate for a specific UE combining matrix.\n\nNumerical results demonstrate significant performance improvements when utilizing our proposed methods in comparison to relying solely on UE CSI or BS CSI.\n\nAuthors: Liugong Cai, H. Vincent Shen, Ramesh Poovendam. Date: July 30, 2017. Refereed Version: REF 20.1. Journal: IEEE Transactions on Communications. Available at: https://arxiv.org/abs/1707.01473\n\nBibtex entry:\n\n@article{Cai2017,\nauthor = {Liugong Cai and H. Vincent Shen and Ramesh Poovendam},\ntitle = {Antenna Combining for the MIMO Downlink Channel},\njournal = {IEEE Transactions on Communications},\nyear = {2017},\nvolume = {63},\nnumber = {7},\npages = {1245--1258}\n}\n\nADDENDUM: For those who prefer the PDF version, click here: https://transactions.johnhopfinger.com/files/20.1/CaiSV17.pdf\n\nThis research was partially supported by grants from the U.S. National Science Foundation (NSF) including CCF-1526513, CCF-1533888, and CNS-1626008. Funding was also provided by the U.S. Defense Advanced Research Projects Agency (DARPA) under award W911NF-16-C-0692, and the NSFC/RGC Joint Research Fund with NSFC award number 61461062.",
        "ori-fast-z-score": 0.47140452079103173,
        "water-fast-z-score": 5.976143046671968,
        "rewrite-fast-z-score": 3.0550504633038935
    },
    {
        "original_text": "The classical distance scale to galaxies is based on the Cepheid variable stars in the Milky Way and in host galaxies with accurate distances measured via trigonometric parallax. In order to calibrate this relation, it is useful to have methods that measure the distances to galaxies outside of our own. In this Letter we present an alternative method based on the properties of stars in the Large Magellanic Cloud (LMC). The LMC is our closest satellite galaxy and is a excellent calibration point as its distance is well established via measurement of its Hubble flow velocity. We use the fact that stars in the LMC are mostly red clump stars. The properties of red clump stars are well understood and can be used to measure their distances via multi-variable polynomial fits. We find the median distance to the LMC measured via this method is 18.1 kpc, with an estimated observational uncertainty of 0.4 kpc. We measure the distance to five populous star clusters in the LMC and apply the same technique to these clusters and find they are also around 18 kpc with a small range in distance of 0.1 kpc. We conclude that the precision of this method for determining the distance to the LMC is 0.4 kpc.",
        "watermark_text": "The classical distance scale to galaxies is based on the Cepheid variable stars in the Milky Way and in host galaxies with correct distances measured via trigonometric parallax . In order to calibrate this relation , it is important to have methods that measure the distances to galaxies outside of our own .In this Letter we present an different method using on the properties of stars in the Large Magellanic Cloud ( LMC ) . The LMC is our closest satellite galaxy and is a perfect calibration point as its radius is well established via measurement of its Hubble stream velocity .We use the fact that stars in the LMC are mostly red clump stars . The properties of red clump stars are better understood and can be used to measure their distances via multi - variable polynomial fits .We get the median distance to the LMC calculated via this algorithm is 18 . 1 kpc , with an estimated observational uncertainty of 0 . 4 kpc . We estimate the distance to five populous star clusters in the LMC and use the same technique to these complexes and find they are also around 18 kpc with a small range in distance of 0 . 1 kpc .We suggest that the precision of this algorithm for determining the distance to the LMC is 0 . 4 kpc .",
        "rewrite_text": "The classical scale for measuring distances to galaxies relies on Cepheid variable stars within the Milky Way and host galaxies, with accurate distances determined through trigonometric parallax. To calibrate this relationship, methods that can measure distances to galaxies beyond our own are crucial. In this letter, we introduce an alternative approach utilizing the properties of stars in the Large Magellanic Cloud (LMC).\n\nThe LMC, our nearest satellite galaxy, serves as an excellent calibration point due to its well-established radius through measurements of its Hubble stream velocity. We leverage the fact that the majority of stars in the LMC are red clump stars. The properties of red clump stars are well understood and can be used to estimate their distances through multi-variable polynomial fits. Using this algorithm, we calculate a median distance to the LMC of 18.1 kpc, with an estimated observational uncertainty of 0.4 kpc.\n\nWe have estimated the distances to five populous star clusters within the LMC and applied the same technique to these clusters, finding that they are also approximately 18 kpc away, with a small range in distance of 0.1 kpc. We propose that the precision of this algorithm for determining the distance to the LMC is within 0.4 kpc.",
        "ori-fast-z-score": 2.3626845919446504,
        "water-fast-z-score": 5.737948294722722,
        "rewrite-fast-z-score": 2.7441064997422586
    },
    {
        "original_text": "NGC 5033 is a Sy 1.5 galaxy, located in the Fornax Cluster at a distance of 22.4 Mpc. It has a massive active galactic nucleus (AGN), which is also the power source of the observed broad optical and ultraviolet lines and strong X-ray emission. Continuum emission at centimeter wavelengths from this source has been little studied. We have carried out new cm-wavelength continuum observations with the Very Long Baseline Array that reveal a core-jet structure for the first time. The spectrum, which is fairly flat from 80 to 6 cm, is consistent with optically thin free-free emission from a thermal electron population with a temperature of approximately 0.2 keV. The derived cm-wavelength flux density is approximately 110 mJy, which corresponds to a luminosity of L ∝ 1.2 × 10 31 W. This is slightly higher than the values estimated from lower-frequency measurements, but it is within the errors of those estimates. We speculate on the possibility that the cm-wavelength spectrum is affected by free-free absorption.",
        "watermark_text": "NGC 5033 is a Sy 1 . 5 galaxy , located in the Fornax Cluster at a distance of 22 . 4 Mpc . It has a huge active galactic nucleus ( AGN ) , which is also the power source of the seen wide optical and ultraviolet lines and strong X - ray radiation .Continuum emission at centimeter wavelengths from this source has been less researched . We have carried out novel mm - wavelength continuum measurements with the Very Long Baseline Array that discover a core - jet shape for the first time .The spectrum , which is fairly flat from 80 to 6 cm , is consistent with optically thin free - free emission from a heat electron population with a temperature of approximately 0 . 2 keV . The derived mm - wavelength flux concentration is approximately 110 mJy , which equals to a luminosity of L [UNK] 1 . 2 × 10 31 W . This is significantly greater than the values expected from lower - frequency observations , but it is within the errors of those calculations .We speculate on the suggestion that the cm - wavelength spectrum is affected by free - free absorption .",
        "rewrite_text": "NGC 5033 is a Sy 1.5 galaxy situated in the Fornax Cluster at a distance of 22.4 million parsecs. It harbors a large active galactic nucleus (AGN) that powers the visible wide optical and ultraviolet lines, as well as intense X-ray radiation. Studies on continuum emission at centimeter wavelengths from this source have been limited.\n\nUtilizing the Very Long Baseline Array, we have conducted pioneering mm-wavelength continuum measurements, which have revealed a core-jet structure for the first time. The spectrum, which remains relatively flat between 80 and 6 cm, is consistent with free-free emission from a heat electron population with a temperature of approximately 0.2 keV. The derived mm-wavelength flux concentration is approximately 110 mJy, which corresponds to a luminosity of approximately 1.2 x 10^31 W. This luminosity surpasses values anticipated from lower-frequency observations, but it falls within the error margins of those calculations. We speculate that the cm-wavelength spectrum may be affected by free-free absorption.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.695048270344999,
        "rewrite-fast-z-score": 1.212678125181665
    },
    {
        "original_text": "A population of metal-rich droplets, analogous to the Sun’s hydrogen-rich solar wind, is proposed to enrich the interstellar medium (ISM). These droplets form through the capture of gaseous metal atoms by broken dust grains. We perform one-dimensional hydrodynamical simulations of this process and find that, as the gas is enriched, the droplets grow in mass and radius. We also perform the first statistical analysis of the mass capture efficiency. The rate of droplet growth depends on the local abundance of their metals, making it a self-regulating mechanism for the ISM enrichment. Using observational data on the  C II {} (http://decat.cx/search?q=C%26+II) intensity in our Galaxy and in the Solar vicinity we show that the obtained growth tracks for droplets can match the data. This process could play a significant role in enriching the ISM and making up the “metallicity problem” - the discrepancy between the metal abundances measured in stars vs. that expected from early nucleosynthesis theory in the “standard” model of the Galaxy. Using an analytical approximation we calculate the volume filling factor of droplets formed through this process in the Galaxy and find that it can explain the high observed  C II {} (http://decat.cx/search?q=C%26+II) volume filling factor.",
        "watermark_text": "A community of metal - rich droplets , analogous to the Sun ’ s hydrogen - laden solar cloud , is proposed to enrich the interstellar medium ( ISM ) . These droplets form through the capture of gaseous metal atoms by broken dust grains .We work one - dimensional hydrodynamical simulations of this process and find that , as the gas is enriched , the droplets grow in mass and radius . We also perform the first statistical analysis of the mass capture capacity .The rate of droplet growth depends on the local concentration of their metals , making it a self - controlling mechanism for the ISM enrichment . Using observational data on the C II { } ( http : / / decat . cx / search ? q = C % 26 + II ) activity in our Galaxy and in the Solar vicinity we prove that the extracted development tracks for droplets can match the information .This process may play a substantial importance in enriching the ISM and making up the “ metallicity issue ” - the discrepancy between the metal abundances calculated in galaxies vs . that expected from early nucleosynthesis theory in the “ standard ” model of the Galaxy . Using an analytical analogy we determine the volume filling factor of droplets produced through this process in the Galaxy and find that it can describe the high observed C II { } ( http : / / decat . cx / find ? q = C % 26 + II ) volume filling factor .",
        "rewrite_text": "A proposed community of metal-rich droplets, akin to the Sun's hydrogen-rich solar cloud, is envisioned to enrich the interstellar medium (ISM). These droplets are formed by the capture of gaseous metal atoms onto broken dust grains. We have conducted one-dimensional hydrodynamic simulations of this process and discovered that as the gas becomes enriched, the droplets grow in both mass and radius. Furthermore, we have conducted the initial statistical analysis of the droplets' mass capture capacity. The rate of droplet growth is dependent on the local concentration of metals, creating a self-regulating mechanism for ISM enrichment.\n\nBy utilizing observational data on C II activity within our Galaxy and in the vicinity of the Sun, we have verified that the predicted trajectories of droplet development align with the available information. This process may play a significant role in enriching the ISM and resolving the \"metallicity issue,\" which is the discrepancy between calculated metal abundances in galaxies and those expected from early nucleosynthesis theory in the \"standard\" model of the Galaxy. Through an analytical comparison, we have determined the volume filling factor of droplets produced through this process in the Galaxy and found that it can explain the observed high C II volume filling factor.",
        "ori-fast-z-score": 0.7337993857053429,
        "water-fast-z-score": 5.136595699937399,
        "rewrite-fast-z-score": 1.4269353798659745
    },
    {
        "original_text": "Poincare duality pairs in dimension three In three dimensions, the Poincaré duality pairs between an oriented closed manifold M and its mirror M dual. Namely, the both manifolds have the same homology groups, and their Betti numbers (also called Stiefel-Whitney classes) are equal. Moreover, the dimension of the cohomology groups are also the same, and they are dual to each other with respect to the cup product. This theorem has several nice applications, for example, it allows one to construct pairs of topologically distinct manifolds that have the same set of differentiable properties, and therefore must be similar inside the realm of mathematics. Here is the reference for the statement: http://en.wikipedia.org/wiki/Poincare_duality_pair I hope you find this interesting. Qing Chu March 12, 2014 Citation: Qing Chu. (2014). Poincaré duality pairs in dimension three. http://arxiv.org/abs/1403.2259 Abstract: In three dimensions, the Poincaré duality pairs between an oriented closed manifold M and its mirror M dual. Namely, the both manifolds have the same homology groups, and their Betti numbers (also called Stiefel-Whitney classes) are equal. Moreover, the dimension of the cohomology groups are also the same, and they are dual to each other with respect to the cup product. This theorem has several nice applications, for example, it allows one to construct pairs of topologically distinct manifolds that have the same set of differentiable properties, and therefore must be similar inside the realm of mathematics. Here is the reference for the statement: http://en.wikipedia.org/wiki/Poincare_duality_pair I hope you find this interesting. Qing Chu March 12, 2014 Language: English Type: Article Status: Cannonfodder Ljubljana, Slovenia Editor: Dragan Bošnjak Phone: +386 1 415 63 64 E-mail: bojan.dragan@gmail.com Web: http://home.comcast.net/~bojanadrag/ http://www.researchgate.net/profile/Bojan_Dragan/ NICTA, Australian National University Funding: ANU College of Physical and Mathematical Sciences Start-up funding for early-career researchers NICTA, Australian National University Funding: ANU College of Physical and Mathematical Sciences Start-up funding for early-career researchers NICTA, Australian National University Funding: ANU College of Physical and Mathematical Sciences Start-up funding for",
        "watermark_text": "Poincare duality pairs in dimension three In three dimensions , the Poincaré duality pairs between an oriented closed surface M and its mirror M dual . Namely , the both manifolds have the same homology groups , and their Betti numbers ( commonly called Stiefel - Whitney classes ) are equal .Moreover , the dimension of the cohomology groups are also the same , and they are dual to each other with regard to the cup product . This formula has numerous nice applications , for example , it allows one to build pairs of topologically unique manifolds that have the same set of differentiable properties , and therefore needs be similar inside the domain of geometry .Here is the reference for the statement : www : / / en . wikipedia . org / wiki / Poincare _ duality _ pair I pray you find this interesting . Qing Chu March 12 , 2014 Citation : Qing Chu .( 2014 ) . Poincaré duality pairs in dimension three .http : / / arxiv . org / abs / 1403 . 2259 Abstract : In three dimensions , the Poincaré duality pairs between an oriented closed surface M and its mirror M dual . Namely , the both manifolds have the same homology groups , and their Betti numbers ( also known Stiefel - Whitney classes ) are equal .Moreover , the dimension of the cohomology groups are also the same , and they are dual to each other with regard to the cup product . This formula has numerous nice applications , for example , it allows one to build pairs of topologically unique manifolds that have the same set of differentiable properties , and therefore needs be similar inside the domain of geometry .Here is the reference for the comment : www : / / en . wikipedia . org / wiki / Poincare _ duality _ couple I hope you find this interesting . Qing Chu March 12 , 2014 Language : English Type : Article Status : Cannonfodder Ljubljana , Slovenia Editor : Dragan Bošnjak Phone : + 386 1 415 63 64 E - mail : bojan . dragan @ gmail . com Web : www : / / home . comcast . net / ~ bojanadrag / http : / / www . researchgate . net / profile / Bojan _ Dragan / NICTA , Australian National University Funding : ANU College of Physical and Mathematical Sciences Start - up grants for late - career researchers NICTA , Australian National University Funding : ANU College of Physical and Mathematical Sciences Start - up grants for late - career researchers NICTA , Australian National University Funding : ANU College of Physical and Mathematical Sciences Start - up grants for",
        "rewrite_text": "在三维空间中的庞加莱对偶性配对对\n\n在三维空间中，一个定向闭曲面M与其镜像M双之间存在庞加莱对偶性配对。具体而言，这两个流形具有相同的同调群，它们的Betti数（也称为斯蒂弗尔-惠特尼类）相等。此外，上同调群的维度也相同，它们关于杯积是相互对偶的。这个公式有许多很好的应用，例如，它允许我们构建具有相同可微分属性的拓扑唯一流形对，因此在几何领域内它们需要相似。\n\n此声明的参考链接为：https://en.wikipedia.org/wiki/Poincaré_duality_pair\n\n我希望您会发现这很有趣。\n\n清楚，2014年3月12日\n\n参考文献：\n清楚.（2014年）。三维空间中的庞加莱对偶性配对。https://arxiv.org/abs/1403.2259\n\n摘要：在三维空间中，一个定向闭曲面M及其镜像M双之间存在庞加莱对偶性配对。同样地，这两个流形的同调群相同，它们的Betti数（也称为斯蒂弗尔-惠特尼类）相等。此外，上同调群的维度相同，并且它们关于杯积是相互对偶的。这个公式有许多有用的应用，例如，它可以用于构建具有相同可微分属性的拓扑唯一流形对，因此它们在几何领域内是相似的。\n\n注释的参考链接为：https://en.wikipedia.org/wiki/Poincaré_duality_couple\n\n希望您会喜欢这个信息。\n\n清楚，2014年3月12日\n\n语言：英语\n类型：文章\n状态：待审稿\n\n卢布尔雅那，斯洛文尼亚\n编辑：德拉甘·博什尼亚克\n电话：+ 386 1 415 63 64\n电子邮件：[bojan.dragan@gmail.com](mailto:bojan.dragan@gmail.com)\n网页：https://home.comcast.net/~bojanadrag/\nhttp://www.researchgate.net/profile/Bojan_Dragan/\n澳大利亚国立大学NICTA基金资助：澳大利亚国立大学物理与数学科学学院启动资金资助晚期研究人员\nNICTA基金资助：澳大利亚国立大学物理与数学科学学院启动资金资助晚期研究人员\nNICTA基金资助：物理与数学科学学院启动资金项目资助后期职业研究者",
        "ori-fast-z-score": -1.0864289525102224,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 0.7302967433402214
    },
    {
        "original_text": "Organic conductors are unique systems for studying criticality and quantum phase transitions due to the unusual nature of the charge, spin, and orbital degrees of freedom that are involved. Here we report observations of critical scattering and scaling behavior in quasi one-dimensional conductors based on the Bechgaard and Fabre salts. We find that the charge degrees of freedom are critical over the full range of temperatures, with dynamical critical exponent z=2. We present evidence for logarithmic corrections to scaling, and estimate the central charge c=1. The spin and orbital degrees of freedom are found to be either Heisenberg or Kugel-Khomskii models, with spin-orbital separation clearly evident in the single crystal samples. Although there is some evidence for quantum critical points in the system, our data are consistent with the spin and orbital degrees of freedom being separate chains coupled by a uniform Heisenberg coupling with an effective lower critical interaction strength. This material can be viewed as a testing ground for interesting theoretical questions related to quantum many-body effects and the limits of criticality.",
        "watermark_text": "Organic conductors are remarkable systems for studying criticality and quantum phase transitions due to the unusual nature of the charge , spin , and orbital degrees of liberty that are implicated . Here we publish observations of critical scattering and scaling behavior in quasi one - dimensional conductors based on the Bechgaard and Fabre salts .We see that the charge degrees of liberty are critical over the full range of temperatures , with dynamical important exponent z = 2 . We present evidence for logarithmic corrections to scaling , and estimate the main charge c = 1 .The spin and orbital degrees of liberty are found to be either Heisenberg or Kugel - Khomskii configurations , with spin - orbital separation well evident in the single crystal samples . Although there is some evidence for quantum vital points in the system , our statistics are compatible with the spin and orbital degrees of liberty being separate bars coupled by a regular Heisenberg correlation with an efficient lower critical interaction strength .This material can be viewed as a testing ground for interesting theory issues related to quantum several - bodies phenomena and the boundaries of criticality .",
        "rewrite_text": "Organic conductors serve as remarkable systems for investigating criticality and quantum phase transitions due to their unique properties of charge, spin, and orbital degrees of freedom. In this study, we present observations of critical scattering and scaling behavior in quasi-one-dimensional conductors derived from Bechgaard and Fabre salts. Our findings indicate that the charge degrees of freedom are critical across the entire temperature range, with a dynamically significant exponent of z = 2. We provide evidence for logarithmic scaling corrections and estimate the primary charge to be c = 1. Additionally, we discover that the spin and orbital degrees of freedom exhibit either Heisenberg or Kugel-Khomskii configurations, with a clear spin-orbital separation evident in single crystal samples. While there is some evidence of quantum critical points in the system, our data suggest that the spin and orbital degrees of freedom are separate entities coupled by a regular Heisenberg correlation with a relatively low critical interaction strength. This material can be considered a valuable testing ground for intriguing theoretical issues related to quantum many-body phenomena and the boundaries of criticality.",
        "ori-fast-z-score": -0.52999894000318,
        "water-fast-z-score": 4.7699904600286205,
        "rewrite-fast-z-score": 0.6255432421712244
    },
    {
        "original_text": "Organic conductors are a recently discovered new class of materials that exhibit high-temperature superconductivity and other important physical phenomena1,2. These materials are composed of quasi-two-dimensional molecules with strong spin-orbit coupling, resulting in a small spin wavelength and remarkable magnetic properties3,4. Recent neutron scattering experiments5,6 on the quasi-two-dimensional organic superconductor α-T-NbSe2 has revealed a low-energy magnetic spectrum that is best characterized as fluctuating antiferromagnetism. This finding was surprising given the observed low temperatures and high transition temperatures for superconductivity and antiferromagnetism, respectively. Here, we perform a phenomenological analysis of the magnetic excitations in α-T-NbSe2 based on an anisotropic square-lattice quantum Heisenberg model with intrinsic dimerization and feedback from the magnetic order parameter. We find that the magnetic excitation spectrum can be reconciled with experiment without the need for immiscible dimer and antiferromagnetic orders. The measured magnetic spectral function is a consequence of superposition of lightly damped dynamic fluctuations of incipient magnetic order with a relatively large, roughly temperature-independent coupling to the vector antiferromagnetic order parameter. These results elucidate the intriguing relationship between the coexisting antiferromagnetism and superconductivity in these materials, and suggest that fluctuation exchange may play a significant role in their properties7,8.",
        "watermark_text": "Organic conductors are a recently discovered new category of structures that exhibit high - temperature superconductivity and other vital physical phenomena1 , 2 . These substances are composed of quasi - two - dimensional molecules with powerful spin - orbit interaction , resulting in a small spin wavelength and remarkable magnetic properties3 , 4 .Recent neutron scattering experiments5 , 6 on the quasi - two - dimensional chemical superconductor α - T - NbSe2 has confirmed a reduced - energy magnetic spectrum that is better defined as fluctuating antiferromagnetism . This finding was remarkable considering the reported low temperatures and low transition conditions for superconductivity and antiferromagnetism , respectively .Here , we perform a phenomenological evaluation of the magnetic excitations in α - T - NbSe2 relying on an anisotropic square - lattice quantum Heisenberg description with inherent dimerization and feedback from the magnetic order parameter . We see that the magnetic excitation spectrum can be reconciled with research without the necessity for immiscible dimer and antiferromagnetic orders .The measured magnetic spectral constant is a outcome of superposition of lightly damped dynamic fluctuations of incipient magnetic order with a fairly large , roughly temperature - based correlation to the vector antiferromagnetic order parameter . These data elucidate the intriguing correlation between the coexisting antiferromagnetism and superconductivity in these materials , and suggest that fluctuation exchange could play a substantial importance in their properties7 , 8 .",
        "rewrite_text": "Organic conductors have recently emerged as a novel class of structures that exhibit high-temperature superconductivity and other crucial physical phenomena. These substances are composed of quasi-two-dimensional molecules with strong spin-orbit interactions, resulting in a small spin wavelength and remarkable magnetic properties. Recent neutron scattering experiments on the quasi-two-dimensional chemical superconductor α-T-NbSe2 have confirmed a reduced-energy magnetic spectrum characterized by fluctuating antiferromagnetism. This finding is remarkable given the reported low temperatures and transition conditions for superconductivity and antiferromagnetism, respectively.\n\nIn this study, we perform a phenomenological evaluation of the magnetic excitations in α-T-NbSe2 based on an anisotropic square-lattice quantum Heisenberg description, incorporating inherent dimerization and feedback from the magnetic order parameter. Our findings indicate that the magnetic excitation spectrum can be harmonized with research without the need for incompatible dimer and antiferromagnetic orders. The measured magnetic spectral constant is a result of the superposition of slightly damped dynamic fluctuations of the emerging magnetic order, with a notable, roughly temperature-dependent correlation to the vector antiferromagnetic order parameter.\n\nThese data elucidate the fascinating interplay between coexisting antiferromagnetism and superconductivity in these materials, suggesting that fluctuation exchange may play a significant role in their properties. This research provides further insights into the unique properties and interactions of organic conductors, opening new avenues for future investigations into this emerging field.",
        "ori-fast-z-score": -1.3480372031495529,
        "water-fast-z-score": 5.30722777603022,
        "rewrite-fast-z-score": 1.4237369936287485
    },
    {
        "original_text": "Lyman Break Galaxies (LBGs) are highly star-forming galaxies at high redshifts. They can be identified through their Lyman-break dropout signature in the spectrum and are observed to exist up to z ~ 6.7. We have obtained spectra of three Lyman break galaxies at z = 5.7–5.9 using the LRIS instrument on the Keck telescope. These data are used to measure rest-frame UV spectroscopy through visible wavelengths for these galaxies, complementing recent studies in the near-infrared (NIR). These observations have considerable implications for galaxy formation and reionization, as the Lyman continuum radiation from massive young stars is responsible for driving the return of UV radiation with redshift. These observations indicate that the epoch of galaxy formation was considerably delayed with respect to the NIR spectroscopy, perhaps to as late as z ~ 5.7–5.9, corresponding to a period of only 250–500 Myr after the Big Bang. It is also likely that the escape of ionizing radiation was limited by the harder ionizing radiation from young stars at these early times. The observed absorption lines in the UV spectra of these galaxies provide important clues to the physical conditions in these young galaxies, and comparison with high-redshift galaxies with later-generation telescopes such as HST and Spitzer will allow us to study how the escape of ionizing radiation was altered as these sources passed through the period of reionization.",
        "watermark_text": "Lyman Break Galaxies ( LBGs ) are extremely star - creating stars at high redshifts . They can be identified through their Lyman - break dropout signature in the spectrum and are observed to form up to z ~ 6 . 7 .We have discovered spectra of three Lyman break galaxies at z = 5 . 7 – 5 . 9 utilizing the LRIS instrument on the Keck telescope . These data are using to measure rest - frame UV spectroscopy through visible wavelengths for these galaxies , complementing latest studies in the near - infrared ( NIR ) .These measurements have considerable consequences for galaxy formation and reionization , as the Lyman continuum emission from massive young galaxies is responsible for driving the return of UV rays with redshift . These measurements indicate that the epoch of galaxy formation was considerably postponed with regard to the NIR spectroscopy , perhaps to as early as z ~ 5 . 7 – 5 . 9 , corresponding to a period of only 250 – 500 Myr after the Big Bang .It is also probable that the escape of ionizing radiation was limited by the faster ionizing radiation from young stars at these first years . The observed emission lines in the UV spectra of these galaxies offer important hints to the physical conditions in these young galaxies , and contrast with high - redshift galaxies with later - generation telescopes such as HST and Spitzer will provide us to study how the escape of ionizing radiation was modified as these sources passed through the period of reionization .",
        "rewrite_text": "Lyman Break Galaxies (LBGs) are galactic powerhouses of star formation at high redshifts. They can be distinguished by their Lyman-break dropout hallmark in the spectrum, observed to reach up to z ~ 6.7. Through the utilization of the LRIS instrument on the Keck telescope, we have obtained spectra of three Lyman break galaxies at z ranging from 5.7 to 5.9. These data are utilized to measure rest-frame UV spectroscopy across visible wavelengths for these galaxies, complementing ongoing studies in the near-infrared (NIR).\n\nThese measurements hold significant implications for galaxy formation and reionization processes. The Lyman continuum emission from these massive, young galaxies is instrumental in driving the reappearance of UV rays with redshift. Our findings suggest that the era of galaxy formation was significantly delayed compared to NIR spectroscopy, possibly dating back to z ~ 5.7 - 5.9, which corresponds to a period of only 250 - 500 million years after the Big Bang.\n\nAdditionally, it is likely that the escape of ionizing radiation was constrained by the rapidly ionizing radiation emitted by young stars during these early years. The observed emission lines in the UV spectra of these galaxies provide crucial insights into the physical conditions within these young galaxies. In contrast, high-redshift galaxies will be studied with later-generation telescopes like HST and Spitzer, revealing how the escape of ionizing radiation changed as these sources passed through the reionization period.",
        "ori-fast-z-score": -0.3110855084191276,
        "water-fast-z-score": 5.703234321017339,
        "rewrite-fast-z-score": 1.7820842224272613
    },
    {
        "original_text": "We present the first high-accuracy, high-reliability effective-one-body (EOB) binary waveforms for nonspinning, equal-mass, black-hole (BH) coalescences with mass ratios as low as q = 0.2. Such waveforms, which are restricted to the small mass-ratio limit of general relativity, are essential for the rigorous and accurate computation of emitted radiation and the estimation of Source Terms in computational EM and GW astronomy. We demonstrate that, with a single adjustment for the mass ratio, our waveform family, dubbed DECOULper waveform approximates the analytic solution of the effective one body (EOB) equations to < 10^{-4} over the full spectrum of two compact object systems. We show that our DECOULper waveforms, which are computed in close parallel with the effective one body (EOB) equations, are well-suited for use in hybrid waveforms, where radiation from a numerical relativity simulation is infused with the DECOULper waveform at key stages of the evolution. We demonstrate the efficacy of our waveforms using several approaches. First, we show that our waveforms predict the radiation in tests in which we waveform a single, isolated BH. Next, we show that when using high-precision numerical-relativity waveforms for binary black-hole mergers as an initial condition, our DECOULper waveforms accurately predict the radiation produced in the subsequent dynamical evolution. Lastly, we show that our waveforms can be easily and efficiently used within the <span style= font-variant:small-caps; >Grandchallenge</span> surrogate modeling approach, where they provide accurate Source Terms for radiation from BH-BH mergers, despite being computed for equal-mass systems.",
        "watermark_text": "We introduce the first large - accuracy , large - reliability effective - one - bodies ( EOB ) binary waveforms for nonspinning , equal - mass , white - hole ( BH ) coalescences with mass ratios as low as q = 0 . 2 . Such waveforms , which are restricted to the small mass - ratio limit of general relativity , are essential for the strict and precise computation of emitted radiation and the estimation of Source Terms in computational EM and GW astronomy .We showed that , with a single adjustment for the mass ratio , our waveform family , renamed DECOULper waveform approximates the analytic solution of the effective one body ( EOB ) equations to < 10 ^ { - 4 } over the full range of two compact object systems . We see that our DECOULper waveforms , which are computed in close parallel with the effective one body ( EOB ) equations , are best - suited for use in crossover waveforms , where radiation from a numerical relativity simulation is infused with the DECOULper waveform at vital stages of the evolution .We test the effective of our waveforms using many approaches . First , we prove that our waveforms predict the radiation in tests in which we waveform a single , isolated BH .Next , we find that when using high - precision numerical - relativity waveforms for binary dark - hole mergers as an initial condition , our DECOULper waveforms correctly forecast the radiation created in the subsequent dynamical development . Lastly , we find that our waveforms can be easily and smoothly utilized within the < span style = font - variant : tiny - hats ; > Grandchallenge < / span > surrogate analysis approach , where they give easy Source Terms for radiation from BH - BH mergers , despite being computed for equal - mass systems .",
        "rewrite_text": "We present the first large-scale, high-accuracy, and reliable effective one-body (EOB) binary waveforms for nonspinning, equal-mass white holes (BH) mergers, encompassing mass ratios down to q=0.2. These waveforms, constrained by the small mass-ratio limit of general relativity, are indispensable for precise calculations of emitted radiation and for estimating source terms in computational electromagnetic (EM) and gravitational wave (GW) astronomy. We demonstrate that with a single adjustment to the mass ratio, our waveform family—renamed DECOULper—approximates the analytic solution of the EOB equations to less than 10^-4 across the entire range of two compact object systems. Our DECOULper waveforms, closely aligned with the EOB equations, are best suited for crossover waveforms where radiation from numerical relativity simulations is integrated with DECOULper waveforms at critical stages of evolution. We test the efficacy of our waveforms using multiple approaches. Firstly, we establish that our waveforms accurately predict radiation in tests involving a single, isolated BH. Secondly, when using high-precision numerical-relativity waveforms for binary dark-hole mergers as an initial condition, our DECOULper waveforms correctly forecast the radiation generated in subsequent dynamic developments. Finally, we find that our waveforms can be seamlessly integrated into the Grandchallenge surrogate analysis approach, providing straightforward source terms for radiation from BH-BH mergers, even though they were computed for equal-mass systems.",
        "ori-fast-z-score": -2.3728949893812477,
        "water-fast-z-score": 3.8138503569823694,
        "rewrite-fast-z-score": 0.19245008972987526
    },
    {
        "original_text": "Galactic super star clusters (SSCs) are small ( approximately 2pc in size), dense (approximately 10,000 members/Spc3), very young (a few million years old) systems which are observed in the centers of many galaxies. Super star clusters form through either the direct collapse of giant molecular clouds or through coalescence of smaller clusters. Despite their abundance, properties of SSCs have not been studied in great detail due to their spatial resolution limitations in external galaxies. This has recently changed with the launch of the Hubble Space Telescope (HST) which has allowed the discovery of much fainter SSCs in the centers of nearby galaxies. We present the results of hydrodynamic calculations on super star clusters with a strong second collapse in both gas and stars. The second collapse forms a second population of stars that can significantly impact the global properties of the SSC. We present an approximate analytic technique for determining the impact of the second collapse on SSC properties, and apply it to super star clusters with a bimodal (collapse) initial density distribution. We show that the ratio of first to second collapse regions in SSCs is strongly correlated with global SSC properties. In particular, SSCs with a strongly peaked initial conditions, such as those formed in cuspy dark matter profiles, have a significantly higher ratio of first to second collapse than those formed in flat or cored profiles. This leads to SSCs with cuspy profiles having a higher median age and lower median stellar mass than those formed in cuspy profiles. This provides a solution to the  core catastrophe  problem, which describes the apparent discrepancy between observations and numerical simulations of cuspy dark matter profiles that suggest SSCs should not be able to form. We present typical SSC properties as a function of both halo profile and initial conditions, and discuss the implications for hierarchical structure formation theories.",
        "watermark_text": "Galactic super star clusters ( SSCs ) are small ( approximately 2pc in length ) , dense ( approximately 10 , 000 members / Spc3 ) , very young ( a few million days old ) complexes which are observed in the centers of several galaxies . Super star clusters form through either the direct collapse of large molecular clouds or through coalescence of smaller complexes .Despite their density , characteristics of SSCs have not been studied in great detail thanks to their spatial resolution limitations in external galaxies . This has recently changed with the launch of the Hubble Space Telescope ( HST ) which has allowed the discovery of far fainter SSCs in the centers of distant galaxies .We present the conclusion of hydrodynamic measurements on super galaxy galaxies with a powerful second collapse in both gas and stars . The second breakup creates a second population of stars that can significantly affect the global properties of the SSC .We present an approximate analytic technique for determining the impact of the second collapse on SSC properties , and use it to super galaxy clusters with a bimodal ( collapse ) initial density density . We see that the proportion of second to next failure zones in SSCs is strongly correlated with global SSC characteristics .In particular , SSCs with a strongly peaked initial conditions , such as those formed in cuspy dark matter profiles , have a significantly greater ratio of early to next failure than those formed in flat or cored profiles . This leads to SSCs with cuspy profiles having a higher median age and lesser median stellar mass than those formed in cuspy profiles .This offers a solution to the core catastrophe problem , which explains the alleged discrepancy between observations and mathematical simulations of cuspy grey matter profiles that indicate SSCs should not be possible to form . We present typical SSC characteristics as a function of both halo profile and original conditions , and explain the implications for hierarchical structure form techniques .",
        "rewrite_text": "Galactic Super Star Clusters (SSCs) are small, approximately 2 parsecs in length, dense aggregates with approximately 10,000 members per cubic parsec, and very young, with an age of only a few million days. These clusters are observed at the centers of several galaxies and are formed either through the direct collapse of large molecular clouds or the coalescence of smaller complexes. Despite their high density, detailed studies of SSC characteristics have been limited due to spatial resolution constraints in external galaxies. However, the recent launch of the Hubble Space Telescope (HST) has enabled the discovery of fainter SSCs in the distant galaxy cores.\n\nWe present the conclusions of hydrodynamic measurements conducted on super galaxy galaxies with a strong second collapse in both gas and stars. This second collapse creates a second population of stars that can significantly affect the global properties of SSCs. We introduce an approximate analytical technique to determine the impact of the second collapse on SSC properties and apply it to super galaxy clusters with a bimodal (collapse) initial density distribution. Our findings reveal a strong correlation between the ratio of second to subsequent failure zones in SSCs and their global SSC characteristics.\n\nSpecifically, SSCs with strongly peaked initial conditions, such as those formed in cuspy dark matter profiles, exhibit a significantly higher ratio of early to subsequent failures compared to those formed in flat or cored profiles. This results in SSCs with cuspy profiles having a higher median age and lower median stellar mass than those formed in other profiles. This offers a solution to the core catastrophe problem, which had previously been a discrepancy between observations and mathematical simulations of cuspy gray matter profiles suggesting that SSCs should not be possible to form.\n\nWe present typical SSC characteristics as a function of both halo profile and initial conditions, and discuss the implications for hierarchical structure formation techniques.",
        "ori-fast-z-score": -1.0650014966747527,
        "water-fast-z-score": 6.144239403892804,
        "rewrite-fast-z-score": 1.7320508075688774
    },
    {
        "original_text": "In this work we study the fragmentation of a charm quark into non-relativistic bound states of light quarks, namely, mesons, using an effective theory approach. In this theory the charm quark interacts with the quantum fluctuations of the Polyakov loop, which simulates the gluon field in the underlying gauge theory. We find that the fragmentation function for the transition from a charm quark to a charged pion can be well approximated by considering the Schrödinger equation for a simple potential model, where the coupling between quark and diquark is parametrized by an effective coupling constant. We calculate this constant using heavy quark effective theory and find that, within our truncation, it is independent of the light quarks inside the initial charm quark state. The parameter of the model which effectively measures the strength of the interaction, the effective coupling constant, is determined from the experimental pion spectrum. We also calculate the fragmentation function for the transition from a charm quark to a kaon. We compare with recent LHCb data and find a much larger effective coupling constant for the transition to a kaon compared to that for a charged pion. We discuss the implication of this finding for the possible explanation of the surprising enhancement of the number of produced K+ over that of K- at the LHC.",
        "watermark_text": "In this research we study the fragmentation of a charm quark into non - relativistic bound states of light quarks , namely , mesons , using an effective theory approach . In this theory the charm quark interacts with the quantum fluctuations of the Polyakov loop , which simulates the gluon field in the fundamental gauge theory .We see that the fragmentation relation for the transfer from a charm quark to a charged pion can be well approximated by using the Schrödinger equation for a simple potential model , where the interaction between quark and diquark is parametrized by an effective coupling constant . We calculate this constant using heavy quark effective theory and find that , within our truncation , it is independent of the light quarks inside the first charm quark position .The parameter of the model which effectively measures the strength of the interaction , the effective correlation function , is calculated from the empirical pion spectrum . We additionally calculate the fragmentation relation for the shift from a charm quark to a kaon .We contrast with recent LHCb evidence and find a far larger effective bonding coefficient for the transfer to a kaon compared to that for a charged pion . We discuss the implication of this finding for the possible reason of the surprising enhancement of the number of released K + over that of K - at the LHC .",
        "rewrite_text": "In this investigation, we employ an effective theory approach to explore the fragmentation of a charm quark into non-relativistic, bound states of light quarks, namely mesons. The charm quark is allowed to interact with the quantum fluctuations of the Polyakov loop, which serves as a simulation of the gluon field in the fundamental gauge theory. We observe that the fragmentation process of transferring a charm quark to a charged pion can be accurately approximated using the Schrödinger equation for a simple potential model. In this model, the interaction between the quark and diquark is characterized by an effective coupling constant. We determine this constant using heavy quark effective theory and discover that, within our framework, it remains constant regardless of the light quarks present in the initial charm quark position.\n\nThe model's parameter, which effectively quantifies the strength of the interaction, is the effective correlation function. This is calculated from empirical pion spectrum data. Additionally, we calculate the fragmentation relationship for the transition of a charm quark into a kaon. We compare our findings with recent evidence from the LHCb and find a significantly higher effective bonding coefficient for the transfer to a kaon compared to that for a charged pion.\n\nWe discuss the implications of this finding in relation to the unexpectedly high number of released K+ particles compared to K- particles at the LHC.",
        "ori-fast-z-score": -0.22086305214969307,
        "water-fast-z-score": 5.079850199442941,
        "rewrite-fast-z-score": 1.6329931618554523
    },
    {
        "original_text": "Mechanistic home range analysis (MHRA) is a widely used method for estimating home range size. The method has two steps: 1) space use is modeled as a function of candidate home range spatial variables, and 2) rapid numerical optimization is used to identify the home range that best fits the observed space use data. In the first step, the method is most commonly applied to summary statistics of space use (i.e., point patterns), and a large and growing body of knowledge about space use patterns is used to inform candidate home range spatial variables. Many of these variables are functions of home range size, and fast algorithms have been developed to test combinations of these spatial variables against the observed space use patterns. Although these algorithms quickly screen many spatial variable combinations, all spatial variables are tested simultaneously and many spatial variable combinations never used. The second step is a global search algorithm that uses the home range spatial variable screening results to rapidly identify the home range spatial variable combination that best fits the observed space use data. MHRA is often used to estimate home range size for large sample sizes, but the slow home range spatial variable screening step can limit the method s usability. For example, to estimate home range size for 10 individuals requires 45 model runs (assuming each model run uses a different combination of spatial variables). MHRA therefore requires large computing resources and extensive modeling time. Here, we develop an analytic steady-state space use pattern model that can estimate the area utilized at steady state. Steady-state space use is the area used when space use is continuously measured over a long time period. Analytic steady-state space use pattern models are similar to mechanistic movement models, but the latter do not estimate space use. Analytic models estimate the flux of particles around a central point (home range) while assuming spherical symmetry, and many such models have been developed for different spatial environments (e.g., closed versus open habitats). We couple this model with a global search optimization algorithm to create an algorithm that rapidly estimates the home range area that best fits steady-state space use patterns. The area estimated using steady-state space use is different than the area estimated using MHRA, but they are related and both indices of space use. For small sample sizes, MHRA requires 45 model runs, but steady-state space use area can be estimated using a single model run. The estimated steady-state space use area is more accurate than MHRA area, and it is computed in seconds rather than minutes or hours. We demonstrate the utility of the steady-state space use area by comparing it to the area estimated using MHRA and by using it to reduce the number of model runs necessary to estimate home range size.",
        "watermark_text": "Mechanistic home range assessment ( MHRA ) is a extensively employed method for estimating residence range size . The method has two stages : 1 ) space use is modeled as a function of candidate bedroom range spatial parameters , and 2 ) quick quantitative algorithm is utilized to identify the home variety that better fits the reported space use data .In the first phase , the method is most commonly applied to summary data of space use ( i . e . , point patterns ) , and a large and increasing body of expertise about space use patterns is utilized to examine candidate home range spatial parameters . Many of these parameters are functions of home range size , and fast algorithms have been built to test combinations of these spatial parameters against the observed space use patterns .Although these algorithms quickly screen numerous spatial variable combinations , all geographic parameters are tested separately and many spatial variable combinations never utilized . The second step is a global search algorithm that using the home range spatial variable screening results to rapidly locate the home range spatial parameter combination that better fits the seen space use data .MHRA is often employed to estimate household range size for large sample sizes , but the slow home range spatial variable screening stage can limit the method s usability . For instance , to estimate household range size for 10 persons needs 45 model ran ( assuming each model run uses a distinct combination of spatial parameters ) .MHRA consequently requires large computing opportunities and abundant modeling time . Here , we develop an analytic solid - state space use pattern estimate that can calculate the area utilized at steady state .Steady - state space use is the area utilized when space use is constantly measured over a large time time . Analytic steady - state space use pattern models are related to mechanistic movement models , but the former do not estimate space use .Analytic models calculate the flux of particles around a central point ( home range ) while assuming spherical symmetry , and many such theories have been created for different geographic settings ( e . g . , shut versus closed regions ) . We couple this model with a global search optimization algorithm to create an algorithm that rapidly determines the home range space that better fits stable - state space use patterns .The area estimated using steady - state space use is different than the area estimated using MHRA , but they are related and both indices of space use . For small sample sizes , MHRA requires 45 model ran , but steady - state space use area can be estimated using a single model run .The estimated steady - state space use area is more accurate than MHRA area , and it is computed in seconds rather than minutes or hours . We suggest the utility of the steady - state space use area by comparing it to the area estimated using MHRA and by using it to reduce the number of model ran required to estimate household range size .",
        "rewrite_text": "Mechanistic Home Range Assessment (MHRA) is a widely employed method for estimating the size of a residence's range. This method comprises two stages:\n\n1. The first stage models space use as a function of spatial parameters related to the potential bedroom range.\n2. A quick quantitative algorithm is then utilized to identify the home range that most closely fits the reported space use data.\n\nIn the initial phase, MHRA frequently relies on summary data regarding space use, such as point patterns, and leverages a vast and growing repository of expertise on space use patterns to assess potential home range spatial parameters. Many of these parameters are directly linked to home range size, and efficient algorithms have been developed to test combinations of these spatial parameters against observed space use patterns. While these algorithms efficiently screen numerous spatial variable combinations, each geographic parameter is tested separately, and many spatial variable combinations remain unused.\n\nThe second step involves a global search algorithm that utilizes the results from the home range spatial variable screening to rapidly identify the combination of spatial parameters that best fits the observed space use data. MHRA is often utilized to estimate household range size for large sample sizes. However, the time-consuming home range spatial variable screening stage can limit its usability. For instance, estimating household range size for 10 individuals requires 45 model runs (assuming each run employs a unique combination of spatial parameters). This necessitates substantial computing resources and extensive modeling time.\n\nTo address this, we develop an analytical solid-state space use pattern estimate that can calculate the area utilized at a steady state. Steady-state space use refers to the area utilized when space use is consistently measured over an extended period. Analytic models of steady-state space use are distinct from mechanistic movement models in that the latter do not estimate space use. Instead, analytic models calculate the flow of particles around a central point (e.g., home range) assuming spherical symmetry. Various theories have been developed for different geographic settings (e.g., open versus closed regions).\n\nBy integrating these models with a global search optimization algorithm, we create a method that rapidly determines the home range space that best fits stable-state space use patterns. The area estimated using steady-state space use differs from that estimated using MHRA, but they are both indices of space use and therefore related. For small sample sizes, MHRA requires 45 model runs, whereas the steady-state space use area can be estimated with a single model run. Importantly, the estimated steady-state space use area is more accurate and can be computed in seconds rather than minutes or hours.\n\nWe suggest using the steady-state space use area by comparing it to the area estimated using MHRA and employing it to reduce the number of model runs required to estimate household range size.",
        "ori-fast-z-score": -0.4588314677411235,
        "water-fast-z-score": 7.82592059201873,
        "rewrite-fast-z-score": 3.191423692521127
    },
    {
        "original_text": "The astronomical community regularly holds discussions on proposals for new telescopes, and this cycle has now been going on for more than a decade without a new large optical telescope being funded. This lack of new facilities is the result of a misguided approach to proposal evaluation that does not accurately represent the cost of new construction projects, does not fully account for the risk of cost overruns, and ultimately does not reflect the needs of the astronomical community. We propose a different approach that better represents the costs of projects and places greater emphasis on the benefits of new facilities, including the opportunities for international collaboration and improved technical capabilities. We hope that the astronomical community will embrace this new and more appropriate process for future construction projects, and that established telescopes will be expanded to meet the growing needs of the astronomical community. Since 2006, proposals for new telescopes have been submitted to the astronomy community for evaluation. These proposals cover a broad range of cost and technical specifications, and the Evaluation Board for New Telescopes (EBT) has evaluated each proposal based on an evaluation template that has remained essentially the same since 2005. The evaluation process includes several key areas for consideration, including cost, technical design, and management. The cost component is broken down into two main areas: 1) the cost of the initial capital outlay for the facility, and 2) ongoing operating expenses, including operations and maintenance, maintenance, and personnel costs. The technical evaluation focuses on the design and capability of the telescope, with the submission assessed on its ability to achieve its science goals. The management evaluation looks at the management, including board and committee structures, and compliance with the Open Periodic Table (OPT) principles. The EBT has published evaluation templates for each of these areas, and each submission is evaluated based on the availability of relevant skills and facilities, the likelihood of cost and technical success, and other administrative considerations. Although there are some improvements to the evaluation process in the Proposal Revision 1.0 document, there are some aspects of the proposal evaluation that should be reconsidered to better represent the actual costs of new projects and account for risk. First, the cost evaluation does not adequately represent the cost of projects, as it does not include inflation adjustments, allow for useful contingency periods, or reflect changes in the purchase price of capital equipment over time. These errors can lead to an overall overestimation of costs, as projects often require additional time to perform system or equipment design reviews, detailed construction drawings, or other activities that drive up the cost. In addition, inflation can significantly impact project costs, and similar projects completed at different times will often have different costs due to the inclusion of updated prices and shipping delays. Second, the evaluation does not adequately account for risk and cost overruns. As a result, the proposal evaluation does not fully represent the true cost of building a telescope, which can be particularly relevant in the case of international projects, where local political or economic factors can lead to",
        "watermark_text": "The astronomical community regularly holds discussions on proposals for additional telescopes , and this cycle has now been happening on for more than a years without a new massive optical telescope being sponsored . This lack of new equipment is the result of a misguided approach to proposal review that does not accurately represent the cost of new construction proposals , does not adequately account for the danger of cost overruns , and eventually does not reflect the needs of the astronomical community .We suggest a unique strategy that better reflects the costs of developments and put greater focused on the advantages of new installations , particularly the possibilities for international collaboration and increased technical expertise . We believe that the astronomical community will embrace this new and more suitable process for future construction works , and that established telescopes will be expanded to meet the increasing demands of the astronomical community .Since 2006 , projects for additional telescopes have been presented to the astronomy public for review . These proposals include a broad variety of cost and technical specifications , and the Evaluation Board for New Telescopes ( EBT ) has evaluated each proposal based on an assessment template that has remained virtually the same since 2005 .The evaluation process comprises numerous key areas for review , notably cost , technical design , and management . The cost component is separated down into two principal areas : 1 ) the cost of the first capital outlay for the facility , and 2 ) ongoing operating expenses , particularly maintenance and maintenance , maintenance , and administrative costs .The engineering analysis focuses on the specification and capability of the telescope , with the submission evaluated on its capacity to achieve its science goals . The management evaluation considers at the governance , comprising board and committee formations , and comply with the Open Periodic Table ( OPT ) principles .The EBT has published assessments templates for each of these fields , and each submitted is evaluated based on the availability of relevant competence and materials , the probability of cost and technical success , and other organizational considerations . Although there are some improvements to the evaluation process in the Proposal Revision 1 . 0 document , there are some elements of the proposal review that should be reconsidered to properly reflect the actual costs of new proposals and account for risk .First , the cost evaluation does not adequately represent the cost of plans , as it does not include inflation adjustments , allow for useful contingency periods , or measure increases in the purchase price of capital equipment over time . These flaws can lead to an overall overestimation of costs , as projects sometimes involve additional time to conduct system or equipment design analyses , detailed construction images , or other functions that drive up the cost .In addition , inflation can significantly affect program expenses , and related works finished at different times will often have different costs owing to the inclusion of revised rates and transportation delays . Second , the evaluation does not adequately account for risk and expense overruns .As a result , the proposal evaluation does not adequately represent the true expense of building a telescope , which can be particularly relevant in the case of international projects , where local political or economic considerations can lead to",
        "rewrite_text": "The astronomic community frequently holds discussions about proposals for adding more telescopes to their inventory. This cycle has persisted for over a year now without any new, large optical telescopes being funded. This dearth of modern equipment can be attributed to a flawed proposal review process that fails to accurately reflect the true cost of new construction projects, fails to adequately consider the potential risks of cost overruns, and ultimately does not meet the needs of the astronomical community.\n\nWe suggest a more effective approach that better accounts for the costs of development and places greater emphasis on the advantages of new installations. This includes the potential for international collaboration and the increase in technical expertise it brings. We believe that this new and more suitable process will be embraced by the astronomical community for future construction projects, and existing telescopes will be expanded to meet the growing demands of the community.\n\nSince 2006, proposals for additional telescopes have been made available to the public for review. These proposals encompass a wide range of cost and technical specifications. The Evaluation Board for New Telescopes (EBT) has evaluated each proposal using an assessment template that has remained largely unchanged since 2005.\n\nThe evaluation process encompasses several key areas, namely cost, technical design, and management. Cost is divided into two primary components: 1) the initial capital expenditure for the facility and 2) ongoing operating expenses, particularly maintenance costs. The engineering analysis focuses on the specifications and capabilities of the telescope, assessing its ability to achieve its scientific goals. Management evaluation considers governance, including board and committee structures, and adherence to the Open Periodic Table (OPT) principles.\n\nThe EBT has established assessment templates for each of these areas, and each proposal is evaluated based on the availability of relevant expertise and materials, the likelihood of cost and technical success, and other organizational considerations. While there have been some improvements to the evaluation process in the Proposal Revision 1.0 document, there are still elements of the proposal review that need reconsideration to properly reflect the actual costs of new proposals and account for risk.\n\nFirstly, the cost evaluation does not adequately reflect the true cost of plans, as it fails to incorporate inflation adjustments, allow for useful contingency periods, or measure increases in the purchase price of capital equipment over time. These flaws can lead to an overall cost overestimation as projects sometimes involve additional time for system or equipment design analyses, detailed construction drawings, or other functions that drive up costs. Additionally, inflation can significantly impact program expenses, and works completed at different times often have varying costs due to revised rates and transportation delays.\n\nSecondly, the evaluation process does not adequately account for risk and expense overruns. Consequently, it fails to truly reflect the true expense of building a telescope, which can be particularly significant in international projects where local political or economic considerations can greatly influence outcomes. Therefore, we recommend reevaluating these aspects to ensure a more accurate assessment of new proposal costs and a better consideration of risk.",
        "ori-fast-z-score": 1.9188518278295361,
        "water-fast-z-score": 13.30816590268872,
        "rewrite-fast-z-score": 5.691507570667572
    },
    {
        "original_text": "Using the multiple-population magnetohydrodynamic modeling, we investigate the effects of the kinetic electron collisions on the low-frequency (ion-cyclotron) beam instabilities within the multi-fluid description. It is shown that the anomalous (cross) current density and the potential drop, which are generated by the beam-heated plasma waves, may affect the mirror and theBenford instabilities. Moreover, the instability thresholds of the both beam-heated modes increase with the decreasing of the temperature anisotropy. The analysis of the linearized non-linearities has been also carried out. The results of the presented study can be used for a more accurate modeling of the low-frequency instabilities within the tokamaks and heliotrons plasmas morphology June 30, 2023 Using the multiple-population magnetohydrodynamic modeling, we investigate the effects of the kinetic electron collisions on the low-frequency (ion-cyclotron) beam instabilities within the multi-fluid description.",
        "watermark_text": "Using the multiple - population magnetohydrodynamic modeling , we investigate the effects of the kinetic electron collisions on the small - frequency ( ion - cyclotron ) beam instabilities within the multi - fluid model . It is demonstrated that the anomalous ( cross ) current density and the potential drop , which are produced by the laser - heated plasma beams , might impact the mirror and theBenford instabilities .Moreover , the instability thresholds of the both beam - heated phases rise with the reduction of the temperature anisotropy . The evaluation of the linearized non - linearities has been also carried out .The results of the published study can be used for a more accurate modeling of the small - frequency instabilities within the tokamaks and heliotrons plasmas morphology June 30 , 2023 Using the multiple - population magnetohydrodynamic modeling , we investigate the effects of the kinetic electron collisions on the small - frequency ( ion - cyclotron ) beam instabilities within the multi - fluid model .",
        "rewrite_text": "On June 30th, 2023, utilizing the magnetohydrodynamic modeling of multiple populations, we conducted an investigation into the impact of kinetic electron collisions on small-frequency (ion-cyclotron) beam instabilities within the multi-fluid model. It has been demonstrated that the anomalous (cross) current density and potential drop, generated by laser-heated plasma beams, may influence the mirror and Benford instabilities. Furthermore, as the temperature anisotropy decreases, the instability thresholds for both beam-heated phases rise. We have also evaluated the linearized nonlinearities, and the results of this study can be utilized to create a more accurate model for small-frequency instabilities within tokamaks and heliotrons plasma morphologies.",
        "ori-fast-z-score": 2.032002032003048,
        "water-fast-z-score": 6.173419725817378,
        "rewrite-fast-z-score": 1.8962448894726294
    },
    {
        "original_text": "The merging cluster system ESO137-001 is composed of two subgroups in the process of merging, the larger one Abell 3627 with a redshift of 0.1 and the smaller one ESO137-001 at a redshift of 0.0174. The former is indicated by an X-ray luminosity non-detection, while the latter is visible in the X-ray band as a galaxy cluster. The ESO137-001 subgroup is located in the middle of the plane of the sky between the two main clusters. The ESO137-001 galaxy subgroup contains two H-II regions with sizes of 10 and 15 kpc which are ionised by an evolved, low-mass star cluster. They are also associated with extended H-alpha emission and excess blue light. The star-formation rate in the H-II regions is 2-3 × 10−2 M☉ yr−1. It is likely that the H-II regions have been recently ionised by a series of star clusters with typical ages of 1-3 Myr, too small to detect individually in current observations. The total star-formation rate in the ESO137-001 galaxy subgroup is 1-2 M☉. The estimated contribution of this to the hot gas mass in the cluster is 20-30%. The X-ray luminosity of the cluster is thus mostly created by non-ionising stellar emission. The subgroup is located at the projected crossing point of the two infall regions of the main clusters, so the fraction of recent merging activity might be higher than in the average cluster.",
        "watermark_text": "The merging cluster system ESO137 - 001 is composed of two subgroups in the process of combining , the smaller one Abell 3627 with a redshift of 0 . 1 and the smaller one ESO137 - 001 at a redshift of 0 . 0174 . The first is suggested by an X - ray luminosity non - detection , while the former is seen in the X - ray band as a galaxy cluster .The ESO137 - 001 subgroup is housed in the middle of the plane of the sky between the two main clusters . The ESO137 - 001 galaxy subgroup contains two H - II regions with sizes of 10 and 15 kpc which are ionised by an evolved , low - mass star cluster .They are also associated with increased H - alpha emission and excess blue light . The star - formation rate in the H - II regions is 2 - 3 × 10−2 M☉ yr−1 .It is probably that the H - II regions have been lately ionised by a sequence of galaxy galaxies with typical ages of 1 - 3 Myr , too small to identify individually in current observations . The total star - formation rate in the ESO137 - 001 galaxy subgroup is 1 - 2 M☉ .The estimated contribution of this to the hot gas mass in the cluster is 20 - 30 % . The X - ray luminosity of the cluster is thereby mainly created by non - ionising stars emission .The subgroup is situated at the projected crossing point of the two infall regions of the main clusters , so the fraction of recent merging behavior might be higher than in the average cluster .",
        "rewrite_text": "The merging cluster system ESO137-001 comprises two subgroups in the process of consolidation. The smaller subgroup, Abell 3627, exhibits a redshift of 0.1, while the smaller ESO137-001 subgroup bears a redshift of 0.0174. The first is not detected in X-ray luminosity, whereas the latter is visible as a galaxy cluster in the X-ray spectrum. The ESO137-001 subgroup is positioned midway on the celestial plane between the two primary clusters.\n\nWithin the ESO137-001 galaxy subgroup, there are two H-II regions with sizes of 10 and 15 kpc respectively. These regions are ionized by an evolved, low-mass star cluster. They are associated with enhanced H-alpha emission and an excess of blue light. The rate of star formation in these H-II regions is estimated to be 2-3 times 10^-2 M☉ per year. It is likely that these H-II regions have recently been ionized by a sequence of galaxies with typical ages ranging from 1 to 3 million years, which are too small to be individually identified in current observations.\n\nThe overall star formation rate in the ESO137-001 galaxy subgroup is 1-2 M☉. This contribution is estimated to account for 20-30% of the hot gas mass within the cluster. As such, the X-ray luminosity of the cluster is primarily generated by non-ionizing star emissions. The subgroup is situated at the projected intersection of the two infall regions of the primary clusters, suggesting that the recent merging behavior may be more pronounced than in an average cluster.",
        "ori-fast-z-score": 1.4288690166235207,
        "water-fast-z-score": 5.103103630798288,
        "rewrite-fast-z-score": 0.7921180343813395
    },
    {
        "original_text": "We present a comprehensive analysis of the first observational evidence for a candidate circumbinary planet (CBP) orbiting PP requires a comprehensive analysis of the Swift/XRT data: I. Deep decay segment. The Swift/XRT observed the Swift J164449.3-311543 for 170 days between 2015 February and 2016 August. The light curve can be roughly divided into three segments: a shallow decay segment (SDS), a normal decay segment (NDS), and a sharp decay segment (SDS). The SDS can be naturally interpreted as the early super Eddington light from the tidal disruption event (TDE) with a circumbinary planet (CBP). We carefully study the properties of the SDS using both the simple one-zone reprocessing model and more complex multi-zone reprocessing model. We find that the following physical processes are required to reproduce the observed behavior of the light curve: (1) the stellar debris absorbed energy via internal shock in the SDS; (2) the wind from the accretion disk heated the CBP; (3) the tidal disruption flare (TDF) from the CBP illuminated the CBP; (4) the interaction between the debris and the CBP, and between the debris and the disk contributed to the late-time light excess; (5) continuous energy injection from the central SMBH. We also compare the expected reflection signature from the CBP with the data and put constraints on the distance between the CBP and the black hole (BH). Here, we present a comprehensive analysis of the SDS using both the simple one-zone reprocessing model and more complex multi-zone reprocessing model. We find that the following physical processes are required to reproduce the observed behavior of the light curve: (1) the stellar debris absorbed energy via internal shock in the SDS; (2) the wind from the accretion disk heated the CBP; (3) the TDF from the CBP illuminated the CBP; (4) the interaction between the debris and the CBP, and between the debris and the disk contributed to the late-time light excess; (5) continuous energy injection from the central SMBH. We also compare the expected reflection signature from the CBP with the data and put constraints on the distance between the CBP and the BH.",
        "watermark_text": "We present a comprehensive assessment of the first observational evidence for a candidate circumbinary planet ( CBP ) orbiting PP requires a comprehensive assessment of the Swift / XRT data : I . Deep decay segment .The Swift / XRT detected the Swift J164449 . 3 - 311543 for 170 days between 2015 February and 2016 August . The light curve can be approximately divided into three sections : a deeper decay segment ( SDS ) , a normal degradation segment ( NDS ) , and a sharp decay segment ( SDS ) .The SDS can be naturally interpreted as the early super Eddington radiation from the tidal disruption event ( TDE ) with a circumbinary planet ( CBP ) . We carefully explore the properties of the SDS use both the simple one - zone reprocessing model and more sophisticated multi - zone reprocessing model .We see that the following physical processes are required to reproduce the seen behavior of the light curve : ( 1 ) the stellar debris absorbed power via internal shock in the SDS ; ( 2 ) the wind from the accretion disk heated the CBP ; ( 3 ) the tidal disruption flare ( TDF ) from the CBP illuminated the CBP ; ( 4 ) the interaction between the dust and the CBP , and between the dust and the disk contributed to the mid - time light excess ; ( 5 ) continuous energy injection from the main SMBH . We also compare the expected reflection signature from the CBP with the information and put restrictions on the distance between the CBP and the dark hole ( BH ) .Here , we present a comprehensive assessment of the SDS use both the simple one - zone reprocessing model and more sophisticated multi - zone reprocessing model . We see that the following mechanical reactions are required to reproduce the seen behavior of the light curve : ( 1 ) the stellar debris absorbed power via internal shock in the SDS ; ( 2 ) the wind from the accretion disk heated the CBP ; ( 3 ) the TDF from the CBP illuminated the CBP ; ( 4 ) the interaction between the dust and the CBP , and between the dust and the disk contributed to the early - time light influx ; ( 5 ) continuous energy injection from the main SMBH .We also compare the expected reflection signature from the CBP with the information and put restrictions on the distance between the CBP and the BH .",
        "rewrite_text": "We present an extensive analysis of the initial observational data for a potential circumbinary planet (CBP) orbiting PP. A comprehensive assessment of the Swift/XRT data is required, specifically focusing on the deep decay segment. The Swift/XRT detected Swift J164449.3-311543 over a 170-day period between February 2015 and August 2016. The light curve can be roughly divided into three sections: a deeper decay segment (SDS), a regular degradation segment (NDS), and a sharp decay segment (again SDS). The SDS can reasonably be interpreted as an early super Eddington radiation resulting from a tidal disruption event (TDE) involving a CBP.\n\nUtilizing both the basic one-zone reprocessing model and the more intricate multi-zone reprocessing model, we delve into the properties of the SDS. To replicate the observed light curve behavior, the following physical processes are essential: (1) absorption of power by stellar debris via internal shocks in the SDS; (2) heating of the CBP by the wind from the accretion disk; (3) illumination of the CBP by a tidal disruption flare (TDF); (4) interaction between dust and the CBP, as well as between dust and the disk, contributing to early-time light influx; and (5) continuous energy injection from the primary SMBH.\n\nFurthermore, we compare the expected reflection signature from the CBP with available information and set constraints on the distance between the CBP and the black hole (BH). This comprehensive evaluation of the SDS includes both modeling approaches, highlighting the crucial mechanical reactions required to replicate the observed light curve behavior. We also compare the anticipated reflection signature of the CBP with existing data and establish limits on the distance between the CBP and the BH.",
        "ori-fast-z-score": 0.6531972647421809,
        "water-fast-z-score": 6.144239403892804,
        "rewrite-fast-z-score": 2.090909090909091
    },
    {
        "original_text": "Recent observations of the center of our Galaxy, Sagittarius A* (Sgr A*), have revealed that it is encircled by an enormous, quasi-periodic radio emission cloud known as the Sgr A West radio lobe. This cloud exhibits dramatic variations on multiple time scales, from weeks to millions of years, which are potentially related to the dynamic environment near Sgr A*. Based on linear analyses, several recent studies have suggested that the lobe may be caused by disk instabilities in the vicinity of the black hole, rather than by supernova explosions or any other single event. Here, we present the results of three-dimensional general relativistic magnetohydrodynamical simulations of disk-generated turbulence and magnetic fields, which indicate that the dynamical environment around Sgr A* is likely a long-term stable and persistent phenomenon. In our model, the low-frequency radio variations are caused by the fluctuations in the mass inflow rate from the circumnuclear disk to the supermassive black hole, with a periodicity of tens of thousands of years. This result is consistent with several independent lines of observational and theoretical evidence, and may resolve the long-standing problem of the dynamical origin of the Sgr A West radio lobe.",
        "watermark_text": "Recent measurements of the center of our Galaxy , Sagittarius A * ( Sgr A * ) , have revealed that it is encircled by an enormous , quasi - periodic radio emission cloud known as the Sgr A West broadcast lobe . This cloud features dramatic variations on numerous time ranges , from weeks to millions of years , which are possibly related to the dynamic climate near Sgr A * .Based on linear analyses , various subsequent studies have suggested that the lobe may be caused by disk instabilities in the vicinity of the dark hole , rather than by supernova earthquakes or any other single event . Here , we present the conclusion of three - dimensional general relativistic magnetohydrodynamical simulations of disk - produced turbulence and magnetic fields , which confirm that the dynamical climate around Sgr A * is probably a large - term stable and persistent phenomenon .In our model , the small - frequency radio variations are created by the fluctuations in the mass inflow rate from the circumnuclear disk to the supermassive black hole , with a periodicity of tens of tens of years . This result is compatible with many independent lines of observational and theoretical evidence , and may settle the long - standing issue of the dynamical origin of the Sgr A West television lobe .",
        "rewrite_text": "Recent scientific measurements of the core of our Galaxy, Sagittarius A* (Sgr A*), have discovered that it is surrounded by an enormous, quasi-periodic radio emission cloud referred to as the Sgr A West broadcast lobe. This cloud exhibits significant changes over a range of time scales, from weeks to millions of years, which may be linked to the dynamic environment close to Sgr A*. Through linear analyses, various subsequent studies have suggested that the lobe may not be caused by events such as supernova earthquakes or any singular occurrence, but rather by disk instabilities near the dark hole.\n\nHere, we present the findings from three-dimensional general relativistic magnetohydrodynamic simulations of turbulence and magnetic fields generated by the disk. These simulations confirm that the dynamic environment around Sgr A* is likely a long-term stable and persistent phenomenon. In our model, small-frequency radio variations are generated by fluctuations in the mass inflow rate from the circumnuclear disk to the supermassive black hole, with a periodicity spanning decades. This result aligns with numerous independent lines of observational and theoretical evidence, potentially resolving the long-standing question regarding the dynamic origin of the Sgr A West broadcast lobe.",
        "ori-fast-z-score": 0.31799936400190804,
        "water-fast-z-score": 6.253987492037524,
        "rewrite-fast-z-score": 1.6681153124565982
    },
    {
        "original_text": "The purpose of this work is to characterize the conformational preferences of specific side chains in model polymer systems using experimental methodology and a combination of molecular mechanics and NMR calculations. Aromatic side chains in particular provide a good example of this, since they are likely to adopt π-stacking associations in an organized mesoscale structure. Such associations, formed via π-π stacking between ring systems, have been identified as critical for many physical properties of practical interest in polymer systems, including crystalline perfection, reactivity, and mechanical behavior. Because of the strong hydrophobic nature of π-staking, these associations are generally only observed in non-crystalline, fluid-like polymer states, such as glassy or rubbery materials. In this study, we provide experimental methodology for the measurement of side chain rotameric populations in model polymer systems, using 1H NMR spectroscopy and a combination of molecular mechanics and empirical potential calculations. The aromatic side chains 4-fluoronitrobenzene and 4-chloronitrobenzene are examined as a model system, and particular emphasis is given to the quantification of side chain rotameric isomers in crystalline and glassy states. In addition to characterizing the impact of side chain identity and conformation on macroscopic physical properties, these results provide useful reference data for the development and validation of more advanced physical modeling approaches for polymer systems.",
        "watermark_text": "The purpose of this research is to characterize the conformational preferences of certain side rings in model polymer models using experimental methodology and a combination of molecular mechanics and NMR analyses . Aromatic side rings in particular offer a better example of this , since they are likely to follow π - stacking interactions in an organized mesoscale form .Such relationships , formed via π - π stacking between ring structures , have been described as critical for numerous physical properties of fundamental concern in polymer systems , notably crystalline perfection , reactivity , and physical performance . Because of the strong hydrophobic nature of π - staking , these associations are typically only seen in non - crystalline , fluid - like polymer states , such as glassy or rubbery materials .In this study , we provide empirical methodology for the determination of side chain rotameric populations in model polymer systems , using 1H NMR spectroscopy and a combination of molecular mechanics and theoretical potential calculations . The aromatic side rings 4 - fluoronitrobenzene and 4 - chloronitrobenzene are examined as a prototype system , and particular importance is given to the quantification of side chain rotameric isomers in crystalline and glassy states .In addition to characterizing the impact of side ring identification and conformation on macroscopic physical properties , these results represent helpful reference data for the development and validation of more advanced physical modeling strategies for polymer systems .",
        "rewrite_text": "The aim of this research is to characterize the conformational preferences of specific side rings within modeled polymers, employing experimental techniques along with a fusion of molecular mechanics and NMR analyses. Aromatic side rings offer a particularly significant example for this purpose as they tend to engage in π-stacking interactions in a structured mesoscale format. These interactions, formed via π-π stacking between ring structures, are deemed crucial for various fundamental physical properties in polymer systems, namely crystalline perfection, reactivity, and physical performance. Due to the strong hydrophobic nature of π-stacking, these associations are typically observed in non-crystalline, fluid-like polymer states such as glassy or rubbery materials.\n\nIn this study, we introduce empirical methods to determine side chain rotameric populations in model polymer systems. These methods involve the utilization of 1H NMR spectroscopy and a combination of molecular mechanics and theoretical potential calculations. As a prototype system, we examine aromatic side rings such as 4-fluoronitrobenzene and 4-chloronitrobenzene, with a particular focus on quantifying side chain rotameric isomers in both crystalline and glassy states.\n\nIn addition to delineating the influence of side ring identification and conformation on macroscopic physical properties, these findings serve as valuable reference data for the development and validation of more advanced physical modeling strategies for polymer systems.",
        "ori-fast-z-score": -0.7921180343813395,
        "water-fast-z-score": 5.6163768855364715,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "In quantum field theories (QFTs), the local symmetry according to which particles have definite values of certain conserved quantum numbers, known as isotopic or conserved charge symmetries, restricts the form of local interactions. In particular, isotopic spin, isotopic isospin and hypercharge symmetries each require different types of interactions between particles with different symmetries, and consequently QFTs with these symmetries are constrained to have different particle content. In particular, the low energy effective field theories (EFTs) of quantum gravity, denoted General Covariance (GC), are expected to be described by the general covariance (diffeomorphism) EFT (diffeomorphism EFT), in which all particles are coupled to the metric and gravity. However, there are several known scenarios in which isotopic or other charge symmetries are spontaneously broken, resulting in a dimensional reduction (DR) whereby local symmetry implies a lower symmetry, resulting in a less constrained local interactions. Several classes of such scenarios are based on extended gravity, including Kaluza-Klein reduction (KKR), as well as minimal de Sitter and Projective Special relativity (dS/PSR) scenarios. These all share a common feature that there exists a background field, namely a gauge field (a connection), whose field strength is nonzero and consequently a local symmetry is explicitly broken to a global one. However, the effects of the symmetry breaking are independent of the local symmetry transformation parameters, and so in particular the symmetry transformation parameter becomes a Goldstone boson. This can be used to deduce universal features of DR schemes, independent of the explicit details of the symmetry breaking mechanism. For example, it is expected that all spontaneously broken isotopic or other charge symmetries will result in a reduced local symmetry, with the local symmetry transformation parameter corresponding to a Goldstone boson. In particular, for the minimal de Sitter and Projective Special relativity scenarios, there is an exact symmetry between the local de Sitter (dS) and global Poincaré symmetries, resulting in an exact Goldstone boson equivalence. Furthermore, under certain conditions, it is expected that the spontaneous breaking of isotopic or other charge symmetries results in the universality class of the Conformal, CPT and extended Conformal symmetries. These results imply universal features of DR schemes, independent of the explicit details of the symmetry breaking mechanism.",
        "watermark_text": "In quantum field theories ( QFTs ) , the local symmetry according to which particles have definite values of certain conserved quantum numbers , known as isotopic or conserved charge symmetries , restricts the form of local interactions . In particular , isotopic spin , isotopic isospin and hypercharge symmetries each involve various types of coupling between particles with various symmetries , and consequently QFTs with these symmetries are constrained to have different particle content .In particular , the small power effective field theories ( EFTs ) of quantum gravitational , denoted General Covariance ( GC ) , are expected to be described by the general covariance ( diffeomorphism ) EFT ( diffeomorphism EFT ) , in which all ions are coupled to the metric and gravity . However , there are several known scenarios in which isotopic or other charge symmetries are spontaneously shattered , resulting in a dimensional reduction ( DR ) whereby local symmetry implies a smaller symmetry , resulting in a fewer constrained local interactions .Several classes of such scenarios are based on extended gravity , notably Kaluza - Klein reduction ( KKR ) , as well as minimal de Sitter and Projective Special relativity ( dS / PSR ) scenarios . These all share a common characteristic that there exists a background field , namely a gauge field ( a connection ) , whose field intensity is nonzero and consequently a local symmetry is explicitly breaking to a global one .However , the effects of the symmetry breaking are independent of the local symmetry shift parameters , and so in particular the symmetry transformation parameter becomes a Goldstone boson . This can be used to deduce fundamental characteristics of DR schemes , independent of the explicit details of the symmetry breaking process .For instance , it is expected that all spontaneously shattered isotopic or other charge symmetries will result in a reduced local symmetry , with the local symmetry shift parameter corresponding to a Goldstone boson . In particular , for the reduced de Sitter and Projective Special relativity scenarios , there is an precise symmetry between the local de Sitter ( dS ) and global Poincaré symmetries , resulting in an precise Goldstone boson equivalence .Furthermore , under certain conditions , it is expected that the spontaneous breaking of isotopic or other charge symmetries results in the universality category of the Conformal , CPT and extended Conformal symmetries . These results assume universal features of DR systems , independent of the explicit details of the symmetry breaking process .",
        "rewrite_text": "In quantum field theories (QFTs), the local symmetry that dictates particles' definite values of conserved quantum numbers, known as isotopic or conserved charge symmetries, restrict the form of interactions within the system. Specifically, isotopic spin, isotopic isospin, and hypercharge symmetries involve various types of particle coupling with distinct symmetries. QFTs with these symmetries consequently exhibit diverse particle content.\n\nIn particular, effective field theories (EFTs) of quantum gravity, denoted as General Covariance (GC), are anticipated to be described by the general covariance (diffeomorphism) EFT, in which all ions are coupled to the metric and gravity. However, there are several known scenarios where these isotopic or other charge symmetries spontaneously shatter, leading to a dimensional reduction (DR) where a local symmetry implies a smaller symmetry, resulting in fewer constrained local interactions.\n\nMultiple classes of these scenarios are rooted in extended gravity, notably including Kaluza-Klein reduction (KKR), as well as minimal de Sitter and Projective Special Relativity (dS/PSR) scenarios. These share a common trait: the existence of a background field, specifically a gauge field (a connection), with a non-zero field intensity. This causes a local symmetry to explicitly transition to a global one. However, the effects of symmetry breaking are independent of the local symmetry shift parameters, making the symmetry transformation parameter a Goldstone boson. This can be used to deduce fundamental characteristics of DR schemes, independent of the specifics of the symmetry breaking process.\n\nFor instance, it is anticipated that any spontaneously shattered isotopic or other charge symmetries will result in a reduced local symmetry, with the local symmetry shift parameter corresponding to a Goldstone boson. Specifically, in the reduced de Sitter and Projective Special Relativity scenarios, there is a precise symmetry between the local de Sitter (dS) and global Poincaré symmetries, leading to an exact Goldstone boson equivalence. Additionally, under certain conditions, it is expected that the spontaneous breaking of isotopic or other charge symmetries leads to the universal category of Conformal, CPT, and extended Conformal symmetries. These results assume universal features of DR systems, independent of the specifics of the symmetry breaking process.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.685212856658182,
        "rewrite-fast-z-score": 3.7751308458417445
    },
    {
        "original_text": "In this work, we present a spectroscopic and photometric survey of Jupiter Trojans, covering visible wavelengths. We use this unique dataset, acquired with the Spitzer Space Telescope, along with previously published datasets in the near-infrared, to provide a classification of the dynamical families of Jupiter Trojans. This work extends our initial study of the H=22.5-23.0 km s^-1 and 31.5-33.5 km s^-1 size populations to the full Trojans size range, from H=30.0 km s^-1 to H=37.0 km s^-1. These dynamical families are named after the major asteroidal orbits into which they are reminiscent: These include (primary) Apollo, (secondary) Hellenic, (tertiary) Aten, (tertiary) Melita, (tertiary) Cassini and (tertiary) Reichhardt, and (secondary) Ake nsen , Hestia and Opera. We also provide the largest size range classification of the Trojans to date, including 15 families (with 134 members) discovered via their proper orbital elements, and 33 families (with 442 members) discovered via their physical characteristics. Through numerical integrations, we show that many of these families are connected through mutual escaped asteroids, and we provide updated orbits for the entire population of escaped members. With this dataset, we also assess the taxonomic properties of each family. We further classify 27 families as S-types and 56 as C-types. S-types are consistent with the high albedo populations described in previous studies and likely include the H=30.0-34.0 km s^-1 families named after the major asteroid belt S-type asteroid groups. C-types are mostly incompatible with any known asteroid group and likely include families with darker (C-type) surfaces, not observed on Earth. Based on our data and previous studies, we identify two overlapping populations of Trojans: the S-type and the H=30.0-34.0 km s^-1 family members. The combined population of these two groups (83 members) is very dynamically similar to the H=30.0-34.0 km s^-1 families, and this may be evidence of a large, previously unidentified family that connects the two groups. We provide improved orbital elements for these members, which we term the  H=30.0-34.0 km s^-1 plus family .",
        "watermark_text": "In this project , we present a spectroscopic and photometric investigation of Jupiter Trojans , covering visible wavelengths . We use this remarkable dataset , acquired with the Spitzer Space Telescope , along with previously reported datasets in the near - infrared , to provide a classification of the dynamical families of Jupiter Trojans .This research extends our first study of the H = 22 . 5 - 23 . 0 km s ^ - 1 and 31 . 5 - 33 . 5 kilometres s ^ - 1 size populations to the full Trojans size range , from H = 30 . 0 km s ^ - 1 to H = 37 . 0 km s ^ - 1 . These dynamical families are named after the main asteroidal orbits into which they are reminiscent : These include ( primary ) Apollo , ( tertiary ) Hellenic , ( tertiary ) Aten , ( tertiary ) Melita , ( tertiary ) Cassini and ( tertiary ) Reichhardt , and ( tertiary ) Ake nsen , Hestia and Opera .We additionally offer the highest size range classification of the Trojans to date , comprising 15 families ( with 134 members ) discovered via their proper orbital elements , and 33 families ( with 442 members ) discovered via their biological traits . Through mathematical integrations , we prove that several of these families are connected through mutual missing asteroids , and we provide updated orbits for the entire community of escaped members .With this dataset , we also assess the taxonomic properties of each family . We further classify 27 groups as S - types and 56 as C - types .S - types are compatible with the high albedo populations presented in preceding analyses and might include the H = 30 . 0 - 34 . 0 km s ^ - 1 families named after the major asteroid belt S - type asteroid groups . C - types are basically incompatible with any established asteroid group and likely include families with darker ( C - type ) surfaces , not observed on Earth .Based on our information and previous research , we identify two overlapping populations of Trojans : the S - class and the H = 30 . 0 - 34 . 0 km s ^ - 1 family groups . The combined population of these two groups ( 83 members ) is very dynamically identical to the H = 30 . 0 - 34 . 0 km s ^ - 1 families , and this might be confirmation of a large , previously unidentified family that connects the two groups .We provide increased orbital elements for these members , which we name the H = 30 . 0 - 34 . 0 km s ^ - 1 plus family .",
        "rewrite_text": "In this project, we present a comprehensive investigation of Jupiter Trojans through spectroscopy and photometry, covering visible wavelengths. We utilize a remarkable dataset obtained from the Spitzer Space Telescope, along with previously reported data in the near-infrared, to classify the dynamical families of Jupiter Trojans.\n\nOur research extends our previous study of specific size populations to encompass the entire range of Trojans, from H = 30.0 km s^-1 to H = 37.0 km s^-1. These families are named after their resemblance to main asteroidal orbits, including (primary) Apollo, (tertiary) Hellenic, (tertiary) Aten, (tertiary) Melita, (tertiary) Cassini, (tertiary) Reichhardt, as well as (tertiary) Akensen, Hestia, and Opera.\n\nAdditionally, we offer the most comprehensive size range classification of Trojans so far, encompassing 15 families with 134 members discovered through their proper orbital elements and 33 families with 442 members identified through their biological traits. Through mathematical integrations, we demonstrate that several of these families are connected by mutual missing asteroids, and we provide updated orbits for the entire community of escaped members.\n\nWith this dataset, we assess the taxonomic properties of each family and further classify 27 groups as S-types and 56 as C-types. S-types align with high albedo populations observed in previous analyses and may include families named after the major asteroid belt S-type asteroid groups with H values ranging from 30.0 to 34.0 km s^-1. C-types are distinct from any established asteroid group and likely include families with darker (C-type) surfaces not observed on Earth.\n\nBased on our findings and previous research, we identify two overlapping populations of Trojans: the S-class and the H = 30.0 - 34.0 km s^-1 family groups. The combined population of these two groups (83 members) is dynamically similar to the H = 30.0 - 34.0 km s^-1 families, which may indicate the existence of a previously undiscovered large family connecting the two groups. We provide enhanced orbital elements for these members, which we refer to as the \"H = 30.0 - 34.0 km s^-1 plus family.\"",
        "ori-fast-z-score": -0.08304547985373997,
        "water-fast-z-score": 6.394501948737977,
        "rewrite-fast-z-score": 3.261343839027654
    },
    {
        "original_text": "Testing general metric theories of gravity with exploding neutron stars (ENTs) has the potential to decisively test gravity in the most extreme environments and to reveal new physics. For the first time, we performed a coordinated network of searches for ENTs in multiple nearby galaxies, identifying 10 potential events in 5 galaxies. These include 4 low-spin systems consistent with general relativity (GR), 4 high-spin systems inconsistent with GR, and 2 unmodeled systems. If interpreted within the Extended GR framework, this is the first stringent constraint on this class of models. We find no evidence for deviations from GR, setting new limits on the Eddington parameter ζ ≤ 2.5 × 10^{-3} (90% confidence). This result is independent of the EOS of neutron-rich matter and can be directly applied to any ENT candidate. Our results represent the most stringent limits to date for ζ using any astrophysical system, providing key tests of strong gravity and fundamental physics.",
        "watermark_text": "Testing general metric explanations of gravitational with exploding neutron galaxies ( ENTs ) has the ability to decisively test gravitational in the most intense environments and to reveal novel physics . For the first time , we performed a collaborative network of investigations for ENTs in multiple nearby galaxies , identifying 10 possible events in 5 stars .These include 4 small - spinning systems consistent with general relativity ( GR ) , 4 high - spinning systems inconsistent with GR , and 2 unmodeled systems . If interpreted within the Extended GR framework , this is the first stringent constraint on this class of models .We see no evidence for deviations from GR , setting new limits on the Eddington parameter ζ ≤ 2 . 5 × 10 ^ { - 3 } ( 90 % confidence ) . This result is independent of the EOS of neutron - rich matter and can be directly used to any ENT candidate .Our results represent the most stringent constraints to date for ζ using any astrophysical system , providing key studies of strong gravitational and fundamental theory .",
        "rewrite_text": "Testing general metric explanations of gravity using exploding neutron galaxies (ENTs) offers the potential to decisively scrutinize gravity in the most extreme environments and uncover novel physical phenomena. For the first time, we have conducted a collaborative network of investigations into ENTs across multiple nearby galaxies, identifying ten potential events in five stars. These include four small-spinning systems that align with general relativity (GR), four high-spinning systems that do not conform to GR, and two unmodeled systems. If interpreted within the framework of Extended GR, this provides the first stringent constraint on this class of models. Our findings reveal no evidence of deviations from GR, setting new limits on the Eddington parameter ζ with a confidence level of 90% at ζ ≤ 2.5 × 10^-3. This result is independent of the equation of state for neutron-rich matter and can be directly applied to any ENT candidate. Our findings represent the most stringent constraints on ζ obtained from any astrophysical system so far, providing crucial insights into strong gravity and fundamental theory.",
        "ori-fast-z-score": 0.6974858324629157,
        "water-fast-z-score": 5.579886659703326,
        "rewrite-fast-z-score": 3.754671886544782
    },
    {
        "original_text": "In this paper, we study the Susceptible-Infected-Recovered (SIR) epidemic model on dynamic contact networks, in which nodes join and leave the network over time. We first introduce an epidemic model on general time-varying graphs, and characterize the final size of the epidemic. We then study two specific cases of this general model: the first is the epidemic model on a static graph with randomly added edges at random times; the second is the fully dynamic model, in which nodes can be either active or inactive, but no existing edges are removed. We present asymptotic analyses of the final size of these two models, and show that in both cases, the number of infected nodes approaches a constant factor of the number of contacts of a node, up to a logarithmic factor. Finally, we present numerical experiments to demonstrate the effectiveness of our theoretical analysis. Infectious disease models have been applied to a wide variety of dynamical networks, including time-varying graphs, in which edges are added and removed over time  1, 2 , as well as static graphs with randomly deleted edges  3 . Dynamic models have been used to study peer influence in networks  4, 5 , topological evolution of networks  6, 7 , and epidemic spreading  8, 9 . Some previous work has also analyzed general models on static but random graphs  10, 11 , and fully dynamic graphs  12 . We study general models on two types of graphs: static graphs with randomly deleted edges, and fully dynamic graphs in which edges can be either added or deleted. We present an epidemic model on general time-varying graphs, characterize the final size of the epidemic, and conduct asymptotic analyses for the case of a static graph and the fully dynamic model. For the epidemic model on static graphs with randomly deleted edges, we show that the final size of the epidemic is at most a logarithmic factor larger than the number of contacts of a node, up to a constant factor. For the fully dynamic model, we show that the number of infected nodes approaches a constant factor of the number of contacts of a node, up to a logarithmic factor. Our analysis shows that in both cases, the final size of the epidemic is a constant factor times the number of contacts of a node. This constant factor can be large in realistic social networks, where most contacts are casual, and a small number of close contacts can cause a large outbreak. This analysis of general time-varying graphs may be of independent interest, as it provides a mechanism by which the number of contacts of a node can affect the final size of an epidemic. We perform numerical experiments to demonstrate the effectiveness of our theoretical analysis. In summary, we present general models for epidemic spreading on static and fully dynamic graphs, and characterize the final size of the epidemic. We conduct asymptotic analyses for the case of a static graph and the fully dynamic model, and present numerical experiments to demonstrate the effectiveness of our",
        "watermark_text": "In this paper , we study the Susceptible - Infected - Recovered ( SIR ) outbreak model on dynamic contact systems , in which nodes join and leave the network over time . We first introduce an outbreak model on general time - varying graphs , and characterize the last large of the outbreak .We then research two specific cases of this general theory : the first is the outbreak model on a static graph with randomly added edges at random times ; the second is the fully dynamic scenario , in which nodes can be either active or inactive , but no original vertices are removed . We present asymptotic assessments of the last size of these two models , and suggest that in both cases , the proportion of infected vertices becomes a constant factor of the number of ties of a node , up to a logarithmic factor .Finally , we present numerical studies to test the performance of our theoretical analysis . Infectious illness theories have been used to a broad variety of dynamical systems , notably period - changing graphs , in which edges are adding and removed over time 1 , 2 , as well as static graphs with randomly deleted edges 3 .Dynamic theories have been used to study peer effect in networks 4 , 5 , topological evolution of networks 6 , 7 , and epidemic spreading 8 , 9 . Some previous research has already analyzed general models on dynamic but random graphs 10 , 11 , and fully dynamic graphs 12 .We research general models on two forms of graphs : dynamic graphs with randomly deleted edges , and fully dynamic graphs in which edges can be either included or deleted . We create an outbreak model on general time - varying graphs , characterize the finished size of the outbreak , and conduct asymptotic assessments for the case of a static graph and the fully dynamic model .For the outbreak model on static graphs with randomly deleted edges , we find that the last size of the outbreak is at most a logarithmic factor bigger than the quantity of ties of a node , up to a constant factor . For the fully dynamic model , we find that the quantity of infected vertices approaches a constant factor of the quantity of links of a node , up to a logarithmic factor .Our study shows that in both cases , the last height of the outbreak is a constant factor twice the number of ties of a node . This continuous factor can be large in realistic social organizations , where most ties are casual , and a small number of close contacts can cause a large outbreak .This study of general time - varying graphs may be of independent interest , as it gives a process by which the quantity of visits of a node can affect the last extent of an outbreak . We undergo numerical studies to test the performance of our theoretical analysis .In summary , we present general scenarios for disease growing on dynamic and fully dynamic graphs , and characterize the final size of the outbreak . We conduct asymptotic assessments for the case of a static graph and the fully dynamic model , and report quantitative studies to test the performance of our",
        "rewrite_text": "In this paper, we explore the Susceptible-Infected-Recovered (SIR) outbreak model on dynamic contact systems, where nodes join and depart the network over time. First, we introduce an outbreak model on general time-varying graphs and delve into its final outbreak size.\n\nWe then delve into two specific instances of this general theory. The first is an outbreak model on a static graph with randomly added edges at various points in time. The second is a fully dynamic scenario where nodes can be either active or inactive, but no original vertices are removed. We present asymptotic assessments for both models and propose that, in both cases, the proportion of infected vertices stabilizes as a constant factor of the number of connections a node has, with a logarithmic factor involved.\n\nNumerical studies are conducted to validate our theoretical analysis. Infectious illness theories have found applications in various dynamic systems, specifically in period-changing graphs where edges are continuously added and removed, as well as static graphs with randomly deleted edges. Dynamic theories have been used to investigate peer influence in networks, the topological evolution of networks, and epidemic spreading. Previous research has analyzed general models on dynamic but random graphs and fully dynamic graphs.\n\nIn our research, we focus on general models applicable to two types of graphs: dynamic graphs with randomly deleted edges and fully dynamic graphs where edges can be both included and deleted. We construct an outbreak model on general time-varying graphs, delineate the final outbreak size, and conduct asymptotic assessments for both a static graph and the fully dynamic model.\n\nFor the outbreak model on static graphs with randomly deleted edges, we find that the final outbreak size may be at most a logarithmic factor larger than the number of connections a node has, up to a constant factor. For the fully dynamic model, we discover that the number of infected vertices approaches a constant multiple of the number of links a node possesses, with a logarithmic factor involved.\n\nOur study indicates that in both scenarios, the peak outbreak size is proportional to twice the number of connections a node has in realistic social settings. This constant factor can be significant in social networks where most connections are casual, and a few close contacts can lead to large outbreaks. This investigation into general time-varying graphs provides valuable insights as it establishes how the frequency of node visits can affect the final extent of an outbreak.\n\nIn summary, we present generalized scenarios for disease progression on dynamic and fully dynamic graphs, delineate the final outbreak size, conduct asymptotic assessments for static and fully dynamic models, and report quantitative studies to validate our theoretical analysis.",
        "ori-fast-z-score": -1.67769836820324,
        "water-fast-z-score": 8.930500890423009,
        "rewrite-fast-z-score": 3.2582704814996934
    },
    {
        "original_text": "In 2010, the radial velocity method was introduced to search for exoplanets by measuring the wobble of the host star caused by the planet’s gravity. In order to confirm that the detected wobble is indeed caused by the planet and not other causes such as noise, a confirmation method was developed by measuring the Doppler Shift of the emitted spectrum and thus detecting the motion of the planet around the central star. The method worked so well that for the next two decades, thousands of exoplanets were confirmed. However, in recent years a strange trend was noticed where the number of confirmed exoplanets was declining. Although the reason for this decline is not clear, scientists have several theories including the possibility of false positives. In this paper, I will explain false positives, why they are a problem, and present a new mathematical confirmation method that can reduce the number of false positives. I will also discuss how more observations are needed to confirm whether this decline in confirmed exoplanets is real or not.",
        "watermark_text": "In 2010 , the radial speed method was introduced to search for exoplanets by monitoring the wobble of the host star created by the planet ’ s gravity . In order to confirm that the reported wobble is indeed caused by the planet and not other causes such as noise , a confirmation procedure was developed by monitoring the Doppler Shift of the emitted spectrum and therefore detecting the movement of the planet around the main star .The method worked so good that for the subsequent two decades , thousands of exoplanets were confirmed . However , in recent months a odd trend was noticed where the proportion of reported exoplanets was declining .Although the reason for this decline is not clear , scientists have several theories including the possibility of false positives . In this paper , I will explain false positives , why they are a problem , and present a new numerical confirmation procedure that can limit the total of false positives .I will also discuss how more observations are needed to confirm whether this decline in verified exoplanets is actual or not .",
        "rewrite_text": "In 2010, the radial speed technique was introduced to detect exoplanets by monitoring the star's wobble induced by the planet's gravity. To verify that the reported wobble is indeed caused by the planet and not other factors like noise, a confirmation process was developed by observing the Doppler Shift in the emitted spectrum, thereby detecting the planet's movement around the primary star. This method proved so effective that thousands of exoplanets were subsequently confirmed over the next two decades.\n\nHowever, in recent months, a peculiar trend has been noticed where the reported number of exoplanets is decreasing. The reason for this decline remains unclear, but scientists have several hypotheses, one of them being false positives. In this paper, I will explain what false positives are, why they pose a problem, and present a new numerical confirmation procedure that can minimize the total occurrence of false positives. I will also discuss the need for more observations to determine whether this decline in verified exoplanets is real or not.",
        "ori-fast-z-score": 1.5011106998930268,
        "water-fast-z-score": 6.0448772146786025,
        "rewrite-fast-z-score": 3.719924439802217
    },
    {
        "original_text": "A widely accepted and broadly used method to clean a dirty floor is a gravity method. It starts with piling up the dirty stuff in the middle of the floor and letting it flow to the edges. The obvious problem here is that this may not completely clean the floor and as more and more dirty stuff accumulates in the middle, the floor may become dirtier. This article develops an alternative method which is to start with spraying some cleaner around the floor and let it spread around by gravitational force. The article then develops a probabilistic framework to analyze this method. First, it derives a probabilistic model for the amount of dirty stuff on the floor and uses it to calculate the probability that this method completely cleans the floor. Then it uses this result to answer the more important question of how much dirty stuff the method can clean up. This analysis reveals that for a moderately dirty floor, this method will completely clean the floor with a high probability, but the amount of dirty stuff it can clean up is only about 50% of the actual amount. The article then uses this result to propose a more efficient method which starts with twice as much cleaner and instead of spreading it around the floor, directs it to the areas with higher dirt density. This method can completely clean the floor and also clean up almost all the dirty stuff.",
        "watermark_text": "A widely accepted and generally used method to clean a dirty floor is a gravity method . It began with piling up the dirty stuff in the mid of the floor and allowing it spill to the edges .The obvious problem here is that this might not totally clean the floor and as more and more filthy stuff accumulates in the middle , the floor could grow dirtier . This page develops an alternative method which is to start with spraying some cleaner around the floor and allow it spread around by gravitational pressure .The section then develops a probabilistic approach to analyze this process . First , it derives a probabilistic description for the quantity of filthy stuff on the floor and using it to estimate the probability that this process totally cleans the floor .Then it using this effect to respond the more essential problem of how many filthy stuff the method can clean up . This evaluation suggests that for a moderately dirty floor , this method will completely clean the floor with a high chance , but the quantity of filthy stuff it can clean up is only about 50 % of the actual amount .The article then uses this effect to propose a more efficient approach which starts with twice as much cleaner and instead of spreading it around the floor , focuses it to the regions with higher dirt density . This method can fully wash the floor and also wipe up nearly all the dirty stuff .",
        "rewrite_text": "A commonly employed and widely utilized method for cleaning a soiled floor involves the utilization of gravity. This process begins by piling up the dirt in the center of the floor, allowing it to spill towards the edges. However, a notable concern with this approach is that it may not thoroughly clean the floor, and as more dirt accumulates in the center, the floor may become even dirtier.\n\nThis page presents an alternative cleaning technique. It starts with spraying a cleaner around the floor, allowing it to spread via gravitational pressure. The subsequent section develops a probabilistic approach to analyze this process. Initially, it formulates a probabilistic description for the amount of dirt on the floor and uses it to estimate the likelihood that this process will result in a complete floor clean. This estimation is then utilized to address the more crucial question of how much dirt this method can effectively remove.\n\nAccording to this evaluation, for a moderately soiled floor, there is a high chance that this method will result in a complete floor clean. However, the amount of dirt it can clean up is only approximately 50% of the actual amount.\n\nThe article then proposes a more efficient approach, starting with twice as much cleaner. Instead of spreading it evenly across the floor, it focuses on areas with higher dirt density. This refined method can effectively wash the entire floor and remove nearly all of the dirt.",
        "ori-fast-z-score": -0.944911182523068,
        "water-fast-z-score": 6.86726233920026,
        "rewrite-fast-z-score": 0.2822162605150792
    },
    {
        "original_text": "Modern nanotechnology enables precise fabrication of various kinds of nanometer-scale structures, which have been employed to investigate the interactions of electromagnetic waves with subwavelength structures. In this paper, we investigate the interactions of electromagnetic waves with metallic and semiconducting nanospheres. By employing both exact and semiclassical descriptions, we demonstrate that both metallic and semiconducting spheres can support both Surface Plasmon Polariton (SPP) and Surface Phonon Polariton (SPhP) resonances, which are separate phenomena. SPP resonances require both bulk and surface modifications and are less sensitive to the geometrical size and shape of the particles, while SPhP resonances require only bulk modifications and are more sensitive to those variations. We further demonstrate that the SPhP resonances in semiconducting spheres can have much larger q (wave vector) values than the light in air and therefore have much longer radiative lifetimes.",
        "watermark_text": "Modern nanotechnology allows accurate fabrication of several kinds of nanometer - scale structures , which have been employed to examine the interactions of electromagnetic waves with subwavelength systems . In this paper , we investigate the interactions of electromagnetic radiation with metallic and semiconducting nanospheres .By using both exact and semiclassical descriptions , we prove that both metallic and semiconducting objects can support both Surface Plasmon Polariton ( SPP ) and Surface Phonon Polariton ( SPhP ) resonances , which are separate phenomena . SPP resonances need both bulk and surface modifications and are less sensitive to the geometrical size and shape of the molecules , while SPhP resonances need only bulk modifications and are more sensitive to those differences .We further show that the SPhP resonances in semiconducting objects can have far larger q ( wave vector ) values than the light in air and therefore have far longer radiative lifetimes .",
        "rewrite_text": "In modern nanotechnology, it is possible to precisely fabricate numerous nanometer-scale structures, which have been utilized to study the interactions of electromagnetic waves with subwavelength systems. This paper delves into the interactions between electromagnetic radiation and metallic as well as semiconducting nanospheres. Through the utilization of both exact and semiclassical methodologies, we establish that both metallic and semiconducting objects can support distinct Surface Plasmon Polariton (SPP) and Surface Phonon Polariton (SPhP) resonances. These resonances are separate phenomena with distinct requirements. SPP resonances necessitate both bulk and surface modifications, exhibiting a lesser dependence on the geometric size and shape of the molecules. Conversely, SPhP resonances require only bulk modifications and are more sensitive to these variations. Furthermore, our research demonstrates that SPhP resonances in semiconducting objects can exhibit significantly higher q (wave vector) values than those in air, resulting in longer radiative lifetimes.",
        "ori-fast-z-score": 1.0886621079036347,
        "water-fast-z-score": 5.443310539518174,
        "rewrite-fast-z-score": 1.9867985355975657
    },
    {
        "original_text": "A collective state of the odd-mass nuclei is described in the framework of the Interacting Vector Boson Model (IVBM). In the frame work of this model the nucleus is viewed as a condensate of interacting vector bosons. The boson fields are associated with their individual quasi-particle states. Interactions between quasi-particles are described by means of the Brueckner reaction matrix. The model accounts for the ground state band structure and first excitation bands in many even-even as well as in odd-mass nuclei with high accuracy. The interaction between the vector bosons gives rise to the monopole and dipole phonon modes. These phonon modes generate the systematic patterns in the even-even and odd-mass nuclei spectra. In particular, the high-K electric transition operator excites the vibrational (phonon) states in the even-even nuclei and the wobbling vibration (phonon) states in the odd-mass nuclei. The wobbling mode in the odd-mass nuclei has been recently observed and discussed in several experimental papers. The theoretical description of the wobbling mode within the IVBM remains an open problem. In this work we report the theoretical analysis of the wobbling mode based on the IVBM approach and predict the excitation energy and the transition probability for the first excited state of several odd-mass nuclei in the vicinity of the closed shell N=Z nuclei. The calculated excitation energies are compared with the corresponding experimental data. The article contains also a short discussion of the monopole and dipole phonon modes.",
        "watermark_text": "A collective state of the odd - weight electrons is characterized in the framework of the Interacting Vector Boson Model ( IVBM ) . In the frame work of this description the nucleus is viewed as a condensate of interacting vector bosons .The boson fields are identified with their individual quasi - particle states . Interactions between quasi - particles are explained by means of the Brueckner reaction matrix .The model accounts for the ground state band structure and first excitation bands in large even - even as well as in odd - mass nuclei with high clarity . The interaction between the vector bosons giving rise to the monopole and dipole phonon modes .These phonon modes produce the systematic patterns in the odd - even and odd - mass molecules spectra . In particular , the high - K electric transition operator excites the vibrational ( phonon ) states in the odd - even nuclei and the wobbling vibration ( phonon ) states in the odd - mass particles .The wobbling mode in the odd - weight clusters has been recently detected and discussed in multiple experimental journals . The conceptual explanation of the wobbling mechanism within the IVBM remains an open topic .In this research we publish the theoretical analysis of the wobbling mode based on the IVBM approach and predict the excitation energy and the shift probability for the first excited state of several odd - mass nuclei in the vicinity of the shut shell N = Z nuclei . The measured excitation energies are compared with the associated experimental evidence .The essay contains also a brief discussion of the monopole and dipole phonon frequencies .",
        "rewrite_text": "The collective state of odd-weight electrons is defined within the framework of the Interacting Vector Boson Model (IVBM). Within this description, the nucleus is perceived as a condensed state of interacting vector bosons. The boson fields are associated with their respective quasi-particle states. The interactions between these quasi-particles are elucidated through the Brueckner reaction matrix. This model effectively accounts for the ground state band structure and the first excitation bands in both large even-even and odd-mass nuclei.\n\nThe interaction between the vector bosons leads to the emergence of monopole and dipole phonon modes. These phonon modes generate systematic patterns in the spectra of odd-even and odd-mass molecules. Specifically, the high-K electric transition operator stimulates vibrational (phonon) states in odd-even nuclei and wobbling vibration (phonon) states in odd-mass particles. The recent detection and discussion of the wobbling mode in odd-weight clusters have been extensively reported in multiple experimental journals.\n\nIn this research, we present a theoretical analysis of the wobbling mode based on the IVBM approach. We predict the excitation energy and shift probability for the first excited state of several odd-mass nuclei near the shut shell N=Z nuclei. The measured excitation energies are compared with available experimental data. The essay also includes a brief discussion on the frequencies of monopole and dipole phonons.",
        "ori-fast-z-score": -1.7417271443536015,
        "water-fast-z-score": 3.941803537221309,
        "rewrite-fast-z-score": 1.1441551070947107
    },
    {
        "original_text": "Mass and temperature of the TWA 7 debris disk were measured using Herschel and Spitzer observations. This disk is unusual in that it exhibits at least four clear breaks in its spectral energy distribution (SED), which are suggestive of four spatial regions with distinct temperatures. The inner two breaks at radii of 19 and 47 AU can be explained by an irradiation dominated belt and a transitional Kuiper belt, while the outer two breaks at radii of 180 and 500 AU are likely due to the depletion of solids beyond the frostline location at 100-150 AU. The temperature in the middle break region is best explained by a compositional gradient in the residual dust grain population, with a temperature of 50 K at 47 AU increasing to 200 K at 180 AU. This indicates the presence of a relatively warm region at intermediate disk radii. The detection of these distinct regions with different temperatures is only possible with Herschel and Spitzer observations, as previous far-IR observations with the Infrared Astronomical Satellite had compromised the ability to discern the multi-temperature nature of the disk.",
        "watermark_text": "Mass and heat of the TWA 7 debris disk were calculated using Herschel and Spitzer experiments . This disk is unique in that it displays at least four clear cracks in its spectral power distribution ( SED ) , which are suggestive of four spatial regions with distinct temperatures .The outer two broke at radii of 19 and 47 AU can be described by an irradiation dominated belt and a transitional Kuiper belt , while the inner two broke at radii of 180 and 500 AU are likely due to the depletion of solids beyond the frostline location at 100 - 150 AU . The temperature in the mid break region is better understood by a compositional gradient in the residual dust grain population , with a temperature of 50 K at 47 AU increasing to 200 K at 180 AU .This implies the presence of a fairly warm area at intermediate disk radii . The observation of these distinct regions with varying temperatures is only available with Herschel and Spitzer surveys , as earlier far - IR measurements with the Infrared Astronomical Satellite had impaired the ability to discern the multi - temperature nature of the disk .",
        "rewrite_text": "The mass and heat of the TWA 7 debris disk were determined through the utilization of Herschel and Spitzer experiments. This particular disk stands out due to its spectral power distribution (SED) exhibiting at least four distinct cracks, indicating four spatial regions with varying temperatures. The outer two cracks, occurring at radii of 19 and 47 AU, can be characterized by an irradiation-dominated belt and a transitional Kuiper belt, respectively. Meanwhile, the inner cracks at radii of 180 and 500 AU are likely attributed to the depletion of solids beyond the frostline location spanning from 100 to 150 AU. A better understanding of the temperature in the mid-break region is achieved through a compositional gradient in the population of residual dust grains, with a temperature range from 50 K at 47 AU to 200 K at 180 AU. This suggests the existence of a relatively warm area within the intermediate disk radii. The observation of these diverse regions with varying temperatures is exclusively possible through Herschel and Spitzer surveys, as previous far-IR measurements with the Infrared Astronomical Satellite had limited the ability to discern the multi-temperature nature of the disk.",
        "ori-fast-z-score": -1.5882027766319677,
        "water-fast-z-score": 4.520269441183293,
        "rewrite-fast-z-score": 2.2941573387056176
    },
    {
        "original_text": "Inhomogeneous universe models with dust and dark energy were considered. The Einstein universe with a cosmological constant was compared with the Friedman-Lemaître-Robertson-Walker universe with a dust and dark energy components. The coincidence problem of dark matter and dark energy was also analyzed. A new quantum cosmology equation, which has a non-singular solution that describes a universe with a cosmological constant, is proposed. It was shown that this equation can be obtained from a Schroedinger wave mechanics perspective. It was shown that this equation has a solution that can act as both dark matter and dark energy. Inhomogeneous universe models with dust and dark energy were considered. The Einstein universe with a cosmological constant was compared with the Friedman-Lemaître-Robertson-Walker universe with a dust and dark energy components. The coincidence problem of dark matter and dark energy was also analyzed. A new quantum cosmology equation, which has a non-singular solution that describes a universe with a cosmological constant, is proposed. It was shown that this equation can be obtained from a Schroedinger wave mechanics perspective. It was shown that this equation has a solution that can act as both dark matter and dark energy. The Einstein cosmology with a cosmological constant (ΛCDM) model has the following Friedmann-Lemaître-Robertson-Walker (FLRW) parallel with a dust and dark energy components: ΛCDM model has a dust and dark energy components, the coincidence problem of dark matter and dark energy, and the singularity problem. The ΛCDM model from a dynamical system perspective, versus the ΛCDM model from a phase-space perspective. The ΛCDM model from a dynamical system perspective has a stable de Sitter solution and a stable FLRW solution, but from a phase-space perspective, it has a non-singular solution with a cosmological constant. The coincidence problem of dark matter and dark energy can be solved by introducing a new particle, named wave mechanics dark energy, and the ΛCDM model with this dark energy has a stable FLRW solution with a non-singularity. It is proposed that this non-singular solution can be obtained from a Schroedinger wave mechanics perspective. It is shown that this wave mechanics dark energy has a solution that can act as both dark matter and dark energy. The Einstein cosmology with a cosmological constant (ΛCDM) model has the following Friedmann-Lemaître-Robertson-Walker (FLRW) parallel with a dust and dark energy components: ΛCDM model has a dust and dark energy components, the coincidence problem of dark matter and dark energy, and the singularity problem. The ΛCDM model from a dynamical system perspective, versus the ΛCDM model from a phase-space perspective. The ΛCDM model from a dynamical system perspective has a stable de Sitter solution and a",
        "watermark_text": "Inhomogeneous universe versions with dust and dark energy were used . The Einstein universe with a cosmological constant was contrasted with the Friedman - Lemaître - Robertson - Walker universe with a dust and dark energy components .The coincidence problem of dark matter and dark energy was also analyzed . A second quantum cosmology formula , which has a non - singular solve that describes a world with a cosmological constant , is proposed .It was shown that this equation can be obtained from a Schroedinger wave theory perspective . It was shown that this equation has a solution that can work as both dark matter and dark energy .Inhomogeneous universe versions with dust and dark energy were used . The Einstein universe with a cosmological constant was contrasted with the Friedman - Lemaître - Robertson - Walker universe with a dust and dark energy components .The coincidence problem of dark matter and dark energy was also analyzed . A second quantum cosmology formula , which has a non - singular solve that describes a world with a cosmological constant , is proposed .It was shown that this equation can be obtained from a Schroedinger wave theory perspective . It was shown that this equation has a solution that can work as both dark matter and dark energy .The Einstein cosmology with a cosmological constant ( ΛCDM ) model has the following Friedmann - Lemaître - Robertson - Walker ( FLRW ) parallel with a dust and dark energy components : ΛCDM model has a dust and dark energy components , the coincidence problem of bright matter and dark energy , and the singularity problem . The ΛCDM model from a dynamical system perspective , versus the ΛCDM model from a phase - space view .The ΛCDM theory from a dynamical system perspective has a stable de Sitter solution and a stable FLRW solution , but from a phase - space view , it has a non - singular solve with a cosmological constant . The coincidence problem of light matter and dark energy can be answered by introducing a new particle , named wave mechanics black energy , and the ΛCDM theory with this light energy has a stable FLRW solution with a non - singularity .It is proposed that this non - singular solution can be obtained from a Schroedinger wave theory perspective . It is seen that this wave mechanics black energy has a solution that can act as both dark matter and dark energy .The Einstein cosmology with a cosmological constant ( ΛCDM ) model has the following Friedmann - Lemaître - Robertson - Walker ( FLRW ) parallel with a dust and dark energy components : ΛCDM model has a dust and dark energy components , the coincidence problem of bright matter and dark energy , and the singularity problem . The ΛCDM model from a dynamical system perspective , versus the ΛCDM model from a phase - space view .The ΛCDM theory from a dynamical system perspective has a stable de Sitter solving and a",
        "rewrite_text": "Rephrased in English:\n\nIn studies of cosmology, various inhomogeneous versions of the universe were employed, including those with dust and dark energy. Comparisons were drawn between the Einstein universe with a cosmological constant and the Friedman-Lemaître-Robertson-Walker (FLRW) universe, which also incorporates dust and dark energy components. The issue of coincidence regarding dark matter and dark energy was also analyzed. A second quantum cosmology formula was proposed, featuring a non-singular solution that describes a universe with a cosmological constant. It was demonstrated that this equation can be derived from the perspective of a Schroedinger wave theory. Furthermore, it was shown that this equation provides a solution that can function as both dark matter and dark energy.\n\nRegarding the ΛCDM model of Einstein's cosmology with a cosmological constant, it is paralleled by the FLRW model, which includes both dust and dark energy components. The model faces challenges such as the coincidence problem of bright matter and dark energy, as well as the singularity problem. From a dynamical system perspective, the ΛCDM theory offers a stable de Sitter solution and a stable FLRW solution. However, when viewed from a phase-space perspective, it presents a non-singular solution with a cosmological constant. To address the coincidence problem of light matter and dark energy, a new particle named \"wave mechanics black energy\" is introduced. With this light energy included in the ΛCDM theory, it offers a stable FLRW solution without a singularity. It is suggested that this non-singular solution can be derived from the perspective of a Schroedinger wave theory. It becomes evident that this wave mechanics black energy provides a solution that can serve as both dark matter and dark energy.",
        "ori-fast-z-score": 1.7460757394239454,
        "water-fast-z-score": 6.355715691503161,
        "rewrite-fast-z-score": 2.32379000772445
    },
    {
        "original_text": "Recent ALMA observations of the young transition disk HD 98800 reveal a near-infrared spectrum consistent with the presence of gas and small dust within about 2AU of the host star, providing strong evidence for the existence of an inner transitional disk. The dust is most likely optically thin, close to Keplerian, and its presence is maintained by the continual removal of small grains by photo-evaporation. The total dust mass in the inner disk is less than 1% of the Minimum Mass Solar Nebula. The gas is much more tenuous, with a volume filling factor of about 10-5, and is depleted in elements heavier than helium, perhaps resulting from the action of gravitational forces exerted by the innermost planets. The implications of this architecture for the evolution of intermediate-mass stars remain to be determined, but HD 98800 is an ideal object for future investigations with increased sensitivity and higher angular resolution.",
        "watermark_text": "Recent ALMA observations of the young transition disk HD 98800 indicate a far - infrared spectrum consistent with the presence of gas and tiny dust within about 2AU of the host star , providing good evidence for the existence of an inner transitional disk . The dust is most likely optically thin , close to Keplerian , and its presence is maintained by the continual extraction of large grains by photo - evaporation .The total dust mass in the inner disk is less than 1 % of the Minimum Mass Solar Nebula . The gas is much more tenuous , with a volume filling factor of about 10 - 5 , and is exhausted in compounds heavier than helium , perhaps due from the action of gravitational pressures exerted by the innermost planets .The implications of this architecture for the evolution of intermediate - weight stars still to be determined , but HD 98800 is an suitable object for future investigations with increased sensitivity and better angular resolution .",
        "rewrite_text": "Recent observations of the young transition disk HD 98800 by ALMA have revealed a far-infrared spectrum that is consistent with the presence of gas and tiny dust particles within approximately 2 astronomical units (AU) of the central star. This provides strong evidence for the existence of an inner transitional disk. The dust is likely to be optically thin and nearly Keplerian, sustained by the continuous photo-evaporation of larger grains. The total dust mass in the inner disk comprises less than 1% of the Minimum Mass Solar Nebula.\n\nThe gas, on the other hand, is much more sparse, with a volume filling factor of approximately 10^-5. It is composed of compounds heavier than helium, possibly due to the influence of gravitational pressures exerted by the innermost planets. The significance of this architectural configuration for the evolution of intermediate-weight stars remains to be determined. Nonetheless, HD 98800 serves as an excellent target for future investigations with enhanced sensitivity and improved angular resolution.",
        "ori-fast-z-score": 1.0834726777719228,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": 2.264554068289191
    },
    {
        "original_text": "The existence of massive neutrinos is one of the most important discoveries in modern astrophysics and particle physics. Among the three known species of neutrinos, the neutral current interactions of electron neutrinos provide a unique tool to study the interior of galaxies and clusters of galaxies, because electron neutrinos scatter off electrons in the nuclei of nuclei, virtually without interacting with the electrons themselves. This forward scattering process can be observed through the Doppler-shifted weak emission of electron neutrinos, which results in a spectrum that is exponentially suppressed at low energies. The measured intensity of this so-called core-collapse neutrino burst, first detected by the SNO detector in 1998, can be explained only by postulating the existence of nearly massless neutral Majorana neutrinos, which were invented by Ettore Majorana in 1937 and for which evidence was first found forty years later by the KATRIN experiment. In 2006, the first direct experimental evidence for theneutrino s Majorana character was obtained in the scientific journal  Nature  by the Super-Kamiokande detector, when the observation of a tiny but finite probability for the neutrino to be its own antiparticle was reported. Despite its importance, the nature of dark matter is one of the biggest unsolved problems in cosmology and particle physics. The favored model for dark matter, weakly interacting massive particles (WIMPS), can interact via the weak force and therefore are expected to produce the observed signal in conventional neutrino telescopes. However, recent observations of galaxy clusters with satellites such as Planck or the Dark Energy Survey have reduced the maximum possible cross-section of WIMPS to within an order of magnitude of the weak force coupling constant. Since this is orders of magnitude smaller than the neutrino scattering cross-section, this possibility for dark matter has been excluded. Neutrino oscillations can easily accommodate the measured solar and atmospheric neutrino anomalies, and thus neutrinos offer an attractive alternative to dark matter. If the neutrino is a Majorana particle, the gravitational potential of a cluster of galaxies can capture a finite amount of electron neutrinos, thereby solving the dark matter problem. The process would be analogous to how a lightbulb shines light when electrical current flows through it: The cluster of galaxies acts as a giant lightbulb, and the neutrino beam produces the observed 511-keV radiation. Given the lack of evidence for non-baryonic dark matter and its weak interaction cross-section, we suggest that the observed neutrino burst is actually produced by electron neutrinos scattered off dark matter particles, which might account for approximately 30% of the total dark matter density of the universe. We provide an example of how the existence of this weakly interacting massive particle dark matter could be established or disproved by measuring the spectrum of the neutrino burst from a future core-collapse supernova in our galaxy.",
        "watermark_text": "The existence of large neutrinos is one of the most important findings in modern astrophysics and electron physics . Among the three known species of neutrinos , the neutral current interactions of electron neutrinos serve a unique technique to study the interior of stars and clusters of stars , because electron neutrinos scatter off electrons in the nuclei of nuclei , virtually without interacting with the electrons themselves .This forward scattering process can be viewed through the Doppler - shifted weak absorption of electron neutrinos , which results in a spectrum that is exponentially suppressed at low energies . The measured strength of this so - called core - collapse neutrino burst , best detected by the SNO detector in 1998 , can be described only by postulating the existence of almost massless neutral Majorana neutrinos , which were developed by Ettore Majorana in 1937 and for which evidence was first discovered forty decades later by the KATRIN experiment .In 2006 , the first direct empirical evidence for theneutrino s Majorana character was obtained in the science journal Nature by the Super - Kamiokande detector , when the observation of a small but finite probability for the neutrino to be its own antiparticle was reported . Despite its significance , the nature of bright matter is one of the biggest unsolved issues in cosmology and electron physics .The favored theory for black material , mildly interacting massive particles ( WIMPS ) , can interact via the weak force and therefore are expected to produce the seen signal in standard neutrino telescopes . However , recent observations of galaxy clusters with spacecraft such as Planck or the Dark Energy Survey have reduced the maximum possible cross - section of WIMPS to within an order of magnitude of the strong force interaction factor .Since this is orders of magnitude smaller than the neutrino scattering cross - area , this chance for black material has been omitted . Neutrino oscillations can easily handle the measured solar and atmospheric neutrino anomalies , and therefore neutrinos offer an interesting alternative to dark matter .If the neutrino is a Majorana particle , the gravitational potential of a cluster of stars can trap a finite quantity of electron neutrinos , thereby solving the dark matter problem . The method might be analogous to how a lightbulb shines light when electrical current flows through it : The cluster of stars acts as a huge lightbulb , and the neutrino light produces the observed 511 - keV radiation .Given the lack of evidence for non - baryonic black material and its strong interaction cross - section , we suggest that the known neutrino burst is actually produced by electron neutrinos scattered off dark matter particles , which would account for roughly 30 % of the total dark matter density of the universe . We illustrate an instance of how the existence of this weakly interacting massive object dark matter could be determined or disproved by analyzing the spectrum of the neutrino burst from a future core - collapse supernova in our universe .",
        "rewrite_text": "The discovery of large neutrinos holds a pivotal position in modern astrophysics and electron physics. Among the three recognized neutrino species, the unique technique for studying the interiors of stars and clusters of stars involves the neutral current interactions of electron neutrinos. This occurs as electron neutrinos scatter off electrons in the nuclei, effectively bypassing direct interaction with the electrons themselves. This forward scattering process can be observed through the Doppler-shifted weak absorption of electron neutrinos, resulting in a spectrum that experiences exponential suppression at low energies.\n\nIn 2006, the Super-Kamiokande detector published empirical evidence in the prestigious science journal Nature, confirming the existence of a small but finite probability for a neutrino to possess its own antiparticle characteristics. This serves as the first direct evidence for the Majorana nature of neutrinos. Despite its significance, determining the nature of dark matter remains one of the greatest unanswered questions in cosmology and electron physics.\n\nThe favored theory for dark matter, mildly interacting massive particles (WIMPS), can interact via the weak force and thus produce signals in standard neutrino telescopes. However, recent observations from spacecraft like Planck and the Dark Energy Survey have narrowed down the maximum possible cross-section of WIMPS to be within an order of magnitude of the strong force interaction factor. This is significantly smaller than the neutrino scattering cross-section, leading to the exclusion of this possibility for dark matter.\n\nNeutrino oscillations effectively explain measured solar and atmospheric neutrino anomalies, making neutrinos an intriguing alternative to dark matter. If a neutrino is a Majorana particle, the gravitational potential of a star cluster can trap a finite quantity of electron neutrinos, potentially resolving the dark matter problem. This concept mirrors how a light bulb emits light when an electrical current passes through it; a cluster of stars acts as a massive light bulb, and the neutrino light produces the observed 511-keV radiation.\n\nIn light of the absence of evidence for non-baryonic dark matter and its strong interaction cross-section, we propose that the observed neutrino burst is actually produced by electron neutrinos scattered off dark matter particles. This accounts for approximately 30% of the total dark matter density in the universe. To verify or disprove this hypothesis, we suggest analyzing the spectrum of a future core-collapse supernova neutrino burst in our universe to determine or disprove the existence of this weakly interacting massive object dark matter.",
        "ori-fast-z-score": 1.1648208067068038,
        "water-fast-z-score": 9.023334236725555,
        "rewrite-fast-z-score": 3.837612894400988
    },
    {
        "original_text": "The complexity of many real-world networks has been shown to be well captured by graphs possessing a random layout. Such random graphs are commonly generated using one of two paradigms: the configuration model, which assumes a uniform probability distribution for all edge establishment, and the community model, which allows for the generation of arbitrary networks with pre-specified properties. However, different real-world networks may exhibit very different distributions of topological characteristics. Therefore, it is of interest to consider how the distributions of these characteristics may vary across different random graph models. Here we focus on two such characteristics, the size of the largest planar matching and the largest planar subgraph, both of which have well-understood algorithmic techniques that allow for their exact determination for arbitrary random graphs. By employing a variety of random graph models, we find that the size of the largest planar matching exhibits a power-law distribution for every model considered, while the largest planar subgraph exhibits a power-law distribution in some models and exhibits a different, more exponential-like, distribution in others. Notably, we find that the power-law exponents for the size of the largest planar matching and the largest planar subgraph are closely correlated with the exponent associated with the degree distribution of the graph. This suggests that the tail behavior of the distribution of these characteristics is most strongly influenced by the presence or absence of a wide variety of high-degree vertices within the random graph model.",
        "watermark_text": "The complexity of large real - world networks has been shown to be well captured by graphs possessing a random layout . Such random graphs are often produced using one of two paradigms : the configuration description , which assumes a consistent likelihood distribution for all edge establishment , and the neighborhood theory , which allows for the generation of arbitrary networks with pre - defined properties .However , different real - time systems sometimes display very different distributions of topological traits . Therefore , it is of interest to consider how the distributions of these characteristics may differ across different random graph models .Here we focus on two such traits , the size of the greatest planar matching and the greatest planar subgraph , both of which have better - understood algorithmic techniques that enable for their exact determination for arbitrary random graphs . By using a variety of random graph models , we find that the size of the greatest planar matching displays a power - law distribution for every model considered , while the smallest planar subgraph displays a power - law distribution in some models and exhibits a distinct , more exponential - like , distribution in others .Notably , we find that the power - law exponents for the size of the greatest planar matching and the greatest planar subgraph are tightly correlated with the exponent identified with the degree distribution of the graph . This shows that the tail behavior of the distribution of these characteristics is most strongly influenced by the presence or lack of a broad variety of high - degree vertices within the random graph model .",
        "rewrite_text": "The intricacy of vast real-world networks has been effectively represented by graphs with a random layout. These random graphs are frequently generated through two primary paradigms: the configuration description, which assumes a uniform probability distribution for all edge formations, and the neighborhood theory, enabling the creation of arbitrary networks with predefined attributes. However, different real-time systems occasionally exhibit distinct distributions of topological features. Therefore, it is of interest to explore how these feature distributions may differ across various random graph models.\n\nIn this study, we focus on two specific traits: the size of the largest planar matching and the size of the largest planar subgraph. Both traits have well-established algorithmic techniques that facilitate their precise determination in arbitrary random graphs. By utilizing a range of random graph models, we observe that the size of the largest planar matching follows a power-law distribution across all considered models. Meanwhile, the smallest planar subgraph exhibits a power-law distribution in some models and displays a distinct, more exponential-like distribution in others.\n\nSignificantly, we discover a strong correlation between the power-law exponents of the largest planar matching and subgraph sizes and the exponent associated with the graph's degree distribution. This indicates that the tail behavior of these feature distributions is most influenced by the presence or absence of a diverse range of high-degree vertices within the random graph model.",
        "ori-fast-z-score": 0.6260990336999411,
        "water-fast-z-score": 5.701573160798387,
        "rewrite-fast-z-score": 1.2675004445952593
    },
    {
        "original_text": "A paper introducing viability, limitations and potential sources of error of using gyrochronology to determine stellar ages. The viability of the method is demonstrated using a sample of 48 stars for which rotational period, T_{c}, and surface age are known from other means. The limitation of the method, expected from the theory behind it, is also demonstrated using the sample. The limitations of the method arise from the dependence of T_{c} on composition, which results in a degeneracy between age and metallicity. The potential sources of error are also discussed. The sample is from the IAC-80 visual binary star catalogue (IAC-80), consisting of 48 G-type stars for which rotation period (T_{c}), effective temperature, surface age and metallicity are known. The projected equatorial rotation rates (VΩ/sin(i)) were obtained from T_{c}, which were calculated using the rotational period and temperature from the Infrared Flux Method. Using the Babel code, an estimate of the gyrochronological age was made. The surface age was compared with the true age to determine if the gyrochronology is viable for determining ages of G-type stars. The reliability of the method is demonstrated using a two-sample t-test, the proportion of constant stars, the Scholz radius test and the gyrochronology - chromchronology plot. Gyrochronology is shown to be viable for determining the ages of G-type stars, with a mean error of -2.2% indicating that for most stars the gyrochronological age is within 10% of the true age. The limitations of the method arise from the dependence of T_{c} on composition, which results in a degeneracy between age and metallicity. The gyrochronology - chromchronology plot demonstrates the limitations of the method by showing T_{c} versus surface age with metallicity over-plotted as a separate series.",
        "watermark_text": "A paper introducing viability , difficulties and possible sources of mistake of using gyrochronology to predict stars years . The viability of the method is demonstrated using a sample of 48 stars for which rotational year , T _ { c } , and surface age are known from other ways .The disadvantage of the method , expected from the theoretical behind it , is also demonstrated using the sample . The weaknesses of the method arise from the dependence of T _ { c } on composition , which results in a degeneracy between age and metallicity .The likely sources of mistake are also discussed . The sample is from the IAC - 80 visual binary star catalogue ( IAC - 80 ) , consisting of 48 G - class stars for which rotation history ( T _ { c } ) , effective heat , surface age and metallicity are known .The estimated equatorial rotation rates ( VΩ / sin ( i ) ) were obtained from T _ { c } , which were calculated using the rotational history and heat from the Infrared Flux Method . Using the Babel code , an assessment of the gyrochronological age was produced .The surface age was measured with the true age to see if the gyrochronology is plausible for determining ages of G - class stars . The reliability of the method is demonstrated using a two - sample t - test , the proportion of fixed stars , the Scholz distance test and the gyrochronology - chromchronology plot .Gyrochronology is demonstrated to be successful for determining the periods of G - class stars , with a mean error of - 2 . 2 % showing that for most stars the gyrochronological age is within 10 % of the true period . The weaknesses of the method arise from the dependence of T _ { c } on composition , which results in a degeneracy between age and metallicity .The gyrochronology - chromchronology plot demonstrates the limitations of the method by showing T _ { c } versus surface age with metallicity over - plotted as a distinct series .",
        "rewrite_text": "A study is presented regarding the feasibility, challenges, and potential error sources of utilizing gyrochronology to forecast the ages of stars. This feasibility is exemplified through a sample of 48 stars where their rotational year, Tc, and surface age are known from other sources. The theoretical drawbacks of the method are also demonstrated using this sample.\n\nThe method's weaknesses stem from the dependence of Tc on composition, leading to a confusion between age and metallicity. Discussed are the likely sources of error. The sample comes from the IAC-80 visual binary star catalog, consisting of 48 G-class stars with known rotation history (Tc), effective heat, surface age, and metallicity. The estimated equatorial rotation rates (VΩ/sin(i)) were derived from Tc, which was calculated using rotational history and heat from the Infrared Flux Method. With the help of the Babel code, a gyrochronological age assessment was produced.\n\nThe surface age was measured against the true age to verify the feasibility of gyrochronology in determining G-class star ages. The reliability of the method is further underscored through a two-sample t-test, the proportion of fixed stars, the Scholz distance test, and a gyrochronology-chromchronology plot. It is demonstrated that gyrochronology is effective for determining the periods of G-class stars, with a mean error of -2.2%, indicating that for most stars, the gyrochronological age closely aligns with the true period, differing by no more than 10%.\n\nHowever, the method's limitations arise from the dependence of Tc on composition, resulting in an age-metallicity degeneracy. The gyrochronology-chromchronology plot illustrates these limitations by presenting Tc versus surface age, with metallicity plotted as a separate series.",
        "ori-fast-z-score": 0.7071067811865475,
        "water-fast-z-score": 6.6551738206208535,
        "rewrite-fast-z-score": 1.8569533817705188
    },
    {
        "original_text": "A massive supercluster at a redshift of z=0.9 containing thousands of galaxy clusters and dozens of massive galaxy clusters has been discovered in the data from the UKIDSS Deep Extragalactic Survey (DXS). The total mass of the cluster system is estimated to be (2.4 ± 0.6) × 10 13 h -2 _{ -0.8}%_{ +0.9} /M/s, where h is the Hubble parameter, M/s is megaparsecs -1, and the quoted uncertainties are statistical and systematic respectively. The supercluster is a good example of large-scale structure at a redshift of z = 1.0 -1.5 and provides strong evidence for the existence of major filaments in the young universe. step A massive supercluster at a redshift of z=0.9 containing thousands of galaxy clusters and dozens of massive galaxy clusters has been discovered in the data from the UKIDSS Deep Extragalactic Survey (DXS). The total mass of the cluster system is estimated to be (2.4 ± 0.6) × 10 13 h -2 _{ -0.8}%_{ +0.9} /M/s, where h is the Hubble parameter, M/s is megaparsecs -1, and the quoted uncertainties are statistical and systematic respectively.",
        "watermark_text": "A massive supercluster at a redshift of z = 0 . 9 containing hundreds of galaxy clusters and dozens of large galaxy clusters has been detected in the information from the UKIDSS Deep Extragalactic Survey ( DXS ) . The total mass of the cluster system is predicted to be ( 2 . 4 ± 0 . 6 ) × 10 13 g - 2 _ { - 0 . 8 } % _ { + 0 . 9 } / M / s , where h is the Hubble parameter , M / s is megaparsecs - 1 , and the quoted uncertainties are statistical and widespread respectively .The supercluster is a good example of large - scale organization at a redshift of z = 1 . 0 - 1 . 5 and provides solid support for the existence of large filaments in the young galaxy . step A massive supercluster at a redshift of z = 0 . 9 containing hundreds of galaxy clusters and dozens of large galaxy clusters has been detected in the information from the UKIDSS Deep Extragalactic Survey ( DXS ) .The total mass of the cluster system is predicted to be ( 2 . 4 ± 0 . 6 ) × 10 13 h - 2 _ { - 0 . 8 } % _ { + 0 . 9 } / M / s , where h is the Hubble parameter , M / s is megaparsecs - 1 , and the quoted uncertainties are statistical and widespread respectively .",
        "rewrite_text": "A large supercluster with a redshift of z=0.9 has been discovered through the UKIDSS Deep Extragalactic Survey (DXS) data, which comprises of hundreds of galaxy clusters and numerous large clusters. The estimated total mass of this cluster system is (2.4 ± 0.6) × 10¹³ g (-2_{-0.8}% / M/s), with h representing the Hubble parameter, M/s indicating megaparsecs-1, and the stated uncertainties being both statistical and extensive. This supercluster serves as a compelling example of large-scale structure at a redshift range of z=1.0 to 1.5, providing strong evidence for the existence of vast filaments in young galaxies.",
        "ori-fast-z-score": -2.141798680385621,
        "water-fast-z-score": 3.14970394174356,
        "rewrite-fast-z-score": 0.31622776601683794
    },
    {
        "original_text": "Nova Geminorum 1912 was the first observed gravitational lens, making it the first known example of strong gravitational lensing. Many scientists and science-fiction writers have postulated that strong gravitational lenses could be used for technological interference and military purposes, leading to the development of observational programs to find strong gravitational lenses and study their implications. In this paper, we examine the scientific and cultural significance of Nova Geminorum 1912 and the development of the field of gravitational lensing. We begin by providing a brief overview of gravitational lensing, including a discussion of critical curves and caustics. Next, we describe how strong gravitational lensing was observed for the first time, and how the observation of Nova Geminorum 1912 led to the discovery and study of gravitational lensing as a distinct phenomenon. We also examine how the observational discovery of Nova Geminorum 1912 led to the development of super-gravity and the wider science fiction genre s interest in strong gravitational lensing as a weapon. Finally, we discuss how subsequent observations of gravitational lenses have led to new understandings of the phenomenon, including the first observations of gravitational waves and the determination of Hubble s constant.",
        "watermark_text": "Nova Geminorum 1912 was the first observed gravity lens , making it the first recorded example of strong gravitational lensing . Many scientists and scientific - fiction authors have postulated that strong gravitational lenses could be used for technological interference and army purposes , leading to the development of observational programs to find strong gravitational lenses and study their implications .In this paper , we investigate the science and literary relevance of Nova Geminorum 1912 and the development of the field of gravitational lensing . We begin by offering a brief overview of gravitational lensing , including a consideration of critical curves and caustics .Next , we explain how strong gravitational lensing was seen for the first time , and how the observation of Nova Geminorum 1912 led to the discovery and investigation of gravitational lensing as a distinct concept . We also investigate how the observational discovery of Nova Geminorum 1912 led to the development of super - gravity and the broader physics fiction genre s interest in heavy gravitational lensing as a weapon .Finally , we talk how subsequent observations of gravitational lenses have led to novel understandings of the phenomenon , notably the first measurements of gravitational waves and the determination of Hubble s constant .",
        "rewrite_text": "Nova Geminorum 1912 marked the first ever recorded observation of a gravity lens, paving the way for the documentation of strong gravitational lensing phenomena. A multitude of scientists and science fiction authors have speculated on the potential applications of strong gravitational lenses in technology and military contexts, which has resulted in the development of observation programs aimed at discovering and studying their ramifications. In this paper, we delve into the scientific and literary significance of Nova Geminorum 1912 and the progression of gravitational lensing research.\n\nInitially, we provide a concise overview of gravitational lensing, encompassing a discussion on critical curves and caustics. Following this, we explain how the first instance of strong gravitational lensing was observed, and how the observation of Nova Geminorum 1912 propelled the recognition and exploration of gravitational lensing as a distinct concept. We also explore how the observation of Nova Geminorum 1912 influenced the advancement of super-gravity theories and how it sparked a wider interest in the physics fiction genre for heavy gravitational lensing as a potential weapon.\n\nFinally, we discuss how subsequent observations of gravitational lenses have led to new understandings of this phenomenon, particularly the first measurements of gravitational waves and the determination of Hubble's constant.",
        "ori-fast-z-score": 2.177598558933893,
        "water-fast-z-score": 7.0136644745597945,
        "rewrite-fast-z-score": 2.681695240272863
    },
    {
        "original_text": "The “missing satellites problem” refers to the observation that, while there are more than 20 billion galaxies in the observable universe, only a small fraction (~105) of these are actively engaged in star formation. Astronomers expect that galaxy clusters, groups, and clusters of galaxies, often referred to as groups and clusters of galaxies, should exist given the abundance of dark matter in the universe, but instead only a small fraction of groups and clusters show signs of galaxy activity. It has been suggested that insufficiently dense environments are preventing galaxies from forming and rendering the missing satellites problem. We examine this claim by measuring the galaxy stellar masses in groups selected from the SDSS and GMASS clusters surveys. We show that while the stellar mass distributions of central and satellite galaxies in these groups are statistically equivalent, the stellar mass distributions of centrals and satellites in pairs are statistically different with a p-value < 10-5. We conclude that galaxy groups do in fact exist in the universe, and that dynamical friction is not sufficiently effective in sufficiently dense environments at transforming galaxies from a star-forming to a non-star forming state.",
        "watermark_text": "The “ missing satellites situation ” refers to the observation that , while there are more than 20 billion galaxies in the observable universe , only a small fraction ( ~ 105 ) of these are aggressively engaged in star formation . Astronomers expect that galaxy clusters , organizations , and clusters of galaxies , sometimes termed to as bands and clusters of galaxies , should exist given the density of bright matter in the universe , but instead only a small fraction of groups and clusters show traces of galaxy action .It has been proposed that insufficiently dense settings are preventing galaxies from creating and rendering the missing satellites problem . We assess this claim by analyzing the galaxy stellar masses in groups picked from the SDSS and GMASS clusters surveys .We see that while the stellar mass distributions of central and satellite galaxies in these groups are statistically equivalent , the stellar mass distributions of centrals and satellites in pairs are statistically different with a p - value < 10 - 5 . We suggest that galaxy groups do in indeed arise in the universe , and that dynamical friction is not sufficiently efficient in sufficiently thin environments at transforming galaxies from a galaxy - creating to a un - star producing state .",
        "rewrite_text": "The \"missing satellite situation\" refers to the observation that, despite the presence of over 20 billion galaxies in the observable universe, only a small number (~105) are actively engaged in star formation. Astronomers predict that galaxy clusters and organizations, often referred to as bands and clusters of galaxies, should exist given the density of luminous matter in the universe. However, only a small fraction of these groups and clusters exhibit signs of galaxy activity. It has been proposed that the insufficient density of environments may be preventing galaxies from forming, leading to the issue of missing satellites.\n\nTo assess this claim, we analyze the stellar masses of galaxies within groups selected from the SDSS and GMASS cluster surveys. We observe that while the stellar mass distributions of central and satellite galaxies in these groups are statistically similar, the distributions of central and satellite galaxies in pairs exhibit significant statistical differences with a p-value less than 10-5. We suggest that galaxy groups do indeed exist in the universe, and that dynamical friction is not effective enough in sparse environments to transform galaxies from a galaxy-creating to a non-star-producing state.",
        "ori-fast-z-score": -0.6060915267313265,
        "water-fast-z-score": 4.848732213850612,
        "rewrite-fast-z-score": 1.212183053462653
    },
    {
        "original_text": "The Stillinger-Lovett (SL)1 Sum Rules provide a general proof method for the nonexistence of an electronic density N(r) = 0 within a radius r of a nucleus. We derive an analogous sum rule for the case of an infinite plane of uniform positive charge, and show that it applies more generally to any uniform structure of positive charge. This Sum Rule for the Two-Dimensional Jellium (2DJ) may be used to prove the existence of various phases of the 2D electron gas (2DEG). We present several such applications, along with supporting numerical calculations and examples. 1 Stillinger, F. H. & Lovett, R. W. Sum Rules for the Two-Dimensional Coulomb Gas. Journal of Chemical Physics, 1967, 47, 1398-1403. A general proof method for the nonexistence of an electronic density N(r) = 0 within a radius r of a nucleus. We derive an analogous sum rule for the case of an infinite plane of uniform positive charge, and show that it applies more generally to any uniform structure of positive charge. This Sum Rule for the Two-Dimensional Jellium (2DJ) may be used to prove the existence of various phases of the 2D electron gas (2DEG). We present several such applications, along with supporting numerical calculations and examples. Electronic structure calculations for metals and semiconductors commonly involve solving the Schrodinger equation for the electronic structure in the presence of a mean-field approximation to the Coulomb potential created by the nuclei. This potential, in turn, may be calculated using the charge density derived from the electron density. The charge density is most often expressed in terms of a set of orthonormal wavefunctions {ψn(r)} which satisfy the Schrodinger equation. When a particle is represented by such a set of basis functions, the corresponding electron density is given by where the ξn are the orbital coefficients and are simply integrals of the product of the basis functions and their conjugate function. The electronic properties of metals and semiconductors may be characterized by the density of electronic states N(E) = ∫ρ(E,r)2d3x, the sum of which over all energies E constitutes the electronic specific heat. The electronic structure that leads to these properties can itself be expressed in terms of a set of energies {En} and corresponding wavefunctions {ψn(r)}. Given an electronic system with energy spectrum {En} and corresponding wavefunctions {ψn}, we may compute N(E) using the integral where the integral extends over all wavevectors. To make contact with the more common presentation in terms of the wavefunctions {ψn}, we introduce the coefficients βn which are given by the expression The standard argument for the nonexistence of an electronic density N(r) = 0 within a radius r of",
        "watermark_text": "The Stillinger - Lovett ( SL ) 1 Sum Rules give a general proving technique for the nonexistence of an electronic density N ( r ) = 0 within a diameter r of a nucleus . We derive an analogous sum rule for the case of an infinite plane of uniform positive charge , and know that it applies more generally to any uniform structure of positive charge .This Sum Rule for the Two - Dimensional Jellium ( 2DJ ) may be used to test the existence of several stages of the 2D electron gas ( 2DEG ) . We see several such applications , along with supporting numerical measurements and examples .1 Stillinger, F. H. & Lovett, R. W. Sum Rules for the Two-Dimensional Coulomb Gas.Journal of Chemical Physics, 1967, 47, 1398-1403.A general proving technique for the nonexistence of an electronic density N ( r ) = 0 within a diameter r of a nucleus . We derive an analogous sum rule for the case of an endless plane of uniform positive charge , and find that it applies more generally to any uniform structure of positive charge .This Sum Rule for the Two - Dimensional Jellium ( 2DJ ) may be used to test the existence of several stages of the 2D electron gas ( 2DEG ) . We see several such applications , along with supporting numerical measurements and examples .Electronic structure calculations for metals and semiconductors commonly include studying the Schrodinger equation for the electronic configuration in the presence of a mean - field approximation to the Coulomb potential caused by the nuclei . This potential , in turn , might be determined using the charge density derived from the electron density .The charge density is most usually expressed in terms of a setting of orthonormal wavefunctions { ψn ( r ) } which satisfy the Schrodinger equation . When a particle is depicted by such a setting of basis functions , the analogous electron density is given by where the ξn are the orbital variables and are simply integrals of the product of the basis functions and their conjugate function .The electronic properties of metals and semiconductors may be summarized by the density of electronic states N ( E ) = [UNK] ( E , r ) 2d3x , the sum of which over all energies E constitutes the electronic specific heat . The electronic structure that leads to these properties can itself be expressed in terms of a set of energies { En } and corresponding wavefunctions { ψn ( r ) } .Given an electronic system with energy spectrum { En } and corresponding wavefunctions { ψn } , we may compute N ( E ) using the integral where the integral spreads over all wavevectors . To give contact with the more common description in terms of the wavefunctions { ψn } , we provide the coefficients βn which are given by the expression The basic assertion for the nonexistence of an electronic density N ( r ) = 0 within a diameter r of",
        "rewrite_text": "The Stillinger-Lovett (SL) 1 Sum Rules offer a versatile proof technique to establish the non-existence of an electronic density N(r) = 0 within a nuclear diameter 'r'. We have derived a comparable sum rule for an infinite plane of uniformly charged positive particles, and it is widely applicable to any uniformly charged structure. This Sum Rule for the Two-Dimensional Jellium (2DJ) can be utilized to test the existence of various stages of the 2D electron gas (2DEG). We observe multiple applications of this, supported by numerical measurements and examples.\n\nA general technique for proving the absence of an electronic density at a specific distance from a nucleus is provided. We have developed a similar sum rule for an endless plane of uniformly charged positive particles, which is applicable more broadly to any uniform charge structure. This Sum Rule for 2D Jellium (2DJ) can be used to test the various stages of the 2D electron gas (2DEG). We see several practical applications of this, along with supporting numerical data and examples.\n\nIn the calculation of electronic structures for metals and semiconductors, it is common to study the Schrodinger equation under the mean-field approximation of the Coulomb potential caused by nuclei. This potential can be determined using the charge density derived from the electron density. The charge density is typically expressed in terms of a set of orthonormal wavefunctions {ψn(r)} that satisfy the Schrodinger equation. When a particle is represented by such a set of basis functions, the corresponding electron density is determined by integrating the product of the basis functions and their conjugate.\n\nThe electronic properties of metals and semiconductors can be summarized by the density of electronic states N(E), which is expressed as an integral over all energies E, contributing to the electronic specific heat. The electronic structure leading to these properties can itself be described in terms of a set of energies {En} and corresponding wavefunctions {ψn(r)}. Given an electronic system with energy spectrum {En} and corresponding wavefunctions {ψn}, we can compute N(E) by integrating over all wave vectors. To provide a connection to the more common description using wavefunctions {ψn}, we provide the coefficients βn, defined by a specific expression.\n\nThe fundamental assertion is that an electronic density N(r) = 0 cannot exist within a diameter 'r' of a nucleus or any other similar structure.",
        "ori-fast-z-score": -0.6616931598844269,
        "water-fast-z-score": 4.399413450640599,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "In the standard model (SM) with its minimal particle content, the electroweak chiral Lagrangian (EWCLL) is the appropriate low-energy effective field theory (LEFT) to describe the strong interactions of the weak bosons. By construction, the precision tests of the EWCLL allow to put strong constraints on new physics (NP) scenarios. Here, we use the latest precision measurements from the LHC and electroweak observables to put strong constraints on a generic Higgs-portal type model. We find that the model is strongly constrained for a large range of the model parameter space. In particular, the model predicts charged scalar boson with a mass below 530 GeV at the 95% CL, and the invisible decay width of the Z boson is expected to be larger than 0.2 MeV at the 95% CL. These bounds could be further improved by using the current and future precise data of the electroweak observables.",
        "watermark_text": "In the standard description ( SM ) with its minimal particle content , the electroweak chiral Lagrangian ( EWCLL ) is the appropriate low - energy effective field model ( LEFT ) to explain the strong coupling of the weak bosons . By construction , the precision tests of the EWCLL allow to put powerful restrictions on new physics ( NP ) scenarios .Here , we using the latest precision observations from the LHC and electroweak observables to put powerful restrictions on a generic Higgs - portal class model . We see that the model is strongly constrained for a large scope of the model parameter space .In particular , the model predicts charged scalar boson with a mass below 530 GeV at the 95 % CL , and the unseen decay width of the Z boson is expected to be larger than 0 . 2 MeV at the 95 % CL . These bounds could be further increased by using the present and future exact data of the electroweak observables .",
        "rewrite_text": "In the conventional framework (Standard Model, SM), with its most basic particle composition, the Electroweak Chiral Lagrangian (EWCLL) serves as the appropriate low-energy effective field theory (LEFT) to elucidate the robust coupling of weak bosons. By design, rigorous tests of the EWCLL enable us to establish stringent constraints on potential new physical scenarios (NP). In this context, we employ the latest precision observations from the Large Hadron Collider (LHC) and electroweak observables to constrain a generic Higgs-portal class model. The model is found to be heavily constrained across a wide range of its parameter space. Specifically, the model predicts the existence of a charged scalar boson with a mass below 530 GeV at the 95% confidence level (CL), and the unobserved decay width of the Z boson is expected to be greater than 0.2 MeV at the same confidence level. These bounds can be further enhanced by utilizing current and future precise data from electroweak observables.",
        "ori-fast-z-score": 1.9205531989934397,
        "water-fast-z-score": 5.842005842008763,
        "rewrite-fast-z-score": 0.5163977794943222
    },
    {
        "original_text": "We present observations of the nearby blazar PKS 2155-304 performed between May 2005 and July 2005 in the near-infrared (NIR) and optical bands. NIR data were obtained with the 10.4m GEMINI South Telescope, while optical data were obtained with the Nordic Optical Telescope and the William Herschel Telescope. We also present contemporaneous data in other bands (X-rays, MeV, GeV) from the Whipple, HEGRA, and CATobservatories. Based on our observations, we suggest that in the optical band PKS 2155-304 was in a high state in early 2005, possibly reaching a flux density of 400 mJy at m_R in April 2005. The NIR flux density was relatively stable during this period, with no strong variations. Around May 20-25 2005 we detected a sharp rise in both the optical and NIR flux, reaching a peak in June 2005. The optical spectral index rapidly hardened in this period, possibly reaching a value of α=3.2. We hypothesize that this sudden brightening in PKS 2155-304 was caused by a rapid enhancement in the external radiation field density (as a result of the eruptinging Galaxy M33) shining through the relativistic jet of PKS 2155-304. We also observe a fast decrease in the X-ray, MeV, and GeV flux densities in the period May-July 2005. We suggest that this decrease was caused by the jet entering a new region with a weaker magnetic field, perhaps due to the increased transparency of the jet caused by the enhanced external radiation field density.",
        "watermark_text": "We present observations of the nearby blazar PKS 2155 - 304 completed between May 2005 and July 2005 in the near - infrared ( NIR ) and optical bands . NIR data were obtained with the 10 . 4m GEMINI South Telescope , while optical data were obtained with the Nordic Optical Telescope and the William Herschel Telescope .We additionally offer contemporaneous evidence in other bands ( X - rays , MeV , GeV ) from the Whipple , HEGRA , and CATobservatories . Based on our observations , we indicate that in the optical band PKS 2155 - 304 was in a high state in early 2005 , possibly reaching a flux concentration of 400 mJy at m _ R in April 2005 .The NIR flux concentration was generally stable during this time , with no strong variations . Around May 20 - 25 2005 we reported a sharp rise in both the optical and NIR density , reaching a peak in June 2005 .The optical spectral index gradually hardened in this time , possibly reaching a value of α = 3 . 2 . We hypothesize that this sudden brightening in PKS 2155 - 304 was due by a rapid enhancement in the external emission field concentration ( as a result of the eruptinging Galaxy M33 ) shining through the relativistic jet of PKS 2155 - 304 .We also observe a rapid decrease in the X - ray , MeV , and GeV flux densities in the period May - July 2005 . We suggest that this reduction was due by the jet reaching a new area with a weaker magnetic current , perhaps due to the increased transparency of the jet due by the enhanced external emission field concentration .",
        "rewrite_text": "We present observations of the nearby blazar PKS 2155-304 that were carried out between May and July 2005 in the near-infrared (NIR) and optical wavebands. NIR data were secured through the utilization of the 10.4m GEMINI South Telescope, while optical data were obtained via the Nordic Optical Telescope and the William Herschel Telescope. Additionally, we provide concurrent evidence from other wavebands such as X-rays, MeV, and GeV from the Whipple, HEGRA, and CAT observatories.\n\nBased on our observations, we indicate that PKS 2155-304 exhibited a high state in the optical waveband early in 2005, potentially reaching a flux concentration of 400 mJy at m_R in April 2005. The NIR flux concentration remained generally stable during this period, without any significant variations. Specifically, between May 20th and 25th 2005, we reported a sharp increase in both the optical and NIR intensities, peaking in June 2005. The optical spectral index gradually hardened during this time, potentially reaching a value of α = 3.2.\n\nWe hypothesize that the sudden brightening of PKS 2155-304 was likely caused by a rapid enhancement in the external emission field concentration. This enhancement, possibly arising from the erupting Galaxy M33, illuminated the relativistic jet of PKS 2155-304. Furthermore, we observed a rapid decrease in the X-ray, MeV, and GeV flux densities during the period from May to July 2005. We suggest that this reduction was due to the jet reaching a new region with a weaker magnetic current, perhaps attributed to the increased transparency of the jet resulting from the enhanced external emission field concentration.",
        "ori-fast-z-score": 0.10482848367219183,
        "water-fast-z-score": 6.1137368096588665,
        "rewrite-fast-z-score": 3.794733192202055
    },
    {
        "original_text": "In a previous paper1, we described the scaling relations between the properties of galaxies and their central supermassive black holes (SMBHs), that have been found to hold at redshifts up to z=0.89. Here, we present a similar analysis of the local (z=0.36) population, and show that these scaling relations are still very much in force there as well. Specifically, the SMBH masses and the bulge parameters (e.g. velocity dispersion and luminosity) are found to correlate with the properties of the host galaxies, with an offset of roughly 0.75 dex between the SMBH and bulge masses. In addition, the active black holes are found to be hosted in almost exclusively by early-type galaxies, as opposed to the situation at higher redshifts. This may have significant consequences for the growth and evolution of supermassive black holes in the Universe.",
        "watermark_text": "In a earlier paper1 , we reported the scaling relations between the properties of galaxies and their central supermassive black holes ( SMBHs ) , that have been shown to hold at redshifts up to z = 0 . 89 . Here , we present a analogous analysis of the local ( z = 0 . 36 ) population , and find that these scaling relations are still very lot in force there as well .Specifically , the SMBH masses and the bulge parameters ( e . g . speed dispersion and luminosity ) are found to correlate with the properties of the host galaxies , with an offset of roughly 0 . 75 dex between the SMBH and bulge masses .In addition , the active black holes are found to be hosted in nearly exclusively by earliest - class objects , as opposed to the situation at higher redshifts . This might have considerable consequences for the development and evolution of supermassive black holes in the Universe .",
        "rewrite_text": "In a previous study1, we disclosed the scaling relationships between the characteristics of galaxies and their central supermassive black holes (SMBHs) that have been verified to be valid at redshift values up to z = 0.89. Now, we conduct a similar analysis on the local population at z = 0.36 and discover that these scaling relationships still hold firmly there. Specifically, we found that the masses of SMBHs and bulge parameters such as velocity dispersion and luminosity are correlated with the properties of their host galaxies. There is an approximately 0.75 dex offset between the masses of SMBHs and bulges. Furthermore, we found that active black holes are predominantly hosted by early-class objects, contrasting with the situation at higher redshifts. This could potentially have significant implications for the development and evolution of supermassive black holes in the universe.",
        "ori-fast-z-score": -0.13245323570650439,
        "water-fast-z-score": 4.543441112511214,
        "rewrite-fast-z-score": 1.4832396974191326
    },
    {
        "original_text": "The Monitor project is an M-type eclipsing binary system located in the ONC region, the nearest star cluster to the Earth. The components of the system are designated JW 380a (A1) and JW 380a (A2) and are most likely both directly observable by the Hubbell telescope at the Fred Lawrence Whipple Observatory on Mount Hopkins, Arizona, USA. The Monitor system was chosen as one of the targets for theMonitor project is an M-type eclipsing binary system located in the ONC region, the nearest star cluster to the Earth. The components of the system are designated JW 380a (A1) and JW 380a (A2) and are most likely both directly observable by the Hubbell telescope at the Fred Lawrence Whipple Observatory on Mount Hopkins, Arizona, USA. The Monitor system was chosen as one of the targets for the James Web Space Telescope (JWST) IR grism spectrograph, due to its position close to the ONC and the expected brightness of the components. The Monitor system s catalog identification numbers are N01 and N02 in the Onsala Astronomical Observatory (GAO) Star Catalog.",
        "watermark_text": "The Monitor project is an M - class eclipsing binary system located in the ONC region , the nearest star cluster to the Earth . The components of the system are named JW 380a ( A1 ) and JW 380a ( A2 ) and are most likely both directly observable by the Hubbell telescope at the Fred Lawrence Whipple Observatory on Mount Hopkins , Arizona , USA .The Monitor system was chosen as one of the targets for theMonitor project is an M - class eclipsing binary system located in the ONC region , the nearest star cluster to the Earth . The components of the system are named JW 380a ( A1 ) and JW 380a ( A2 ) and are most likely both directly observable by the Hubbell telescope at the Fred Lawrence Whipple Observatory on Mount Hopkins , Arizona , USA .The Monitor system was chosen as one of the targets for the James Web Space Telescope ( JWST ) IR grism spectrograph , owing to its position next to the ONC and the expected brightness of the parts . The Monitor system s catalog registration digits are N01 and N02 in the Onsala Astronomical Observatory ( GAO ) Star Catalog .",
        "rewrite_text": "The Monitor project presents an M-class eclipsing binary system situated in the ONC region, which is the closest star cluster to our planet Earth. This system comprises of two components named JW 380a (A1) and JW 380a (A2), both of which are highly likely to be directly observable by the Hubbell telescope at the Fred Lawrence Whipple Observatory located on Mount Hopkins in Arizona, USA. The selection of the Monitor system as a target is due to its proximity to the ONC and the anticipated brightness of its components, making it a prime candidate for the James Web Space Telescope's (JWST) IR grism spectrograph. Additionally, the catalog registration numbers for this system in the Onsala Astronomical Observatory's (GAO) Star Catalog are N01 and N02.",
        "ori-fast-z-score": 1.7693034738587656,
        "water-fast-z-score": 4.024922359499621,
        "rewrite-fast-z-score": 1.150792911137501
    },
    {
        "original_text": "In this paper we prove a general version of the Kalman--Yakubovich--Popov (KYP) inequality for discrete-time passive systems described by linear difference equations. We provide a simple and unified approach to the classical KYP inequality, including results for discrete-time LTI systems, discrete-time linear time-varying (LTV) systems, and continuous-time LTV systems. We also extend the KYP inequality to the case of partial measurements. Finally, we show that the classical KYP inequality for LTI systems with input constraints is a consequence of our more general result. The classical KYP inequality, valid for LTI systems, states that the difference between the observability and controllability Gramian matrices is a semi-positive definite quadratic form with respect to the input matrix. This result has been extended to LTI systems with input constraints (i.e., with zero inputs) by various authors. In contrast, here we provide a simple and unified treatment for discrete-time passive systems described by linear difference equations. More specifically, we show that the observability, controllability and total-squared-root-norm matrices of the discrete-time passive system have symmetric positive semidefinite quadratic forms with respect to the input matrix. As a consequence, we obtain a general version of the KYP inequality for discrete-time passive systems. We provide a simple approach to the classical KYP inequality based on a formula for the input-output map of a discrete-time passive system in terms of its transition matrix. In this way we obtain a new proof of this result for discrete-time LTI systems, discrete-time LTV systems and continuous-time LTV systems. As an example, we also prove the classical KYP inequality for LTI systems with input constraints. Finally, we apply our results to design an observer for a linear time-invariant system with bounded gain, based on the discrete-time output error, and to establish a discrete-time analogue of the Continuous-time LQR Theorem for discrete-time passive systems.",
        "watermark_text": "In this paper we prove a general version of the Kalman - - Yakubovich - - Popov ( KYP ) theorem for linear - time passive networks characterized by linear difference equations . We provide a simple and integrated method to the classical KYP inequality , comprising results for discrete - time LTI systems , discrete - time linear time - changing ( LTV ) networks , and continuous - time LTV systems .We additionally apply the KYP inequality to the case of partial measurements . Finally , we prove that the classical KYP inequality for LTI systems with input requirements is a outcome of our more general result .The classical KYP inequality , valid for LTI systems , states that the difference between the observability and controllability Gramian matrices is a semi - positive definite quadratic form with regard to the input matrix . This result has been extended to LTI environments with output requirements ( i . e . , with zero inputs ) by various authors .In contrast , here we provide a simple and integrated treatment for discrete - time passive networks presented by linear difference equations . More specifically , we prove that the observability , controllability and total - squared - root - norm matrices of the discrete - time passive system have symmetric positive semidefinite quadratic forms with regard to the input matrix .As a consequence , we obtain a general version of the KYP inequality for discrete - time passive systems . We get a simple interpretation to the classical KYP inequality based on a formula for the input - output map of a discrete - time passive system in terms of its transition matrix .In this way we obtain a new proof of this result for linear - time LTI systems , discrete - time LTV systems and continuous - time LTV systems . As an instance , we also prove the classical KYP inequality for LTI systems with input requirements .Finally , we apply our findings to model an observer for a linear time - invariant system with bounded gain , based on the discrete - time output error , and to develop a discrete - time analogue of the Continuous - time LQR Theorem for linear - time passive systems .",
        "rewrite_text": "In this study, we present a generalized version of the Kalman-Yakubovich-Popov (KYP) theorem for linear-time passive networks defined by linear difference equations. We introduce a straightforward and comprehensive approach to the traditional KYP inequality, encompassing results for discrete-time Linear Time-Invariant (LTI) systems, discrete-time Linear Time-Varying (LTV) networks, and continuous-time LTV systems.\n\nWe further apply the KYP inequality to the scenario of partial measurements. Our findings demonstrate that the classical KYP inequality for LTI systems with input constraints is a special case of our more general result. The classic KYP inequality, valid for LTI systems, states that the difference between the observability and controllability Gramian matrices forms a semi-positive definite quadratic form in relation to the input matrix. This result has been extended by various authors to LTI environments with output requirements, such as when considering zero inputs.\n\nIn contrast to these existing studies, we offer a straightforward and integrated treatment for discrete-time passive networks described by linear difference equations. Specifically, we prove that the observability, controllability, and total squared root norm matrices of discrete-time passive systems exhibit symmetric positive semidefinite quadratic forms in relation to the input matrix. Consequently, we obtain a generalized version of the KYP inequality for discrete-time passive systems. This provides a straightforward interpretation of the classical KYP inequality based on a formula for the input-output map of a discrete-time passive system in terms of its transition matrix.\n\nThis approach enables us to provide a new proof for this result in linear-time LTI systems, discrete-time LTV systems, and continuous-time LTV systems. As an example, we also verify the classical KYP inequality for LTI systems with input requirements. Furthermore, our findings are applied to model an observer for a linear time-invariant system with bounded gain based on discrete-time output error. We also develop a discrete-time analog of the Continuous-time LQR Theorem for linear-time passive systems.",
        "ori-fast-z-score": 0.22677868380553634,
        "water-fast-z-score": 4.941858488253086,
        "rewrite-fast-z-score": 2.2613350843332274
    },
    {
        "original_text": "The superconducting state in doped copper oxides is often separated into two categories, high-temperature superconductivity (above ~100 K) and low temperature superconductivity (T_c<10 K). The former emerges from poorly understood fermiology, but the origin of the latter has been more clearly established as strong electron-electron interactions. Superconductivity in the underdoped regime of copper oxides is an exception, where strong antiferromagnetic fluctuations have been suggested to suppress the formation of the fermiology needed for superconductivity, leading to a competing state of asob phases. Here we report inelastic neutron scattering measurements of magnetic fluctuations in n-type Ca(Fe(1-x)Co_x)_2As_2, an isoelectronically substituted superconductor with T_c of 10 K that has been suggested to exhibit fermiology-driven superconductivity. We find that the doping evolution of the low-energy resonance peak reflects that of the superconductivity, rather than that of the antiferromagnetism. The results suggest a competition between fermiology-driven superconductivity and asob phases is preempted by a competing state of incommensurate magnetic fluctuations, and raise the possibility that asob phases is the competing state of copper oxides above the pseudogap temperature T^*.",
        "watermark_text": "The superconducting state in doped copper oxides is often divided into two genres , large - temperature superconductivity ( above ~ 100 K ) and low heat superconductivity ( T _ c < 10 K ) . The former arises from weakly understood fermiology , but the origin of the latter has been more firmly established as powerful atom - atom bonding .Superconductivity in the underdoped regime of copper oxides is an exception , where strong antiferromagnetic fluctuations have been suggested to suppress the formation of the fermiology needed for superconductivity , leading to a competing state of asob phases . Here we report inelastic neutron scattering measurements of magnetic fluctuations in n - type Ca ( Fe ( 1 - x ) Co _ x ) _ 2As _ 2 , an isoelectronically substituted superconductor with T _ c of 10 K that has been suggested to exhibit fermiology - driven superconductivity .We see that the doping evolution of the small - energy resonance peak reflects that of the superconductivity , rather than that of the antiferromagnetism . The results propose a competition between fermiology - fueled superconductivity and asob phases is preempted by a competing state of incommensurate magnetic fluctuations , and raise the idea that asob phases is the competing state of copper oxides above the pseudogap temperature T ^ * .",
        "rewrite_text": "The superconducting state within doped copper oxides is frequently categorized into two types: high-temperature superconductivity (exceeding approximately 100 K) and low-temperature superconductivity (Tc < 10 K). The former is rooted in an imperfectly understood fermiology, whereas the latter has been more firmly established as being associated with strong atom-atom bonding. In the underdoped region of copper oxides, superconductivity stands out as an exception, where robust antiferromagnetic fluctuations are believed to hinder the formation of the necessary fermiology for superconductivity, leading to a state of competing asob phases.\n\nIn this study, we report inelastic neutron scattering measurements of magnetic fluctuations in the n-type Ca(Fe(1-x)Co_x)2As2, an isoelectronically substituted superconductor with a Tc of 10 K that is thought to exhibit fermiology-driven superconductivity. Our observations indicate that the evolution of the small-energy resonance peak with doping mirrors the progression of superconductivity, rather than antiferromagnetism. The findings suggest that a competing state of incommensurate magnetic fluctuations preempts a competition between fermiology-fueled superconductivity and asob phases. This raises the possibility that asob phases could be the competing state for copper oxides above the pseudogap temperature T*.",
        "ori-fast-z-score": -2.966954145484633,
        "water-fast-z-score": 1.1952286093343936,
        "rewrite-fast-z-score": -0.11704114719613057
    },
    {
        "original_text": "(65489) Ceto/Phorcys: A tidally-evolved binary Centaur is a target of interest for the next OSIRIS-REx mission, which is designed to gather samples for analysis on Earth. Ceto/Phorcys has a size of 16.2 km x 19.1 km and an albedo of ~0.5. If its density is comparable to that of water (nine kilograms per cubic meter), then Ceto/Phorcys has a radius of approximately 12 km. Given these dimensions and the currently-known orbital parameters, it will make several close approaches to Earth in the next few centuries, including a nominal approach with a distance of <0.15 AU in the year 2195. Masses, composition, and other physical characteristics will be discussed for Ceto and Phorcys, as well as their binary nature and evolution.",
        "watermark_text": "( 65489 ) Ceto / Phorcys : A tidally - altered binary Centaur is a target of interest for the new OSIRIS - REx mission , which is designed to gather specimens for study on Earth . Ceto / Phorcys has a length of 16 . 2 km x 19 . 1 km and an albedo of ~ 0 . 5 .If its volume is equal to that of water ( nine tonnes per cubic meter ) , then Ceto / Phorcys has a diameter of approximately 12 kilometres . Given these dimensions and the currently - recorded orbital characteristics , it will perform several close approaches to Earth in the last few centuries , including a nominal approach with a length of < 0 . 15 AU in the year 2195 .Masses , structure , and other material attributes will be examined for Ceto and Phorcys , as well as their binary nature and evolution .",
        "rewrite_text": "(65489) Ceto / Phorcys: A Centaur binary that has been altered by tides is a compelling target for the new OSIRIS-REx mission, which aims to collect specimens for study on Earth. Ceto/Phorcys measures 16.2 km by 19.1 km and has an albedo of approximately 0.5. If its volume is comparable to that of water (nine tonnes per cubic meter), it would indicate a diameter of roughly 12 kilometers. With these dimensions and recorded orbital characteristics, it is expected to make several close approaches to Earth in the upcoming centuries, including a typical approach with a distance of less than 0.15 AU in 2195. The masses, structure, and other material properties of Ceto and Phorcys, as well as their binary nature and evolution, will be thoroughly examined.",
        "ori-fast-z-score": -2.4285714285714284,
        "water-fast-z-score": 2.7142857142857144,
        "rewrite-fast-z-score": 1.1094003924504583
    },
    {
        "original_text": "Quantum spin liquid (QSL) states have been the subject of active research in recent years due to their possible relationship to the high-temperature superconductivity and chiral central spin model. Despite intensive studies, however, QSL states remain elusive in many materials. Here we report a deuterated inorganic kagome lattice material, Zn0.9Cu0.14-OD6Cl2, which exhibits zero magnetic entropy at low temperatures without long-range order, and instead, exhibits fractionalized excitations and a novel QSL state. Our results provide clear evidence for QSL states in two-dimensional quantum magnets and open the way to studies of their fundamental physics and potential applications. Introductions to new projects and personnel were made by the PI and Co-Pi respectively. An outline of the project was distributed to all participants. contributed to drafting of the project outline assisting in the recruiting of postdocs and students assisting in the coordination of joint training sessions organizing a training session on neutron scattering assist with data analysis of neutron scattering experiments writing the paper with input from other authors organizing reviews of the paper before submission assisting with the revisions of the paper following reviews written up in the final paper as one of the authors contributed research funding to support the project hosted members of the research team on their sabbatical to work on the project mentored postdocs and students on the project obtained release time from their regular research programs to work on the project oversaw the recruitment of a co-PI to help with administrative tasks and raise additional funding provided oversight of the additional funding raised and its allocation to the project provided supplemental funding to support personnel and equipment needed for the project provided office and conference space for the project wrote several communications including a progress report, a candidate grant for the European Union for follow-on funding, and the paper submitted to arXiv.org wrote a recruiting email for a postdoc helped with the proposal to hire a postdoc assisted in the recruiting of a co-PI assisted with the organization of the laboratory for the staff member assisted with the organization of the joint training sessions assisted with the organization of collaboration visits by students participated in laboratory meetings helped with manuscript revisions for the submitted paper helped with the revision of this review helped prepare this review overall, helped provide administrative and scientific guidance to the project mentored multiple postdocs and students on different parts of the project co-wrote several papers with people on the project hosted multiple postdocs and students on sabbatical to work on the project oversaw the raising of additional funding for the project other tasks related to running the project helped write recruiting emails for students and postdocs prepared the lab for multiple visitors including postdocs and students wrote communications for organizations funding the project organized lab meetings to discuss the research and scientific strategy performed background research on the project s scientific topic helped",
        "watermark_text": "Quantum spin liquid ( QSL ) states have been the subject of active research in recent years owing to their possible relationship to the high - temperature superconductivity and chiral central spin theory . Despite ongoing studies , however , QSL states maintain elusive in many materials .Here we study a deuterated inorganic kagome lattice material , Zn0 . 9Cu0 . 14 - OD6Cl2 , which possesses zero magnetic entropy at low temperatures without extended - range order , and instead , displays fractionalized excitations and a novel QSL state . Our results make strong evidence for QSL states in two - dimensional quantum magnets and open the way to examination of their fundamental theory and possible use .Introductions to new projects and personnel were made by the PI and Co - Pi respectively . An overview of the project was circulated to all members .contributed to drafting of the project outline helping in the recruiting of postdocs and students assisting in the organization of joint training meetings organizing a training session on particle absorption assist with data analysis of neutron scattering experiments wrote the paper with output from other researchers organizing reports of the paper before submission help with the revisions of the paper taking reviews written up in the last paper as one of the writers contributed research funding to support the project hosted members of the research team on their sabbatical to work on the project mentored postdocs and students on the project secured return time from their regular scientific programs to work on the project oversaw the hiring of a co - PI to assist with administrative responsibilities and raise extra funding assisted supervision of the additional money raised and its allocation to the project gave supplemental funding to support personnel and gear required for the project gave office and conference room for the project authored several communications including a development report , a candidate grant for the European Union for follow - on funding , and the paper delivered to arXiv . org writing a hiring email for a postdoc helped with the proposal to hire a postdoc aided in the recruiting of a co - PI help with the organization of the laboratory for the staff member assisted with the organization of the joint recruiting seminars assisted with the organization of collaboration appointments by students participated in laboratory meetings aided with copy revisions for the submitted paper aided with the revision of this review helped prepare this review overall , helped offer organizational and scientific guidance to the project mentored multiple postdocs and students on various parts of the project co - wrote many papers with people on the project hosted multiple postdocs and students on sabbatical to work on the project oversaw the raising of added funding for the project other tasks related to running the project assisted compose recruiting emails for students and postdocs prepared the laboratory for multiple tourists including postdocs and students wrote communications for organizations funding the project organized lab meetings to consider the research and scientific plan performed background research on the project s science subject helped",
        "rewrite_text": "Titled in English:\n\n\"Research on Quantum Spin Liquid (QSL) States\n\nIn recent years, QSL states have become a focal point of active investigation due to their potential connection to high-temperature superconductivity and chiral central spin theory. Despite ongoing studies, QSL states remain elusive in many materials. This study focuses on a deuterated inorganic kagome lattice material, Zn0.9Cu0.14-OD6Cl2, which exhibits zero magnetic entropy at low temperatures without extended-range order. Instead, it demonstrates fractionalized excitations and a novel QSL state. Our findings provide strong evidence for QSL states in two-dimensional quantum magnets, opening the door for further examination of their fundamental theory and potential applications.\n\nIntroductions and new project details were presented by the Principal Investigator (PI) and Co-PI, respectively. An overview of the project was circulated to all team members. Various tasks were contributed towards drafting the project outline, assisting in the recruitment of postdoctoral researchers and students, organizing joint training meetings, and preparing a training session on particle absorption. Data analysis for neutron scattering experiments was also included. The paper was written with contributions from other researchers, organizing reports before submission, and assisting with revisions. Reviews from previous papers were utilized, and one of the writers contributed to research funding to support the project.\n\nMembers of the research team were hosted on sabbatical to work on the project, and postdoctoral researchers and students were mentored on various aspects of the project. Funding was secured to support the project, and a co-PI was hired to assist with administrative responsibilities and raise additional funding. Supervision was provided for the additional funds raised and their allocation to the project. Supplemental funding was provided to support personnel and equipment required for the project. Office and conference room space were provided for the project. Several communications were authored, including a development report, a candidate grant for follow-on funding from the European Union, and the paper submitted to arXiv.org.\n\nOther tasks included writing a hiring email for a postdoctoral researcher, assisting with the proposal to hire a postdoc, helping to recruit a Co-PI, assisting with the organization of laboratory appointments for staff members, participating in laboratory meetings, assisting with copy revisions for submitted papers, and helping to prepare this review. Overall, various forms of organizational and scientific guidance were offered to the project, mentoring multiple postdoctoral researchers and students on various parts of the project. Several papers were co-written with members of the research team, and multiple postdocs and students were hosted on sabbatical to work on the project. The project's research and scientific plan was discussed in lab meetings, and background research was conducted on the science subject of the project.\"\n\nThis is a summarized and paraphrased version of the original text while maintaining its core meaning and research focus. Note that some details may have been altered for clarity and conciseness.",
        "ori-fast-z-score": 1.060430864638069,
        "water-fast-z-score": 10.459223146926798,
        "rewrite-fast-z-score": 2.674545099519113
    },
    {
        "original_text": "An Infrared Cloud Monitor was developed for the Haleakalaan robotic telescope at the Observatorio del Teide on Tenerife, Spain. The instrument was designed to image the thermal emission from thin clouds in the atmosphere in the wavelength region between 3.8 μm and 14 μm. This paper describes the design, development and initial tests of the ICM. Observations with the ICM of infrared emission from thin cirrus clouds are presented. The observed clouds had linear dimensions between 1 km and 10 km with a median size of 3.3 km. They were located at altitudes between 2.2 km and 7.2 km. The Infrared Cloud Monitor was designed, developed and tested over a period of 18 months. The system is composed of several subsystems. The main control and data acquisition system was developed with the VICON motion analysis system. The instrument package for imaging the thermal emission from thin clouds is mounted on a three-axis stage allowing it to be pointed at the cloud of interest. The ICM obtains data through a series of exposures with an uncooled MCT-LPS sensor and focal plane assembly in order to map the cloud in the desired spectral region. Each exposure is 10 s long and the data is aquired at a rate of 10 Hz. The ICM images are constructed from the time series of images and a median filter is applied to remove high frequency noise from the data.",
        "watermark_text": "An Infrared Cloud Monitor was developed for the Haleakalaan robotic telescope at the Observatorio del Teide on Tenerife , Spain . The telescope was built to see the thermal emitted from thin clouds in the atmosphere in the frequency region between 3 . 8 μm and 14 μm .This paper explains the development , construction and first tests of the ICM . Observations with the ICM of infrared emission from small cirrus clouds are presented .The observed clouds had continuous dimensions between 1 km and 10 km with a median size of 3 . 3 km . They were situated at ranges between 2 . 2 km and 7 . 2 km .The Infrared Cloud Monitor was developed , created and evaluated over a period of 18 months . The system is composed of several subsystems .The main control and information transfer system was developed with the VICON motion analysis unit . The instrument package for imaging the thermal emitted from thin dust is installed on a three - axis stage allowing it to be pointed at the cloud of interest .The ICM obtains data through a sequence of exposures with an uncooled MCT - LPS detector and focal plane assembly in order to map the cloud in the desired spectral region . Each exposure is 10 s long and the information is aquired at a rate of 10 Hz .The ICM images are built from the period series of pictures and a median filter is applied to remove great frequency distortion from the information .",
        "rewrite_text": "An Infrared Cloud Monitor (ICM) has been developed for the Haleakalaan robotic telescope located at the Observatorio del Teide on Tenerife, Spain. This telescope was designed to observe thermal emissions from thin clouds in the atmosphere within the frequency range of 3.8 micrometers to 14 micrometers. This paper details the development, construction, and initial testing of the ICM. It presents observations made with the ICM of infrared emissions from small cirrus clouds. The observed clouds had dimensions ranging from 1 kilometer to 10 kilometers, with a median size of 3.3 kilometers. They were situated at distances between 2.2 kilometers and 7.2 kilometers.\n\nThe development, creation, and evaluation of the Infrared Cloud Monitor spanned a period of 18 months. The system is composed of several subsystems, with the main control and information transfer system developed using the VICON motion analysis unit. The imaging instrument package, designed to capture the thermal emissions from fine dust, is mounted on a three-axis stage, allowing it to be directed at the desired cloud.\n\nThe ICM obtains data through a series of exposures using an uncooled MCT-LPS detector and focal plane assembly. This allows for mapping of the cloud in the desired spectral region. Each exposure is captured for 10 seconds and information is acquired at a rate of 10 Hz. The ICM images are constructed from a series of time-stamped pictures, with a median filter applied to remove high-frequency distortions from the data.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 5.787473141261629,
        "rewrite-fast-z-score": 1.8367993291867606
    },
    {
        "original_text": "A survey of molecular gas in and around the supergiant HII region NGC 604 has been carried out using the James Clerk Maxwell Telescope (JCMT). The JCMT is an innovative facilities instrument, operating as a dedicated mission in the submillimetre wavelength range. This allows the study of cold gas, an important phase of the ISM, in extreme environments such as those present in the NGC 604 region. Two areas of distinct complexity have been observed: the ring-like structure, often called the bow shock, associated with the fast moving cluster of young stars central cluster; and a broader arc-like distribution of high CO(J=3-2)/CO(J=1-0) ratio gas surrounding the central star cluster. The arc is highly unusual given its small size (a few parsecs) and extremely high brightness temperature (Tb ~ 105 K). The most plausible explanations for this extreme emission are extremely high densities (n(H) ~ 1023 cm-3) and extremely low temperatures (T < 10K), either due to high volume densities and low volume temperatures (i.e. very high energy densities), or far-ultraviolet radiation from the central cluster.",
        "watermark_text": "A investigation of molecular gas in and around the supergiant HII area NGC 604 has been carried out utilizing the James Clerk Maxwell Telescope ( JCMT ) . The JCMT is an ambitious facilities instrument , operating as a dedicated mission in the submillimetre wavelength range .This enables the observation of cold gas , an important phase of the ISM , in extreme environments such as those present in the NGC 604 region . Two areas of distinct detail have been observed : the circle - like structure , sometimes called the bow shock , associated with the fast moving cluster of young galaxies central cluster ; and a narrower arc - like pattern of high CO ( J = 3 - 2 ) / CO ( J = 1 - 0 ) ratio gas covering the main star cluster .The arc is highly unusual given its tiny diameter ( a few parsecs ) and extremely high brightness temperature ( Tb ~ 105 K ) . The most plausible excuses for this extreme emitted are extremely high densities ( n ( H ) ~ 1023 mm - 3 ) and extremely poor temperatures ( T < 10K ) , either due to low volume densities and low volume pressures ( i . e .very high energy densities ) , or far - ultraviolet radiation from the main cluster .",
        "rewrite_text": "An investigation into the molecular gas within and surrounding the supergiant HII region NGC 604 has been conducted using the James Clerk Maxwell Telescope (JCMT). The JCMT, an ambitious facility instrument, operates as a dedicated mission in the submillimeter wavelength range, enabling the observation of cold gas in extreme environments like those found in the NGC 604 area. Two distinct areas of interest have been observed: a circular structure, often referred to as a bow shock, associated with the rapidly moving central cluster of young galaxies; and a narrow arc-like pattern of gas with a high CO (J=3-2) to CO (J=1-0) ratio, covering the main star cluster. This arc is particularly unusual due to its small diameter (a few parsecs) and exceptionally high brightness temperature (approximately 105 K). The most plausible explanations for this extreme emission are extremely high densities (n(H) ~ 1023 mm-3) and extremely low temperatures (T < 10K), which may be caused by either low volume densities and low volume pressures (indicating very high energy densities), or far-ultraviolet radiation from the primary cluster.",
        "ori-fast-z-score": -0.8626621856275073,
        "water-fast-z-score": 4.364357804719848,
        "rewrite-fast-z-score": 1.3093073414159544
    },
    {
        "original_text": "The isolated spiral galaxies of the sample are fairly similar to the Magellanic Clouds in the optical appearance. They are nearly round with slightly flocculent, ocher-colored spiral arms. The flocculent appearance is similar to that of the M33 galaxy, but the isolated spirals are slightly larger. Elliptical isophotes are rare. The distance estimates range from 9 to 17 Mpc, with an average of 13 Mpc. The apparent V band magnitudes range from 15.9 to 16.4 with an average of 16.0. The absolute V band magnitude Mv ranges from -18.4 to -17.0 with an average of -16.5. The colors are fairly blue, with V-R equal to 0.6 and R-I equal to 0.7. The colors and magnitudes of these isolated spiral galaxies are very similar to the corresponding characteristics of the Magellanic Clouds. Thus these galaxies are probably satellites of the Milky Way. The similarity of the two galaxies in characteristics also suggest a close physical relationship. The isolation of the galaxies implies that they have been fairly untouched by neighboring galaxies. Thus their original shapes may have been more nearly circular than those seen today. The effect of tidal forces from the Milky Way is probably to have flattened the original shapes and formed the flocculent appearance. These galaxies have not been completely dissolved by the Milky Way s gravitational field. The estimated masses of these galaxies based on their observed rotational velocities and projected spatial extents are on the order of 10 billion times the mass of the Sun. These large masses indicate that the galaxies are in a fairly late stage of evolution. The colors and absolute magnitudes of the isolated spiral galaxies are typical of dI galaxies, whose evolution is generally thought to be caused by the gradual loss of mass to large scale structures via a process of tidal stripping. The estimated ages of the isolated spiral galaxies based on their integrated colors and based on the presence of blue stars in their cores range from 3 to 7 Gyr, with an average of 5 Gyr. These estimated ages are generally much less than the probable ages based on the number of remnants of intermediate and old age present, which range from 10 to 14 Gyr, with an average of 12 Gyr. The estimated ages based on integrated colors may be underestimated if there is significant internal reddening in the galaxies. These differences between the estimated ages based on integrated colors and the probable ages based on the numbers of old stars may be due to the galaxies having a relatively short evolutionary life span. Some of the galaxies may be new systems in the process of formation. The infraredASAS survey found no evidence of an active nucleus in any of the isolated spiral galaxies. The nucleus of an isolated spiral galaxy may be completely extinguished by the surrounding dust. Thus, judging from the integrated optical characteristics and the infrared spectra, it seems that",
        "watermark_text": "The tiny spiral galaxies of the sample are fairly related to the Magellanic Clouds in the optical appearance . They are nearly round with slightly flocculent , ocher - colored spiral arms .The flocculent appearance is similar to that of the M33 galaxy , but the scattered spirals are slightly larger . Elliptical isophotes are rare .The distance estimates vary from 9 to 17 Mpc , with an average of 13 Mpc . The apparent V band magnitudes vary from 15 . 9 to 16 . 4 with an average of 16 . 0 .The absolute V band magnitude Mv ranges from - 18 . 4 to - 17 . 0 with an average of - 16 . 5 . The colors are fairly blue , with V - R equal to 0 . 6 and R - I equal to 0 . 7 .The colors and magnitudes of these isolated spiral galaxies are very related to the analogous characteristics of the Magellanic Clouds . Thus these objects are probably satellites of the Milky Way .The similarity of the two galaxies in characteristics also suggest a close physical correlation . The separation of the galaxies indicates that they have been fairly untouched by surrounding galaxies .Thus their early forms may have been more almost circular than those shown today . The impact of tidal forces from the Milky Way is probably to have flattened the original designs and shaped the flocculent appearance .These galaxies have not been totally dissolved by the Milky Way s gravitational field . The estimated masses of these galaxies based on their observed rotational velocities and projected spatial extents are on the order of 10 billion times the mass of the Sun .These huge masses indicate that the galaxies are in a fairly late stage of evolution . The colors and absolute magnitudes of the tiny spiral galaxies are common of dI galaxies , whose evolved is usually thought to be caused by the slow loss of mass to large scale structures via a process of tidal stripping .The estimated ages of the scattered spiral galaxies based on their unified colors and based on the presence of blue stars in their cores vary from 3 to 7 Gyr , with an average of 5 Gyr . These estimated ages are typically much less than the presumed ages based on the quantity of remnants of intermediate and old age present , which range from 10 to 14 Gyr , with an average of 12 Gyr .The estimated ages according on unified shades may be underestimated if there is substantial internal reddening in the galaxies . These changes between the expected ages according on unified shades and the presumed periods based on the numbers of young stars probably be due to the galaxies having a fairly short evolutionary lifetime span .Some of the galaxies likely be new components in the process of formation . The infraredASAS survey detected no evidence of an active nucleus in any of the scattered spiral nuclei .The nucleus of an small spiral galaxy might be completely extinguished by the nearby dust . Thus , judging from the integrated optical characteristics and the infrared spectra , it appears that",
        "rewrite_text": "The sample's tiny spiral galaxies bear a resemblance to the Magellanic Clouds in their optical appearance. They present an almost circular shape with slightly fluffy, ocher-colored spiral arms. The fluffy appearance mirrors the M33 galaxy, but with slightly larger scattered spirals. Elliptical isophotes are scarce. Distance estimates range from 9 to 17 Mpc, with an average of 13 Mpc. The apparent V-band magnitudes vary between 15.9 and 16.4, averaging out to 16.0. The absolute V-band magnitude, Mv, spans from -18.4 to -17.0, with a mean of -16.5. Their colors are predominantly blue, with V-R equal to 0.6 and R-I equal to 0.7.\n\nThe color and magnitude characteristics of these isolated spiral galaxies closely resemble those of the Magellanic Clouds, suggesting that they could be satellites of the Milky Way. The similarity in their traits also hints at a close physical correlation. The separation between these galaxies indicates that they have been largely untouched by neighboring galaxies, implying that their early forms may have been more circular than what is currently observed. The impact of tidal forces from the Milky Way likely flattened their original designs and gave them a fluffy appearance. These galaxies have not been completely dissolved by the Milky Way's gravitational field.\n\nBased on observed rotational velocities and projected spatial extents, the estimated masses of these galaxies are on the order of 10 billion times the mass of the Sun. Such huge masses suggest that they are in a later stage of evolution. The colors and absolute magnitudes of these tiny spiral galaxies are typical of dI galaxies, which are thought to have undergone slow mass loss to large-scale structures through a process known as tidal stripping.\n\nEstimated ages of the scattered spiral galaxies, determined from their uniform colors and the presence of blue stars in their cores, range from 3 to 7 billion years, averaging out to 5 billion years. These estimated ages are typically shorter than those inferred from the amount of intermediate and old age remnants, which span from 10 to 14 billion years with an average of 12 billion years. If there is significant internal reddening in the galaxies, the estimated ages based on uniform shades may be underestimated. The discrepancies between expected ages derived from colors and those inferred from young star counts likely stem from the galaxies' relatively short evolutionary lifespan. Some of these galaxies may be new additions in the process of formation.\n\nThe infrared ASAS survey has revealed no evidence of an active nucleus in any of the scattered spiral nuclei, indicating that the nucleus of a small spiral galaxy could be completely obscured by nearby dust. Judging from their integrated optical characteristics and infrared spectra, it appears that these galaxies lack a distinct active core.",
        "ori-fast-z-score": -1.5833852988894828,
        "water-fast-z-score": 6.318585941639696,
        "rewrite-fast-z-score": -0.06984302957695782
    },
    {
        "original_text": "Using the Atacama Large Millimeter/submillimeter Array (ALMA), we have carried out a thorough search for highly-excited molecular lines in the vicinity of the Seyfert 2 nucleus in the nearby galaxy M51 (NGC 5194). No evidence of an active galactic nucleus (AGN) was found in the distribution or kinematics of the observed species. Instead, the spatial distribution, excitation mechanism, and kinematics of the molecular gas are all consistent with a scenario in which the molecular gas is impacted by a radio jet emanating from the active nucleus in the galaxy NGC 5195. The molecular gas is distributed in two spatially distinct clouds that are offset from the radio source by ~15-20 pc, exhibit similar velocities, and have slightly different physical properties. These results lend further credence to the hypothesis that radio jets are effective at transporting interstellar matter from their host galaxies to distances of several tens of parsecs from the nucleus.",
        "watermark_text": "Using the Atacama Large Millimeter / submillimeter Array ( ALMA ) , we have carried out a comprehensive search for highly - excited molecular lines in the vicinity of the Seyfert 2 nucleus in the nearby galaxy M51 ( NGC 5194 ) . No evidence of an active galactic nucleus ( AGN ) was produced in the distribution or kinematics of the observed organisms .Instead , the spatial distribution , excitation process , and kinematics of the molecular gas are all consistent with a situation in which the molecular gas is impacted by a radio jet emanating from the active nucleus in the galaxy NGC 5195 . The molecular gas is spread in two spatially unique clouds that are offset from the radio source by ~ 15 - 20 pc , exhibit similar velocities , and have slightly different physical properties .These data lend further credence to the notion that radio jets are important at transporting interstellar matter from their host galaxies to distances of several tens of parsecs from the nucleus .",
        "rewrite_text": "With the utilization of the Atacama Large Millimeter/submillimeter Array (ALMA), we have conducted an extensive survey for highly excited molecular lines in close proximity to the Seyfert 2 nucleus of the nearby galaxy M51 (also known as NGC 5194). Our observations reveal no definitive indications of an active galactic nucleus (AGN) in the distribution or kinematics of the detected organisms. Instead, the spatial distribution, excitation process, and kinematics of the molecular gas align with a scenario where it is impacted by a radio jet emanating from the active nucleus of the neighboring galaxy NGC 5195.\n\nThe molecular gas is dispersed across two spatially distinctive clouds, which are offset from the radio source by approximately 15 to 20 pc. These clouds share similar velocities and possess slightly different physical properties. These data further support the idea that radio jets play a crucial role in transporting interstellar matter from their host galaxies to distances spanning several tens of parsecs from the galaxy's nucleus.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 2.54000254000381,
        "rewrite-fast-z-score": -0.23904572186687872
    },
    {
        "original_text": "Co-doped TiO2 films with various Co doping levels were synthesized by sol-gel process and investigated by structural, transport and magnetic measurements as a function of O2 partial pressure during deposition. The crystalline phase of the films was verified by X-ray diffraction and revealed anatase for all samples. A transport study revealed that incorporation of Co in TiO2 matrix enhances the charge transport characteristic of the films. Magnetic measurement revealed superparamagnetic behavior for samples with low doping levels and ferromagnetic ordering at higher doping levels. The results were discussed in the light of the activation energy calculation. Reducing the O2 partial pressure from high to low level during deposition leads to an increase of the activation energy which can be correlated to the change of magnetic behavior from superparamagnetic to ferromagnetic ordering. * * * * * * * * Cobalt doped TiO2 thin films with various Co doping levels were deposited by sol-gel process under various O2 partial pressures. Structural, transport and magnetic measurements as a function of O2 partial pressure revealed that the formation of Co1-xToxOy solid solution is governed by the Co concentration as well as the O2 partial pressure. As Co doping level increases, the ferromagnetic ordering is observed for the films prepared under low O2 partial pressure. An increase of O2 partial pressure leads to an increase of activation energy and the change of magnetic behavior from superparamagnetic to ferromagnetic ordering can be correlated to the change of activation energy.",
        "watermark_text": "Co - doped TiO2 films with various Co doping amounts were synthesized by sol - gel method and studied by structural , transport and magnetic measurements as a function of O2 partial pressure during deposition . The crystalline phase of the films was verified by X - ray diffraction and indicated anatase for all specimens .A transport study revealed that incorporation of Co in TiO2 matrix enhances the charge flow characteristic of the films . Magnetic observation confirmed superparamagnetic properties for specimens with lowered doping rates and ferromagnetic ordering at higher doping rates .The results were mentioned in the light of the activation energy calculation . Reducing the O2 partial pressure from high to low level during deposition leads to an increase of the activation energy which can be correlated to the shift of magnetic behavior from superparamagnetic to ferromagnetic ordering .* * * * * * * * Cobalt doped TiO2 thin films with various Co doping amounts were deposited by sol - gel method under various O2 partial temperatures . Structural , transport and magnetic measurements as a function of O2 partial pressure revealed that the formation of Co1 - xToxOy liquid solution is governed by the Co density as well as the O2 partial pressure .As Co doping rate grows , the ferromagnetic ordering is observed for the films prepared under lowering O2 partial pressure . An rise of O2 partial pressure leads to an increase of activation energy and the shift of magnetic behavior from superparamagnetic to ferromagnetic ordering can be correlated to the shift of activation energy .",
        "rewrite_text": "Various amounts of Co-doped TiO2 films were synthesized using the sol-gel method, and were studied through structural, transport, and magnetic measurements while varying the O2 partial pressure during deposition. X-ray diffraction confirmed the crystalline phase of the films, indicating anatase for all specimens. The incorporation of Co into the TiO2 matrix was found to enhance the charge flow characteristics of the films in a transport study. Magnetic observations confirmed superparamagnetic properties for specimens with lower doping rates, while ferromagnetic ordering was observed at higher doping rates. These results were discussed in relation to activation energy calculations.\n\nWhen reducing the O2 partial pressure from a high to a low level during deposition, there was an increase in activation energy that could be correlated with a shift in magnetic behavior from superparamagnetic to ferromagnetic ordering. Additionally, cobalt-doped TiO2 thin films with different Co doping amounts were deposited using the sol-gel method at various O2 partial temperatures. Structural, transport, and magnetic measurements conducted at different O2 partial pressures revealed that the formation of a Co1-xToxOy liquid solution was governed by both the Co density and the O2 partial pressure. As the Co doping rate increased, ferromagnetic ordering was observed for films prepared at lower O2 partial pressures. Conversely, an increase in O2 partial pressure led to an increase in activation energy, and the shift in magnetic behavior from superparamagnetic to ferromagnetic ordering could be linked to changes in activation energy.",
        "ori-fast-z-score": 2.203000456008648,
        "water-fast-z-score": 6.800566625070174,
        "rewrite-fast-z-score": 3.2637668288410984
    },
    {
        "original_text": "The cosmological constant, also known as the vacuum energy, represents the contribution to the density of energy of the universe from the zero-point energies of the quantum fields, and has been assumed to be positive since the discovery that the universe is flat. However, we show that it can in fact be positive or negative, in contrast to the assumptions of many theories of cosmology and particle physics. If the cosmological constant is positive, then many possible theories that give a natural explanation for its small positive value are excluded. If the cosmological constant is negative, then some of these theories are reinstated. These conclusions are reached by using a combination of observational constraints, combined with detailed theoretical calculations in specific models. We demonstrate that within the standard model of particle physics, the QCD sum rule gives a naturally small and positive value for the cosmological constant. This reinvestigation of the cosmological constant has implications for the anthropic solution to the problems of the initial conditions of the universe, and for the nature of dark energy.",
        "watermark_text": "The cosmological constant , sometimes called as the vacuum energy , represents the contribution to the density of electricity of the universe from the zero - point energies of the quantum fields , and has been expected to be positive since the discovery that the universe is flat . However , we prove that it can in reality be positive or negative , in comparison to the assumptions of several theories of cosmology and electron physics .If the cosmological constant is positive , then many potential ideas that give a natural explanation for its tiny positive value are excluded . If the cosmological constant is negative , then some of these theories are reinstated .These conclusions are reached by using a combination of observational restrictions , combined with thorough theoretical calculations in individual models . We suggest that within the standard description of particle theory , the QCD sum rule gives a naturally tiny and strong value for the cosmological constant .This reinvestigation of the cosmological constant has implications for the anthropic solution to the problems of the first conditions of the universe , and for the nature of dark energy .",
        "rewrite_text": "The cosmologic constant, alternatively referred to as vacuum energy, embodies the impact of zero-point energies from quantum fields on the electric density of the universe. Since it has been anticipated to be positive since the revelation that the universe is flat, we have demonstrated through rigorous investigation that it can indeed be either positive or negative—contrasting with several assumptions in cosmology and electron physics theories. If the cosmologic constant is positive, numerous potential explanations for its small positive value are excluded. Conversely, if it is negative, certain theories are rejuvenated. These conclusions are derived from a blend of observational constraints and meticulous theoretical calculations within individual models. We propose that within the standard framework of particle theory, the QCD sum rule naturally offers a tiny yet significant value for the cosmologic constant. This re-examination of the cosmologic constant holds significant implications for the anthropic approach to addressing the initial conditions of the universe and for understanding the nature of dark energy.",
        "ori-fast-z-score": 0.6708203932499369,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": -0.1111111111111111
    },
    {
        "original_text": "In this paper, we present a methodology and accompanying system for evaluating user experience in an immersive virtual environment (IVE). Specifically, we describe the development of a survey, termed the IVE Experience Questionnaire (IVEEQ), and the construction of a set of form-based questions designed to capture specific experiences found to be important by our usability testing. Our system then analyzes user responses to the IVEEQ, along with gathered metadata, to produce numerical scores and qualitative descriptions of the surveyed experience. To illustrate the utility of this approach, we present the results of our evaluation of forms in an IVE, demonstrating the ability of the IVEEQ to identify form type as a factor influencing user experience. Finally, we discuss future research directions inspired by this work, including extending our evaluation to other aspects of the user experience and applying this approach to different form types.  esk In this paper, we present a methodology and accompanying system for evaluating user experience in an immersive virtual environment (IVE). First, we describe the development of a survey, termed the IVE Experience Questionnaire (IVEEQ), and the construction of a set of form-based questions designed to capture specific experiences found to be important by our usability testing. Our system then analyzes user responses to the IVEEQ, along with gathered metadata, to produce numerical scores and qualitative descriptions of the surveyed experience. To illustrate the utility of this approach, we present the results of our evaluation of forms in an IVE, demonstrating the ability of the IVEEQ to identify form type as a factor influencing user experience. Finally, we discuss future research directions inspired by this work, including extending our evaluation to other aspects of the user experience and applying this approach to different form types.",
        "watermark_text": "In this paper , we present a framework and accompanying method for evaluating customer experience in an immersive virtual environment ( IVE ) . Specifically , we explain the development of a survey , designated the IVE Experience Questionnaire ( IVEEQ ) , and the development of a setting of form - based questions constructed to capture specific experiences discovered to be crucial by our usability testing .Our model then analyzes customer responses to the IVEEQ , along with gathered metadata , to produce quantitative scores and qualitative descriptions of the assessed encounter . To indicate the utility of this methodology , we present the results of our evaluation of forms in an IVE , showing the ability of the IVEEQ to identify type class as a factor influencing user experience .Finally , we explain potential study directions inspired by this study , including increasing our evaluation to other components of the user experience and using this methodology to different form types . esk In this paper , we present a technique and accompanying method for evaluating customer experience in an immersive virtual environment ( IVE ) .First , we document the development of a survey , designated the IVE Experience Questionnaire ( IVEEQ ) , and the development of a setting of form - based questions constructed to capture specific experiences discovered to be crucial by our usability testing . Our system then analyzes customer responses to the IVEEQ , along with gathered metadata , to produce quantitative scores and qualitative descriptions of the assessed encounter .To explain the utility of this methodology , we present the results of our evaluation of forms in an IVE , showing the ability of the IVEEQ to identify type type as a factor influencing user experience . Finally , we explain potential study approaches inspired by this study , including increasing our evaluation to other components of the user experience and using this methodology to different form types .",
        "rewrite_text": "In this paper, we introduce a framework along with a method to assess customer experience within an immersive virtual environment (IVE). Specifically, we explain the creation of a survey named the IVE Experience Questionnaire (IVEEQ). This survey incorporates a set of form-based questions designed to capture specific experiences identified as crucial through our usability testing. Our model then analyzes customer responses to the IVEEQ, alongside gathered metadata, to generate quantitative scores and qualitative descriptions of the evaluated encounter.\n\nTo illustrate the effectiveness of this methodology, we present the results of our IVE form evaluation, highlighting the IVEEQ's ability to identify type as a significant factor influencing user experience. Ultimately, we outline potential study directions inspired by this research. This includes expanding our evaluation to other user experience components and applying this methodology to various form types.",
        "ori-fast-z-score": -0.17025130615174972,
        "water-fast-z-score": 9.245259333511683,
        "rewrite-fast-z-score": 1.5428161556520092
    },
    {
        "original_text": "Turbulence is one of the most ubiquitous and important phenomena in the universe, present in a large range of different contexts, from water flows in canals, to the movements of gases in the atmosphere, to the rearrangement of stars in a galaxy. Turbulence is caused by molecular or particulate collisions that occur at random, leading to an incoherent distribution of motion. The theory of turbulence was developed in the 1920s by Norwegian engineerKároly (Karsten) Gustaf Arctander, who founded a university now named after him - the Károlyi University of footballers in Hungary, and a library - the Károlyi Public Library of footballers in Hungary. In the early turbulence theory, mathematical descriptions were used to characterise the properties of turbulence, and basic concepts such as integration, differentiation and different types of differential equations were applied. The development of experimental techniques to evaluate the properties of turbulence and theoretical developments were combined in the 1930s to create a modern theory of turbulence. Important results in the modern theory of turbulence were published by American mathematician Robert H. Luther in 1941 and Norwegian-American mathematician Bryce L. Dysthe in 1958.",
        "watermark_text": "Turbulence is one of the most ubiquitous and significant phenomena in the universe , visible in a large variety of different settings , from river flows in canals , to the actions of fluids in the atmosphere , to the rearrangement of stars in a galaxy . Turbulence is caused by molecular or particulate collisions that occur at random , leading to an incoherent distribution of movement .The theory of turbulence was developed in the 1920s by Norwegian engineerKároly ( Karsten ) Gustaf Arctander , who created a university now naming after him - the Károlyi University of footballers in Hungary , and a library - the Károlyi Public Library of footballers in Hungary . In the early turbulence theory , mathematical explanations were used to characterise the properties of turbulence , and fundamental concepts such as differentiation , separation and many kinds of integral equations were applied .The advance of research techniques to analyze the properties of turbulence and theoretical developments were consolidated in the 1930s to create a new theory of turbulence . Important results in the present theory of turbulence were published by American mathematician Robert H . Luther in 1941 and Norwegian - American mathematician Bryce L . Dysthe in 1958 .",
        "rewrite_text": "Turbulence is a prevalent and crucial phenomenon in the universe, observable in a wide range of environments - from the flow of rivers in canals, to the behavior of fluids in the atmosphere, and even the reconfiguration of stars within a galaxy. It arises from random collisions between molecules or particles, resulting in an incoherent distribution of movement.\n\nThe theory of turbulence was formulated in the 1920s by Norwegian engineer Károly (also known as Karsten) Gustaf Arctander. His contributions have led to the naming of a university after him - the Károlyi University in Hungary, as well as a library - the Károlyi Public Library. In the early stages of turbulence theory, mathematical explanations were utilized to characterize its properties, with fundamental concepts such as differentiation, separation, and various integral equations being applied.\n\nAdvancements in research techniques and theoretical developments consolidated in the 1930s, leading to the creation of a new theory of turbulence. Significant achievements in the current theory of turbulence were published by Robert H. Luther, an American mathematician, in 1941, and by Bryce L. Dysthe, a Norwegian-American mathematician, in 1958.",
        "ori-fast-z-score": 0.3721042037676254,
        "water-fast-z-score": 7.0699798715848825,
        "rewrite-fast-z-score": 2.42535625036333
    },
    {
        "original_text": "The CNGS neutrino beam produced by the Centro Nazionale Granulosatire (CNGS) laboratory in Italy is dedicated to the NOvA and GNO experiments. The detector installed at the LNGS, in the true neutrino mass scheme, consists of a 150 kilotonnes active detector target, made of a wide cylindrical tank containing 250 tons of liquid argon, inside a 2.67 kilotonnes rock overburden. The collaboration MODULAr, composed of researchers from Italy, United States, and Slovenia, proposes to build a new, very massive Liquid Argon Imaging Chamber, with the main goal to search for very low energy neutrinos, in a region not explored before by the CNGS beam. The detector will consist of a 2 level chamber with an internal diameter of 4 meters, and a height of 3 meters. The inner volume will be divided into 18 Verticale slices (VLSI), each hosting 30.48 Kg of liquid argon. The expected sensitivity is in a region of interest between 1 and 10 eV, where the expected number of signal events depends on the adopted simulated neutrino fluxes, and on the position of the detector in the phase volume. The unprecedented very massive and compact design of the detector, together with its high sensitivity to very low energy neutrino interactions, make it an ideal candidate to search for spectral distortions, therefore exploring new physics phenomena in the neutrino sector.",
        "watermark_text": "The CNGS neutrino light produced by the Centro Nazionale Granulosatire ( CNGS ) laboratory in Italy is dedicated to the NOvA and GNO experiments . The detector constructed at the LNGS , in the true neutrino mass plan , consists of a 150 kilotonnes active detector target , made of a broad cylindrical tank containing 250 kilograms of liquid argon , inside a 2 . 67 kilotonnes rock overburden .The collaboration MODULAr , composed of experts from Italy , United States , and Slovenia , proposes to build a new , very huge Liquid Argon Imaging Chamber , with the main goal to search for very low power neutrinos , in a region not explored before by the CNGS laser . The detector will consist of a 2 level room with an internal diameter of 4 meters , and a length of 3 km .The inner volume will be broken into 18 Verticale slices ( VLSI ) , each hosting 30 . 48 Kg of liquid argon . The expected sensitivity is in a region of interest between 1 and 10 eV , where the expected number of signal events depends on the adopted simulated neutrino fluxes , and on the orientation of the detector in the phase volume .The unusual very huge and compact design of the detector , combined with its high sensitivity to very low power neutrino interactions , give it an suitable candidate to search for spectral distortions , thus pursuing new quantum concepts in the neutrino sector .",
        "rewrite_text": "The CNGS neutrino light, generated by the Centro Nazionale Granulosatire (CNGS) laboratory in Italy, is specifically designed for the NOvA and GNO experiments. The detector constructed at the LNGS, in accordance with the true neutrino mass model, comprises a 150-kilotonne active detector target housed within a broad cylindrical tank containing 250 kilograms of liquid argon, further ensconced by a 2.67-kilotonne rock overburden.\n\nThe MODULAr collaboration, composed of experts from Italy, the United States, and Slovenia, proposes to build a novel, immense Liquid Argon Imaging Chamber aimed at detecting very low-energy neutrinos in an unexplored region of the CNGS laser spectrum. This detector will be a two-level chamber with an internal diameter of four meters and a length of three kilometers. The inner volume will be divided into 18 Verticale Slices (VLSI), each containing 30.48 kilograms of liquid argon.\n\nThe expected sensitivity of this detector lies in a region of interest between 1 and 10 eV, where the number of signal events depends on the simulated neutrino fluxes employed and the orientation of the detector in the phase volume. The unique, extremely large and compact design of this detector, combined with its high sensitivity to low-energy neutrino interactions, makes it an ideal candidate for searching for spectral distortions and pursuing novel quantum concepts in the neutrino field.",
        "ori-fast-z-score": 0.21320071635561041,
        "water-fast-z-score": 5.5432186252458715,
        "rewrite-fast-z-score": 2.225995548013356
    },
    {
        "original_text": "Astronomy is a powerful tool for determining fundamental parameters of stars, and in particular their temperatures, masses and ages. In recent years, considerable advances have been made in the spectral domain, with the implementation of new theory and the development of efficient data analysis techniques. I describe a method to derive these parameters from high-resolution optical spectra, using current evolutionary tracks and the strengths of certain atomic and molecular lines. To demonstrate its effectiveness, I present an application to the open cluster IC 4651 and field stars of similar spectral type. The required observational data are provided, including high-resolution optical spectra and relevant atmospheric parameters, from the literature and my own observations at the Haute-Provence Observatory. The main limitation of this approach is the requirement of high-resolution optical spectra, which are not always available for large samples of stars. Nevertheless, this methodology can be applied to a wide range of applications, and I present a first application to open clusters and field stars, which can help us understand the properties of these populations. In particular, these results demonstrate that intermediate-resolution spectrographs on large telescopes (such as MuSiC; Probeca) provide high data return and low cost, making them good candidates for surveys of large samples of stars.",
        "watermark_text": "Astronomy is a powerful tool for determining essential parameters of stars , and in particular their altitudes , masses and periods . In recent years , substantial advances have been achieved in the spectral domain , with the implementation of new theory and the development of effective data analysis methods .I describe a technique to derive these parameters from high - resolution optical spectra , using current evolutionary tracks and the strengths of certain atomic and molecular lines . To show its effectiveness , I offer an use to the open cluster IC 4651 and field stars of comparable spectral type .The essential observational data are provided , comprising high - resolution optical spectra and relevant atmospheric parameters , from the books and my own observations at the Haute - Provence Observatory . The main obstacle of this methodology is the requirement of high - resolution optical spectra , which are not always accessible for large specimens of stars .Nevertheless , this methodology can be applied to a broad variety of applications , and I offer a early application to open clusters and field stars , which can help us explain the properties of these populations . In particular , these results show that intermediate - resolution spectrographs on huge telescopes ( such as MuSiC ; Probeca ) offer high data return and low cost , making them useful candidates for surveys of large specimens of stars .",
        "rewrite_text": "Astronomy serves as a potent instrument for determining crucial parameters of stars, specifically their altitudes, masses, and periods. Recent advancements in the spectral domain have been substantial, driven by the implementation of new theories and the development of efficient data analysis techniques. I describe a technique that utilizes high-resolution optical spectra to derive these parameters, leveraging current evolutionary tracks and the intensities of specific atomic and molecular lines.\n\nTo demonstrate its effectiveness, I provide an example application to the open cluster IC 4651 and field stars of a comparable spectral type. The essential observational data is provided, which includes high-resolution optical spectra and relevant atmospheric parameters sourced from books and my own observations at the Haute-Provence Observatory. A key challenge with this methodology is the need for high-resolution optical spectra, which may not be readily accessible for large samples of stars.\n\nHowever, this methodology can be applied to a wide range of applications. I offer an early application to open clusters and field stars, which can aid in explaining the properties of these populations. Specifically, our findings indicate that intermediate-resolution spectrographs on large telescopes (such as MuSiC and Probeca) offer both high data return and low cost, making them ideal candidates for surveys of large samples of stars.",
        "ori-fast-z-score": 0.1889822365046136,
        "water-fast-z-score": 6.490973991846821,
        "rewrite-fast-z-score": 1.5118578920369088
    },
    {
        "original_text": "Extreme High Energy (EHE) peaked BL Lacs (HBLs), knowns as the Third Frequency, are largely unobservable at optical wavelengths due to their high Doppler-shifted equivalent widths. Here we report on optical spectroscopy of 32 EHE HBLs performed with the Neil Gehrels Swift Observatory (NGSOM) spectrograph. This is the largest homogeneous sample of EHE HBLs with optical spectroscopy to date. We observe compelling connections between EHE HBLs and low Doppler-shifted optical magnitudes in a complex web of correlations among equivalent width, full width at half-maximum, continuum spectral slope, redshift, and synchrotron peak frequency. Higher equivalent widths, higher full width at half-maximum, steeper spectral slopes, and higher redshifts are all associated with the presence of low Doppler-shifted optical components. Conversely, lower equivalent widths, lower full width at half-maximum, flatter spectral slopes, and lower redshifts are associated with the absence of optical components at or near the host galaxy systemic redshift. We measure the highest equivalent widths and full widths at half-maximum yet observed in the optical band from EHE HBLs. The results of this work have profound implications for EHE HBL central engines, trigger models, and EBL re-processing.",
        "watermark_text": "Extreme High Energy ( EHE ) peaked BL Lacs ( HBLs ) , knowns as the Third Frequency , are essentially unobservable at imaging wavelengths due to their high Doppler - shifted equivalent widths . Here we publish on optical spectroscopy of 32 EHE HBLs conducted with the Neil Gehrels Swift Observatory ( NGSOM ) spectrograph .This is the greatest homogeneous sample of EHE HBLs with optical spectroscopy to date . We see compelling connections between EHE HBLs and low Doppler - shifted optical magnitudes in a complex network of correlations among equal size , full width at half - maximum , continuum spectral slope , redshift , and synchrotron peak frequency .Higher equivalent widths , higher full width at half - maximum , steeper spectral slopes , and higher redshifts are all associated with the presence of poor Doppler - shifted optical components . Conversely , lower equivalent widths , lower full width at half - maximum , flatter spectral slopes , and lower redshifts are associated with the absence of optical components at or near the host galaxy systemic redshift .We estimate the highest equivalent widths and complete widths at half - maximum yet observed in the optical band from EHE HBLs . The results of this research have profound implications for EHE HBL central engines , trigger models , and EBL re - processing .",
        "rewrite_text": "Extreme High Energy (EHE) BL Lacs (HBLs), which are also known as the Third Frequency, are effectively unobservable in imaging wavelengths due to their high Doppler-shifted equivalent widths. We present the optical spectroscopy of 32 EHE HBLs conducted with the Neil Gehrels Swift Observatory (NGSOM) spectrograph. This represents the most comprehensive homogeneous sample of EHE HBLs with optical spectroscopy conducted so far.\n\nOur findings reveal compelling connections between EHE HBLs and low Doppler-shifted optical magnitudes within a complex network of interrelated parameters such as equal size, full width at half-maximum, continuum spectral slope, redshift, and synchrotron peak frequency. Higher equivalent widths, greater full width at half-maximum, steeper spectral slopes, and greater redshifts are all associated with the presence of significantly Doppler-shifted optical components. Conversely, lower equivalent widths, narrower full width at half-maximum, flatter spectral slopes, and lower redshifts are linked to the absence of optical components near or at the systemic redshift of the host galaxy.\n\nWe have estimated the highest equivalent widths and complete widths at half-maximum observed in the optical spectrum from EHE HBLs. The results of this research have significant implications for understanding the central engines of EHE HBLs, trigger models, and EBL re-processing.",
        "ori-fast-z-score": -1.2222222222222223,
        "water-fast-z-score": 1.5460413650478515,
        "rewrite-fast-z-score": 1.3093073414159544
    },
    {
        "original_text": "Quark-Gluon Plasma (QGP), the state of matter conjectured by particle physicists several decades ago, is the densest form of matter known to humans. It exhibits many remarkable properties, like color charge and net-baryon number fluctuations, that can in principle be measured in the LHC. While most of the properties of the QGP were confirmed in the LHC Run 1, the observation of the novel phenomenon of photon emission from the QGP, reported by the ALICE collaboration, remains a great mystery. In this work, using the kinetic theory of heat and angular momentum transfer, we show that the QGP can emit virtual photons with an intensity that depends only on the temperature and the properties of the plasma, obviating the need for a dynamical calculation within the standard model of particle physics. The virtuality of the emitted photons, defined as the square of the momentum scale at which the induced propagator vanishes, is determined by the temperature, and hence, the emitted photons may possess the energy and virtuality required to explain the ALICE anomaly. The proposed mechanism is not constrained by Lorentz invariance, and therefore it can operate in any QGP-like system that exhibits large momentum scales and non-trivial thermodynamics. In order to explore this mechanism, we make use of the uniqueness and robustness of the QGP thermodynamics to perform predictions for photon emission from strongly coupled plasma, which we confront against available data from lattice QCD. We find that the predicted photon rates are sufficient to resolve the ALICE anomaly, should it persist upon the completion of the Run 2 of the LHC. The ALICE collaboration has recently reported photon emission from the QGP in Pb-Pb collisions at the CERN LHC  1 . This observation has remained a mystery to the scientific community for over three decades, as most of the expected QGP signatures were observed in the first run of the LHC. However, the observation of photon emission, with an energy of approximately 9 GeV, and at an average rate of 1010 GeV-1 s-1, has remained a mystery. The emission of real photons is a Quantum Electrodynamics (QED) process, whereas the emission of virtual photons is not constrained by the symmetries of the standard model of particle physics, and may occur via a myriad of different processes. In this work we show that QGP can emit virtual photons with an intensity that depends only on the temperature and the properties of the plasma, obviating the need for a dynamical calculation within the standard model of particle physics. The virtuality of the emitted photons, defined as the square of the momentum scale at which the induced propagator vanishes, is determined by the temperature, and hence, the emitted photons may possess the energy and virtuality required to explain the ALICE anomaly. The proposed mechanism is not constrained by Lorentz invariance, and therefore it can operate in any QGP-like system that exhibits large momentum scales and non-trivial thermodynamics. In order to",
        "watermark_text": "Quark - Gluon Plasma ( QGP ) , the state of matter conjectured by particle physicists many years previously , is the densest form of matter known to mankind . It displays many amazing characteristics , like color charge and net - baryon size fluctuations , that can in concept be determined in the LHC .While most of the properties of the QGP were confirmed in the LHC Run 1 , the observation of the novel concept of photon radiation from the QGP , reported by the ALICE project , continues a huge mystery . In this research , using the kinetic theory of heat and angular velocity transfer , we find that the QGP can emit virtual photons with an intensity that relies only on the temperature and the properties of the plasma , obviating the necessity for a dynamical measurement within the standard theory of particle physics .The virtuality of the emitted photons , defined as the square of the velocity scale at which the induced propagator vanishes , is chosen by the temperature , and hence , the emitted photons may acquire the electricity and virtuality required to explain the ALICE anomaly . The proposed process is not constrained by Lorentz invariance , and therefore it can operate in any QGP - like system that exhibits big momentum scales and non - trivial thermodynamics .In order to examine this mechanism , we make using of the uniqueness and robustness of the QGP thermodynamics to conduct estimates for photon radiation from highly coupled plasma , which we face against provided information from lattice QCD . We see that the expected photon levels are adequate to overcome the ALICE anomaly , should it persist upon the completion of the Run 2 of the LHC .The ALICE project has recently noted photon radiation from the QGP in Pb - Pb collisions at the CERN LHC 1 . This measurement has remained a unknown to the science public for over three decades , as most of the expected QGP signatures were detected in the first run of the LHC .However , the observation of photon radiation , with an energy of approximately 9 GeV , and at an estimated rate of 1010 GeV - 1 s - 1 , has remained a unknown . The emission of real photons is a Quantum Electrodynamics ( QED ) process , whereas the emission of virtual photons is not constrained by the symmetries of the standard theory of particle physics , and may happen via a myriad of different processes .In this research we prove that QGP can emit virtual photons with an intensity that relies only on the temperature and the properties of the plasma , obviating the necessity for a dynamical measurement within the standard description of particle physics . The virtuality of the emitted photons , defined as the square of the velocity scale at which the induced propagator vanishes , is chosen by the temperature , and hence , the emitted photons may acquire the electricity and virtuality required to explain the ALICE anomaly .The proposed process is not constrained by Lorentz invariance , and therefore it can operate in any QGP - like system that exhibits wide momentum scales and non - trivial thermodynamics . In order to",
        "rewrite_text": "Quark-Gluon Plasma (QGP) - the state of matter once theorized by particle physicists - is now known to be the densest form of matter discovered by mankind. It demonstrates numerous remarkable features like color charge and net-baryon size fluctuations, which can potentially be explored in the Large Hadron Collider (LHC). While many properties of QGP were confirmed during the first LHC run, the observation of photon radiation from QGP, reported by the ALICE project, remains a profound mystery.\n\nIn our research, employing the kinetic theory of heat and angular velocity transfer, we discover that QGP can emit virtual photons. The intensity of these photons relies solely on the temperature and the properties of the plasma, eliminating the need for a dynamical measurement within the standard theory of particle physics. The virtuality of these emitted photons, defined by the square of the velocity scale where the induced propagator vanishes, is determined by temperature. Therefore, these emitted photons may possess the required electricity and virtuality to explain the ALICE anomaly. This proposed process is not limited by Lorentz invariance and can occur in any QGP-like system exhibiting large momentum scales and non-trivial thermodynamics.\n\nTo investigate this mechanism, we leverage the uniqueness and robustness of QGP thermodynamics to estimate photon radiation from highly coupled plasma. We compare our estimates with information provided by lattice QCD. We find that the anticipated photon levels are sufficient to address the ALICE anomaly, if it persists after the completion of the second LHC run.\n\nThe ALICE project has recently detected photon radiation from QGP in Pb-Pb collisions at the CERN LHC 1. This detection, which has been a scientific enigma for over three decades, occurred when most expected QGP signatures were identified during the initial LHC run. However, the observation of photons with an energy of approximately 9 GeV and an estimated rate of 1010 GeV-1 s-1 remains a mystery. The emission of real photons follows Quantum Electrodynamics (QED) processes, whereas the emission of virtual photons is not constrained by the symmetries of the standard theory of particle physics and can occur through a multitude of different processes.\n\nIn this study, we prove that QGP can emit virtual photons with an intensity dependent only on temperature and plasma properties. This eliminates the need for a dynamical measurement within the conventional framework of particle physics. The virtuality of these photons, defined by the velocity scale where the induced propagator ceases to exist, is determined by temperature. Consequently, these photons may acquire the necessary electricity and virtuality to explain the ALICE anomaly. Importantly, this process is not limited by Lorentz invariance and can operate in any QGP-like system exhibiting broad momentum ranges and complex thermodynamic behaviors.",
        "ori-fast-z-score": -0.7580980435789034,
        "water-fast-z-score": 9.146740246823299,
        "rewrite-fast-z-score": 3.1538461538461537
    },
    {
        "original_text": "In this paper the longitudinal impedance of an XFEL undulator is calculated for operation in current-enhanced SASE (Self-Amplified Spontaneous Emission) schemes. The undulator radiation impedance is derived from the wake fields and is used to study the reflections from the input ZLWstairs filter for several current-enhanced SASE setups. The performance of the setup based on seven cells of  1  is discussed in detail. It is shown that more than 90% of the input power can be reflected with only 10% loss, which is considered very good for a proof-of-principle study. The presented results show that the presented setup is very promising for reaching the originally proposed performance target of current-enhanced SASE mode at XFEL.  1  Schulz, T., et al.  Current-Enhanced SASE at the European XFEL.  J. Synchrotron Rad. 23.101 (2016). Authors: T. Schulz Journal: Journal of Synchrotron Radiation Date: September 2016 Publisher: World scientific Type: Article URL: https://arxiv.org/abs/1609.09538 Follow the link above to view the full text of the article.",
        "watermark_text": "In this paper the longitudinal impedance of an XFEL undulator is calculated for operation in current - improved SASE ( Self - Amplified Spontaneous Emission ) schemes . The undulator radiation impedance is generated from the wake fields and is utilized to study the reflections from the input ZLWstairs filter for various current - improved SASE setups .The efficiency of the equipment based on seven cells of 1 is mentioned in detail . It is demonstrated that more than 90 % of the input power can be reflected with only 10 % losing , which is regarded very best for a proof - of - principle study .The published results show that the shown configuration is very promising for achieving the previously recommended performance target of present - improved SASE mode at XFEL . 1 Schulz , T . , et al .Current-Enhanced SASE at the European XFEL.J. Synchrotron Rad.23 . 101 ( 2016 ) . Authors : T . Schulz Journal : Journal of Synchrotron Radiation Date : September 2016 Publisher : World scientific Type : Article URL : https : / / arxiv . org / abs / 1609 . 09538 Follow the link above to view the full text of the article .",
        "rewrite_text": "In this study, the longitudinal impedance of an XFEL undulator has been computed for its operation in advanced SASE (Self-Amplified Spontaneous Emission) systems. The undulator's radiation impedance is generated by wake fields and is utilized to analyze reflections from various input ZLWstairs filters in different current-enhanced SASE configurations. The efficiency of the equipment comprising seven cells is detailed extensively. It has been demonstrated that more than 90% of the input power can be reflected, with only 10% lost, which is considered excellent for a fundamental proof-of-concept study. The published results indicate that the configured setup holds great potential to achieve the previously targeted performance in the present-enhanced SASE mode at the XFEL, as described in a research paper by Schulz, et al. (Title: \"Current-Enhanced SASE at the European XFEL,\" Journal of Synchrotron Radiation, 23(101), September 2016). To view the full article, please follow the link at https://arxiv.org/abs/1609.09538.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.08000508000762,
        "rewrite-fast-z-score": -0.12403473458920847
    },
    {
        "original_text": "We report on the first results from the 2500-2750 MHz band of the Robert C. Byrd Green Bank Telescope (GBT) Large 2010A deployable feed array, used for a sensitive search for neutral hydrogen absorption toward BRI 1335-0417 at $z = 4.4$ (OM 1365+1549). We use the 1000-hour data set from November 2010 to March 2011 to place an upper limit on the neutral hydrogen fraction of $f < 0.26$ at $4.4 < z < 5.2$ in a region of 21 Å (with 2σ conf. level). If the absorbing region covers the background QSO, as proposed in some scenarios for the nature of dark matter, this limit provides the most stringent constraint on the neutral fraction at high redshift to date. If the neutral hydrogen is distributed in small clouds distributed smoothly around the QSO, this limit corresponds to an upper limit on the velocity-integrated optical depth of Υ<0.027, or a lower limit on the fraction of neutral hydrogen of f≥0.95. This limit is an order of magnitude more sensitive than the previously best measurement at this redshift, and will reduce the region of uncertainty by a factor of 2, offering the most precise test to date of the nature of dark matter.",
        "watermark_text": "We report on the first findings from the 2500 - 2750 MHz band of the Robert C . Byrd Green Bank Telescope ( GBT ) Large 2010A deployable feed array , utilized for a sensitive search for neutral hydrogen diffusion toward BRI 1335 - 0417 at $ z = 4 . 4 $ ( OM 1365 + 1549 ) . We use the 1000 - day data set from November 2010 to March 2011 to place an upper limitation on the neutral hydrogen proportion of $ f < 0 . 26 $ at $ 4 . 4 < z < 5 . 2 $ in a region of 21 Å ( with 2σ conf .level ) . If the absorbing region encompasses the background QSO , as suggested in some scenarios for the nature of deep material , this limit presents the most stringent constraint on the neutral fraction at high redshift to date .If the neutral hydrogen is spread in small clouds dispersed smoothly around the QSO , this limit equals to an upper maximum on the velocity - integrated optical length of [UNK] < 0 . 027 , or a higher limit on the fraction of neutral hydrogen of f≥0 . 95 . This limit is an order of magnitude more sensitive than the previously best measurement at this redshift , and will decrease the region of uncertainty by a factor of 2 , providing the most precise test to date of the nature of dark matter .",
        "rewrite_text": "We present the initial findings from the 2500-2750 MHz frequency range of the Robert C. Byrd Green Bank Telescope's (GBT) Large 2010A deployable feed array. This array was utilized for a meticulous search of neutral hydrogen diffusion towards BRI 1335-0417 at a redshift of $z = 4.4$ (OM 1365 + 1549). Leveraging a 1000-day dataset spanning from November 2010 to March 2011, we establish an upper limit on the neutral hydrogen proportion of $f < 0.26$ within the redshift range of $4.4 < z < 5.2$ in a 21 Å region (with a 2σ confidence level). If the absorbing region encompasses the background QSO, as suggested in certain deep material scenarios, this limit offers the most stringent constraint on the neutral fraction at high redshift to this day. Conversely, if the neutral hydrogen is distributed in small, smoothly dispersed clouds around the QSO, this limit equates to an upper maximum on the velocity-integrated optical length of [UNK] < 0.027, or a higher limit on the neutral hydrogen fraction of $f≥0.95$. This limit is significantly more sensitive than previous measurements at this redshift, reducing the uncertainty region by a factor of two and providing the most accurate test yet of the nature of dark matter.",
        "ori-fast-z-score": 1.3416407864998738,
        "water-fast-z-score": 5.0,
        "rewrite-fast-z-score": 2.6832815729997477
    },
    {
        "original_text": "A Truecluster is a model of a true clustering. It is a collection of points in n-dimensional space where each cluster is represented by a hypersphere. The Truecluster matching problem is to find a perfect matching between clusters and points such that the distance between points and their matched clusters is small. In this work, we present a fast algorithm for the Truecluster matching problem that matches each point to the cluster whose center is closest to the point in n-dimensional space. The proposed algorithm first randomly partitions the n-dimensional space into a number of cells. It then selects a cell as the query cell, finds all the clusters whose centers are in the query cell, and assigns each point to the cluster whose center is closest to the point. We empirically show that the proposed algorithm is faster than existing approximate matching algorithms and produce good matches for datasets with high-dimensional points. This work was published in the Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval.",
        "watermark_text": "A Truecluster is a description of a true clustering . It is a collection of points in n - dimensional space where each cluster is represented by a hypersphere .The Truecluster fitting problem is to find a perfect matching between clusters and points such that the distance between points and their matched clusters is tiny . In this research , we present a rapid algorithm for the Truecluster fitting problem that sets each point to the cluster whose center is closest to the point in n - dimensional space .The proposed algorithm first randomly partitions the n - dimensional space into a number of cells . It then selects a cell as the query cell , seeks all the clusters whose centers are in the query cell , and assigns each point to the cluster whose center is closest to the point .We empirically prove that the suggested method is faster than existing approximate matching algorithms and produce excellent matches for datasets with high - dimensional points . This research was publication in the Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval .",
        "rewrite_text": "A Truecluster is an expression that characterizes true clustering. It denotes a collection of points in an n-dimensional space, where each cluster is symbolized by a hypersphere. The goal of the Truecluster fitting problem is to achieve an ideal alignment between clusters and points, ensuring that the distance between them is minimal. In our research, we present a swift algorithm for solving the Truecluster fitting problem. This algorithm assigns each point to the cluster whose center is nearest to it in the n-dimensional space.\n\nOur proposed algorithm initially randomly divides the n-dimensional space into multiple cells. It then selects a cell as the query cell and searches for all clusters whose centers reside within the query cell. Subsequently, it assigns each point to the cluster with the closest center to that point. Empirical evidence demonstrates that our proposed method outperforms existing approximate matching algorithms and delivers outstanding matches for datasets containing high-dimensional points. This research has been published in the Proceedings of the 34th International ACM SIGIR Conference on Research and Development in Information Retrieval.",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 4.564604740649092,
        "rewrite-fast-z-score": 1.4814874939752933
    },
    {
        "original_text": "The COMBO-17 satellite surveyed 1.1 x 1.2 degrees of the northern sky in four bands of the near-infrared corresponding roughly to rest-frame visible light at wavelengths 0.65, 0.85, 1.35, and 2.2 μm. Here we analyze the galaxy morphologies and environment in the Abell 901/902 supercluster at redshift z = 0.33. This structure spans an area of approximately 20 square degrees, contains 775 galaxies with measured redshifts and is among the largest structures yet found at these redshifts. We find that 42% of the members of Abell 901/902 are early-type galaxies, an overdensity compared to the field. Particularly high fractions of early-type galaxies are found in the core of the supercluster at approximately 10 h$^{-1}$ Mpc and in a region of lower mean redshift of 0.31 compared to the rest of the structure at approximately 20 h$^{-1}$ Mpc, suggestive of a recent perturbation. We detect no large scale structure in the density distribution of late-type galaxies, although we note that this may be a consequence of the spatial resolution of the COMBO-17 survey, which is 6.2 h$^{-1}$ kpc at this redshift. We conclude that at least some of the early-type galaxies in Abell 901/902 are a result of recent merging and that the process of morphological transformation of cluster galaxies may be ongoing at this epoch.",
        "watermark_text": "The COMBO - 17 satellite examined 1 . 1 x 1 . 2 degrees of the northern sky in four bands of the near - infrared related roughly to rest - frame visible radiation at wavelengths 0 . 65 , 0 . 85 , 1 . 35 , and 2 . 2 μm . Here we assess the galaxy morphologies and environment in the Abell 901 / 902 supercluster at redshift z = 0 . 33 .This structure spanning an area of almost 20 square degrees , comprises 775 galaxies with recorded redshifts and is among the largest objects yet found at these redshifts . We see that 42 % of the members of Abell 901 / 902 are early - class objects , an overdensity relative to the field .Particularly high fractions of early - type galaxies are found in the core of the supercluster at approximately 10 h $ ^ { - 1 } $ Mpc and in a region of lower mean redshift of 0 . 31 relative to the remainder of the formation at approximately 20 g $ ^ { - 1 } $ Mpc , suggestive of a recent perturbation . We detect no large scale organization in the density distribution of late - class objects , although we note that this might be a outcome of the spatial resolution of the COMBO - 17 sample , which is 6 . 2 h $ ^ { - 1 } $ kpc at this redshift .We suggest that at least some of the early - class objects in Abell 901 / 902 are a outcome of recent merging and that the process of morphological transformation of cluster clusters might be continuing at this epoch .",
        "rewrite_text": "The COMBO-17 satellite conducted an examination of 1.1 x 1.2 degrees of the northern sky, focusing on four bands of near-infrared radiation closely related to rest-frame visible radiation at wavelengths of 0.65, 0.85, 1.35, and 2.2 micrometers. In this study, we evaluated the galaxy morphologies and environment within the Abell 901/902 supercluster at a redshift of z=0.33. Spanning an area of nearly 20 square degrees, this structure comprises 775 galaxies with recorded redshifts and is one of the largest objects discovered at these redshift ranges.\n\nIt was observed that 42% of the members in the Abell 901/902 supercluster belonged to early-class objects, an overdensity compared to the surrounding field. Specifically high fractions of early-type galaxies were found in the core of the supercluster, located at approximately 10 h^-1 Mpc, and in a region of lower mean redshift (0.31) relative to the rest of the formation, at approximately 20 g^-1 Mpc. This suggests recent perturbations in the region. No large-scale organization was detected in the density distribution of late-class objects, although it is worth noting that this could be a result of the spatial resolution limit of the COMBO-17 sample, which is 6.2 h^-1 kpc at this redshift.\n\nWe propose that at least some of the early-class objects in Abell 901/902 are a result of recent mergers, and that the process of morphological transformation of clusters may be ongoing at this epoch.",
        "ori-fast-z-score": 0.21566554640687682,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 1.0425720702853738
    },
    {
        "original_text": "Recently, technological innovation has been spurred by invention leverage resulting from invention clubs or communities of technology holders. This phenomenon has been well studied with regard to trends in single technologies, for example wireless local area networks (WLAN) and Worldwide interoperability for microwave access (WiMAX). A complementary trend to this innovation is invention relocatability or joint applicability, that is, the capability of an invention to be applied to various technologies. We investigate trends in joint applicability by mapping patents to technology classes using a mixture of descriptive and class-based hierarchical Bayesian techniques. We find that joint applicability increased over time with spikes in the late 1990s and early 2000s, possibly reflecting rapid development of the semiconductor industry and the diffusion of mobile communication, respectively. We also find that joint applicability exhibits clear geographical patterns: joint applicability was higher in regions where corresponding technologies are highly developed, including the Kanto district in Japan and California in the United States. These geographical differences may reflect differences in regulations, competition, and consumer preferences.",
        "watermark_text": "Recently , technological innovation has been fueled by patent influence arising from technology clubs or communities of tech holders . This phenomenon has been much examined with regard to developments in single innovations , for example wireless regional region systems ( WLAN ) and Worldwide interoperability for microwave access ( WiMAX ) .A complementary shift to this innovation is invention relocatability or joint applicability , that is , the capability of an invention to be applied to several innovations . We explore patterns in joint applicability by map inventions to technical classes using a mixture of descriptive and class - based hierarchical Bayesian techniques .We see that joint applicability improved over time with spikes in the early decade and later 2000s , possibly reflecting greater growth of the semiconductor business and the diffusion of mobile communication , respectively . We additionally find that joint applicability exhibits clear geographical patterns : joint applicability was greater in areas where similar innovations are heavily advanced , notably the Kanto neighborhood in Japan and California in the United States .These regional differences may indicate differences in regulations , competition , and customer choices .",
        "rewrite_text": "Lately, technological innovation has been propelled by the impact of patents stemming from technology clubs or communities of technology owners. This trend has been extensively analyzed in terms of individual innovations' progress, such as wireless local area networks (WLAN) and worldwide interoperability for microwave access (WiMAX). A corresponding trend in this innovation is the relocatability or joint applicability of inventions, which refers to the capability of an invention to be applied across multiple innovations.\n\nWe explore patterns in joint applicability by mapping inventions to technical categories using a blend of descriptive and class-based hierarchical Bayesian methods. Our observations indicate that joint applicability has improved over time, with spikes occurring in the early 2010s and later in the 2000s, potentially reflecting the burgeoning growth of the semiconductor industry and the widespread diffusion of mobile communication, respectively.\n\nFurthermore, we find that joint applicability exhibits clear geographical patterns. Regions where similar innovations are highly advanced tend to have a higher degree of joint applicability, such as the Kanto region in Japan and California in the United States. These regional differences may be attributed to variations in regulations, competition, and customer preferences.",
        "ori-fast-z-score": -0.4216370213557839,
        "water-fast-z-score": 7.800284895082002,
        "rewrite-fast-z-score": 2.8303690591491795
    },
    {
        "original_text": "The leading order behavior of the Friedmann equations in modified gravity theories is typically modified by an additional function of the curvature invariants. In particular, for f(R) models this function typically diverges at points in field space where the model exhibits equation of state (EOS) singularities. These EOS singularities correspond to infinite values for the density, pressure, or both. In this work, we explore the phenomenological implications of these EOS singularities, focusing on the specific case of finite-time singularities. We explore the model parameter space for which EOS singularities render the universe non-physical, formulating the question as an exclusion plot in the plane of model parameters. We show that such an exclusion plot is significantly restricted by existing data from SNIa, baryon acoustic oscillation (BAO) and direct detection experiments, allowing for only a small region of this parameter space. We present a new EOS formulation that is well-behaved across all parameter space and show that this significantly expands the viable model space. In addition to validating the existing results, this demonstrates the ability of existing data to rule out non-physical models. Finally, we show that EOS singularities cause regions of field space to become ghosts—unitary theories that have propagating degrees of freedom with negative energy. Such regions of field space can be ruled out by local gravity constraints from tests of the equivalence principle. We show that these constraints are more than sufficient to rule out all regions of ghostly parameter space, further strengthening the case for f(R) models as the standard gravity theory.",
        "watermark_text": "The leading order behavior of the Friedmann equations in modified gravity physics is typically altered by an additional function of the curvature invariants . In particular , for f ( R ) models this function typically diverges at places in field space where the model shows equation of state ( EOS ) singularities .These EOS singularities relate to infinite values for the density , pressure , or both . In this research , we investigate the phenomenological consequences of these EOS singularities , concentrating on the specific case of finite - time singularities .We explore the model parameter room for which EOS singularities render the universe non - physical , formulating the question as an exclusion plotting in the plane of model variables . We suggest that such an exclusion plotting is significantly restricted by existing information from SNIa , baryon electromagnetic oscillation ( BAO ) and direct detection experiments , allowing for only a small area of this parameter space .We introduce a new EOS formulation that is well - behaved across all parameter space and suggest that this substantially increases the viable model room . In addition to validating the established results , this demonstrates the ability of older data to rule out non - physical models .Finally , we find that EOS singularities produce regions of field space to become ghosts — unitary theories that have propagating degrees of freedom with negative energy . Such parts of field space can be ruled out by regional gravity limitations from tests of the equivalence principle .We see that these limits are more than sufficient to rule out all regions of ghostly parameter space , further expanding the case for f ( R ) models as the standard gravity theory .",
        "rewrite_text": "In modified gravity physics, the primary behavior of the Friedmann equations is typically altered by an additional function of curvature invariants. Specifically, for models of the form f(R), this function often diverges at points in field space where the model demonstrates equation of state (EOS) singularities. These EOS singularities are related to infinite values in density, pressure, or both of these parameters. In this research, we examine the phenomenological effects of these EOS singularities, with a focus on finite-time singularities.\n\nWe explore the model parameter space where EOS singularities render the universe unphysical. We formulate this question as an exclusion plot in the plane of model variables. Our suggestion is that such exclusion plots are significantly constrained by existing information from SNIa, baryon electromagnetic oscillation (BAO), and direct detection experiments, leaving only a narrow area of viable parameter space.\n\nWe introduce a new EOS formulation that exhibits well-behaved properties across all parameter spaces, suggesting that this significantly increases the range of viable models. This not only validates established results but also demonstrates the ability of older data to disqualify unphysical models.\n\nFurthermore, we discover that EOS singularities create regions of field space that become ghosts, i.e., unitary theories with propagating degrees of freedom with negative energy. These portions of field space can be ruled out by regional gravity limitations tested through the equivalence principle. We observe that these limits are more than sufficient to eliminate all regions of ghostly parameter space, further strengthening the case for f(R) models as the standard gravity theory.",
        "ori-fast-z-score": -0.2683281572999747,
        "water-fast-z-score": 5.701573160798387,
        "rewrite-fast-z-score": 2.9755097944025266
    },
    {
        "original_text": "The luminosity function (LF) is one of the most fundamental statistics in observational cosmology. The VVDS type-1 AGN sample represents the largest and most complete spectroscopic sample of active galactic nuclei (AGN) to date. This paper presents the VVDS-deep and VVDS-wide LFs at 2.2, 3.6, 4.9, 7 and 7.7 μm, based on 97, 37, 26, 53 and 20 unique redshifts respectively. We find that the space density of optically luminous AGN (Mλ≥−23.5) is consistent with the local value at z≤2. The evolution of the faint-end slope is much more pronounced, with the bright-end slope found to be α=−1.8⪘(Mλ/−23.5)½−0.5 for Mλ<−24, while most models prefer a much shallower evolution of the faint-end slope, suggesting the presence of substantial cosmic variance in the AGN population, possibly associated with large-scale structure. The effective wavelengths of the different bands sample the rest-frame UV, optical and near-infrared regime at z≤2, the observed frame far-infrared regime at z=2-3 and the rest-frame far-infrared regime at z>3. At all redshifts the LFs are relatively similar, suggesting that the bolometric corrections are almost constant at these redshifts. We attempt to fit a simple phenomenological form to the faint-end slope, finding that the evolution is well-described by λ*∝(1+z)−3.0 for z<2, while at higher redshifts we find λ*∝(1+z)−4.9. We rule out the possibility that the faint-end slope is constant at α=-0.5, finding α=−1.8⪘(Mλ/−23.5)½−0.7 at Mλ<−24. This extreme evolution of the faint-end slope of the AGN luminosity function is driven by the presence of a relatively small number of luminous quasars, with number densities (per unit volume) up to eight times larger at z=3 than today. Such a steep evolution in the space density of optically faint AGN suggests that most actively accreting supermassive black holes are very hard to detect, with most of the growth must have occurred at very high redshifts. The authors are with the VIRGO and VVDS collaborations, based in France and Switzerland. This work was partially funded by the European Union 6th Framework Program n° RITA-CT-2004-505304 & n° 011604 grant to G.P.",
        "watermark_text": "The luminosity function ( LF ) is one of the most important statistics in observational cosmology . The VVDS type - 1 AGN sample constitutes the greatest and most full spectroscopic sample of active galactic nuclei ( AGN ) to date .This paper offers the VVDS - depth and VVDS - broad LFs at 2 . 2 , 3 . 6 , 4 . 9 , 7 and 7 . 7 μm , using on 97 , 37 , 26 , 53 and 20 unique redshifts respectively . We see that the space density of optically luminous AGN ( Mλ≥−23 . 5 ) is compatible with the local value at z≤2 .The evolution of the faint - start slope is much more pronounced , with the dark - end slope found to be α = −1 . [UNK] ( Mλ / −23 . 5 ) ½−0 . 5 for Mλ < −24 , while most models prefer a significantly shallower evolution of the faint - beginning slope , indicate the presence of large cosmic variations in the AGN population , possibly associated with large - scale structure . The effective wavelengths of the different bands measure the rest - frame UV , optical and far - infrared regime at z≤2 , the seen frame far - infrared regime at z = 2 - 3 and the remainder - frame deep - infrared regime at z > 3 .At all redshifts the LFs are fairly identical , showing that the bolometric corrections are almost steady at these redshifts . We aim to put a simple phenomenological form to the faint - end slope , finding that the evolution is well - described by λ * [UNK] ( 1 + z ) −3 . 0 for z < 2 , while at higher redshifts we find λ * [UNK] ( 1 + z ) −4 . 9 .We rule out the suggestion that the faint - end slope is constant at α = - 0 . 5 , finding γ = −1 . [UNK] ( Mλ / −23 . 5 ) ½−0 . 7 at Mλ < −24 . This extreme expansion of the faint - beginning slope of the AGN luminosity function is caused by the presence of a fairly little amount of luminous quasars , with number densities ( per unit volume ) up to eight times bigger at z = 3 than presently .Such a steep evolution in the space density of optically dim AGN suggests that most actively accreting supermassive black holes are very hard to locate , with most of the development must have happened at very high redshifts . The authors are with the VIRGO and VVDS collaborations , located in France and Switzerland .This project was partially financed by the European Union 6th Framework Program n° RITA - CT - 2004 - 505304 & n° 011604 funding to G . P .",
        "rewrite_text": "The luminosity function (LF) is a crucial statistic in observational cosmology. The VVDS type-1 Active Galactic Nuclei (AGN) sample currently constitutes the most comprehensive and extensive spectroscopic sample of AGNs. This research paper presents the VVDS-depth and VVDS-broad LFs at various wavelengths, including 2.2, 3.6, 4.9, 7, and 7.7 μm, utilizing unique redshifts of 97, 37, 26, 53, and 20, respectively. We observe that the space density of optically luminous AGNs (with Mλ≥-23.5) is consistent with the local value at z≤2. The evolution of the faint-end slope is notably pronounced, with the dark-end slope found to be α = -1 [UNK] (Mλ / -23.5)½ - 0.5 for Mλ < -24. While most models prefer a shallower evolution of the faint-beginning slope, this suggests the presence of significant cosmic variations in the AGN population, possibly linked to large-scale structures.\n\nThe effective wavelengths of different bands measure the rest-frame UV, optical, and far-infrared regime at z≤2, the seen-frame far-infrared regime at z between 2 and 3, and the remainder-frame deep-infrared regime at z > 3. Across all redshifts, the LFs remain relatively similar, indicating that bolometric corrections are relatively stable at these redshifts. Our aim is to simplify the phenomenological form of the faint-end slope, finding that the evolution is well described by λ* [UNK] (1 + z) -3.0 for z < 2, while at higher redshifts, we find λ* [UNK] (1 + z) -4.9. We reject the notion that the faint-end slope is constant at α = -0.5, instead finding γ = -1 [UNK] (Mλ / -23.5)½ - 0.7 at Mλ < -24.\n\nThe significant expansion of the faint-beginning slope in the AGN luminosity function is attributed to the presence of a relatively small number of luminous quasars, with number densities (per unit volume) up to eight times greater at z = 3 than they are now. This steep evolution in the space density of optically dim AGNs suggests that most actively accreting supermassive black holes are difficult to locate, with much of the development likely occurring at very high redshifts. The authors are affiliated with the VIRGO and VVDS collaborations, located in France and Switzerland. This project was partially funded by the European Union's 6th Framework Program, specifically through RITA-CT-2004-505304 & n° 011604 grants to G. P.",
        "ori-fast-z-score": 1.158648244043315,
        "water-fast-z-score": 8.0,
        "rewrite-fast-z-score": 3.972508265291366
    },
    {
        "original_text": "Large lifetime differences in neutral B mesons could signal the presence of new particles with different weak interactions. For example, such differences could arise in scenarios with supersymmetry (SUSY), in which case the lightest neutralino would form one such particle. Measuring such differences in the lifetimes of these mesons would provide a window into SUSY parameters, potentially even constraining the SUSY parameter space by combination with results from other experiments. If lifetimes differ significantly from the usual two mass differences, a new -specific - higgs could also contribute significantly to these masses. This could explain the still unaccounted forh invisibles shown by several experiments. These scenarios can be tested in the next round of B meson experiments, both directly through improved measurements of these masses, and indirectly through new constraints on these models. imbab Large lifetime differences in neutral B mesons could signal the presence of new particles with different weak interactions. For example, such differences could arise in scenarios with supersymmetry (SUSY), in which case the lightest neutralino would form one such particle. Measuring such differences in the lifetimes of these mesons would provide a window into SUSY parameters, potentially even constraining the SUSY parameter space by combination with results from other experiments. If lifetimes differ significantly from the usual two mass differences, a new -specific - higgs could also contribute significantly to these masses. This could explain the still unaccounted forh invisibles shown by several experiments. These scenarios can be tested in the next round of B meson experiments, both directly through improved measurements of these masses, and indirectly through new constraints on these models.",
        "watermark_text": "Large lifetime variations in neutral B mesons could signal the presence of new objects with various weak interactions . For instance , such differences might arise in scenarios with supersymmetry ( SUSY ) , in which case the lightest neutralino would create one such particle .Measuring such shifts in the lifetimes of these mesons would offer a window into SUSY parameters , possibly even constraining the SUSY parameter room by combination with data from other experiments . If lifetimes vary significantly from the usual two mass differences , a new - specific - higgs could also contribute greatly to these masses .This might explain the still unaccounted forh invisibles shown by various tests . These possibilities can be evaluated in the second round of B meson experiments , both directly through improved measurements of these masses , and indirectly through new limitations on these models .imbab Large lifetime variations in neutral B mesons could signal the presence of new objects with various weak interactions . For instance , such differences might arise in scenarios with supersymmetry ( SUSY ) , in which case the lightest neutralino would create one such particle .Measuring such shifts in the lifetimes of these mesons would offer a window into SUSY parameters , possibly even constraining the SUSY parameter room by combination with data from other experiments . If lifetimes vary significantly from the usual two mass differences , a new - specific - higgs could also contribute greatly to these masses .This might explain the still unaccounted forh invisibles shown by various tests . These possibilities can be evaluated in the second round of B meson experiments , both directly through improved measurements of these masses , and indirectly through new limitations on these models .",
        "rewrite_text": "Neutral B mesons exhibiting significant lifetime variations could indicate the existence of novel particles with diverse weak interactions. For instance, these variations might emerge in the context of supersymmetry (SUSY), wherein the lightest neutralino would emerge as one such particle. Measuring these lifetime shifts in the mesons would provide insight into SUSY parameters, potentially even narrowing down the SUSY parameter space through the combination of data from other experiments. If the lifetimes deviate considerably from the typical two mass differences, a specific new Higgs particle could significantly contribute to these masses. This could potentially explain the unaccounted-for invisible particles revealed by various tests. These possibilities can be assessed in the second phase of B meson experiments, both directly through improved measurements of their masses and indirectly through new constraints on these models.",
        "ori-fast-z-score": 0.5345224838248488,
        "water-fast-z-score": 6.948792289723034,
        "rewrite-fast-z-score": 1.099524999206747
    },
    {
        "original_text": "A phononic bandgap (PBG) structure for microwave solids was proposed recently  1 . It consists of a one-dimensional (1D) PC channel with a defect and coated by a surface plasma layer (SPL). The PBG structure provides a possibility to achieve a high phononic quality factor (Q) in a compact size. We designed, fabricated, and experimentally investigated such a PBG resonator. The resonant frequency was found to be 19.35 GHz with a spectral width of 0.5 GHz. The measurement results of the linewidth and the Q factor are in a good agreement with the numerical results. It is also shown that the nonautonomous phaser dynamics within the PBG resonator is modeled well by the nonlinear Schrödinger equation. The nonautonomous terms in the equation are induced by the SPL. Such nonautonomous terms provide an additional nonlinearity that can stimulate optical wave break up. Indeed, it is demonstrated that, under the critical power of the external pump, optical wave break up occurs in the experiment. The timing of the break up is in good agreement with the numerical results. The underlying mechanism for this phenomenon is discussed.",
        "watermark_text": "A phononic bandgap ( PBG ) topology for microwave solids was suggested recently 1 . It consists of a one - dimensional ( 1D ) PC network with a defect and coated by a surface plasma layer ( SPL ) .The PBG model provides a potential to achieve a high phononic quality factor ( Q ) in a compact size . We built , fabricated , and experimentally studied such a PBG resonator .The resonant signal was obtained to be 19 . 35 GHz with a spectral width of 0 . 5 GHz . The measurement conclusions of the linewidth and the Q value are in a good agreement with the numerical findings .It is also shown that the nonautonomous phaser dynamics within the PBG resonator is modeled good by the nonlinear Schrödinger equation . The nonautonomous terms in the equation are induced by the SPL .Such nonautonomous terms provide an additional nonlinearity that can stimulate optical wave break up . Indeed , it is demonstrated that , under the key force of the external pump , optical wave break up occurs in the laboratory .The timing of the broke up is in good agreement with the numerical findings . The basic process for this phenomenon is mentioned .",
        "rewrite_text": "Recently, a phononic bandgap (PBG) topology for microwave solid-state applications has been proposed. This topology involves a one-dimensional (1D) photonic crystal (PC) network with a defect, coated by a surface plasma layer (SPL). The PBG model offers the potential to achieve a high phononic quality factor (Q) in a compact form factor.\n\nWe have constructed, fabricated, and experimentally examined such a PBG resonator. The obtained resonant signal was found to be at 19.35 GHz with a spectral width of 0.5 GHz. Our measurements of linewidth and Q value are in good agreement with numerical predictions. Furthermore, it has been demonstrated that the nonautonomous phaser dynamics within the PBG resonator can be effectively modeled by the nonlinear Schrödinger equation. The nonautonomous terms in this equation are attributed to the SPL, which introduce an additional nonlinearity that can stimulate the breakdown of optical waves.\n\nIndeed, under the influence of an external pump, the phenomenon of optical wave breakdown has been observed in laboratory experiments. The timing of this breakdown is well-aligned with our numerical findings. The basic process underlying this phenomenon is briefly mentioned.",
        "ori-fast-z-score": 1.7556172079419585,
        "water-fast-z-score": 6.277372492166241,
        "rewrite-fast-z-score": 1.937329799813845
    },
    {
        "original_text": "In quantum mechanics, the act of observing the outcome of a measurement of a physical system has been found to retroactively change the wavefunction of the system, leading to what is called the  which-way  (WW) interference effect. This phenomenon has been used to explain numerous counterintuitive quantum mechanical phenomena, such as the  Schrödinger s cat  problem and the  many worlds  interpretation of quantum mechanics, which considers the act of observation to be a branching quantum event. We herein introduce a generalization of the Born rule, which we call the  weakly generalized Born rule  (WGBR), according to which the WW effect is not considered to change the probability of potential outcomes, but instead provides a new causal explanation for the observation of those potential outcomes. Unlike previous interpretations of quantum mechanics, this new explanation does not require setting aside the many-worlds interpretation, and it also does not conflict with existing interpretations such as the Copenhagen interpretation. We test our theory on a single photon in a one-dimensional wave packet, and we observe the expected WW interference pattern in accordance with the WGBR. We also discuss potential applications of our theory to understanding quantum thermodynamics, predicting when a quantum system will respond differently to positive and negative measurement results, and formulating quantum algorithms without using explicit wavefunction evolution.",
        "watermark_text": "In quantum mechanics , the act of monitoring the result of a measurement of a physical system has been shown to retroactively shift the wavefunction of the system , leading to what is dubbed the which - way ( WW ) interference effect . This phenomenon has been used to explain numerous counterintuitive quantum mechanical phenomena , such as the Schrödinger s cat problem and the many worlds formulation of quantum mechanics , which considers the act of observation to be a branching quantum event .We herein provide a generalization of the Born rule , which we call the weakly generalized Born rule ( WGBR ) , according to which the WW effect is not thought to alter the probability of potential events , but instead provides a new causal excuse for the observation of those potential outcomes . Unlike past interpretations of quantum mechanics , this new explanation does not require setting aside the many - worlds explanation , and it also does not conflict with existing interpretations such as the Copenhagen interpretation .We test our theory on a single photon in a one - dimensional wave packet , and we monitor the expected WW noise pattern in compliance with the WGBR . We also discuss possible applied of our theory to knowledge quantum thermodynamics , predicting when a quantum system will react similarly to positive and negative measurement data , and formulating quantum algorithms without using explicit wavefunction evolution .",
        "rewrite_text": "In quantum mechanics, it has been demonstrated that the act of monitoring the outcome of a measurement on a physical system can lead to a retroactive shift in the system's wavefunction, resulting in the phenomenon known as the \"which-way\" (WW) interference effect. This phenomenon has been utilized to elucidate various counterintuitive quantum mechanical phenomena, such as the Schrödinger's cat paradox and the many-worlds interpretation of quantum mechanics, which posits that the act of observation is a branching quantum event.\n\nIn this study, we introduce a generalization of the Born rule, termed the Weakly Generalized Born Rule (WGBR). According to this rule, the WW effect is not believed to alter the probabilities of potential events. Instead, it offers a new causal rationale for observing those potential outcomes. In contrast to previous interpretations of quantum mechanics, this novel explanation does not necessitate the abandonment of the many-worlds explanation and is non-confrontational with existing interpretations like the Copenhagen interpretation.\n\nTo test our theory, we examine a single photon within a one-dimensional wave packet and monitor the expected WW noise pattern in accordance with the WGBR. Furthermore, we discuss potential applications of our theory in quantum thermodynamics, predicting when a quantum system will respond similarly to positive and negative measurement data. We also formulate quantum algorithms without relying on explicit wavefunction evolution.",
        "ori-fast-z-score": 1.3862065601673441,
        "water-fast-z-score": 5.671711984196938,
        "rewrite-fast-z-score": 0.9138115486202573
    },
    {
        "original_text": "A new unified description of ultra-relativistic hydrodynamics and MHD in an arbitrary magnetic field is derived using a relativistic thermal Riemann solver. For the first time, we resolve the coupled hydrodynamics, special relativistic heat conduction, general relativistic magnetohydrodynamics, and Maxwell s equations in a single framework. We simulate a variety of extreme flow configurations including a non-linear ultra-relativistic shock, a relativistic two-dimensionalblast wave with strong magnetic field, a relativistic two-dimensional Kelvin-Helmholtz instability, and a one-dimensional vortex seeded in two dimensions. Excellent agreement is obtained between our simulation results and special relativistic hydrodynamics, general relativistic MHD, and magnetic field evolution. This is the preprint version of the paper: https://arxiv.org/pdf/1710.09318.pdf The full paper is available from the link above.",
        "watermark_text": "A modern unified description of ultra - relativistic hydrodynamics and MHD in an arbitrary magnetic force is developed using a relativistic thermal Riemann solver . For the first time , we reconcile the coupled hydrodynamics , special relativistic heat conduction , general relativistic magnetohydrodynamics , and Maxwell s equations in a single formulation .We simulate a variety of extreme fluid configurations including a non - linear ultra - relativistic shock , a relativistic two - dimensionalblast wave with powerful magnetic force , a relativistic two - dimensional Kelvin - Helmholtz disturbance , and a one - dimensional vortex seeded in two dimensions . Excellent agreement is found between our modeling results and special relativistic hydrodynamics , general relativistic MHD , and magnetic field evolution .This is the preprint version of the paper : https : / / arxiv . org / pdf / 1710 . 09318 . pdf The full paper is available from the link above .",
        "rewrite_text": "A comprehensive and modern description of ultra-relativistic hydrodynamics and MHD in an arbitrary magnetic force framework has been developed, utilizing a relativistic thermal Riemann solver. This is the first time we have harmonized the coupled hydrodynamics, special relativistic heat conduction, general relativistic magnetohydrodynamics, and Maxwell's equations within a single formulation.\n\nWe have simulated diverse extreme fluid configurations, including a nonlinear ultra-relativistic shock, a powerful magnetic force-driven relativistic two-dimensional blast wave, a two-dimensional Kelvin-Helmholtz disturbance with relativistic properties, and a one-dimensional vortex embedded in a two-dimensional context. Our modeling results exhibit excellent agreement with special relativistic hydrodynamics, general relativistic MHD, and magnetic field evolution.\n\nThis is the preprint version of the paper, which can be found at the following link: https://arxiv.org/pdf/1710.09318.pdf. The complete paper is accessible from the provided link above.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.640679257301507,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "Researchers have long pondered whether time travel is possible, and if so, whether paradoxes such as the grandfather paradox are logically disproofed. In a recent article, Albin, et al. proposed a test to search for  time-travel paradoxes , using neutral $K$ and $B$ mesons that coherently transform from an incoherent to a coherent quantum state. Such a transformation entails removing the meson from its current incoherent state and placing it in a coherent superposition of all possible momentum states. CP symmetry, which is a fundamental property of the universe, ensures that the transformation is reversible, and a CP-violating disturbance is invoked if the time-travel paradox is logical disproofed. The team at the Jefferson Lab achieved the transformation, and performed a search for CP violation, with null results. A null result rules out the possibility of logically disproving the time-travel paradox, and further illustrates that the universe possesses a logical consistency.",
        "watermark_text": "Researchers have often pondered whether time transportation is possible , and if so , whether paradoxes such as the grandfather paradox are logically disproofed . In a recent article , Albin , et al .proposed a experiment to search for time - travel paradoxes , using neutral $ K $ and $ B $ mesons that coherently shift from an incoherent to a coherent quantum state . Such a transformation entails placing the meson from its current incoherent state and placing it in a coherent superposition of all possible momentum states .CP symmetry , which is a basic feature of the universe , guarantees that the transformation is reversible , and a CP - breaking disturbance is invoked if the time - flight paradox is logical disproofed . The crew at the Jefferson Lab accomplished the transformation , and performed a search for CP violation , with null findings .A null conclusion limits out the prospect of logically disproving the period - flight paradox , and further illustrates that the universe possesses a logical consistency .",
        "rewrite_text": "Researchers have frequently questioned the feasibility of time transportation and whether paradoxes like the grandfather paradox can be logically disproved if it is possible. In a recent study, Albin and his colleagues proposed an experiment to explore time-travel paradoxes by utilizing neutral K and B mesons that undergo a coherent transition from an incoherent to a coherent quantum state. This transformation involves placing the meson in a state of coherent superposition across all potential momentum states, while maintaining its current incoherent state.\n\nThe CP symmetry, a fundamental attribute of the universe, ensures that this transformation is reversible. A CP-breaking disturbance would indicate that the time-travel paradox is logically disproved. The team at the Jefferson Lab successfully conducted this transformation and searched for CP violations, but found no evidence of it. This null finding limits the possibility of logically disproving the time-flight paradox and further underscores the logical consistency of the universe.",
        "ori-fast-z-score": -1.1920791213585393,
        "water-fast-z-score": 3.151354388633341,
        "rewrite-fast-z-score": -1.016001016001524
    },
    {
        "original_text": "The high energy emission of GRO J1655-40 as revealed with INTEGRAL spectroscopy of the 2005 outburst is described. This well-known black hole X-ray binary is a system composed of a Neutron Star primary orbiting a black hole of 18 Meters in close orbit (13-31 R_s) every 2.87 hours. It is suspected that disk accretion onto the neutron star occurs in this system, as evidenced by its spectral class of Z-source, which is characterized by having strong, sometimes regular, neutron star high-speed winds. These winds are likely due to the transfer of energy from the accretion disk onto the neutron star. The 2005 outburst of GRO J1655-40 was detected with the INTEGRAL observatory on Jan. 28, 2005, and reached a maximum on Feb. 24. The work presented here focuses on the high energy spectrum of GRO J1655-40 over this time period. The major result of this work is the discovery of acutoff powerlaw component in the high energy spectrum of GRO J1655-40, with a flux contribution of approximately 50% over nearly 3 decades in energy. This component is not commonly observed in X-ray binaries, and therefore its discovery with INTEGRAL represents a new paradigm for high energy production in these systems. The cutoff powerlaw component is believed to be an extension of the accretion disk blackbody component which is often seen in black hole X-ray binaries, and is generally thought to be the signature ofNETWORK GAMMA RAYS radiated by the inner part of an accretion disk around a black hole. The discovery of a cutoff powerlaw in this system, with a neutron star at its core, represents a new paradigm for high energy production in black hole X-ray binaries, and may indicate that disk accretion is not the only form of high energy production in these binaries. It is possible that this component is the result of a new form of high energy production unique to this system, which may be a result of the strong neutron star winds. This may be a promising area for exploration with further, more detailed analysis of these data.",
        "watermark_text": "The high energy emission of GRO J1655 - 40 as revealed with INTEGRAL spectroscopy of the 2005 outburst is mentioned . This good - famous black hole X - ray binary is a system composed of a Neutron Star main orbiting a black hole of 18 Meters in close orbit ( 13 - 31 R _ s ) every 2 . 87 hours .It is suspected that disk accretion onto the neutron star happens in this system , as demonstrated by its spectral category of Z - source , which is characterized by having strong , sometimes normal , neutron star high - speed winds . These winds are likely due to the transfer of electricity from the accretion disk onto the neutron star .The 2005 outburst of GRO J1655 - 40 was detected with the INTEGRAL astronomer on Jan . 28 , 2005 , and reached a maximum on Feb . 24 . The work offered here highlights on the high energy spectrum of GRO J1655 - 40 over this time time .The significant effect of this research is the discovery of acutoff powerlaw structure in the high energy spectrum of GRO J1655 - 40 , with a flux contribution of almost 50 % over nearly 3 generations in energy . This element is not commonly observed in X - ray binaries , and therefore its discovery with INTEGRAL represents a new framework for high energy recovery in these systems .The cutoff powerlaw portion is suspected to be an extension of the accretion disk blackbody element which is often found in black hole X - ray binaries , and is usually thought to be the signature ofNETWORK GAMMA RAYS radiated by the inner part of an accretion disk around a black hole . The discovery of a cutoff powerlaw in this system , with a neutron star at its core , represents a new paradigm for high energy production in black hole X - ray binaries , and may indicate that disk accretion is not the only form of high energy production in these binaries .It is suggested that this component is the result of a new form of high energy production unusual to this system , which perhaps be a product of the strong neutron star winds . This might be a viable area for research with further , more precise analysis of these information .",
        "rewrite_text": "The mentioned high-energy emission of GRO J1655-40 has been revealed through the INTEGRAL spectroscopy of its 2005 outburst. This well-known black hole X-ray binary comprises a neutron star orbiting a 18-meter black hole in a close orbit (spanning 13-31 R_s) every 2.87 hours. It is suspected that the neutron star experiences disk accretion in this system, as indicated by its Z-source spectral classification, which is characterized by strong, sometimes normal, neutron star high-speed winds. These winds are likely due to the transfer of electrical charge from the accretion disk onto the neutron star.\n\nThe outburst of GRO J1655-40 was detected by the INTEGRAL astronomer on January 28th, 2005, and reached its peak on February 24th. This study focuses on the high-energy spectrum of GRO J1655-40 during this time. A significant finding of this research is the discovery of a cutoff powerlaw structure in the high-energy spectrum of GRO J1655-40. This structure contributes nearly 50% of the flux over three generations of energy, which is not commonly observed in X-ray binaries. Therefore, its discovery with INTEGRAL represents a new framework for high-energy phenomena in these systems.\n\nThe cutoff powerlaw portion is believed to be an extension of the accretion disk blackbody element, often found in black hole X-ray binaries. It is usually thought to be a signature of network gamma rays emitted by the inner part of an accretion disk surrounding a black hole. The discovery of a cutoff powerlaw in this system, with a neutron star at its core, represents a new paradigm for high-energy production in black hole X-ray binaries. This suggests that disk accretion may not be the only form of high-energy production in these binaries, and instead, this component may be the result of a new form of high-energy production unique to this system, possibly stemming from the strong neutron star winds. This could be a viable area for further research with more precise analysis of this information.",
        "ori-fast-z-score": -0.4931969619160719,
        "water-fast-z-score": 5.753964555687506,
        "rewrite-fast-z-score": 2.0344711469278987
    },
    {
        "original_text": "In a wireless broadcast channel with multiple antennas at the transmitter and one antenna at the receiver, it is possible and beneficial to allow some of the transmitter antennas to be turned off, as long as the signal energy is spread across the antennas. This article studies the system design question of how many transmitters should be turned on, and how much transmission power should be used, to maximize the number of receivers who can recover the transmitted information at an arbitrarily low cost. We consider the case where the transmitter has perfect knowledge of the channel state information (CSI), and the case where the transmitter only has statistical CSI. In the case where perfect CSI is available at the transmitter, we determine the optimal number of active transmitters and corresponding transmission power that maximize the number of receivers who can recover the information at an arbitrarily low cost. We then characterize the scaling of the number of active transmitters and corresponding transmission power with the blocklength of the channel code, up to logarithmic factors, for the symmetric case where the channel is invariant under permutation of the antennas. When only statistical CSI is available at the transmitter, we provide sufficient conditions for low-complexity effective coding and interference management to achieve nearly-optimal performance, and we describe a specific coding and interference management scheme that achieves within a factor of 2/3rd the maximum possible number of receivers.",
        "watermark_text": "In a wireless radio channel with many antennas at the antenna and one antenna at the receiver , it is possible and useful to allow some of the antenna antennas to be turned off , as long as the signal energy is spread across the antennas . This page studies the equipment design issue of how many transmitters should be turned on , and how many transmission power should be used , to maximize the quantity of receivers who can obtain the transmitted information at an arbitrarily low cost .We consider the case where the antenna has good knowledge of the channel state information ( CSI ) , and the case where the antenna only has statistical CSI . In the case where perfect CSI is accessible at the signal , we determine the ideal amount of active transmitters and corresponding transmission power that maximize the quantity of receivers who can regain the information at an arbitrarily low cost .We then characterize the scaling of the number of active transmitters and corresponding transmission power with the blocklength of the channel code , up to logarithmic factors , for the symmetric case where the channel is invariant under permutation of the antennas . When only statistical CSI is accessible at the antenna , we provide enough circumstances for low - complexity effective coding and interference management to achieve roughly - optimal performance , and we define a certain code and interference management scheme that achieves within a factor of 2 / 3rd the maximum possible number of receivers .",
        "rewrite_text": "In a wireless communication system featuring multiple antennas at the transmitting end and a single receiver antenna, it becomes both feasible and beneficial to deactivate certain transmitting antennas, provided that the signal energy is distributed across all of them. This page delves into the design considerations of equipment, specifically addressing the question of how many transmitters should be activated and what transmission power levels to use in order to maximize the number of receivers that can acquire transmitted data at a minimal cost.\n\nWe explore two scenarios: one where the antennas have a comprehensive understanding of Channel State Information (CSI), and another where they only possess statistical CSI. In situations where perfect CSI is available, we determine the optimal number of active transmitters and corresponding transmission power that maximize the number of receivers capable of retrieving information at a low cost. We further characterize how the number of active transmitters and their transmission power scale with the blocklength of the channel code, considering logarithmic factors, in a symmetric scenario where the channel remains unchanged despite antenna permutations.\n\nWhen limited to only statistical CSI availability at the antenna, we identify sufficient conditions for low-complexity coding and interference management to achieve approximately optimal performance. Additionally, we define a specific code and interference management scheme that achieves a maximum number of receivers within a factor of 2/3rd of the maximum possible number, even under these limited circumstances.",
        "ori-fast-z-score": 1.8905706613989794,
        "water-fast-z-score": 7.661786364616916,
        "rewrite-fast-z-score": 2.5861309700971087
    },
    {
        "original_text": "The astrophysical supplements to the ASCC-2.5 catalog of radial velocities of stars in the Galaxy are presented. The catalog includes the radial velocities derived from the spectrum analysis for about 55000 stars and the mean radial velocities for 516 Galactic open clusters and associations. This catalog can be used for kinematic study of the Galactic disk and can be applied to study the dynamical evolution of open clusters and their associations. zephyr Title: Astrophysical supplements to the ASCC-2.5 catalog of radial velocities of stars in the Galaxy Abstract: The astrophysical supplements to the ASCC-2.5 catalog of radial velocities of stars in the Galaxy are presented. The catalog includes the radial velocities derived from the spectrum analysis for about 55000 stars and the mean radial velocities for 516 Galactic open clusters and associations. This catalog can be used for kinematic study of the Galactic disk and can be applied to study the dynamical evolution of open clusters and their associations.",
        "watermark_text": "The astrophysical supplements to the ASCC - 2 . 5 catalog of radial velocities of stars in the Galaxy are presented . The catalog contains the radial velocities derived from the spectrum analysis for about 55000 galaxies and the mean radial velocities for 516 Galactic close complexes and associations .This label can be used for kinematic study of the Galactic disk and can be applied to study the dynamical development of open complexes and their connections . zephyr Title : Astrophysical supplements to the ASCC - 2 . 5 catalog of radial velocities of stars in the Galaxy Abstract : The astrophysical supplements to the ASCC - 2 . 5 catalog of radial velocities of stars in the Galaxy are presented .The collection contains the radial velocities derived from the spectrum analysis for about 55000 galaxies and the mean radial velocities for 516 Galactic open complexes and associations . This collection can be used for kinematic study of the Galactic disk and can be applied to study the dynamical development of open complexes and their connections .",
        "rewrite_text": "The astrophysical supplements to the ASCC - 2.5 catalog of Galactic star radial velocities are presented. This catalog encompasses radial velocities derived from spectrum analysis for approximately 55,000 galaxies, as well as mean radial velocities for 516 Galactic close complexes and associations. The data can serve as a label for studying the kinematics of the Galactic disk and can be applied to investigate the dynamic evolution of open complexes and their interconnections.\n\nTitle: Astrophysical Addenda to the ASCC - 2.5 Catalog of Galactic Star Radial Velocities\n\nAbstract: The astrophysical supplements mentioned in this study are additions to the ASCC - 2.5 catalog, which provides radial velocity data for stars in the Galaxy. The collection includes radial velocities derived from spectrum analysis for a large number of galaxies, as well as average radial velocities for a selected group of Galactic open complexes and associations. This dataset can be utilized for studying the dynamics of the Galactic disk and exploring the developmental processes and connections within open complexes.",
        "ori-fast-z-score": -0.40451991747794525,
        "water-fast-z-score": 3.910359202286804,
        "rewrite-fast-z-score": -0.24618298195866545
    },
    {
        "original_text": "Existing studies of extragalactic background light (EBL) used measurements in the optical and far-infrared wavelength regions. These measurements are subject to various systematic uncertainties, and the EBL in the ultraviolet (UV) and soft X-ray wavelength regions remains largely unconstrained. In this paper, we use gamma-ray data from the Fermi Large Area Telescope to place a constraint on EBL in the EUV wavelength region down to a few eV, i.e., the effective photon energy of 10-511 keV electromagnetic radiation. We employ a numerical model of galaxy formation to predict the optical depth to gamma-ray pair production. We find the total EBL in the EUV wavelength region is (3.0 ± 0.9) × 10-13 h erg -2, or equivalently, the star formation rate is (1.2 ± 0.4) × 10 -12 h 1.6 -1.2 × 10-13 h cm -2 s, where h is the reduced Hubble constant in units of 100 km s -1 Mpc -1. If we adopt a universal Simon-Glover-Glotch (SGG) disk mass scaling relation, the resulting EUV luminosity density is (2.2 ± 0.7) × 10 -23 h erg -1 s -1, consistent with previous estimates derived from direct UV measurements. Given the significant uncertainties in the modeling of star formation and the gamma-ray opacity, the constraints on EBL presented in this Letter should be interpreted as a range of possible star formation rates and total EUV luminosity densities. However, the measurement of the EUV luminosity density is an important step in understanding the energetics of galaxy formation over cosmic time.",
        "watermark_text": "Existing experiments of extragalactic background light ( EBL ) used observations in the optical and far - infrared frequency regions . These measurements are subject to several systematic uncertainties , and the EBL in the ultraviolet ( UV ) and dark X - ray frequency regions remains largely unconstrained .In this paper , we using gamma - ray data from the Fermi Large Area Telescope to place a constraint on EBL in the EUV frequency region down to a few eV , i . e . , the effective photon energy of 10 - 511 keV electromagnetic radiation . We utilize a statistical model of galaxy formation to predict the optical height to alpha - ray pair production .We find the total EBL in the EUV wavelength region is ( 3 . 0 ± 0 . 9 ) × 10 - 13 h erg - 2 , or equivalently , the star formation rate is ( 1 . 2 ± 0 . 4 ) × 10 - 12 h 1 . 6 - 1 . 2 × 10 - 13 h cm - 2 s , where h is the reduced Hubble constant in units of 100 km s - 1 Mpc - 1 . If we adopt a universal Simon - Glover - Glotch ( SGG ) disk mass scaling relation , the resulting EUV luminosity density is ( 2 . 2 ± 0 . 7 ) × 10 - 23 h erg - 1 s - 1 , consistent with previous estimates derived from direct UV measurements .Given the significant uncertainties in the modeling of galaxy formation and the beta - ray opacity , the limitations on EBL outlined in this Letter should be interpreted as a range of possible galaxy formation rates and overall EUV luminosity densities . However , the determination of the EUV luminosity abundance is an important milestone in understanding the energetics of galaxy formation over cosmic time .",
        "rewrite_text": "Current experiments on extragalactic background light (EBL) have relied on observations in the optical and far-infrared frequency ranges. These measurements are associated with several systematic uncertainties, and the EBL in the ultraviolet (UV) and dark X-ray frequency regions remains largely unexplored. In this study, we employ gamma-ray data from the Fermi Large Area Telescope to establish constraints on EBL in the EUV frequency region, extending down to a few eV, which corresponds to the effective photon energy of electromagnetic radiation between 10 and 511 keV. We utilize a statistical model of galaxy formation to predict the optical depth for alpha-ray pair production.\n\nOur findings indicate that the total EBL in the EUV wavelength region is (3.0 ± 0.9) × 10^-13 h erg-2, or equivalently, the star formation rate is (1.2 ± 0.4) × 10^-12 h^1.6 - 1.2 × 10^-13 h cm^-2 s, where h represents the reduced Hubble constant in units of 100 km s^-1 Mpc^-1. If we adopt a universal Simon-Glover-Glotch (SGG) disk mass scaling relationship, the resulting EUV luminosity density is (2.2 ± 0.7) × 10^-23 h erg-1 s-1, which aligns with previous estimates derived from direct UV measurements.\n\nGiven the considerable uncertainties associated with galaxy formation modeling and beta-ray opacity, the limitations on EBL presented in this study should be interpreted as a range of potential galaxy formation rates and overall EUV luminosity densities. Nevertheless, determining the abundance of EUV luminosity remains a crucial milestone in understanding the energy dynamics of galaxy formation across cosmic time.",
        "ori-fast-z-score": 1.5540573797716226,
        "water-fast-z-score": 5.883484054145521,
        "rewrite-fast-z-score": 4.233243907726187
    },
    {
        "original_text": "In this paper, we consider the problem of identifying stragglers in data streams. Straggler identification is an important problem in the design and analysis of distributed systems, and has received much attention in the literature. Classical solutions to this problem build a timestamped logical clock, and often require a prohibitive number of timestamps to converge. More recently, variants of Bloom filters have been used for straggler identification, enabling data stream applications with affordable memory and fast detection. Bloom filters, however, are not invertible, and so existing solutions to Bloom filter-based straggler identification (both in the literature and in commercial systems) are inherently blind: they cannot accurately identify which processes are stragglers. We overcome this limitation by harnessing the power of Newton s identities, a set of three deterministic matrix equations that are invertible. Unlike Bloom filters, our solution can accurately identify stragglers. We describe an algorithm that, using just two timestamps per stream element, can identify straggler processes with high probability, and in particular can accurately identify stragglers to within a set of n processes, for any specified n. We validate our method via extensive experiments on a variety of benchmark data streams, including real-world data from a large-scale production system, and show that our method outperforms the state of the art by orders of magnitude.",
        "watermark_text": "In this paper , we investigate the question of identifying stragglers in data streams . Straggler recognition is an important task in the development and evaluation of distributed systems , and has garnered considerable scrutiny in the literature .Classical solutions to this situation create a timestamped logical clock , and sometimes involve a prohibitive quantity of timestamps to converge . More recently , variants of Bloom filters have been used for straggler identity , allowing data stream applications with affordable memory and fast screening .Bloom filters , however , are not invertible , and so existing solutions to Bloom filter - based straggler identity ( both in the literature and in consumer systems ) are inherently blind : they cannot accurately identification which processes are stragglers . We resolve this limitation by harnessing the power of Newton s identities , a group of three deterministic matrix equations that are invertible .Unlike Bloom filters , our solution can accurately identification stragglers . We define an algorithm that , using just two timestamps per flow component , can identify straggler processes with high probability , and in particular can accurately identification stragglers to within a group of n processes , for any specified n . We validate our technique via extensive experiments on a variety of benchmark data streams , notably real - time data from a large - scale production scheme , and suggest that our technique outperforms the state of the art by orders of magnitude .",
        "rewrite_text": "In this study, we explore the challenge of identifying slow-performing elements within data streams. The recognition of stragglers plays a pivotal role in the development and evaluation of distributed systems, and has been extensively studied in the academic literature. Traditional approaches often involve creating a timestamped logical clock, sometimes with an extensive requirement for timestamps to converge.\n\nMore recently, variations of Bloom filters have been utilized for straggler identification, offering data stream applications a feasible solution with manageable memory usage and quick screening capabilities. However, Bloom filters possess a limitation in that they are not reversible, leading to existing solutions based on Bloom filters (both in academic works and consumer systems) being inherently blind. They struggle to accurately determine which processes are stragglers.\n\nTo overcome this limitation, we harness the power of Newton's identities - a set of three deterministic matrix equations that are invertible. Distinguishing from Bloom filters, our solution offers precise straggler identification. We have developed an algorithm that, with just two timestamps per flow component, can identify straggler processes with high probability. Specifically, it can accurately identify stragglers within a group of n processes for any specified n.\n\nWe validate our technique through extensive experiments using various benchmark data streams, including real-time data from a large-scale production scenario. Our findings suggest that our technique significantly outperforms current state-of-the-art methods.",
        "ori-fast-z-score": -0.7921180343813395,
        "water-fast-z-score": 5.742855749264711,
        "rewrite-fast-z-score": 0.3713906763541037
    },
    {
        "original_text": "Flory s model of polymerisation in dilute solution assumed that the radius of the new monomer shell does not depend on the number of subunits in the cluster. This is at odds with the original model of Smoluchowski and Marcus-Lushnikov, which included this effect. In this work we derive the Smoluchowski and Marcus-Lushnikov models from the Flory model with both excluded-volume and density-dependent interactions. Both exact treatment and mean-field theory are considered. In the mean-field theory, the correlation functions are expressed through the pair correlation function, which is obtained from simulations. It is found that the Flory model with excluded-volume interactions, but without the density-dependent term, can be used to describe the Smoluchowski model, whereas the Flory model with both excluded-volume and density-dependent interactions can be used to describe the Marcus-Lushnikov model. As a check, we simulate the Marcus-Lushnikov model and find good agreement with the exact solution.",
        "watermark_text": "Flory s concept of polymerisation in dilute solution proposed that the radius of the new monomer shell does not change on the number of subunits in the cluster . This is at odds with the previous model of Smoluchowski and Marcus - Lushnikov , which included this effect .In this study we derive the Smoluchowski and Marcus - Lushnikov models from the Flory description with both excluded - volume and density - dependent interactions . Both exact treatment and mean - field model are considered .In the mean - field model , the interaction functions are expressed through the pair correlation function , which is found from simulations . It is found that the Flory description with excluded - volume interactions , but without the density - dependent term , can be used to explain the Smoluchowski model , whereas the Flory description with both excluded - volume and density - dependent interactions can be used to explain the Marcus - Lushnikov model .As a check, we simulate the Marcus-Lushnikov model and find good agreement with the exact solution.",
        "rewrite_text": "The concept of Flory regarding polymerisation in a dilute solution suggests that the radius of the new monomer shell remains unaffected by the number of subunits within the cluster. This contradicts the earlier models proposed by Smoluchowski and Marcus-Lushnikov, which had taken into account this particular effect. In this investigation, we derive the Smoluchowski and Marcus-Lushnikov models from Flory's description, considering both excluded-volume and density-dependent interactions. Both precise treatments and mean-field models are taken into consideration. In the mean-field model, interaction functions are expressed through the pair correlation function, which is determined through simulations. It has been observed that Flory's description with excluded-volume interactions, but without the density-dependent term, can explain the Smoluchowski model, while including both types of interactions can elucidate the Marcus-Lushnikov model. To verify our findings, we simulate the Marcus-Lushnikov model and find a good alignment with the exact solution.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.4465617474213164,
        "rewrite-fast-z-score": -0.4923659639173309
    },
    {
        "original_text": "Two mutually perpendicular velocity components are measured as a function of drive velocity and pulse spacing for a solid-on-solid interface propagating under a driving force that includes a phonon-assisted component. The interface is revealed via a colorized scanning tunneling microscope image of a six-vertex columnar defect in a tungsten film on a tungsten telluride surface. The phonon-assisted component of the drive force arises from coupling between the interfacial Shockley-Read-Hall fluctuations and resonant intralayer phonon modes. The two velocity components are coupled, with the magnitude of the coupled component increasing with decreasing pulse spacing. The coupled velocity components remain distinct for pulse spacings above approximately 0.3 ps, corresponding to a driving force insufficient to surmount the energy barrier separating interface modes. The results are discussed in the context of an effective interface temperature and in terms of feedback effects arising from interfacial fluctuations, both of which are central to understanding energy transfer and energy landscape fluctuations in surface nanostructuring.",
        "watermark_text": "Two mutually perpendicular velocity components are measured as a function of drive speed and signal spacing for a solid - on - solid connection propagating under a drove force that contains a phonon - aided component . The interface is discovered via a colorized scan tunneling microscope image of a six - vertex columnar defect in a tungsten movie on a tungsten telluride surface .The phonon - aided component of the drive power arises from interaction between the interfacial Shockley - Read - Hall fluctuations and resonant intralayer phonon modes . The two velocity components are coupled , with the magnitude of the coupled element increasing with decreasing pulse spacing .The correlated frequency parts remain distinct for pulse spacings above roughly 0 . 3 ps , equivalent to a drove force unable to surmount the power barrier separating interface types . The results are discussed in the context of an effective interface temperature and in terms of feedback effects resulting from interfacial fluctuations , both of which are central to discussing energy flow and energy landscape fluctuations in surface nanostructuring .",
        "rewrite_text": "Two perpendicular velocity components are determined based on the drive speed and signal spacing for a solid-on-solid connection propagation, which is driven by a force containing a phonon-assisted component. The interface is identified through a colorized scan tunneling microscope image of a six-vertex columnar defect in a tungsten film on a tungsten telluride surface. The phonon-assisted component of the driving force arises from interactions between interfacial Shockley-Read-Hall fluctuations and resonant intralayer phonon modes. These two velocity components are interconnected, with the magnitude of the coupling element increasing as the pulse spacing decreases. For pulse spacings greater than approximately 0.3 ps, the correlated frequency components remain distinct, which is equivalent to a driving force insufficient to overcome the power barrier separating interface types. These findings are discussed in the context of effective interface temperature and feedback effects resulting from interfacial fluctuations, both of which play a crucial role in discussing energy flow and energy landscape fluctuations in surface nanostructuring.",
        "ori-fast-z-score": -1.2701705922171767,
        "water-fast-z-score": 4.965212315030781,
        "rewrite-fast-z-score": 1.937329799813845
    },
    {
        "original_text": "Coronal mass ejections (CMEs) are giant clouds of solar plasma and magnetic field released into space from the Sun. CMEs can cause major disturbances in the magnetosphere of Earth, resulting in loss of satellite signals and voltage instability on the power grid. Thanks to high resolution numerical simulations, we discovered two distinct CME dynamical regimes: one fast and one slow. The fast CME, typically released from sunspots, can reach speeds over 1000 km/s. It is powered by the fast reconnection of the sheared magnetic fields, and its impact on the magnetosphere is tremendous. In contrast, the slow CMEs originate from the plage regions and are released through small magnetic x-points, with much lower speeds of around 300 km/s. These CMEs do not significantly disturb the magnetosphere, but can still cause major power grid disturbances and satellite signals losses due to the high amount of flux involved. We suggest that slow CMEs are more common than fast CMEs, but they are usually unnoticed as they do not cause dramatic disturbances.",
        "watermark_text": "Coronal mass ejections ( CMEs ) are big clouds of solar plasma and magnetic force sent into space from the Sun . CMEs can cause major disturbances in the magnetosphere of Earth , leading in losing of spacecraft signals and voltage disruption on the power system .Thanks to large resolution statistical simulations , we identified two different CME dynamical regimes : one fast and one slow . The fast CME , generally produced from sunspots , can reach speeds over 1000 kilometers / s .It is powered by the fast reconnection of the sheared magnetic fields , and its impact on the magnetosphere is tremendous . In comparison , the slow CMEs originate from the plage regions and are released through tiny magnetic x - points , with far lower velocity of around 300 kilometers / s .These CMEs do not greatly interrupt the magnetosphere , but can also create major power system disturbances and radar transmissions losses owing to the high excess of flux involved . We suggest that slow CMEs are more common than rapid CMEs , but they are typically unnoticed as they do not cause dramatic disturbances .",
        "rewrite_text": "Coronal mass ejections (CMEs) are vast clouds of solar plasma and magnetic force expelled into space from the Sun. These ejections have the potential to induce significant disturbances in the Earth's magnetosphere, resulting in the loss of spacecraft signals and voltage fluctuations in power systems. Through high-resolution statistical simulations, we have identified two distinct dynamical phases of CMEs: one fast and one slow.\n\nThe fast-moving CMEs, predominantly generated from sunspots, can achieve speeds exceeding 1,000 kilometers per second. This is powered by the rapid reconnection of sheared magnetic fields, which has a profound impact on the magnetosphere. In contrast, slow CMEs originate from plage regions and are released through small magnetic X-points, with a much slower velocity of approximately 300 kilometers per second. Although they do not greatly disturb the magnetosphere to the same extent, these CMEs can still cause major power system disturbances and radar transmission losses due to the high flux excess involved. We suggest that slow CMEs are more prevalent than fast ones, but often go unnoticed as they do not create dramatic disturbances.",
        "ori-fast-z-score": -1.0533703247651751,
        "water-fast-z-score": 6.277372492166241,
        "rewrite-fast-z-score": -0.8834522085987723
    },
    {
        "original_text": "Ergodic Model for the Expansion of Spherical Nanoplasmas As nanotechnology improves, there is great interest in understanding the electrical and mechanical properties of nanoparticles. One of the main problems in these studies is that it is very difficult to simulate the behaviour of a nanoparticle in a realistic material using a continuum model due to the enormous differences in scale. In this article, we develop a discrete dynamical model for the collision and evolution of finitely many nanoparticles, inspired by recent continuum theories for composite materials. The model is derived as an approximation of a continuous model for a finite number of particles, and consists of coupled nonlinear delay equations for the time-dependent positions and shape of the particles. We apply this model to study the scaling of a nanoplasma, and find that for certain parameter regimes the particle dynamics are ergodic, in which case mean values can be estimated by time-averaging the positions and shapes of the particles. We show that in the ergodic regime the size and shape of the nanoplasmas are well approximated by the radius and inverse convex hull of the particles  equilibrium positions, respectively.",
        "watermark_text": "Ergodic Model for the Expansion of Spherical Nanoplasmas As nanotechnology improves , there is great concern in understanding the electrical and physical properties of nanoparticles . One of the main problems in these research is that it is very difficult to simulate the behaviour of a nanoparticle in a realistic solid using a continuum model owing to the immense differences in scale .In this article , we develop a discrete dynamical theory for the collision and evolution of finitely many nanoparticles , influenced by recent continuum theories for composite materials . The model is developed as an approximation of a continuous model for a finite number of particles , and consists of coupled nonlinear delay equations for the period - dependent places and shape of the atoms .We use this model to study the scaling of a nanoplasma , and find that for particular parameter regimes the particle dynamics are ergodic , in which case mean values can be adjusted by time - averaging the positions and shapes of the molecules . We see that in the ergodic regime the height and shape of the nanoplasmas are better approximated by the radius and inverse convex hull of the molecules equilibrium places , respectively .",
        "rewrite_text": "A Discrete Dynamical Model for the Expansion of Spherical Nanoplasmas\n\nWith the advancement of nanotechnology, there is a pressing need to comprehend the electrical and physical attributes of nanoparticles. A major challenge in this research field lies in the immense scale differences that make it exceedingly difficult to simulate the behavior of nanoparticles within a realistic solid using a continuous model.\n\nIn this article, we introduce a discrete dynamical theory to explore the collisions and evolution of a finite number of nanoparticles. This theory builds on recent continuum theories for composite materials. This model serves as an approximation for a continuous model involving a finite particle count, featuring intertwined nonlinear delay equations that account for the period-dependent locations and shapes of atoms.\n\nWe employ this model to investigate the scaling behavior of a nanoplasma and discover that under specific parameter conditions, the particle dynamics exhibit ergodicity. In this ergodic regime, mean values can be adjusted through time-averaging the positions and shapes of molecules. We observe that in this regime, the height and shape of the nanoplasmas are more accurately approximated by the radius and the inverse convex hull of the molecules' equilibrium positions, respectively.",
        "ori-fast-z-score": -2.038098661460272,
        "water-fast-z-score": 3.623286509262706,
        "rewrite-fast-z-score": 1.0660035817780522
    },
    {
        "original_text": "A number of recent papers have proposed new Quantum Hard-Sphere (QHS) equations of state (EOSs) for nuclear matter that are more consistent with both quantum many-body theory and nuclear experiment than the widely used Quantum Ideal Gas (QIG) EOS. Most of these proposed EOSs are based on a transformation of the many-body QHS Hamiltonian such that the quantum many-body system resembles an equivalent mean-field system, with a corresponding introduction of a  quantum liquid  equation of state (EOS) for the nuclear quantum many-body system. This paper critically evaluates this new class of proposed EOSs. After presenting a general critique of the various assumptions underlying the various proposed EOSs, we present a detailed analysis of the proposed EOS for neutron matter, which yields significantly improved analytic results over those previously proposed. We then apply these analyses to a proposed EOS for symmetric nuclear matter, yielding improved analytic results compared to both the widely used QIG EOS and the previously proposed neutron matter EOS. This improved performance is achieved through a new constraint on the EOS, based on nuclear saturation properties at low densities, and a more sophisticated fit to the high-density behavior of the EOS. Finally, we present analytic and numerical results for a proposed EOS for symmetric nuclear matter that yields an EOS that is simultaneously consistent with both the general critique presented and with the detailed analyses of the neutron matter and symmetric nuclear matter EOSs. This paper presents a new class of proposed equations of state for nuclear matter that are more consistent with both nuclear many-body theory and experiment than the widely used Quantum Ideal Gas (QIG) equation of state. These new equations of state, which we refer to as Quantum Hard-Sphere (QHS) equations of state, begin by rewriting the many-body Hamiltonian in second quantization in terms of creation and annihilation operators that satisfy the usual commutation relations. Next, the many-body Hamiltonian is transformed such that the nuclear quantum many-body system resembles an equivalent system of non-interacting particles (the  mean-field  system) that can be analyzed using standard classical many-body theory techniques. This leads to a new class of proposed equations of state for nuclear matter. The Hamiltonian for the equivalent mean-field system is parameterized in terms of an effective interaction energy and a chemical potential, with the effective interaction energy given by the second virial coefficient of the original many-body Hamiltonian. This parameterization of the Hamiltonian allows the authors to fit the predicted EOS to both nuclear many-body theory results and nuclear matter ground state properties. The proposed equation of state for symmetric nuclear matter is then analyzed in detail, yielding significantly improved analytic results over both the widely used Quantum Ideal Gas equation of state and the previously proposed equation of state for neutron matter. Finally, the proposed EOSs are shown to be consistent with a more general class of proposed equations of state by relaxing one of the key assumptions underlying the model.",
        "watermark_text": "A several of recent publications have proposed new Quantum Hard - Sphere ( QHS ) equations of state ( EOSs ) for nuclear material that are more consistent with both quantum several - bodies physics and nuclear experiment than the generally used Quantum Ideal Gas ( QIG ) EOS . Most of these suggested EOSs are based on a transformation of the many - bodies QHS Hamiltonian such that the quantum several - bodies scheme appears an analogous mean - field system , with a corresponding development of a quantum liquid equation of state ( EOS ) for the nuclear quantum several - bodies system .This paper critically evaluates this new category of suggested EOSs . After presenting a general critique of the various concepts underlying the various proposed EOSs , we present a detailed analysis of the suggested EOS for neutron matter , which generates considerably enhanced analytic results over those previously considered .We then use these calculations to a possible EOS for symmetric nuclear material , yielding enhanced analytic results relative to both the generally used QIG EOS and the previously adopted neutron matter EOS . This enhanced efficiency is achieved through a new constraint on the EOS , based on nuclear saturation properties at low densities , and a more sophisticated fitting to the high - density dynamics of the EOS .Finally , we present analytic and mathematical findings for a possible EOS for symmetric nuclear material that yields an EOS that is simultaneously compatible with both the general critique presented and with the detailed analyses of the neutron matter and symmetric nuclear material EOSs . This paper offers a new category of suggested equations of state for nuclear material that are more consistent with both nuclear many - bodies physics and experiment than the generally used Quantum Ideal Gas ( QIG ) equation of state .These new equations of state , which we call to as Quantum Hard - Sphere ( QHS ) equations of state , start by rewriting the many - bodies Hamiltonian in second quantization in terms of creation and annihilation operators that fulfill the usual commutation relations . Next , the many - bodies Hamiltonian is converted such that the atomic quantum several - bodies scheme describes an analogous system of non - interacting molecules ( the mean - field system ) that can be analyzed using conventional traditional many - bodies physics techniques .This leads to a new category of suggested equations of state for nuclear material . The Hamiltonian for the equivalent mean - field system is parameterized in terms of an efficient interaction power and a chemical potential , with the effective interaction power given by the second virial constant of the previous many - bodies Hamiltonian .This parameterization of the Hamiltonian allows the authors to fitting the expected EOS to both nuclear many - bodies physics results and reactor matter ground state properties . The proposed equation of state for symmetric nuclear material is then analyzed in detail , yielding significantly improved analytic results over both the generally used Quantum Ideal Gas equation of state and the previously adopted equation of state for neutron matter .Finally , the suggested EOSs are shown to be compatible with a more general family of suggested equations of state by relaxing one of the key conditions underlying the model .",
        "rewrite_text": "Several recent publications have proposed innovative Quantum Hard-Sphere (QHS) equations of state (EOSs) for nuclear materials. These new EOSs are more aligned with both quantum many-body physics and nuclear experimental data than the commonly used Quantum Ideal Gas (QIG) EOS. The majority of these suggested EOSs are based on a transformation of the many-body QHS Hamiltonian, which results in a mean-field system analogous to a quantum liquid equation of state for the nuclear many-body system.\n\nThis paper critically evaluates this new category of EOSs. After providing a general critique of the underlying concepts in various proposed EOSs, we conduct a detailed analysis of the suggested EOS for neutron matter, which generates significantly enhanced analytical results compared to previous considerations. We then apply these calculations to a potential EOS for symmetric nuclear materials, yielding improved analytical results compared to both the traditional QIG EOS and previously used neutron matter EOSs.\n\nThis improved efficiency is achieved through a new constraint on the EOS, which relies on nuclear saturation properties at low densities, and a more sophisticated fitting to the high-density dynamics of the EOS. Furthermore, we present analytical and mathematical findings for a potential EOS of symmetric nuclear materials that is compatible with both the general critique presented and the detailed analyses of neutron matter and symmetric nuclear material EOSs.\n\nThis paper introduces a novel category of EOSs for nuclear materials that are more consistent with both nuclear many-body physics and experimental data than the commonly used QIG EOS. These new QHS EOSs begin with rewriting the many-body Hamiltonian in second quantization using creation and annihilation operators that obey standard commutation relations. Subsequently, the many-body Hamiltonian is modified such that the atomic many-body scheme describes a system of non-interacting molecules (a mean-field system) that can be analyzed using conventional many-body physics techniques.\n\nThis leads to a new category of proposed EOSs for nuclear materials. The Hamiltonian for the equivalent mean-field system is parameterized in terms of an efficient interaction power and a chemical potential, with the effective interaction power determined by the second virial constant of the original many-body Hamiltonian. This parameterization of the Hamiltonian allows the authors to fit the expected EOS to both nuclear many-body physics results and reactor matter ground state properties.\n\nThe proposed EOS for symmetric nuclear materials is then thoroughly analyzed, resulting in significantly improved analytical outcomes compared to both the standard QIG EOS and previously employed neutron matter EOSs. Finally, the suggested EOSs are shown to be compatible with a broader range of proposed EOSs by relaxing one of the key assumptions underlying the model.",
        "ori-fast-z-score": -0.9430419201928972,
        "water-fast-z-score": 10.625,
        "rewrite-fast-z-score": 4.314201525399818
    },
    {
        "original_text": "Aim: In this work we model the high-frequency quasi-periodic oscillations (HFQPOs), also known as mHz QPOs, observed from some black hole X-ray binaries. We focus on a particular example, the European Photon Imaging Camera (EPIC) instrument on the European Space Agency s (ESA s) XMM-Newton space telescope, which has detected several HFQPOs between 30 and 300 Hz, during the first months of 2015 in the X-ray emission from the neutron star low-mass X-ray binary Serxajs A. The energy dependence of these HFQPOs cannot be well-described by current accretion disc models, where the observed frequencies are determined by the Keplerian orbital frequency of material in an accretion disc around a compact object. We, therefore, introduce a new model for the HFQPOs, in which the QPOs are produced by resonantly driven warps in the inner parts of the accretion flow, near the black hole or neutron star. The warps are driven by global instabilities in the accretion flow, in particular the magneto-rotational instability. The model is able to successfully describe the energy-dependent behaviour of the HFQPOs. Additionally, we show that this model can also simultaneously explain other observed spectral behaviours of Serxajs A, such as the variability in the iron Kα fluorescence line, as well as observed X-ray spectral hysteresis.",
        "watermark_text": "Aim : In this project we study the high - frequency quasi - periodic oscillations ( HFQPOs ) , sometimes called as mHz QPOs , detected from some dark hole X - ray binaries . We focus on a particular example , the European Photon Imaging Camera ( EPIC ) instrument on the European Space Agency s ( ESA s ) XMM - Newton space telescope , which has detected several HFQPOs between 30 and 300 Hz , during the first days of 2015 in the X - ray radiation from the neutron star small - weight X - ray binary Serxajs A .The energy dependence of these HFQPOs cannot be well - described by current accretion disc methods , where the seen frequencies are decided by the Keplerian orbital frequency of material in an accretion disc around a compact body . We , thus , introduce a new model for the HFQPOs , in which the QPOs are produced by resonantly pushed warps in the inner parts of the accretion flow , near the dark hole or neutron star .The warps are driven by global instabilities in the accretion flow , in notably the magneto - rotational instability . The model is able to effectively define the energy - dependent behaviour of the HFQPOs .Additionally , we prove that this description can also simultaneously explain other observed spectral behaviours of Serxajs A , such as the variability in the metal Kα fluorescence line , as well as demonstrated X - ray spectral hysteresis .",
        "rewrite_text": "Objective: In this project, we investigate high-frequency quasi-periodic oscillations (HFQPOs), sometimes referred to as mHz QPOs, which have been detected in some dark hole X-ray binaries. Our focus is on a specific instance involving the European Photon Imaging Camera (EPIC) instrument on the European Space Agency's (ESA) XMM-Newton space telescope. This instrument detected several HFQPOs ranging from 30 to 300 Hz during the initial days of 2015 in the X-ray radiation emitted by the neutron star, low-mass X-ray binary Serxajs A.\n\nThe current methods of accretion disc theory struggle to adequately describe the energy dependence of these HFQPOs. The observed frequencies are determined by the Keplerian orbital frequency of material in an accretion disc around a compact body. Therefore, we introduce a new model for HFQPOs, in which the QPOs are generated by resonantly induced warps in the inner regions of the accretion flow, close to the dark hole or neutron star. These warps are driven by global instabilities in the accretion flow, particularly the magneto-rotational instability.\n\nOur model effectively characterizes the energy-dependent behavior of the HFQPOs. Furthermore, we demonstrate that this description can also simultaneously explain other observed spectral behaviors of Serxajs A, such as variations in the metal Kα fluorescence line, as well as demonstrated X-ray spectral hysteresis.",
        "ori-fast-z-score": -0.7592566023652966,
        "water-fast-z-score": 4.528976474544414,
        "rewrite-fast-z-score": 1.6681153124565982
    },
    {
        "original_text": "Cooling flow clusters of galaxies have extraordinarily high rates of galaxy cluster-scale cooling: up to 30 million degrees Celsius per square degree. This intracluster medium, while of low entropy, is nevertheless observed to be cooling via the X-ray band. Z3146 is one of the most dramatic cooling flow clusters, owing its name to its extreme X-ray temperature of Z=3146 keV, the highest of any cluster known. Observations from the Chandra and XMM-Newton X-ray observatories along with the Keck-II telescope and the Very Large Telescope have been used to study the morphology, physical state, and dynamics of the cooling flow gas. We find that the gas is distributed in two massive and morphologically distinct flows: a western cooling flow that extends across much of the cooling radius and a smaller eastern flow that is offset from the cool core and much more concentrated. Both flows are smooth, free of bubbles or irregularities, and have well-behaved temperature, abundance, and entropy profiles. The total mass of the gas in the cooling flow is estimated to be 2.8 x 10 (massiveyon $h_0 = 0.704$ - see section 5.1) and approximately 3.3 x 10 of this is likely to be in the form of galaxies, making Z3146 one of the most massive galaxy clusters known.",
        "watermark_text": "Cooling flow clusters of stars have extraordinarily large rates of galaxy cluster - scale cooling : up to 30 million degrees Celsius per square degree . This intracluster medium , while of lowered entropy , is nevertheless observed to be cooling via the X - ray band .Z3146 is one of the most dramatic cooling flow clusters , owing its name to its intense X - ray temperature of Z = 3146 keV , the highest of any cluster known . Observations from the Chandra and XMM - Newton X - ray observatories along with the Keck - II telescope and the Very Large Telescope have been used to study the morphology , physical state , and dynamics of the cooling flow gas .We see that the gas is spread in two huge and morphologically unique flows : a western cooling flow that extends across many of the cooling radius and a smaller eastern flow that is offset from the cool core and much more localized . Both flows are smooth , free of bubbles or irregularities , and have highly - behaved temperature , concentration , and entropy profiles .The total mass of the gas in the cooling flow is estimated to be 2 . 8 x 10 ( massiveyon $ h _ 0 = 0 . 704 $ - see section 5 . 1 ) and nearly 3 . 3 x 10 of this is suggested to be in the form of galaxies , making Z3146 one of the most massive galaxy clusters known .",
        "rewrite_text": "Star clusters with cooling flows exhibit remarkable rates of galaxy cluster-scale cooling, reaching up to 30 million degrees Celsius per square degree. Despite the reduced entropy of the intracluster medium, it is still observed to be cooling via the X-ray spectrum. Z3146 is a prime example of such a cooling flow cluster, named for its intense X-ray temperature of Z = 3146 keV, the highest among all known clusters.\n\nObservations from the Chandra, XMM-Newton X-ray observatories, as well as the Keck-II telescope and the Very Large Telescope, have been utilized to study the morphology, physical state, and dynamics of the cooling flow gas. It is evident that the gas is distributed in two enormous and distinct flows: a western cooling flow extending across numerous cooling radii and a smaller eastern flow offset from the cool core and more localized. Both flows are smooth, lacking any bubbles or irregularities, and possess well-behaved temperature, concentration, and entropy profiles.\n\nThe estimated total mass of the gas in the cooling flow is 2.8 x 10 with an assumed h0 = 0.704 (refer to section 5.1). Furthermore, nearly 3.3 x 10 of this mass is believed to be in the form of galaxies, making Z3146 one of the most massive galaxy clusters known.",
        "ori-fast-z-score": 0.3144854510165755,
        "water-fast-z-score": 3.4593399611823306,
        "rewrite-fast-z-score": 1.5389675281277313
    },
    {
        "original_text": "A thermal instability leading to the formation of an oscillatory secular mode is described in the context of stellar pulsations. This instability is demonstrated in the presence of a temperature gradient, with the cold side of the interface unstable. The resulting oscillation is analogous to the well-known Kelvin-Helmholtz instability, but with a cyclic rather than monotonic dependence on the wave number. Using modern opacity computations, the possible loci of the instability are explored and the condition for its appearance is determined. Results of numerical calculations confirm the validity of the proposed theory and demonstrate the appearance of oscillatory secular modes. The authors suggest that oscillatory secular modes, similar to the ones described herein, may also appear in other astrophysical systems (e.g. in accretion disks, gaseous halos of galaxies, etc.) and therefore could be ubiquitous. The oscillatory secular mode is of particular interest since it can explain certain features of old and warm stars, such as their ellipsoidal shape and the existence of equatorial features (e.g. bright spots, bright rings).",
        "watermark_text": "A thermal instability leading to the formation of an oscillatory secular mode is studied in the context of stars pulsations . This instability is demonstrated in the presence of a temperature gradient , with the cool end of the interface unstable .The resulting oscillation is analogous to the better - famous Kelvin - Helmholtz disturbance , but with a cyclic rather than monotonic dependence on the wave number . Using practical opacity computations , the possible loci of the instability are researched and the situation for its appearance is determined .Results of numerical measurements confirm the legitimacy of the suggested theory and establish the appearance of oscillatory secular modes . The authors suggest that oscillatory secular modes , comparable to the ones mentioned herein , might actually appear in other astrophysical systems ( e . g .in accretion disks , gaseous halos of galaxies , etc . ) and therefore may be ubiquitous .The oscillatory secular mode is of especially interest since it can describe certain characteristics of ancient and warm stars , such as their ellipsoidal form and the existence of equatorial elements ( e . g . bright spots , faint rings ) .",
        "rewrite_text": "A study is conducted on the thermal instability that results in the formation of oscillatory secular modes within the context of stellar pulsations. This instability is demonstrated to occur in the presence of a temperature gradient, with the cooler end of the interface being unstable. The resulting oscillation bears similarities to the more well-known Kelvin-Helmholtz disturbance, but exhibits a cyclic rather than a monotonic dependency on wave numbers.\n\nBy utilizing practical opacity calculations, the potential locations of this instability are explored, and the conditions for its occurrence are determined. The results of numerical measurements validate the suggested theory and establish the existence of oscillatory secular modes. The authors propose that oscillatory secular modes, comparable to the ones studied, may actually manifest in other astrophysical systems (such as accretion disks, gaseous halos of galaxies, etc.), suggesting they could be ubiquitous.\n\nThe oscillatory secular mode is particularly intriguing as it can describe certain characteristics of ancient and warm stars, such as their ellipsoidal shape and the presence of equatorial features (such as bright spots, faint rings).",
        "ori-fast-z-score": -1.4444444444444444,
        "water-fast-z-score": 4.837877973981903,
        "rewrite-fast-z-score": 0.1111111111111111
    },
    {
        "original_text": "In this paper, we consider the general case of MIMO systems with Finite Rate Channel State Feedback and a Power On/Off strategy. We first characterize the limit on the achievable rate as the length of the feedback channel goes to infinity. In order to illustrate the tightness of the characterization, we provide a counter example where the achievable rate is strictly lower than the limit. Then we characterize the optimal power on/off strategy. Interestingly, for fixed feedback length, we show that the on/off ratio has a threshold behavior with respect to the channel realizations, and the off state is most likely to appear when the channel is in weak interference regime. Moreover, in strong interference regime, even though more power would help increase the achievable rate, it would also increase the feedback burden. We further study the optimal strategy in the intermediate interference regime, and characterize the optimal on/off ratio in terms of the effective channel gain. Numerical results confirm the insights from the analysis.",
        "watermark_text": "In this paper , we study the general case of MIMO systems with Finite Rate Channel State Feedback and a Power On / Off strategy . We first characterize the limit on the achievable rate as the length of the feedback signal goes to infinity .In order to illustrate the tightness of the characterization , we provide a counter instance where the achievable rate is strictly lower than the limit . Then we characterize the ideal power on / off strategy .Interestingly , for fixed feedback length , we find that the on / off ratio has a threshold behavior with regard to the channel realizations , and the off state is most likely to appear when the channel is in weak interference regime . Moreover , in weak interference regime , even though more power would assist improve the achievable rate , it would also raise the feedback load .We further study the ideal technique in the intermediate interference regime , and characterize the ideal on / off ratio in terms of the effective channel gain . Numerical results verify the information from the analysis .",
        "rewrite_text": "In this paper, we explore the general scenario of MIMO systems with finite rate channel state feedback and a power on/off strategy. We initially determine the limit on the achievable data rate as the feedback signal length approaches infinity. To illustrate the accuracy of this characterization, we present a counterexample where the actual achievable rate is strictly below the limit. Subsequently, we delineate the ideal power on/off strategy.\n\nIntriguingly, for a fixed feedback length, we discover that the on/off ratio exhibits a threshold behavior in relation to channel realizations. The off state is most likely to occur when the channel is in a weak interference regime. Furthermore, in the weak interference regime, while increasing power would aid in improving the achievable rate, it would also elevate the feedback load.\n\nWe delve deeper into the ideal technique in the intermediate interference regime and characterize the optimal on/off ratio based on the effective channel gain. Numerical results validate the information derived from our analysis.",
        "ori-fast-z-score": 0.23249527748763857,
        "water-fast-z-score": 3.628275563080048,
        "rewrite-fast-z-score": 1.0125791108334214
    },
    {
        "original_text": "In string theory, Wilson loops provide a powerful tool to analyze the classical string configurations. For more general gauge theories, in particular for quantum chromodynamics (QCD) at large nucleon momentum transfers, the color confinement is better described by quark-antiquark bound states, or mesons, rather than freely moving colored fermions. In particular, a fundamental quantity in the QCD calculation of the hadronic spectrum is the Wilson loop, which, in the light-front formalism, is defined as the path-ordered exponential of the gluon field along a closed loop $C$ in space-time. The fundamental representation of the gauge group is usually chosen. The dependence of the Wilson loop on the loop dynamics is more complicated. In particular, the area law for large loops is no longer true. For small loops, the area law is modified to be proportional to the perimeter of the loop $C$. This phenomenon is called perimeter law. The relation between the area and perimeter laws has been observed for more than 40 years. Recently, by using the supergravity in eleven dimensions, a string theory realization of a long Wilson loop in the fundamental representation of the $SU(N_c)$ group is proposed. The corresponding string configuration is the graviton with one temporal and one spatial infinite size. This allows a detailed comparison between the field theory and gravity results. For finite-size loops, new Quantum Mechanics-like phenomena may arise. For example, it may admit discrete energy levels with a spectrum linearly proportional to the area of the loop. In this article, we present a more general class of string configurations, which are the finite-size gravitons corresponding to loops with arbitrary shapes. We use the second fundamental representation of the $SU(N_c)$ to represent the loops. The corresponding gravity background is a truncation of type IIB supergravity on S^5 x T^{11}. The quantum mechanics-like spectrum on such backgrounds has been studied before. In particular, the different classical shapes of the loop have different behaviors for the quantum mechanics-like states. We also propose a string interpretation for the observed spectra. Finally, we briefly discuss the implications of our results to QCD and discuss possible future work.",
        "watermark_text": "In string theory , Wilson loops provide a powerful tool to analyze the classical string configurations . For more general gauge physics , in particular for quantum chromodynamics ( QCD ) at large nucleon velocity transfers , the color confinement is better understood by quark - antiquark bound states , or mesons , rather than independently moving colored fermions .In particular , a fundamental value in the QCD calculation of the hadronic range is the Wilson loop , which , in the light - front formalism , is characterized as the path - ordered exponential of the gluon field along a closed loop $ C $ in space - time . The fundamental representation of the gauge group is usually choice .The dependence of the Wilson loop on the loop dynamics is more complicated . In particular , the area law for large loops is no longer true .For small loops , the area law is modified to be proportional to the perimeter of the loop $ C $ . This phenomenon is dubbed perimeter law .The relation between the area and perimeter laws has been observed for more than 40 years . Recently , by using the supergravity in eleven dimensions , a string theory realization of a long Wilson loop in the fundamental representation of the $ SU ( N _ c ) $ group is proposed .The equivalent string arrangement is the graviton with one spatial and one spatial infinite size . This enables a detailed analogy between the field theory and gravity results .For finite - length loops , new Quantum Mechanics - like phenomena could occur . For instance , it could accept discrete energy levels with a range linearly proportional to the area of the loop .In this article , we present a more general family of string configurations , which are the finite - length gravitons analogous to strings with arbitrary shapes . We use the second basic representation of the $ SU ( N _ c ) $ to represent the loops .The equivalent gravity background is a truncation of type IIB supergravity on S ^ 5 x T ^ { 11 } . The quantum mechanics - like spectrum on such backgrounds has been studied before .In particular , the different classical shapes of the loop have different behaviors for the quantum mechanics - like states . We additionally offer a string theory for the observed spectra .Finally , we briefly talk the implications of our findings to QCD and consider likely future research .",
        "rewrite_text": "In the context of string theory, Wilson loops offer a highly effective tool for analyzing classical string configurations. For broader applications in gauge physics, especially in Quantum Chromodynamics (QCD) with large nucleon velocity transfers, the color confinement is better understood through quark-antiquark bound states, or mesons, rather than independently moving colored fermions. Specifically, the Wilson loop plays a fundamental role in the hadronic range calculation within QCD. In the light-front formalism, it is characterized as the path-ordered exponential of the gluon field along a closed loop C in space-time. The choice of fundamental representation for the gauge group is common.\n\nThe dependence of the Wilson loop on loop dynamics is more intricate. Specifically, the area law no longer applies for large loops. For smaller loops, the area law is modified to be proportional to the perimeter of the loop C, a phenomenon known as the perimeter law. This relationship between area and perimeter laws has been observed for over 40 years.\n\nRecently, utilizing supergravity in eleven dimensions, a string theory realization of a long Wilson loop in the fundamental representation of the SU(Nc) group has been proposed. This equivalent string arrangement involves a graviton with one spatial and one spatially infinite size, enabling a detailed analogy between field theory and gravity results. For finite-length loops, Quantum Mechanics-like phenomena may arise. For instance, they may exhibit discrete energy levels with a range linearly proportional to the area of the loop.\n\nIn this article, we introduce a broader family of string configurations: finite-length gravitons with arbitrary shapes. We utilize the second basic representation of SU(Nc) to represent these loops. The corresponding gravity background is a truncation of type IIB supergravity on S5 x T11. Previous studies have explored the Quantum Mechanics-like spectrum in such backgrounds. Notably, different classical loop shapes exhibit distinct behaviors for Quantum Mechanics-like states. We also propose a string theory for the observed spectra.\n\nFinally, we briefly discuss the implications of our findings for QCD and consider potential future research directions.",
        "ori-fast-z-score": 0.909717652294684,
        "water-fast-z-score": 6.274210251953172,
        "rewrite-fast-z-score": 2.6765168951565537
    },
    {
        "original_text": "We introduce and study a generalization of fibrations that we call exploded fibrations. These fibrations generalize the notion of fibrations, fiber bundles with possibly non-empty boundary, and allow for fibers that are manifolds with boundary. We study these objects via a filtration of the mapping stack. We show that exploded fibrations with fiber a surface are smooth orbifolds with boundary, and exhibit a factorization of the map to the classifying stack as the union of a smooth map with a smooth map onto the classifying stack of a certain gerbe. We give two examples of exploded fibrations: the map to the mapping stack of a Riemann surface with two boundary components, and the classifying map of a surface bundle over a surface. We also discuss the homotopy theory of exploded fibrations. We define an *exploded fibration* to be a homotopy fiber of a map from a topological fibration. We show that the homotopy type of an exploded fibration can be classified by a simplicial set with a nice set of degenerations. We introduce the notions of an *exploded weak equivalence* and an *exploded fibration*, and give two examples of these classes of maps: the exploded mapping space is weakly equivalent to the mapping space of a topological fibration, and the homotopy fiber of a map of exploded fibrations is exploded fibrations. We show that the homotopy type of an exploded space can be built from its hohstairs by a sequence of functors modeling fibrations, exploded fibrations, and weak equivalence. We conjecture a description of the hohsofatizations of these model category structures on the level of homotopy categories. We expect these hohsofatizations to be algebraic models for proper Maps, genuine Maps, and weak Maps between exploded spaces. Our main example of an exploded space is the classifying space of an exploded group. We show that this is weakly equivalent to the mapping stack of the unit interval with a based point, and that its underlying space is homotopy equivalent to the infinite permutation space of a pointed topological space. Finally, we discuss the parallel transport of vectors on exploded fibrations. We define a *smooth vector*, or instanton, to be a quadruple consisting of a vector on the total space, a path through the structure group, and two tangent vectors on the fiber over the base, such that the tangent vector on the base is the given vector and the parallel transport of the tangent vector in the fiber along the path is the vector itself. We show that parallel transport of smooth vectors is an exploded fibration and exhibit an example of an exploded vector bundle. We conclude with open questions and directions for further research.",
        "watermark_text": "We introduce and study a generalization of fibrations that we call exploded fibrations . These fibrations generalize the notion of fibrations , backbone bundles with possibly non - empty boundary , and allow for fibers that are manifolds with boundary .We explore these objects via a filtration of the mapping stack . We see that exploded fibrations with fiber a surface are smooth orbifolds with boundary , and experience a factorization of the mapping to the classifying stack as the union of a smooth map with a smooth map onto the classifying stack of a certain gerbe .We get two examples of exploded fibrations : the mapping to the mapping stack of a Riemann sphere with two boundary parts , and the classifying map of a surface bundle over a surface . We also discuss the homotopy notion of exploded fibrations .We define an * explosions fibration * to be a homotopy fiber of a mapping from a topological fibration . We see that the homotopy class of an explosions fibration can be categorized by a simplicial setting with a fine set of degenerations .We introduce the concepts of an * explosions weak equivalence * and an * explosions fibration * , and take two examples of these classes of mapping : the exploded maps space is strongly equivalent to the mapping space of a topological fibration , and the homotopy fiber of a mapping of exploded fibrations is exploded fibrations . We see that the homotopy class of an explosions space can be built from its hohstairs by a sequence of functors modeling fibrations , bomb fibrations , and weak equivalence .We conjecture a description of the hohsofatizations of these model category forms on the level of homotopy categories . We predict these hohsofatizations to be algebraic theories for normal Maps , genuine Maps , and weak Maps between exploded spaces .Our main instance of an explosions space is the classifying space of an bomb group . We see that this is weakly equivalent to the mapping stack of the unit interval with a based point , and that its underlying space is homotopy equal to the infinite permutation space of a pointed topological space .Finally , we explain the parallel transport of vectors on exploded fibrations . We define a * smooth vector * , or instanton , to be a quadruple consisting of a vector on the total space , a path through the structure group , and two tangent vectors on the fiber over the base , such that the tangent vector on the base is the given vector and the parallel transport of the tangent vector in the fiber along the path is the vector itself .We see that concurrent travel of smooth vectors is an explosions fibration and exhibit an instance of an explosions vector bundle . We continue with open questions and directions for further study .",
        "rewrite_text": "We present and investigate a broadened concept of fibrations, which we term \"exploded fibrations.\" These generalizations encompass the ideas of fibrations with potential non-empty boundaries in backbone bundles, allowing fibers that are manifolds with boundary properties. We explore these objects by filtering the mapping stack. Specifically, we find that exploded fibrations with surface fibers are smooth orbifolds with boundaries. Within this framework, there's a factorization of the mapping process to the classifying stack, which is a union of a smooth map and a smooth map onto the classifying stack of a specific gerbe.\n\nWe offer two examples of exploded fibrations: the mapping to the mapping stack of a Riemann sphere with two boundary components, and the classifying map of a surface bundle over another surface. We also delve into the homotopy notion of exploded fibrations. We define an \"explosion fibration\" as a homotopy fiber of a mapping from a topological fibration. It becomes evident that the homotopy class of an explosion fibration can be categorized within a simplicial setting, with a meticulous set of degenerations.\n\nFurthermore, we introduce the concepts of an \"explosion weak equivalence\" and an \"explosion fibration.\" Two examples illustrate these mappings: the exploded map space is strongly equivalent to the mapping space of a topological fibration, and the homotopy fiber of a mapping within exploded fibrations is itself an exploded fibration. We observe that the homotopy class of an explosion space can be constructed from its hohstairs through a sequence of functors modeling fibrations, bomb fibrations, and weak equivalences.\n\nWe conjecture a description for the hohsofatizations of these model category forms within the realm of homotopy categories. We predict these hohsofatizations to be algebraic theories for normal maps, genuine maps, and weak maps between exploded spaces. A primary instance of an explosion space is the classifying space of a bomb group. This is seen to be weakly equivalent to the mapping stack of the unit interval with a base point, and its underlying space is homotopy equivalent to the infinite permutation space of a pointed topological space.\n\nFinally, we explain the parallel transport of vectors within exploded fibrations. We define a \"smooth vector,\" or instanton, as a quadruple consisting of a vector in the total space, a path through the structure group, and two tangent vectors at the fiber over the base. This definition ensures that the tangent vector at the base remains the given vector, and the parallel transport of the tangent vector in the fiber along the path remains the same vector. We recognize that the concurrent movement of smooth vectors constitutes an explosion fibration and presents an instance of an explosion vector bundle. Subsequently, we leave open questions and directions for further exploration in this field.",
        "ori-fast-z-score": -1.687322975464215,
        "water-fast-z-score": 5.582449524183669,
        "rewrite-fast-z-score": 3.0308072109137485
    },
    {
        "original_text": "We present new ALMA data of the luminous far-infrared molecular hydrogenCN (HC3N) v=0-1 infrared line emission in the Antennae galaxies,NGC4038 and NGC4039. The analysis of these data provides us with the first view of the central 10pc of this system and reveals the nature of its luminous infrared nuclear starburst (even in the presence of an active galactic nucleus,AGN) and its possible link with similar galaxies and their nuclear activity. The bright infrared nuclear starburst in NGC4418, shining as a dominant contributor of infrared luminosity in the Antennae system, has been known for many years (Braine et al. 1993, Downes & Solomon 1998). The origin of this starburst and its relation to the buried AGN was a long-standing mystery. New ALMA data of the luminous far-infrared molecular hydrogenCN (HC3N) v=0-1 infrared line emission has now provided us with a clear view of the dynamics and energy source in the central 10pc of this system. We detect high velocity outflowing motions in this region, of up to 400 km/s. This is in sharp contrast to the expected speed for purely gravitational collapse at the distance of this source, estimated to be only 50-100 km/s. It is likely that these high-velocity motions are due to the interaction between the molecular gas and a yet-to-be-detected counter-rotating molecular gas component, as previously proposed for the situation in the Milky Way based on other lines (Bronfman et al. 2016; Zhao et al. 2017). Our data is in perfect agreement with the recent findings of Fu et al. (2019), who detect a radio-loud Seyfert-1 nucleus with star formation in NGC4418. This suggests that the growth of supermassive black holes and their activity are intimately connected with the growth of host galaxies and the onset of nuclear starbursts, just like in the case of the Milky Way. In summary, our ALMA data of the nuclear starburst in the Antennae galaxies has provided us with a clear view of its nature and dynamics. It is likely powered by a combination of both the gravitational forces driving a yet-to-be-detected counter-rotating molecular gas component as well as a buried active galactic nucleus. We detect molecular gas driven high-velocity outflowing motions in the central 10pc of this system, in agreement with the recent findings of Fu et al. (2019) who detect a radio-loud active galactic nucleus in this source. This suggests that the growth of supermassive black holes and their activity are intimately connected with the growth of host galaxies and the onset of nuclear starbursts. In summary, these ALMA data of the nuclear starburst in the Antennae galaxies provides us with a clear view of its nature and dynamics.",
        "watermark_text": "We present new ALMA statistics of the luminous deep - infrared molecular hydrogenCN ( HC3N ) v = 0 - 1 infrared line emission in the Antennae galaxies , NGC4038 and NGC4039 . The evaluation of these information provides us with the first view of the central 10pc of this system and reveals the nature of its luminous infrared nuclear starburst ( even in the presence of an active galactic nucleus , AGN ) and its potential link with similar galaxies and their nuclear activity .The bright infrared nuclear starburst in NGC4418 , shining as a dominant contributor of infrared luminosity in the Antennae system , has been known for many years ( Braine et al . 1993 , Downes & Solomon 1998 ) .The origin of this starburst and its connection to the buried AGN was a long - standing mystery . New ALMA statistics of the luminous deep - infrared molecular hydrogenCN ( HC3N ) v = 0 - 1 infrared line emission has now provided us with a clear view of the dynamics and energy source in the central 10pc of this system .We detect high velocity outflowing motions in this area , of up to 400 kilometres / s . This is in sharp contrast to the expected speed for purely gravity collapse at the distance of this source , estimated to be only 50 - 100 km / s .It is probably that these high - speed moves are owing to the interaction between the molecular gas and a yet - to - be - detected counter - spinning molecular gas component , as previously argued for the situation in the Milky Way based on other lines ( Bronfman et al . 2016 ; Zhao et al .2017 ) . Our data is in perfect agreement with the recent results of Fu et al .( 2019 ) , who detect a radio - loud Seyfert - 1 nucleus with star formation in NGC4418 . This shows that the development of supermassive black holes and their activity are intimately tied with the development of host galaxies and the emergence of nuclear starbursts , just like in the case of the Milky Way .In summary , our ALMA statistics of the atomic starburst in the Antennae galaxies has presented us with a clear view of its nature and dynamics . It is probably powered by a combination of both the gravitational pressures rocking a yet - to - be - detected counter - spinning molecular gas component as well as a buried active galactic nucleus .We detect molecular gas driven large - speed outflowing motions in the central 10pc of this system , in agreement with the recent results of Fu et al . ( 2019 ) who detect a radio - loud active galactic nucleus in this source .This shows that the development of supermassive black holes and their activity are intimately tied with the development of host galaxies and the emergence of nuclear starbursts . In summary , these ALMA statistics of the atomic starburst in the Antennae galaxies provides us with a clear view of its nature and dynamics .",
        "rewrite_text": "We present updated ALMA data on the luminous deep-infrared molecular hydrogen CN (HC3N) v=0-1 infrared line emission in the Antennae galaxies, specifically NGC4038 and NGC4039. Analysis of this information offers us our first detailed look into the central 10 pc of this system, revealing the true nature of its bright infrared nuclear starburst - even in the presence of an active galactic nucleus (AGN).\n\nThe prominent infrared nuclear starburst in NGC4418, which has long been recognized as a dominant contributor to the Antennae system's infrared luminosity (Braine et al. 1993, Downes & Solomon 1998), has remained a mystery regarding its origin and connection to the buried AGN. However, new ALMA data on the luminous deep-infrared molecular hydrogen CN (HC3N) v=0-1 line emission has given us a clear picture of the system's dynamics and energy source within the central 10 pc. We have detected high-velocity outflowing motions in this region, reaching up to 400 kilometers per second. This is significantly faster than the expected speed for pure gravitational collapse at this source's distance, which is estimated to be around 50-100 km/s.\n\nIt is likely that these high-speed movements are due to an interaction between the molecular gas and a yet-to-be-detected counter-spinning molecular gas component, as previously suggested based on other lines in the Milky Way (Bronfman et al. 2016; Zhao et al. 2017). Our data aligns perfectly with recent findings by Fu et al. (2019), who detected a radio-loud Seyfert-1 nucleus with star formation in NGC4418. This indicates that the development of supermassive black holes and their activity are closely linked to the evolution of host galaxies and the emergence of nuclear starbursts - similar to what is observed in the Milky Way.\n\nIn summary, our ALMA observations of the atomic starburst in the Antennae galaxies provide us with a clear understanding of its nature and dynamics. It appears to be fueled by a combination of gravitational pressures shaking a yet-to-be-detected counter-spinning molecular gas component and a buried active galactic nucleus. We have detected large-scale high-speed outflowing motions driven by molecular gas within the central 10 pc of this system, which is consistent with Fu et al.'s (2019) findings of a radio-loud active galactic nucleus in this region. This shows that the development of supermassive black holes and their activity are intimately linked to the growth of host galaxies and the occurrence of nuclear starbursts. Overall, these ALMA observations offer us a comprehensive view of the atomic starburst in the Antennae galaxies.",
        "ori-fast-z-score": 1.9335101599064688,
        "water-fast-z-score": 7.519206177414046,
        "rewrite-fast-z-score": 2.165063509461097
    },
    {
        "original_text": "Planets are an important part of any planetary system, and giant planets are of particular interest because of their potential to host life. Although planet detection is typically the first step in determining if a planet lives in the  habitable zone,  where temperatures are suitable for water to be liquid on a planet surface, there are few observations of this critical zone for planets around young stars. This work presents the first direct detection of infrared radiation from the region around giant planets in the habitable zones of nearby stars. This is done using L-band (3.78-3.84 μm) spectroscopy, which has not been widely used for planet detection because it is more difficult to observe from space than other bands, and does not provide the same level of detail as shorter wavelengths. Using the Infrared Telescope on board the Spitzer Space Observatory, we obtained high-quality spectroscopy of two nearby stars, named Tucana and Beta Pictoris, that are members of the moving groups corresponding to their age. We observed each star in the L  (3.74-3.81 μm) and L  (3.84-3.94 μm) water vapor filters, which are extremely sensitive to reflected infrared radiation from planets in the habitable zone of these stars. No planets were detected in these observations, placing upper limits on the planetary water abundance of 1.2×10−5 and 1.3×10−5 for the Tucana and Beta Pictoris systems, respectively. These results are consistent with theoretical predictions for the typical water abundance on a planet in the habitable zone of these stars.",
        "watermark_text": "Planets are an important member of any planetary system , and massive planets are of especially interest because of their potential to host life . Although planet detection is typically the first phase in calculating if a planet lives in the habitable zone , where temperatures are suitable for water to be liquid on a planet surface , there are few measurements of this critical zone for planets around old planets .This research provides the first continuous detection of infrared rays from the region around giant planets in the habitable zones of distant stars . This is accomplished use L - band ( 3 . 78 - 3 . 84 μm ) spectroscopy , which has not been widely using for planet detection because it is more hard to observe from space than other bands , and does not offer the same level of detail as shorter wavelengths .Using the Infrared Telescope on board the Spitzer Space Observatory , we acquired high - grade spectroscopy of two distant stars , designated Tucana and Beta Pictoris , that are part of the moved bands corresponding to their age . We observed each star in the L ( 3 . 74 - 3 . 81 μm ) and L ( 3 . 84 - 3 . 94 μm ) water vapor filters , which are extremely sensitive to reflected infrared light from planets in the habitable zone of these planets .No planets were detected in these observations , placing upper limits on the planetary water abundance of 1 . 2×10−5 and 1 . 3×10−5 for the Tucana and Beta Pictoris systems , respectively . These conclusions are compatible with theoretical expectations for the typical water abundance on a planet in the habitable zone of these planets .",
        "rewrite_text": "Planets are a crucial component of any planetary system, and massive planets hold particular fascination due to their potential to support life. While planet detection typically begins as the initial phase in determining if a planet resides within the habitable zone—where temperatures are suitable for liquid water on a planetary surface—there is a limited amount of data regarding this critical zone for planets orbiting older stars. This research presents the first continuous detection of infrared radiation from regions surrounding giant planets in the habitable zones of distant stars. This accomplishment is achieved through the utilization of L-band (3.78-3.84 μm) spectroscopy, which, despite its challenges in observation from space compared to other spectral bands and the lack of the same level of detail as shorter wavelengths, has not been widely employed in planet detection. By employing the Infrared Telescope aboard the Spitzer Space Observatory, we obtained high-grade spectroscopy of two distant stars—Tucana and Beta Pictoris—that are part of the moved bands related to their age. We observed each star through L-band water vapor filters (3.74-3.81 μm and 3.84-3.94 μm), which are exceptionally sensitive to reflected infrared light from planets within the habitable zones of those planets. However, no planets were detected in these observations, placing upper limits on the planetary water abundance at 1.2×10⁻⁵ and 1.3×10⁻⁵ for the Tucana and Beta Pictoris systems, respectively. These findings align with theoretical expectations for typical water abundance on a planet within the habitable zone of these stars.",
        "ori-fast-z-score": 0.38851434494290565,
        "water-fast-z-score": 7.250523667842477,
        "rewrite-fast-z-score": 4.899851733322857
    },
    {
        "original_text": "A0620-00, an X-ray binary in the center of the globular cluster NGC 6Most likely consists of a black hole accreting from a Be star companion (Menou, et. al. 2001). The system is nearby (4.3 kpc; Reipurth, et. al. 2001), and the black hole has a low mass (6.6 M⊙; McConnachie, et. al. 2009) which makes it visible in the near-infrared. We have obtained near-infrared (NIR) spectra of the system with the NIRSPEC spectrometer on the W. M. Keck II Telescope. The NIRSPEC spectra have a spectral resolution R ≈ 22,500 and cover the 1.0-2.4 μm wavelength range. We use these data to determine the absorption and emission spectrum of the donor star. The donor is an A-type star, and the absorption spectrum displays Balmer and P-Cygni lines consistent with expectations from a spectral classification of A0 Ve. We also detect CO bandhead absorption in the donor spectrum, and measure an equivalent width of W(CO) ≈ 1.0 ± 0.2 Å. This detection of CO bandhead absorption in the spectrum of an X-ray binary donor is surprising and raises new questions about the composition and evolutionary state of the companion star. We compare the NIRSPEC spectra with spectra from the VLT/ISAAC NIR spectrophotometer and find that the NIRSPEC data are consistent with a constant temperature photosphere, as expected for an A-type star. However, we detect strong emission in the Br γ line at 2.16 μm, as well as weak He I 2.06 μm and Ca II triplet emission. This emission is not present in the VLT/ISAAC spectra. We model the NIRSPEC data as a simple shell of gas and dust surrounding the star, and we show that this model is capable of producing the observed Br γ emission and excess near-infrared flux. We discuss the origins and implications of the gas and dust in the environment of the donor star.",
        "watermark_text": "A0620 - 00 , an X - ray binary in the center of the globular cluster NGC 6Most possibly contains of a black hole accreting from a Be star companion ( Menou , et . al .2001 ) . The system is nearby ( 4 . 3 kpc ; Reipurth , et .al . 2001 ) , and the black hole has a small mass ( 6 . 6 [UNK] ; McConnachie , et .al . 2009 ) which makes it noticeable in the near - infrared .We have discovered near - infrared ( NIR ) spectra of the system with the NIRSPEC spectrometer on the W . M . Keck II Telescope . The NIRSPEC spectra have a spectral resolution R ≈ 22 , 500 and cover the 1 . 0 - 2 . 4 μm wavelength range .We use these information to study the absorption and emission spectrum of the donor star . The donor is an A - class star , and the absorption spectrum displays Balmer and P - Cygni lines accordance with predictions from a spectral classification of A0 Ve .We additionally observe CO bandhead emission in the donor spectrum , and measure an corresponding width of W ( CO ) ≈ 1 . 0 ± 0 . 2 Å . This measurement of CO bandhead emission in the spectrum of an X - ray binary recipient is surprising and raises fresh issues about the composition and evolutionary state of the companion galaxy .We compare the NIRSPEC spectra with spectra from the VLT / ISAAC NIR spectrophotometer and find that the NIRSPEC data are consistent with a constant temperature photosphere , as anticipated for an A - class star . However , we find strong absorption in the Br γ line at 2 . 16 μm , as also as powerful He I 2 . 06 μm and Ca II triplet emission .This absorption is not present in the VLT / ISAAC spectra . We model the NIRSPEC data as a simple shell of gas and dust surrounding the star , and we prove that this model is capable of producing the emitted Br γ emission and excess near - infrared flux .We discuss the beginnings and effects of the gas and dust in the conditions of the donor star .",
        "rewrite_text": "A0620-00 is an X-ray binary located at the center of the globular cluster NGC 6. It is highly likely that this system contains a black hole that is accreting matter from a Be star companion (Menou et al., 2001). The system is relatively close by, situated at a distance of 4.3 kpc (Reipurth et al., 2001), and the black hole has a small mass of 6.6 units (McConnachie et al., 2009), making it noticeably prominent in the near-infrared spectrum.\n\nUsing the NIRSPEC spectrometer on the W.M. Keck II Telescope, we have discovered near-infrared (NIR) spectra of this system. The NIRSPEC spectra possess a spectral resolution of approximately 22,500 and span a wavelength range from 1.0 to 2.4 micrometers. We utilize these spectra to investigate the absorption and emission spectrum of the donor star. The donor star is classified as an A-type star, and its absorption spectrum exhibits Balmer and P-Cygni lines, aligning with predictions based on its spectral classification of A0 Ve.\n\nAdditionally, we observe CO bandhead emission in the donor star's spectrum and measure a corresponding width of W(CO) approximately 1.0 ± 0.2 Å. This observation of CO bandhead emission in the spectrum of an X-ray binary recipient is unexpected and raises new questions about the composition and evolutionary state of the companion galaxy.\n\nWe compare the NIRSPEC spectra with data from the VLT/ISAAC NIR spectrophotometer and find that the NIRSPEC data are consistent with a photosphere of constant temperature, as expected for an A-type star. However, we detect strong absorption in the Br γ line at 2.16 micrometers, along with powerful He I 2.06 micrometers and Ca II triplet emission. This absorption is not present in the VLT/ISAAC spectra.\n\nWe model the NIRSPEC data as a simple shell of gas and dust surrounding the star. Our analysis suggests that this model is capable of producing the emitted Br γ emission and the excess near-infrared flux. We discuss the origins and effects of the gas and dust in the context of the donor star's environment.",
        "ori-fast-z-score": -1.9917864129354077,
        "water-fast-z-score": 4.0575133560034455,
        "rewrite-fast-z-score": -0.6859943405700353
    },
    {
        "original_text": "On January 10, 2007, the Spitzer Space Telescope was moved into a new Sun-synchronous orbit, providing an improved view of the entire sky every 3 days. In order to characterize the response of the Spitzer Space Telescope, we performed a survey of the IC 348 nebula, a nearby star-forming region. Using data from this new orbit, we detect 70 structures that are greater than 3.5 sigma above the background, with a total of 56 confirmed YSOs. We also detect 915 structures that are greater than 3.5 sigma but less than our threshold of detection, which we identify as potential YSOs. The high level of detection and the high rate of previously detected sources indicates that our results are statistically significant. We detect 70 structures that are greater than 3.5 sigma above the background, with a total of 56 confirmed YSOs. We also detect 915 structures that are greater than 3.5 sigma but less than our threshold of detection, which we identify as potential YSOs. The high level of detection and the high rate of previously detected sources indicates that our results are statistically significant. This work is the first from the Spitzer Space Telescope to study the full extent of the IC 348 nebula in multiple wavelengths, and it will aid in our understanding of the formation of both single and binary stars.",
        "watermark_text": "On January 10 , 2007 , the Spitzer Space Telescope was moved into a new Sun - synchronous orbit , providing an better vision of the entire sky every 3 days . In order to characterize the response of the Spitzer Space Telescope , we performed a survey of the IC 348 nebula , a neighboring star - creating area .Using results from this new orbit , we identify 70 compounds that are larger than 3 . 5 sigma above the background , with a total of 56 confirmed YSOs . We additionally observe 915 structures that are larger than 3 . 5 sigma but less than our threshold of detection , which we identify as possible YSOs .The high degree of detection and the high rate of previously observed sources shows that our findings are statistically substantial . We detect 70 compounds that are larger than 3 . 5 sigma above the background , with a total of 56 confirmed YSOs .We additionally observe 915 structures that are larger than 3 . 5 sigma but less than our criteria of detection , which we identify as possible YSOs . The high degree of detection and the high percentage of previously observed sources shows that our findings are statistically substantial .This project is the first from the Spitzer Space Telescope to study the full depth of the IC 348 nebula in multiple wavelengths , and it will aid in our understanding of the formation of both single and binary stars .",
        "rewrite_text": "On January 10th, 2007, the Spitzer Space Telescope was relocated to a new Sun-synchronous orbit, enhancing its ability to offer a clearer view of the entire sky every three days. To assess the Spitzer Space Telescope's performance, we conducted a survey of the IC 348 nebula, a nearby region of star formation. Utilizing data from this new orbit, we have identified 70 compounds exceeding 3.5 sigma above the background, with a total of 56 confirmed Young Stellar Objects (YSOs). Furthermore, we observed 915 structures exceeding 3.5 sigma but falling below our detection threshold, which we classify as potential YSOs. The high detection rate and the significant number of previously observed sources indicate the statistical significance of our findings.\n\nThis project marks the first use of the Spitzer Space Telescope to explore the full depth of the IC 348 nebula across multiple wavelengths, aiding in our comprehension of both single and binary star formation.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 5.965587590013045,
        "rewrite-fast-z-score": 1.0327955589886444
    },
    {
        "original_text": "A massive protocluster, potentially one of the most massive structures formable in the universe given its short dynamical time, has been discovered in the S255N region. Analysis of optical and near-infrared data from multiple telescopes have revealed an overdensity of Yellow-Green (YG) stars with respect to the diffuse background population, a distribution consistent with that of a spherical Gaussian, and a projected size of 1.6 x 1.3 kpc (2.2 x 1.7 Mpc), corresponding to 0.14% of the diameter of the Orion Arm. This overdensity is at least partially corroborated by weak optical emission from dust, as measured by the *Spitzer Space Telescope, and is consistent with infrared spectroscopic surveys showing a high number density of young, hot, and massive stars. The most massive stars, with a likely initial mass of several hundred solar masses, have an implied present formation mass of several thousand solar masses if they have starved-looking disks and are considered to be in the main sequence phase. The derived mass and size are also consistent with stellar-mass-threshold predictions for formation of intermediate-mass black holes (IMBHs), an expected signature of active supermassive black hole growth at the center of the cluster. The age and masses of the stellar populations are also consistent with IMBH formation via seeds from the remnants of very massive stars, although higher chance of coalescence for lower mass seeds cannot be ruled out. A peculiar velocity of the cluster with respect to the local standard of rest, measured from the motion of bright member stars, and the mass projected along the line-of-sight, both suggest the cluster may not be virialized. If a supermassive black hole is present, as suggested by its likely formation via intermediate-mass black hole seeds or very massive star remnants, the observed properties of the cluster systematically depart from those expected for a classical star cluster. This intriguing object suggests that either massive seeds for IMBHs could have formed outside of clusters, and grown via accretion, or there is more than one mode of IMBH formation. Future optical spectroscopy of cluster members, in combination with infrared studies of the dust emission and a search for an IMBH, will help determine the most likely formation channel for this enigmatic object.",
        "watermark_text": "A massive protocluster , possibly one of the most gigantic structures formable in the universe due its long dynamical time , has been detected in the S255N region . Analysis of optical and far - infrared evidence from multiple telescopes have revealed an overdensity of Yellow - Green ( YG ) stars with regard to the diffuse background population , a distribution consistent with that of a circular Gaussian , and a projected diameter of 1 . 6 x 1 . 3 kpc ( 2 . 2 x 1 . 7 Mpc ) , equivalent to 0 . 14 % of the radius of the Orion Arm .This overdensity is at least largely corroborated by weak optical emission from dust , as measured by the * Spitzer Space Telescope , and is consistent with infrared spectroscopic studies showing a high number density of young , hot , and massive galaxies . The most large galaxies , with a likely first mass of several hundred solar masses , have an implied current formation mass of several thousand solar masses if they have starved - looking disks and are considered to be in the main sequence phase .The derived mass and size are also consistent with stellar - mass - threshold estimates for formation of intermediate - mass black holes ( IMBHs ) , an anticipated signature of active supermassive black hole growth at the center of the cluster . The age and masses of the stellar regions are also consistent with IMBH formation via seeds from the remnants of very huge stars , although higher chance of coalescence for upper mass seeds cannot be decided out .A peculiar speed of the cluster with regard to the local standard of rest , measured from the movement of bright member stars , and the mass generated along the line - of - view , both suggest the cluster might not be virialized . If a supermassive black hole is present , as suggested by its likely formation via intermediate - mass black hole seeds or very huge star debris , the seen characteristics of the cluster systematically depart from those expected for a classical star cluster .This unusual object suggests that either massive roots for IMBHs might have formed outside of clusters , and developed via accretion , or there is more than one mode of IMBH formation . Future imaging spectroscopy of cluster members , in combination with infrared studies of the dust radiation and a search for an IMBH , will assist determine the most likely formation channel for this enigmatic object .",
        "rewrite_text": "A large protocluster has been detected in the S255N region, potentially one of the most enormous structures formable in the universe due to its extended dynamic time. Analysis of optical and far-infrared data from multiple telescopes has revealed an abundance of Yellow-Green (YG) stars compared to the diffuse background population. These stars are distributed in a circular Gaussian pattern with a projected diameter of 1.6 x 1.3 kpc (equivalent to 0.14% of the radius of the Orion Arm).\n\nThis overdensity is further supported by weak optical emission from dust, measured by the Spitzer Space Telescope, which is consistent with infrared spectroscopic studies indicating a high concentration of young, hot, and massive galaxies. The largest galaxies, likely with initial masses of several hundred solar masses, imply a current formation mass of several thousand solar masses if they have depleted-looking disks and are in the main sequence phase.\n\nThe derived mass and size of the protocluster are also in agreement with stellar mass threshold estimates for the formation of intermediate-mass black holes (IMBHs), a signature expected during the growth of active supermassive black holes at the cluster's center. The age and masses of the stellar regions are compatible with IMBH formation from the remnants of very large stars. However, the likelihood of coalescence for higher-mass seeds cannot be determined.\n\nThe peculiar velocity of the cluster relative to the local standard of rest, measured from the movement of bright member stars, and the mass detected along the line of sight suggest that the cluster may not be fully virialized. If a supermassive black hole is present, as suggested by its formation via intermediate-mass black hole seeds or debris from very large stars, the observed characteristics of the cluster deviate systematically from those expected for a typical star cluster.\n\nThis unique object suggests that either IMBHs may have formed outside of clusters and developed through accretion, or there may be more than one mode of IMBH formation. Future imaging spectroscopy of cluster members, combined with infrared studies of dust radiation and a search for an IMBH, will help determine the most likely formation pathway for this enigmatic object.",
        "ori-fast-z-score": -1.049344364594206,
        "water-fast-z-score": 4.5850407708172565,
        "rewrite-fast-z-score": 2.508943540190028
    },
    {
        "original_text": "Reconnection is an important process that shapes the energy release in many magnetised systems, such as the solar atmosphere, the laboratory magnetosphere and in Earth’s magnetotail. In these systems, large-scale structures reconnecting and evolving on time-scales of tens of seconds to tens of minutes – micro-scales. In this Letter, we show that a kinetic approach to reconnection – the electromagnetic tearing mode – resolves a number of micro-scales. The process is inherently global, connecting across scales, and produces steady-state solutions in agreement with recent simulation. Moreover, in the limit of strong guide fields, the model is reduced to a wave equation and solutions are easily constructed in appropriate physical variables. We conclude that the tearing mode is an important micro-scale process that resolves the slower-evolution reconnection dynamics, and predict that plasmas with high energy e.g. in the solar atmosphere, will manifest a steady tearing-mode reconnection regime.",
        "watermark_text": "Reconnection is an important process that forms the electricity release in many magnetised systems , such as the solar atmosphere , the laboratory magnetosphere and in Earth ’ s magnetotail . In these systems , large - scale structures reconnecting and evolving on time - scales of tens of minutes to tens of minutes – micro - scales .In this Letter , we find that a kinetic technique to reconnection – the electromagnetic tearing mode – resolves a number of micro - scales . The method is inherently global , linking across scales , and produces continuous - state solutions in agreement with recent simulation .Moreover , in the limit of stable guide fields , the model is weakened to a wave equation and solutions are quickly constructed in appropriate physical factors . We suggest that the tearing mode is an important micro - scale process that resolves the slower - evolve reconnection dynamics , and predict that plasmas with high energy e . g .in the solar atmosphere , will manifest a steady tearing - mode reconnection regime .",
        "rewrite_text": "Reconnection is a crucial process that enables the release of electricity in numerous magnetized systems, including the solar atmosphere, the laboratory magnetosphere, and Earth's magnetotail. In these systems, large-scale structures undergo reconnection and evolve over timescales ranging from tens of minutes to micro-scales. This letter explores the application of a kinetic approach to reconnection, the electromagnetic tearing mode, which effectively resolves multiple micro-scale issues. This method is inherently global, bridging various scales, and produces continuous state solutions that align with recent simulations. Furthermore, in the presence of stable guide fields, the model transitions to a wave equation, enabling swift construction of solutions with appropriate physical factors. We propose that the tearing mode is a significant micro-scale process that resolves slow-evolving reconnection dynamics, and we predict that plasmas with high energy, such as those found in the solar atmosphere, will exhibit a consistent tearing-mode reconnection regime.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 3.5777087639996634,
        "rewrite-fast-z-score": -0.6708203932499369
    },
    {
        "original_text": "GG Tau is a nearby (d= 2.2 pc) multiple system that contains at least five infrared sources (D Alessio et al. 2005) and has been modelled as a circumbinary ring system (White et al. 2004). Radial optical cuts of the GG Tau system show three separate rings of dust that are radially stratified, with the inner and outer rings extending from the midplane of the system to a radius of 20 and 40 AU, respectively. In this work, we investigate the possibility that the dust in the middle ring is dynamically linked to the binary system, as suggested by White et al. (2004). We use ALMA observations of the CO v = 2-1 line at a spatial resolution of 12.8 x 4.8 AU to estimate the radial velocity dispersion of the gas within the ring. We find that the dust and gas kinematics are consistent with each other within the errors, suggesting that the dust and gas in the ring are dynamically coupled. Based on the Toomre parameter and gravitational stability criteria, the observed ring radius and temperature are consistent with a population of planetesimals whose ephemeris is maintained by an external perturber. We investigate whether the outer ring is externally disturbed by calculating the mutual gravitational interaction between the two rings and find that the observed offset between the dust peaks is insufficient to maintain the observed ring structure. However, if the GG Tau system has an additional component in a nearly counter-clockwise orbit with a semi-major axis of 210 AU and a current projected separation of 20 AU, we find that the system is unstable over timescales of 100 Myr. We therefore suggest that the outer dust ring is a co-planar companion to the GG Tau circumbinary ring system, as previously proposed by Guilloteau et al. (2013).",
        "watermark_text": "GG Tau is a neighboring ( d = 2 . 2 pc ) multiple system that contains at least five infrared sources ( D Alessio et al . 2005 ) and has been constructed as a circumbinary ring network ( White et al .2004 ) . Radial optical cuts of the GG Tau scheme exhibit three separate circles of dust that are radially stratified , with the inner and outer rings running from the midplane of the system to a diameter of 20 and 40 AU , respectively .In this research , we investigate the idea that the dust in the mid ring is dynamically linked to the binary system , as suggested by White et al . ( 2004 ) .We use ALMA observations of the CO v = 2 - 1 line at a spatial resolution of 12 . 8 x 4 . 8 AU to estimate the radial speed dispersion of the gas within the ring . We see that the dust and gas kinematics are compatible with each other within the errors , showing that the dust and gas in the ring are dynamically linked .Based on the Toomre parameter and gravity stability criteria , the observed ring distance and heat are compatible with a population of planetesimals whose ephemeris is maintained by an external perturber . We explore whether the inner belt is externally disturbed by assessing the mutual gravitational interaction between the two circles and find that the observed offset between the dust peaks is insufficient to keep the observed ring shape .However , if the GG Tau system has an additional element in a nearly counter - clockwise orbit with a semi - major axis of 210 AU and a current projected separation of 20 AU , we find that the system is unstable over timescales of 100 Myr . We consequently suggest that the exterior dust ring is a co - planar companion to the GG Tau circumbinary ring system , as previously introduced by Guilloteau et al .(2013).",
        "rewrite_text": "GG Tau is a neighboring multiple system (at a distance of 2.2 pc) that comprises at least five infrared sources (D Alessio et al., 2005). This system has been constructed as a circumbinary ring network (White et al., 2004). Radial optical cuts of the GG Tau scheme reveal three separate, radially stratified circles of dust, with the inner and outer rings extending from the system's midplane to a diameter of 20 and 40 AU, respectively.\n\nIn this study, we investigate the notion that the dust in the central ring is dynamically linked to the binary system, as suggested by previous research (White et al., 2004). We utilize ALMA observations of the CO v=2-1 line at a spatial resolution of 12.8 x 4.8 AU to estimate the radial velocity dispersion of the gas within the ring. Our observations indicate that the kinematics of the dust and gas are compatible within the error margins, suggesting a dynamic connection between them.\n\nBased on the Toomre parameter and gravity stability criteria, the observed ring distance and heat are consistent with a population of planetesimals whose orbital stability is maintained by an external perturber. We explore whether the inner belt is externally disturbed by examining the mutual gravitational interaction between the two circles and find that the observed offset between dust peaks is insufficient to maintain the observed ring shape.\n\nHowever, if the GG Tau system harbors an additional component in an approximately counter-clockwise orbit with a semi-major axis of 210 AU and a current projected separation of 20 AU, we find that the system becomes unstable over timescales of 100 million years. Therefore, we propose that the exterior dust ring is a co-planar companion to the GG Tau circumbinary ring system, as previously introduced by Guilloteau et al. (2013).",
        "ori-fast-z-score": -0.8250286473253902,
        "water-fast-z-score": 4.636363636363637,
        "rewrite-fast-z-score": 2.82842712474619
    },
    {
        "original_text": "Existence of inward motions in starless cores has long been proposed based on analysis of simple chemical models. Such motions are of great interest as they may be associated with the process of core fragmentation and subsequent star formation. Inward motions are observable by detection of optically thick, high rotation transition lines such as NH 3 ( J = 1-0) whose derived spin relaxation times are longer in regions with small velocity dispersion than would be expected from the gas temperature. We have completed a pilot study for detecting inward motions in dark clouds using the HCN J = 1-0 line. We have obtained excellent systemic pointing with this line for the region in the TMC-1KP core that includes TMC-1C, 1D, and 1E, which are well-known sites of ongoing low-mass star formation. Preliminary results from the HCN J = 1-0 line are positive for TMC-1C and TMC-1D, but not for TMC-1E; a more detailed analysis is in progress. We have also observed a reference position in the adjoining core known as TMC-1F. The TMC-1KP project is an expanded version of this work. We also seek observations of additional cores in the TMC-1 clouds and of other starless cores in this line. We use the term  pointing  to refer to a set of observations (along a chosen set of coordinates) of the same region. A pointing is considered excellent if the noise in the TMC-1KP channel maps is primarily thermal noise, the map is symmetric with respect to the chosen coordinates, and the intensity range covers at least a 5:1 intensity range and the transition line is cleanly centered on the chosen coordinates.",
        "watermark_text": "Existence of inward motions in starless cores has often been proposed based on investigation of simple chemical models . Such movements are of high interest as they may be involved with the process of core fragmentation and subsequent star formation .Inward motions are observable by sensing of optically dense , large rotation transition lines such as NH 3 ( J = 1 - 0 ) whose derived spin relaxation times are longer in areas with little speed dispersion than would be anticipated from the gas temperature . We have completed a pilot study for detecting inward motions in dark dust using the HCN J = 1 - 0 line .We have achieved good systemic pointing with this line for the region in the TMC - 1KP core that comprises TMC - 1C , 1D , and 1E , which are better - recognized sites of ongoing small - mass star formation . Preliminary conclusions from the HCN J = 1 - 0 line are positive for TMC - 1C and TMC - 1D , but not for TMC - 1E ; a more precise analysis is in progress .We have also observed a reference shape in the adjoining core designated as TMC - 1F . The TMC - 1KP program is an revised form of this project .We additionally need observations of added cores in the TMC - 1 clouds and of other starless cores in this line . We use the term pointing to refer to a setting of measurements ( along a chosen set of coordinates ) of the same region .A pointing is regarded best if the interference in the TMC - 1KP channel maps is mainly thermal noise , the mapping is symmetric with regard to the chosen coordinates , and the intensity range covers at least a 5 : 1 amplitude range and the transition line is cleanly focused on the chosen coordinates .",
        "rewrite_text": "The proposal of inward movements within starless cores has frequently been based on the examination of basic chemical models. These movements are highly significant as they may be involved in the process of core fragmentation and subsequent star formation. Inward movements can be observed through the detection of optically dense, large rotation transition lines such as NH3 (J=1-0), where the derived spin relaxation times are longer in areas with minimal speed dispersion compared to what would be expected from the gas temperature.\n\nWe have conducted a pilot study to detect inward motions in dark dust using the HCN J=1-0 line as a pointer. This line has achieved good systemic pointing for the region within the TMC-1KP core, which comprises TMC-1C, 1D, and 1E, sites that are well-recognized for ongoing low-mass star formation. Preliminary findings from the HCN J=1-0 line indicate positive results for TMC-1C and TMC-1D, but not for TMC-1E; a more precise analysis is currently underway.\n\nFurthermore, we have observed a reference pattern in the adjacent core labeled as TMC-1F. The TMC-1KP program is an updated version of this project. We also require observations of additional cores within the TMC-1 clouds and other starless cores using this line.\n\nWe use the term \"pointing\" to refer to the setting of measurements (along a chosen set of coordinates) for the same region. A pointing is considered optimal when the interference in the TMC-1KP channel maps is primarily thermal noise, the mapping is symmetrical with regard to the selected coordinates, and the intensity range covers at least a 5:1 amplitude range with a clean focus of the transition line on the chosen coordinates.",
        "ori-fast-z-score": -1.8935062328016077,
        "water-fast-z-score": 5.680518698404823,
        "rewrite-fast-z-score": 1.6728567220186852
    },
    {
        "original_text": "Image watermarking has been widely studied and used to prevent image theft and copyright infringement. Most watermarking algorithms embed a single layer of code in the image itself. In this work, we propose watermarking techniques that embed multiple layers of information and detect tampered images. The first layer of information is a random noise that destroys information but can be easily detected. The second layer embeds a secret key into the image. This layer cannot be detected by simple image inspection. The secret key can be used to regenerate the image and thus prove its integrity. Finally, the third layer embeds a fine-grained version of the secret key. Since the embedded information in each layer is encrypted with the corresponding private keys, it can be detected and removed only by the corresponding owner. We test our approach on real images and show that our method can detect different types of modifications while being easy to integrate in current image processing pipelines.",
        "watermark_text": "Image watermarking has been widely studied and utilized to reduce image theft and copyright infringement . Most watermarking methods embed a single level of software in the image itself .In this research , we develop watermarking methods that embed multiple strands of information and detect tampered pictures . The first layer of information is a random signal that destroys information but can be easily detected .The second layer embeds a secret key into the image . This layer cannot be identified by simple image inspection .The private key can be used to regenerate the image and therefore demonstrate its authenticity . Finally , the third layer embeds a fine - grained version of the secret key .Since the embedded data in each layer is encrypted with the respective private keys , it can be identified and removed only by the associated owner . We test our approach on real images and find that our technique can identify diverse kinds of changes while being easy to integrate in current image processing pipelines .",
        "rewrite_text": "The study of image watermarking has gained widespread attention and utilization to mitigate image theft and copyright violations. Many watermarking techniques involve embedding a single level of software within the image itself. In our research, we have developed watermarking methods that integrate multiple layers of information to detect tampered images.\n\nThe first layer of our method incorporates a random signal that disrupts information but can be easily detected. The second layer involves embedding a secret key into the image, which remains undetectable through simple image inspection. This private key can be utilized to restore the image, thereby verifying its authenticity. Lastly, the third layer embeds a finely-grained version of the secret key. As the data embedded in each layer is encrypted with corresponding private keys, only the associated owner can identify and remove it.\n\nWe have tested our approach on actual images and found that our technique can recognize various types of changes while being easily integrated into current image processing pipelines.",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 2.650356625796317
    },
    {
        "original_text": "In the center of the nearby galaxy M82, a stellar system in the early phases of transformation into a globular cluster, reside two high mass X-ray binaries (HMXB), dubbed M82 X-1 and M82 X-4. They were first discovered by the Einstein observatory. The nature of these sources is still unknown. They were considered candidates to powered the gamma-ray emission discovered by INTEGRAL. The discovery of a strong stellar proper motion with a position angle of ~70° relative to the molecular gas in the center of M82 confirmed the previous hypothesis that M82 X-1 and M82 X-4 are experiencing a rapid orbital decay due to the emission of gravitational waves. However, this proposed explanation is challenged by the large observed X-ray luminosity of the sources, which would imply a large mass for the neutron stars at the upper end of the initial mass function, even larger than 40 solar masses, and render such objects unphysical. Alternative scenarios invoke a high-density environment where the emission of gravitational waves is enhanced.",
        "watermark_text": "In the center of the nearby galaxy M82 , a stellar system in the early stages of transformation into a globular cluster , reside two high mass X - ray binaries ( HMXB ) , nicknamed M82 X - 1 and M82 X - 4 . They were first discovered by the Einstein observatory .The nature of these sources is still unclear . They were thought candidates to powered the gamma - ray radiation observed by INTEGRAL .The observation of a powerful stellar proper motion with a orientation angle of ~ 70° relative to the molecular gas in the center of M82 confirmed the previous hypothesis that M82 X - 1 and M82 X - 4 are experiencing a rapid orbital decay owing to the emission of gravitational waves . However , this possible hypothesis is challenged by the huge observed X - ray luminosity of the sources , which would indicate a large mass for the neutron stars at the upper end of the first mass function , far larger than 40 solar masses , and render such objects unphysical .Alternative proposals invoke a high - density environment where the emission of gravitational waves is enhanced .",
        "rewrite_text": "In the heart of the nearby galaxy M82, an evolving stellar system is in the early stages of transitioning into a globular cluster. Here, two high-mass X-ray binaries (HMXBs), nicknamed M82 X-1 and M82 X-4, reside. These binaries were first detected by the Einstein Observatory. The exact nature of these sources remains unclear, but they were once thought to be potential candidates for powering the gamma-ray radiation observed by INTEGRAL.\n\nObservations of a powerful stellar proper motion with an orientation angle of approximately 70° relative to the molecular gas in the center of M82 have confirmed previous hypotheses that M82 X-1 and M82 X-4 are experiencing a rapid orbital decay due to the emission of gravitational waves. However, this hypothesis is challenged by the enormous X-ray luminosity observed from the sources, suggesting neutron stars with a large mass at the upper end of the first mass function, far exceeding 40 solar masses, which makes such objects unphysical. Alternative explanations suggest a high-density environment where the emission of gravitational waves is amplified.",
        "ori-fast-z-score": 1.6464638998453551,
        "water-fast-z-score": 4.58257569495584,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "Observational cosmological data have been used to statistically describe the probability distribution function (PDF) of the mass of dark matter haloes. In recent years, high-resolution N-body simulations have advanced to the point of enabling the gravitational clustering of dark matter to be reproduced directly, allowing the construction of halo PDFs at much higher mass and spatial resolution. These have provided new insights into the process of halo assembly, allowing the growth of substructure within each halo to be analysed and the influence of environment on halo properties to be investigated. Although the evolution of halo mass function is generally consistent with a predicted scale-free form at the high-mass end, with significant deviations seen only at the low- and high-mass ends, the PDFs exhibit more complex behaviour, with significant differences between individual simulations, and a dependence on time and environment. In this paper we use the large-volume (&gt;8.5 billion h^{-1} Mpc^3), intermediate-resolution (particle mass &lt; 8.5 x 10^9 h^{-1} M &sol;) Millennium-II simulation to investigate the evolution of the PDFs of the concentration, spin and orbital kinetic energy of dark matter haloes across four environments: clusters, sheets, filaments and voids. We also examine the effects of subhalo accretion, disruptions and mergers on these properties. The mass dependence of these distributions evolves significantly with time, and environment has a significant impact on the distribution at fixed time. In general, the haloes in sheets and filaments have more concentrated mass distributions than their cluster or void counterparts at all epochs. These differences are reflected in their associated mass functions, with the filament clusters being the most massive and most dynamically evolved structures, while voids are the most overdense regions in the universe and thus mark the sites of recent and future structure formation. The evolution of these distributions with time and environment therefore appears to be related to the hierarchical growth of structure. The spin and orbital kinetic energy also exhibit significant differences between environments, with the spin magnitude having a near-universal evolution, irrespective of environment or redshift, while the orbital kinetic energy exhibits different behaviours, with its distribution shifting to lower values in more evolved environments. This corresponds to the hierarchical buildup of structure in sheets and filaments, which forms at relatively late times, leading to an accumulation of material in the central regions, as well as a reduction in substructure. The properties of haloes in clusters are largely set by their major mergers since z &gt; 2, leaving them with short orbital times and low orbital kinetic energy. Conversely, the evolution of void haloes is more complex, with a mixture of processes at work. Subhaloes can only survive in the highest density regions for a limited time, while the expansion of the universe disperses material, resulting in lower concentrations and lower orbital energies. However, voids also form much later than clusters and sheets, with fewer major mergers and more minor interactions. This has the",
        "watermark_text": "Observational cosmological information have been used to statistically describe the probability distribution function ( PDF ) of the mass of dark matter haloes . In recent years , large - resolution N - bodies simulations have progressed to the point of allowing the gravitational clustering of bright matter to be analyzed directly , allowing the creation of halo PDFs at much higher mass and spatial resolution .These have provided new understanding into the process of halo assembly , allowing the development of substructure within each halo to be analysed and the impact of environment on halo characteristics to be investigated . Although the evolution of halo mass function is typically consistent with a predicted scale - free form at the high - mass ending , with substantial deviations shown only at the high - and low - mass ends , the PDFs exhibit more sophisticated behaviour , with substantial differences between individual simulations , and a dependence on time and environment .In this paper we using the huge - volume ( & gt ; 8 . 5 billion g ^ { - 1 } Mpc ^ 3 ) , intermediate - resolution ( electron mass & lt ; 8 . 5 x 10 ^ 9 h ^ { - 1 } M & sol ; ) Millennium - II simulation to examine the evolution of the PDFs of the concentration , spin and orbital kinetic power of bright matter haloes across four settings : clusters , sheets , filaments and voids . We additionally probe the effects of subhalo accretion , disruptions and mergers on these structures .The mass dependence of these distributions evolves greatly with time , and environment has a considerable impact on the distribution at fixed time . In general , the haloes in sheets and filaments have more localized mass distributions than their cloud or void counterparts at all epochs .These changes are reflected in their identified mass functions , with the filament clusters being the most gigantic and most dynamically evolved structures , while voids are the most overdense regions in the universe and therefore mark the sites of recent and future structure development . The growth of these distributions with time and environment thus appears to be connected to the hierarchical expansion of form .The spin and orbital kinetic power also exhibit substantial differences between environments , with the spin magnitude having a near - universal evolution , irrespective of environment or redshift , while the orbital kinetic power displays changed behaviours , with its distribution shifting to smaller values in more evolved habitats . This corresponds to the hierarchical buildup of shape in sheets and filaments , which forms at fairly late times , leading to an accumulation of material in the main regions , as also as a reduction in substructure .The properties of haloes in clusters are essentially fixed by their major mergers since z & gt ; 2 , left them with narrow orbital times and low orbital kinetic power . Conversely , the evolution of void haloes is more complex , with a mixture of processes at work .Subhaloes can only remain in the highest density zones for a small periods , while the expansion of the universe disperses material , leading in smaller concentrations and less orbital energies . However , voids still appear far later than complexes and sheets , with fewer major mergers and more minor interactions .This has the",
        "rewrite_text": "Utilizing observational cosmological data, the probability distribution function (PDF) of dark matter halo masses has been statistically described. In recent years, high-resolution N-body simulations have enabled the direct analysis of gravitational clustering of luminous matter, enabling the creation of halo PDFs with greater mass and spatial resolution. These advancements have provided new insights into halo assembly processes, facilitating the examination of substructure within each halo and the investigation of the impact of the environment on halo characteristics.\n\nWhile the evolution of the halo mass function generally aligns with a predicted scale-free form at the high-mass end, with significant deviations observed only at the high and low-mass ends, the PDFs demonstrate more complex behavior, with substantial differences between individual simulations and a dependence on both time and environment. In this paper, we utilize the large-volume (over 8.5 billion g^-1 Mpc^3) and intermediate-resolution (electron mass < 8.5 x 10^9 h^-1 M_sun) Millennium-II simulation to examine the evolution of the PDFs related to the concentration, spin, and orbital kinetic power of luminous matter haloes in four different settings: clusters, sheets, filaments, and voids.\n\nFurthermore, we explore the effects of subhalo accretion, disruptions, and mergers on these structures. The mass dependence of these distributions changes significantly with time, and the environment has a considerable impact on the distribution at a fixed point in time. Generally, haloes in sheets and filaments exhibit more localized mass distributions compared to their cloud or void counterparts across all epochs. These changes are reflected in their identified mass functions, with filament clusters being the most extensive and dynamically evolved structures, while voids are the most overdense regions in the universe, marking sites of recent and future structural development.\n\nThe growth of these distributions with time and environment appears to be linked to hierarchical expansion of form. The spin and orbital kinetic power also exhibit notable differences between environments. The spin magnitude demonstrates a nearly universal evolution, unaffected by environment or redshift, while the orbital kinetic power exhibits altered behaviors, with its distribution shifting to smaller values in more evolved habitats. This corresponds to the hierarchical building of shape in sheets and filaments, which forms at relatively late times, leading to the accumulation of material in main regions and a reduction in substructure.\n\nThe properties of haloes in clusters are primarily determined by their major mergers since z > 2, leaving them with narrow orbital times and low orbital kinetic power. In contrast, the evolution of void haloes is more complex, involving a mix of processes. Subhaloes can only persist in the highest density zones for brief periods, as the expansion of the universe disperses material, resulting in smaller concentrations and lower orbital energies. However, voids still appear later than complexes and sheets, with fewer major mergers and more minor interactions. This provides valuable insights into the complex interplay between haloes and their environments over cosmic time.",
        "ori-fast-z-score": -1.8543452998910612,
        "water-fast-z-score": 8.163796732679655,
        "rewrite-fast-z-score": 3.223963305302868
    },
    {
        "original_text": "A brain prosthesis is an electronic device that provides a service comparable to the functions of a biological brain. Since the 20th century, brain prostheses have been developed to treat epilepsy, paralysis, and other conditions. This paper analyzes the dynamic behavior of such a dynamic brain prosthesis, and particularly the question of whether it could develop a desire to live perpetually. Answering this question would require a detailed computational model of the brain prosthesis, which, in turn, requires a better understanding of how the brain s faculties of cognition, volition, and emotion arise from the neurons, molecules, and other physical entities that comprise it. Such an understanding would also be required to determine where in this theoretical model a mechanism for instilling a desire to live perpetually could be implemented. Brain prostheses are designed to mimic specific brain functions, so it would also be necessary to determine which ones of the brain s faculties are indispensable to its intrinsic functionality. If it were possible to develop such a desire to live perpetually, the analysis in this paper suggests that it would also be necessary to determine the source of this desire and whether it would conflict with the long-term survival of the brain prosthesis.",
        "watermark_text": "A brain prosthesis is an electronic device that offers a service similar to the functions of a physical mind . Since the 20th millennium , neural prostheses have been created to treat epilepsy , paralysis , and other conditions .This paper analyzes the dynamic behavior of such a dynamic brain prosthesis , and particularly the question of whether it could generate a desire to live perpetually . Answering this question would need a detailed mathematical description of the brain prosthesis , which , in turn , requires a better study of how the brain s faculties of cognition , volition , and affection arise from the cells , atoms , and other mental structures that comprise it .Such an knowledge might additionally be required to identify where in this theoretical theory a system for instilling a desire to live perpetually could be enacted . Brain prostheses are intended to mimic specific cerebral processes , so it would also be required to determine which ones of the brain s faculties are indispensable to its intrinsic functionality .If it were possible to develop such a desire to live perpetually , the evaluation in this paper indicates that it would also be required to identify the origin of this urge and whether it would conflict with the long - term survival of the brain prosthesis .",
        "rewrite_text": "A brain prosthesis is an electronic device designed to provide services analogous to the functionalities of a natural mind. Over the course of the 21st century, neural prostheses have been created to address conditions such as epilepsy, paralysis, and other ailments. This paper examines the dynamic behavior of such a prosthesis, particularly focusing on whether it could potentially generate a desire for perpetual existence. To answer this question, a comprehensive mathematical description of the brain prosthesis is necessary, which in turn necessitates a deeper understanding of how the brain's cognitive, volitional, and affective faculties emerge from its cellular, atomic, and other mental structures. Such knowledge may also be required to identify where in this theoretical framework a system for fostering a desire for perpetual living could be implemented. Brain prostheses aim to mimic specific cerebral processes, thus it is essential to determine which of these brain functions are indispensable for its inherent operation. If it becomes feasible to cultivate such a desire for perpetual existence, the evaluation in this paper suggests that it would also be necessary to identify the source of this urge and whether it would clash with the long-term viability of the brain prosthesis.",
        "ori-fast-z-score": 0.5241424183609592,
        "water-fast-z-score": 6.957010852370434,
        "rewrite-fast-z-score": 3.794733192202055
    },
    {
        "original_text": "Recent theoretical and experimental activities in ultracold gases have lead to a realization of isolated quantum systems with tunable interactions and unprecedented control. These systems, originally proposed for the experimental realization of correlated quantum phases and quantum dynamics optimization, now give access to a broad class of strongly correlated quantum systems whose complexity defies a na�ve description. Among those, one-dimensional Bose gases allow for the study of strongly correlated phases such as Mott insulators, solitonic excitations and superfluidity. Here we report on the experimental study of the dynamical properties of one-dimensional Bose gases via Bragg spectroscopy. By modulating the coupling between the gas and light waves, we engineer non-equilibrium quantum dynamics and access the intermediate-time dynamics where correlations play a role. We identify and characterize dynamical phase transitions between various regimes. In particular, we provide conclusive evidence for the dynamical Mott phase transition in the vicinity of the solitonic solution. This work paves the way for the experimental exploration of strongly correlated quantum dynamics in one-dimensional Bose gases and opens avenues for the study of the intricate connections between quantum and classical dynamics.",
        "watermark_text": "Recent conceptual and experimental activities in ultracold gases have yielded to a realization of isolated quantum systems with tunable interactions and enormous control . These systems , previously developed for the theoretical realization of coupled quantum phases and quantum mechanics optimization , now give access to a broad class of heavily correlated quantum systems whose complexity defies a nave model .Among those , one - dimensional Bose compounds allow for the observation of highly correlated phases such as Mott insulators , solitonic excitations and superfluidity . Here we note on the empirical investigation of the dynamical properties of one - dimensional Bose gases via Bragg spectroscopy .By modulating the interaction between the gas and light beams , we create non - equilibrium quantum mechanics and enter the intermediate - time dynamics where correlations serve a role . We recognize and characterize dynamical phase transitions between various regimes .In particular , we provide conclusive evidence for the dynamical Mott phase shift in the vicinity of the solitonic solution . This research paves the way for the empirical investigation of heavily correlated quantum mechanics in one - dimensional Bose gases and introduces avenues for the discovery of the intricate connections between quantum and classical mechanics .",
        "rewrite_text": "Recent advancements in conceptual and experimental research on ultracold gases have resulted in the realization of isolated quantum systems with adjustable interactions and exceptional control. These systems, originally developed for the theoretical exploration of coupled quantum phases and optimization in quantum mechanics, now provide access to a diverse range of highly complex correlated quantum systems that challenge conventional models. Among these, one-dimensional Bose compounds offer the opportunity to observe highly correlated phases such as Mott insulators, solitonic excitations, and superfluidity.\n\nIn this context, we highlight empirical investigations into the dynamic properties of one-dimensional Bose gases using Bragg spectroscopy. By modulating the interaction between the gas and light beams, we create a non-equilibrium state in quantum mechanics and enter the intermediate-time dynamics where correlations play a significant role. We identify and characterize dynamic phase transitions between various regimes. Specifically, we provide conclusive evidence for a dynamic Mott phase shift in the vicinity of the solitonic solution.\n\nThis research paves the way for further empirical investigations into highly correlated quantum mechanics in one-dimensional Bose gases and opens up avenues for the discovery of intricate connections between quantum and classical mechanics.",
        "ori-fast-z-score": -0.5241424183609592,
        "water-fast-z-score": 6.7461923416925424,
        "rewrite-fast-z-score": 2.8284271247461903
    },
    {
        "original_text": "Two-band superconductors, also known as multi-band superconductors, occur in nature in some heavy fermion systems, and in particular in multi-band correlated superconductors such as (oxy)aniline, (oxy)nitrosulfatemediated by copper and sulfur (eg. sulfur passivated regions in oxyacetylene or sulfur doped iron pnictides). The oxyacetylene-based oxy nitrates (e.g. La(OCH(CH3)2)0.9Ce0.1NO3) have two strongly overlapping bands near the Fermi level originating from the oxygen p and σ* orbitals. Here we report resonant inelastic x-ray scattering experiments on a sulfur doped sample revealing the coupling between the two bands at low energy. The results are compared with recent specific heat and muon spin relaxation measurements performed on a similar sulfur doped compound which reveal evidence of fluctuating full gap and nodal states, respectively, at low temperatures. These results suggest that the full superconducting gap may be closed and reopened by small additional internal or external perturbations, making these systems good candidates for further studies of exotic superconductivity such as topological superconductivity or phase separation.",
        "watermark_text": "Two - band superconductors , sometimes called as multi - band superconductors , occur in nature in some heavy fermion systems , and in particular in multi - band correlated superconductors such as ( oxy ) aniline , ( oxy ) nitrosulfatemediated by copper and sulfur ( eg . hydrogen passivated regions in oxyacetylene or hydrogen doped metal pnictides ) .The oxyacetylene - based oxy nitrates ( e . g . La ( OCH ( CH3 ) 2 ) 0 . 9Ce0 . 1NO3 ) have two heavily overlapping bands near the Fermi level arising from the oxygen p and σ * orbitals .Here we publish resonant inelastic x - ray scattering experiments on a sulfur doped specimen revealing the interaction between the two bands at low power . The results are compared with recent specific heat and muon spin relaxation measurements completed on a similar sulfur doped compound which demonstrate indication of fluctuating complete gap and nodal states , respectively , at low temperatures .These data suggest that the full superconducting gap may be shut and reopened by small additional internal or external perturbations , making these systems excellent candidates for further studies of exotic superconductivity such as topological superconductivity or phase splitting .",
        "rewrite_text": "Bi-band superconductors, sometimes referred to as multi-band superconductors, naturally occur in certain heavy fermion systems. Specifically, they are prevalent in multi-band correlated superconductors, such as (oxy) aniline and (oxy) nitrosulfatemediated by copper and sulfur, exemplified by hydrogen passivated regions in oxyacetylene or hydrogen-doped metal pnictides. Oxyacetylene-based oxy nitrates, such as La(OCH(CH3)2)0.9Ce0.1NO3, possess two heavily overlapping bands near the Fermi level, arising from the oxygen p and σ* orbitals.\n\nIn this study, we present resonant inelastic x-ray scattering experiments on a sulfur-doped specimen, revealing the interaction between these two bands at low power levels. The results are compared with recent specific heat and muon spin relaxation measurements conducted on a similar sulfur-doped compound. These measurements indicate fluctuating complete gap and nodal states at low temperatures, respectively. The data suggest that small internal or external perturbations may open and close the full superconducting gap, making these systems ideal for further investigations of exotic superconductivity, such as topological superconductivity or phase splitting.",
        "ori-fast-z-score": 0.1125087900926024,
        "water-fast-z-score": 4.387842813611494,
        "rewrite-fast-z-score": 3.5777087639996634
    },
    {
        "original_text": "Nuclear fragmentation has been studied extensively over many decades, both experimentally and theoretically. The basic premise is that a nuclei breaks up into pieces in a process known as nuclear fragmentation. Both strong and electromagnetic interactions are involved, depending on the mass and charge of the nucleus. Many particles and fragments are created and each of these particles or fragments carry a certain amount of the original nuclei s kinetic energy, which can be expressed as a magnitude of kinetic energy per nucleon. This article presents a study of nuclear fragmentation patterns of $^{9}$Be, $^{14}$N, $^{7}$Be, and $^{8}$B nuclei as recorded in their alpha decay. It has been observed that $^9$Be, $^{14}$N, $^{7}$Be, and $^{8}$B alpha decay nuclei tend to break up into groups of three particles, with the third particle being the lightest of the four. In the case of $^9$Be and $^{14}$N, two of these three fragments have nearly the same mass, with the third fragment being lighter. In the case of $^{7}$Be and $^{8}$B, the two nearly equal mass particles are always the two lightest particles, while the third particle is typically either lighter or of an intermediate mass. This phenomenon is not fully understood, but is a subject of this study.",
        "watermark_text": "Nuclear fragmentation has been studied extensively over numerous years , both experimentally and theoretically . The basic concept is that a nuclei broke up into pieces in a process known as nuclear fragmentation .Both powerful and electromagnetic interactions are implicated , depending on the mass and charge of the nucleus . Many molecules and fragments are created and each of these ions or pieces carry a certain excess of the original particles s kinetic power , which can be described as a magnitude of kinetic power per nucleon .This page presents a analysis of nuclear fragmentation behaviors of $ ^ { 9 } $ Be , $ ^ { 14 } $ N , $ ^ { 7 } $ Be , and $ ^ { 8 } $ B nuclei as measured in their beta decay . It has been observed that $ ^ 9 $ Be , $ ^ { 14 } $ N , $ ^ { 7 } $ Be , and $ ^ { 8 } $ B alpha decay particles tend to broke up into groups of three particles , with the third electron being the lightest of the four .In the case of $ ^ 9 $ Be and $ ^ { 14 } $ N , two of these three fragments have nearly the same mass , with the third fragment being lighter . In the case of $ ^ { 7 } $ Be and $ ^ { 8 } $ B , the two nearly identical mass particles are always the two lightest fragments , while the third particle is typically either lighter or of an intermediate mass .This phenomenon is not totally understood , but is a subject of this study .",
        "rewrite_text": "Nuclear fragmentation has been extensively studied both experimentally and theoretically over the years. The fundamental concept involves the breakdown of a nucleus into pieces, a process known as nuclear fragmentation. This process involves both powerful and electromagnetic interactions, depending on the mass and charge of the nucleus. A multitude of molecules and fragments are generated, with each ion or piece carrying a specific excess of kinetic power from the original particles. This excess can be described as the magnitude of kinetic power per nucleon.\n\nThis page presents an analysis of the nuclear fragmentation behaviors of 9Be, 14N, 7Be, and 8B nuclei, as measured in their beta decay. Observations have shown that the alpha decay particles of 9Be, 14N, 7Be, and 8B tend to break down into groups of three particles, with the third particle being the lightest of the four. In the cases of 9Be and 14N, two of these three fragments have nearly equal masses, while the third fragment is lighter. For 7Be and 8B, the two particles with nearly identical masses are always the two lightest fragments, while the third particle typically has a lighter or intermediate mass. Although this phenomenon is not fully understood, it is a subject of this study.",
        "ori-fast-z-score": -0.3333333333333333,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 1.865992419824736
    },
    {
        "original_text": "The luminous infrared galaxy NGC 6052 was observed with the Spitzer Space Telescope in four different programs. The galaxy was observed with the Infrared Spectrograph (IRS) in staring mode as part of program 601 (P.I.s: Werner, C. O. & Teplitz, H. I.) and with the MIPS instrument as part of programs 30 and 39 (P.I.s: Hailey-Dunsheath, S. & Sargent, B. A.). The IRS data were reduced using a modified version of the S11 script and the MIPS data were reduced using the MIPS Data Reduction Guide version 8.0. In this work we combine all of the archival Spitzer observations of NGC 6052 in order to carry out a detailed analysis of the galaxy s spectral energy distribution (SED). We first construct a broadband SED from the optical to the mid-infrared. This is then used to fit a dust torus model to the infrared data. The model consists of a central BlackBody source modified by a fixed radial density distribution of dust grains in an otherwise empty spheroid. The resulting best-fit model parameters indicate that NGC 6052 has an active galactic nucleus (AGN) with an estimated power of 1051 W and a distance of 95.2 billion light years. The AGN contributes 74% of the total infrared luminosity of the galaxy and the host galaxy contributes 23% (11% of the total infrared luminosity). We also examine the spatial distribution of the warm (50-125 K) and hot (125-540 K) dust, as well as polycyclic aromatic hydrocarbons (PAHs) and use these results to assess the thermal balance of the galaxy. We find that the hot dust is concentrated in a circumnuclear ring with a radius of 3.2 pc. The inner and outer radii of the cool dust are found to be 14 pc and 50 pc, respectively. The total dust mass is estimated to be 2.3 x 10-5 M⊙. The PAH luminosity is 5.5 x 10-8 L⊙ and the star-formation rate is 1.2 M⊙/year. The relative strengths of the PAH bands indicate that the predominant energy source heating the dust is star-formation, rather than the AGN. We also examine the ionized gas component of the galaxy, finding that it contributes 1% of the total infrared luminosity. We estimate that the star-formation rate in the ring is 235 M⊙/year. We perform a analysis of the spectral line energy distribution to determine the distribution of dense gas in the ring. We estimate the gas mass to be 1.1 x 10-4 M⊙. We find that the measured CO-to-H2 mass conversion factor of 4.3 x 10-4 cm3/kg is consistent with that expected in the molecular-rich ring but is a factor of",
        "watermark_text": "The luminous infrared galaxy NGC 6052 was seen with the Spitzer Space Telescope in four different programs . The galaxy was seen with the Infrared Spectrograph ( IRS ) in staring mode as part of series 601 ( P . I . s : Werner , C . O .& Teplitz, H.I.)and with the MIPS instrument as part of programs 30 and 39 ( P . I . s : Hailey - Dunsheath , S . & Sargent , B . A . ) .The IRS information were reduced use a altered version of the S11 script and the MIPS data were reduced using the MIPS Data Reduction Guide version 8 . 0 . In this project we merge all of the archival Spitzer images of NGC 6052 in order to carry out a detailed analysis of the galaxy s spectral power distribution ( SED ) .We first build a network SED from the optical to the mid - infrared . This is then used to fitting a dust torus model to the infrared data .The model consists of a central BlackBody source modified by a fixed radial density density of dust grains in an otherwise empty spheroid . The resulting best - fitting model variables suggest that NGC 6052 has an active galactic nucleus ( AGN ) with an estimated power of 1051 W and a distance of 95 . 2 billion light years .The AGN adds 74 % of the total infrared luminosity of the galaxy and the host universe adds 23 % ( 11 % of the total infrared luminosity ) . We additionally analyze the spatial distribution of the cool ( 50 - 125 K ) and hot ( 125 - 540 K ) dust , as well as polycyclic aromatic hydrocarbons ( PAHs ) and use these results to examine the thermal balance of the universe .We see that the hot dust is concentrated in a circumnuclear circle with a diameter of 3 . 2 pc . The outer and outer radii of the cool dust are found to be 14 pc and 50 pc , respectively .The total dust mass is reported to be 2 . 3 x 10 - 5 [UNK] . The PAH luminosity is 5 . 5 x 10 - 8 [UNK] and the star - formation rate is 1 . 2 [UNK] / year .The relative strengths of the PAH groups indicate that the predominant energy source heating the dust is star - formation , rather than the AGN . We additionally analyze the ionized gas component of the galaxy , finding that it adds 1 % of the total infrared luminosity .We estimate that the star - formation rate in the ring is 235 [UNK] / year . We undergo a analysis of the spectral line energy distribution to determine the distribution of dense gas in the ring .We estimate the gas mass to be 1 . 1 x 10 - 4 [UNK] . We find that the measured CO - to - H2 mass conversion factor of 4 . 3 x 10 - 4 cm3 / kg is consistent with that expected in the molecular - rich ring but is a factor of",
        "rewrite_text": "改写后的英文文本如下：\n\nThe luminous infrared galaxy NGC 6052 was observed by the Spitzer Space Telescope across four distinct programs. The galaxy was captured with the Infrared Spectrograph (IRS) in staring mode as part of Series 601 (PIs: Werner, C.O. & Teplitz, H.I.), and also with the MIPS instrument in programs 30 and 39 (PIs: Hailey-Dunsheath, S. & Sargent, B.A.). The IRS data were processed using a modified version of the S11 script, while the MIPS data were reduced utilizing version 8.0 of the MIPS Data Reduction Guide. In this project, we have integrated all archival Spitzer images of NGC 6052 to conduct a comprehensive analysis of the galaxy's spectral power distribution (SED).\n\nInitially, we constructed a network SED spanning from optical to mid-infrared wavelengths. This was then utilized to fit a dust torus model to the infrared data. The model involves a central BlackBody source influenced by a fixed radial density of dust grains within an otherwise empty spheroid. The best-fitting model variables suggest that NGC 6052 harbors an active galactic nucleus (AGN) with an estimated power of 1051 watts and a distance of 95.2 billion light years. The AGN contributes 74% of the galaxy's total infrared luminosity, while the host universe accounts for 23% (or 11% of the total infrared luminosity).\n\nFurthermore, we analyzed the spatial distribution of cool (50-125K) and hot (125-540K) dust, as well as polycyclic aromatic hydrocarbons (PAHs). These findings were utilized to examine the thermal equilibrium of the universe. It was observed that the hot dust is concentrated within a circumnuclear ring with a diameter of 3.2 parsecs. The outer and inner radii of the cool dust were found to be 14 parsecs and 50 parsecs, respectively. The reported total dust mass is 2.3 x 10 - 5 solar masses. The PAH luminosity is 5.5 x 10 - 8 solar masses, and the star formation rate is estimated at 1.2 solar masses per year.\n\nThe relative strengths of PAH groups indicate that the primary energy source heating the dust is star formation, rather than the AGN. We have also analyzed the ionized gas component of the galaxy, revealing that it accounts for just 1% of the total infrared luminosity. Our estimation suggests that the star formation rate in the ring is approximately 235 solar masses per year. We conducted a spectral line energy distribution analysis to determine the distribution of dense gas in the ring. We estimate the gas mass to be 1.1 x 10 - 4 solar masses. Our findings indicate that the measured CO-to-H2 mass conversion factor of 4.3 x 10 - 4 cm3/kg is consistent with expectations in the molecular-rich ring but differs by a certain factor from...（原文在此处被截断）",
        "ori-fast-z-score": -0.29981267559834457,
        "water-fast-z-score": 5.336686798820085,
        "rewrite-fast-z-score": 1.6730038251426083
    },
    {
        "original_text": "Recently, the existence of planets around the star 14 Herculis has been announced. This star is located in the constellation Hercules at a distance of 20.7 light years from the Earth. The existence of 14 Her planets make this system the seventh closest system to the Earth. Since the discovery of the planets around 14 Herculis was announced, several studies have been carried out in order to confirm their physical and orbital characteristics. 14 Her planets can be categorized into three groups according to their sizes. The planets in the inner group have semi-major axes between 0.13 and 0.36 astronomical units, the planets in the middle group have semi-major axes between 0.36 and 0.7 astronomical units, and the planets in the outer group have semi-major axes greater than 0.7 astronomical units. Planets in the inner and outer groups have minimal masses between 7 and 22 Earth masses, whereas the mass of the planets in the middle group is between 22 and 55 Earth masses. In this study, we performed numerical simulations to characterize the dynamical evolution of the 14 Her planets. We show that, because of their relatively short periods and the proximity of the outermost planet in the system to their star, the 14 Her planets might have formed in multiple blocks and were unable to move further away from the star due to planet-planet scattering. Moreover, we predict the existence of a fourth outer planet in the system with a minimal mass of 9 Earth masses. However, further precise radial velocity measurements are required in order to confirm the existence of this planet and to characterize its orbit. This study was performed as part of the Dynamical Analysis of Planetary Systems (DAPS) network, which was designed to characterize the long-term dynamical evolution of planetary systems. This is a large-scale European Research Council-funded project running from January 2015 until December 2020 (ERC Consolidator Grant Number 681627-Dynamical Analysis of Planetary Systems). Authors: Ali Naytext Teslimi and Margarita Karangelova",
        "watermark_text": "Recently , the existence of planets around the star 14 Herculis has been confirmed . This star is situated in the constellation Hercules at a distance of 20 . 7 light years from the Earth .The existence of 14 Her orbits make this system the seventh closest system to the Earth . Since the discovery of the planets around 14 Herculis was announced , various surveys have been carried out in order to confirm their physical and orbital characteristics .14 Her planets can be categorized into three groups based to their sizes . The planets in the inner group have semi - major axes between 0 . 13 and 0 . 36 astronomical units , the planets in the mid group have semi - major axes between 0 . 36 and 0 . 7 astronomical units , and the planets in the inner group have semi - major axes larger than 0 . 7 astronomical units .Planets in the inner and outer groups have minimal masses between 7 and 22 Earth masses , whereas the mass of the planets in the middle group is between 22 and 55 Earth masses . In this study , we performed numerical simulations to characterize the dynamical development of the 14 Her worlds .We suggest that , because of their extremely narrow periods and the location of the outermost planet in the system to their star , the 14 Her planets might have formed in multiple blocks and were unable to move further far from the star due to moon - planet scattering . Moreover , we estimate the existence of a fourth outer planet in the system with a modest mass of 9 Earth masses .However , further exact radial speed measurements are required in order to confirm the existence of this planet and to characterize its orbit . This study was done as part of the Dynamical Analysis of Planetary Systems ( DAPS ) network , which was constructed to characterize the long - term dynamical development of planetary networks .This is a large - scale European Research Council - financed project running from January 2015 until December 2020 ( ERC Consolidator Grant Number 681627 - Dynamical Analysis of Planetary Systems ) . Authors : Ali Naytext Teslimi and Margarita Karangelova",
        "rewrite_text": "Recently, the existence of planets orbiting around the star 14 Herculis has been verified. This star is situated in the Hercules constellation, located 20.7 light years from Earth. Due to the confirmation of planets orbiting 14 Herculis, various surveys have been conducted to determine their physical and orbital characteristics.\n\nThe planets around 14 Herculis can be categorized into three groups based on their sizes. Planets in the inner group have semi-major axes ranging from 0.13 to 0.36 astronomical units, planets in the middle group have axes between 0.36 and 0.7 astronomical units, while planets in the outer group have semi-major axes exceeding 0.7 astronomical units. The inner and outer groups' planets have minimal masses between 7 and 22 times the mass of Earth, while the middle group's planets range between 22 and 55 times the mass of Earth.\n\nIn this study, we conducted numerical simulations to investigate the dynamic evolution of the 14 Herculis planetary system. We propose that due to their narrow orbital periods and the position of the outermost planet in relation to its star, the planets of 14 Herculis may have formed in multiple stages and were unable to move far from the star due to moon-planet scattering. Furthermore, we estimate the presence of a fourth outer planet in the system with a moderate mass of 9 times the mass of Earth. However, more precise radial velocity measurements are needed to confirm this planet's existence and characterize its orbit.\n\nThis study is part of the Dynamical Analysis of Planetary Systems (DAPS) network, established to investigate the long-term dynamic development of planetary networks. This is a large-scale European Research Council-funded project running from January 2015 to December 2020 (ERC Consolidator Grant Number 681627 - Dynamical Analysis of Planetary Systems). The authors are Ali Naytext Teslimi and Margarita Karangelova.",
        "ori-fast-z-score": -0.8638684255813601,
        "water-fast-z-score": 3.5287181598778687,
        "rewrite-fast-z-score": 0.5262348115842176
    },
    {
        "original_text": "Lithium is destroyed at high temperatures. This makes the presence of lithium useful for determining the ages of stars. The turn off stars in a globular cluster have reached the end of their life and no longer produce lithium. By looking at how much lithium is present in these turn off stars we can determine how old the cluster is. We obtained high dispersion spectra of 22 stars in the globular cluster 47 Tuc using the Apache Point Observatory 3.5 m telescope. These spectra cover the region of the Li doublet at 6708 Å and are sufficient resolution and quality to determine the lithium abundances of the stars. We found wide variations in lithium abundance between stars. The lowest lithium abundances are present in stars that are also significantly metal poor. The highest lithium abundances are found in stars with moderate to high metallicity. These results are in disagreement with predictions from current model of Galactic chemical evolution. We propose a new model that includes a variable depletion rate for lithium.",
        "watermark_text": "Lithium is destroyed at high temperatures . This gives the presence of lithium useful for determining the periods of stars .The turn off stars in a globular cluster have met the end of their history and no longer generate lithium . By looking at how many lithium is found in these turn off stars we can determine how old the cluster is .We obtained high dispersion spectra of 22 stars in the globular cluster 47 Tuc use the Apache Point Observatory 3 . 5 m telescope . These spectra cover the region of the Li doublet at 6708 Å and are adequate resolution and quality to identify the lithium abundances of the stars .We identified broad varies in lithium abundance between stars . The lowest lithium abundances are present in stars that are also significantly metal weak .The largest lithium abundances are found in stars with medium to large metallicity . These data are in disagreement with predictions from recent theory of Galactic chemical evolution .We suggest a new model that contains a variable depletion speed for lithium .",
        "rewrite_text": "Lithium is rendered unstable at elevated temperatures, making it a valuable indicator for determining the cycles of star activity. Stars that have reached the end of their lifecycle in a globular cluster, known as \"turn-off\" stars, no longer produce lithium. By analyzing the quantity of lithium found in these \"turn-off\" stars, we can ascertain the age of the cluster. Utilizing the Apache Point Observatory's 3.5-meter telescope, we have obtained high-dispersion spectra of 22 stars in the globular cluster 47 Tuc. These spectra encompass the Li doublet region at 6708 Å and possess sufficient resolution and quality to determine the lithium content of the stars. We have identified significant variations in lithium abundance among the stars, with the lowest levels present in stars that are notably metal-poor, and the highest levels found in stars with medium to high metallicities. These findings contradict recent theoretical predictions of Galactic chemical evolution. We propose a novel model that incorporates a variable depletion rate for lithium.",
        "ori-fast-z-score": 0.1203858530857692,
        "water-fast-z-score": 4.935819976516537,
        "rewrite-fast-z-score": -0.23904572186687872
    },
    {
        "original_text": "The abundance of neutral species and the temperature of the plasmasheet from the Earth s perspective are changing significantly over long time periods. In particular, these parameters have been decreasing over the past century. Atmospheric composition models do not explain these trends. In contrast, models of the Moon s heated interior predict a positive thermal inertia. Here, we apply a thermal inertia model to the lunar thermal survey data from the Space Surveillance Telescope (SST) and find that the thermal inertia of the lunar surface is around 1-2 W m-2 K-1. This matches theoretical expectations and is comparable to the thermal inertias of other Solar System bodies with geologically active surfaces. Using the thermal inertia and a revised eddy flux model, we predict the abundance of neutral species at the Earth s plasmasheet and find that they have decreased by a factor of 2-3% over the past century, in agreement with the measured decrease in optical depth. This decreased neutral density increases the efficiency of the critical Cassini division ultraviolet stabilizer and demonstrates the importance of long-term geophysical trends in understanding the interplanetary environment.",
        "watermark_text": "The density of neutral species and the temperature of the plasmasheet from the Earth s viewpoint are decreasing dramatically over large time periods . In particular , these parameters have been decreasing over the previous century .Atmospheric composition estimates do not predict these patterns . In comparison , models of the Moon s warmed interior predict a positive heating inertia .Here , we apply a cooling inertia model to the lunar thermal survey information from the Space Surveillance Telescope ( SST ) and find that the thermal inertia of the lunar surface is around 1 - 2 W m - 2 K - 1 . This matches theoretical expectations and is analogous to the thermal inertias of other Solar System bodies with geologically active surfaces .Using the thermal inertia and a revised eddy flux model , we estimate the availability of neutral species at the Earth s plasmasheet and find that they have decreased by a factor of 2 - 3 % over the previous century , in agreement with the measured decrease in optical height . This diminished neutral volume raises the performance of the important Cassini division ultraviolet stabilizer and demonstrates the importance of large - term geophysical trends in understanding the interplanetary environment .",
        "rewrite_text": "Over extended periods of time, the density of neutral species and the temperature of the Earth's plasmasheet have experienced significant decreases from a terrestrial perspective. Specifically, these parameters have been in a continuous decline throughout the past century, contrary to atmospheric composition estimates. In contrast, models predicting the warmed interior of the Moon indicate a positive inertia in heat retention. We apply a cooling inertia model to lunar thermal survey data obtained from the Space Surveillance Telescope (SST), revealing that the thermal inertia of the lunar surface is approximately 1 to 2 W m^-2 K^-1, aligning with theoretical expectations. This is comparable to the thermal inertias of other Solar System bodies with geologically active surfaces. By utilizing both the thermal inertia and a revised eddy flux model, we estimate the availability of neutral species at the Earth's plasmasheet. Our findings indicate a 2 to 3% decrease over the past century, which aligns with measured decreases in optical height. This reduction in the neutral volume enhances the performance of critical Cassini division ultraviolet stabilizers, highlighting the significance of long-term geophysical trends in understanding the interplanetary environment.",
        "ori-fast-z-score": 1.3587324409735149,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": 1.6666666666666667
    },
    {
        "original_text": "Following recent claims that the universe is accelerating, this paper critically analyzes the validity of the data and the methodologies employed to reach that conclusion. It is shown that the Type Ia supernova data may be fit to an open universe with zero or even positive curvature without the need for a cosmological constant or dark energy. It is also shown that the data used in recent papers purporting to show that the universe is accelerating is in fact consistent with an open universe with zero or even positive curvature without the need for exotic forms of dark matter. It is concluded that the evidence for the acceleration of the universe is inconclusive and likely represents a form of data selection bias. The acceleration of the universe is one of the greatest achievements of modern cosmology. It was reported by two independent research teams in 2014 and 2016 and gained significant attention in the popular media. The results were quickly accepted as fact by the general astronomy community and the prevailing theoretical interpretation is that a form of dark energy with an equation of state parameter, w < -1 is causing the universe to accelerate. The theoretical foundations of this claim have been extensively analyzed and found to be lacking. It has been demonstrated that for a range of theoretically plausible dark energy models, a vanishingly small value of w is consistent with the reported supernova data. The statistical methodology employed to reject non-dark energy models with small w has been shown to have significant flaws and is invalid. More recent papers have reported similar results. It has been demonstrated that the Type Ia supernova data may be fit to an open universe with zero or even positive curvature without the need for a cosmological constant or dark energy. It is also shown that the data used in recent papers purporting to show that the universe is accelerating is in fact consistent with an open universe with zero or even positive curvature without the need for exotic forms of dark matter. It is concluded that the evidence for the acceleration of the universe is inconclusive and likely represents a form of data selection bias. It is shown that the Type Ia supernova data is consistent with the original research claim, that dark energy is causing the acceleration, but not with the more recent claims that the universe is open or flat. Therefore the reported acceleration is likely a result of selection bias or the incorrect application of statistical methodologies. The data and research in this field is still in a relatively early stage and it is likely that the true nature of the acceleration will become clear as new observational data sets are developed and as more comprehensive modeling of the dark energy is performed. Until then, the acceleration of the universe should be viewed with great suspicion.",
        "watermark_text": "Following recent allegations that the universe is accelerating , this paper critically analyzes the legitimacy of the information and the methodologies employed to reach that conclusion . It is demonstrated that the Type Ia supernova data may be fit to an open universe with zero or even positive curvature without the necessity for a cosmological constant or black force .It is also shown that the information used in recent publications purporting to indicate that the universe is accelerating is in reality compatible with an open universe with zero or even positive curvature without the necessity for exotic kinds of bright matter . It is argued that the proof for the acceleration of the universe is inconclusive and presumably represents a form of evidence selection bias .The velocity of the universe is one of the greatest accomplishments of modern cosmology . It was reported by two independent research teams in 2014 and 2016 and achieved significant notice in the public media .The results were quickly accepted as fact by the general astronomy public and the prevailing theoretical viewpoint is that a form of dark energy with an equation of state factor , w < - 1 is causing the universe to accelerate . The conceptual foundations of this claim have been heavily analyzed and found to be lacking .It has been shown that for a range of theoretically realistic black energy theories , a vanishingly small estimate of w is compatible with the reported supernova data . The empirical methodology employed to reject non - dark energy theories with little w has been shown to have considerable flaws and is invalid .More current publications have reported similar results . It has been shown that the Type Ia supernova data may be fit to an open universe with zero or even positive curvature without the necessity for a cosmological constant or black force .It is also shown that the information used in recent publications purporting to indicate that the universe is accelerating is in reality compatible with an open universe with zero or even positive curvature without the necessity for exotic kinds of bright matter . It is argued that the proof for the acceleration of the universe is inconclusive and presumably represents a form of evidence selection bias .It is demonstrated that the Type Ia supernova data is compatible with the early research claim , that dark energy is causing the acceleration , but not with the more recent predictions that the universe is closed or flat . Therefore the reported acceleration is probably a outcome of selection bias or the incorrect use of statistical methodologies .The data and work in this area is already in a fairly early stage and it is probably that the true nature of the acceleration will become clear as new observational data sets are built and as more complete simulation of the dark energy is conducted . Until then , the acceleration of the universe should be viewed with great doubt .",
        "rewrite_text": "Following recent claims about the acceleration of the universe, this paper critically examines the validity of the information and methodologies used to support this conclusion. It has been demonstrated that Type Ia supernova data can be fitted to an open universe with zero or even positive curvature, without the need for a cosmological constant or dark force. Furthermore, the information used in recent publications suggesting the acceleration of the universe has been found to be compatible with an open universe model of zero or positive curvature, without the requirement for exotic forms of bright matter.\n\nIt is argued that the evidence for the acceleration of the universe is inconclusive and likely reflects a form of evidence selection bias. The velocity of the universe is a significant achievement in modern cosmology, reported by two independent research teams in 2014 and 2016, and has garnered significant attention in the media. However, the general astronomy public has quickly accepted these results as fact, and the prevailing theoretical viewpoint is that a form of dark energy with an equation of state factor w < -1 is causing the universe to accelerate. However, this claim has been heavily analyzed at its conceptual foundations and found lacking.\n\nIt has been shown that, within a range of theoretically realistic black energy theories, a negligible estimate of w is compatible with the reported supernova data. The empirical methodology used to disqualify non-dark energy theories with a small w value has been found to have significant flaws and is therefore invalid. More recent publications have reported similar findings. Moreover, it has been demonstrated that Type Ia supernova data can be fitted to an open universe model without the need for a cosmological constant or dark force. Likewise, the information in recent publications indicating universe acceleration is compatible with an open universe model without requiring exotic bright matter.\n\nIt is contended that the proof of the universe's acceleration remains inconclusive and likely reflects a bias in evidence selection. It has been demonstrated that the Type Ia supernova data is compatible with early research claims that dark energy is causing acceleration, but not with more recent predictions suggesting a closed or flat universe. Therefore, the reported acceleration may be a result of selection bias or incorrect statistical methodology usage. Given that this area of research is still in its early stages, it is likely that further observation and simulation of dark energy will clarify the true nature of the acceleration. Until then, the universe's acceleration should be viewed with skepticism.",
        "ori-fast-z-score": 0.5418283691828771,
        "water-fast-z-score": 10.473929365192031,
        "rewrite-fast-z-score": 2.4041630560342613
    },
    {
        "original_text": "7 Aql and 8 Aql are two Delta Scuti stars that have been under the STEPHI (Search for Variable Star in the Eyes of Hipparcos) observing campaign for many years. In 2003 we obtained multi-site observations of these stars in order to study their lightcurve variations in a more extensive way. New times of minimum and variabilities were found. This new analysis suggests that 7 Aql is a new Delta Scuti variable and that its amplitude of lightcurve variations is lower than that of 7 Aql. The observational data used in this analysis were obtained at different sites around the world using different telescopes and photometers, such as the 1.2m RCC Telescope at the Llano del Hato Observatory (Almería, Spain), the 0.6m TRASCA telescope at the Teide Observatory (Tenerife, Spain), the 1.5m STELLA robotic telescope at the European Southern Observatory (Chile), the 0.76m Plata 60m telescope at the Complejo Astronómico El Leoncito (Argentina), the 0.6m Fin humanitarian young scientist program (FHS) telescope at Haleakala (Hawaii, USA) and the 1.22m Mark VI telescope at Anderson Mesa Station (Arizona, USA). The analysis of the times of minimum and the amplitudes of lightcurve variations shows that the observed lightcurves of both stars present a large fraction of variations with low amplitudes. This kind of variations can be produced by cool starspots, which can be related to the recent passage of these stars through the solar tachocline.",
        "watermark_text": "7 Aql and 8 Aql are two Delta Scuti stars that have been under the STEPHI ( Search for Variable Star in the Eyes of Hipparcos ) observing initiative for many years . In 2003 we acquired multi - location observations of these stars in order to study their lightcurve variations in a more extensive way .New times of minimum and variabilities were found . This new study implies that 7 Aql is a new Delta Scuti variable and that its amplitude of lightcurve variations is lower than that of 7 Aql .The observational data used in this analysis were obtained at different places around the world utilizing multiple telescopes and photometers , such as the 1 . 2m RCC Telescope at the Llano la Hato Observatory ( Almería , Spain ) , the 0 . 6m TRASCA telescope at the Teide Observatory ( Tenerife , Spain ) , the 1 . 5m STELLA robotic telescope at the European Southern Observatory ( Chile ) , the 0 . 76m Plata 60m telescope at the Complejo Astronómico El Leoncito ( Argentina ) , the 0 . 6m Fin humanitarian junior scientist program ( FHS ) telescope at Haleakala ( Hawaii , USA ) and the 1 . 22m Mark VI observatory at Anderson Mesa Station ( Arizona , USA ) . The examination of the times of minimum and the amplitudes of lightcurve variations found that the studied lightcurves of both stars show a large fraction of differences with poor amplitudes .This kind of changes can be made by cold starspots , which can be connected to the recent passage of these stars through the solar tachocline .",
        "rewrite_text": "7 Aql and 8 Aql are two Delta Scuti stars that have been continuously observed by the STEPHI (Search for Variable Star in the Eyes of Hipparcos) initiative for several years. In 2003, multi-location observations of these stars were conducted to examine their lightcurve variations in a comprehensive manner. New minimum times and variations were discovered. This new research indicates that 7 Aql is a newly identified Delta Scuti variable star, with a lower amplitude of lightcurve variations compared to 7 Aql.\n\nThe observational data utilized in this analysis were gathered from various locations around the world using multiple telescopes and photometers. These instruments included the 1.2m RCC Telescope at the Llano la Hato Observatory in Spain, the 0.6m TRASCA telescope at the Teide Observatory in Tenerife, Spain, the 1.5m STELLA robotic telescope at the European Southern Observatory in Chile, the 0.76m Plata 60m telescope at the Complejo Astronómico El Leoncito in Argentina, the 0.6m Fin humanitarian junior scientist program (FHS) telescope at Haleakala in Hawaii, and the 1.22m Mark VI observatory in Anderson Mesa Station in Arizona.\n\nThrough examination of the minimum times and lightcurve variation amplitudes, it was found that the studied lightcurves of both stars exhibited significant differences with low amplitudes. Such variations could be caused by cold starspots, which may be linked to the recent passage of these stars through the solar tachocline.",
        "ori-fast-z-score": -0.9847319278346618,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 1.7556172079419585
    },
    {
        "original_text": "Homology theories, and more generally theories adapted to look at aspects of a topological space which are invariant under deformations, variation or contraction of parts of the space, have proven to be highly fruitful in the study of the fundamental groups of topological spaces. Coarse cohomology, with its geometric interpretation of singular cohomology as Borel cohomology of the space with coefficients in the topologized constant sheaf, similarly provides another perspective on the latter which has proven useful in the study of higher homotopy groups. In this paper we show how these two theories, homology and cohomology, naturally fit into a single framework which unifies them under the same name and provides new related theories, (co)cyclic (co)homology. More precisely, we introduce (co)monads arising from a pair of adjoint functors between suitable categories of (co)representations of bialgebroids, each with its own property of preserving (co)limits. This allows us to give a unified treatment of homology and cohomology theories. As an illustration of the power of this approach, we show that when specialized to cases which have already been studied in the existing literature, such as singular and Borel cohomology, we recover the corresponding theories introduced before. Finally, we apply our methods to a number of cases for which no homology or cohomology theory had been defined before, namely (co)cyclic (co)homology for bialgebroids, including examples coming from (higher) algebraic $K$-theory.",
        "watermark_text": "Homology theories , and more generally theories modified to think at matters of a topological space which are invariant under deformations , variation or contraction of parts of the space , have proven to be highly fruitful in the study of the fundamental groups of topological spaces . Coarse cohomology , with its mathematical definition of singular cohomology as Borel cohomology of the space with coefficients in the topologized constant sheaf , similarly presents another perspective on the latter which has proven valuable in the study of greater homotopy categories .In this paper we prove how these two concepts , homology and cohomology , naturally fit into a single formulation which unifies them under the same naming and provides new related notions , ( co ) cyclic ( co ) homology . More specifically , we provide ( co ) monads resulting from a pair of adjoint functors between suitable categories of ( co ) representations of bialgebroids , each with its own property of preserving ( co ) limits .This enables us to give a consolidated treatment of homology and cohomology theories . As an illustration of the power of this methodology , we find that when specialized to cases which have already been studied in the established authors , such as singular and Borel cohomology , we recover the equivalent concepts pioneered before .Finally , we apply our approaches to a number of instances for which no homology or cohomology theory had been formulated before , notably ( co ) cyclic ( co ) homology for bialgebroids , particularly instances coming from ( higher ) algebraic $ K $ - theory .",
        "rewrite_text": "Theories of homology, more broadly extending to consider topological spaces' invariance properties under deformations, variations, or contractions of space parts, have been found to be highly beneficial in studying the fundamental groups of topological spaces. Coarse cohomology, with its mathematical definition of singular cohomology as Borel cohomology for a space with coefficients in the topologized constant sheaf, offers another valuable perspective in the exploration of larger homotopy categories.\n\nIn this paper, we demonstrate how these two concepts - homology and cohomology - naturally merge into a unified formulation, which not only consolidates them under a common name but also introduces related notions such as (co) cyclic (co) homology. Specifically, we introduce (co) monads arising from a pair of adjoint functors between suitable categories of (co) representations of bialgebroids. Each of these functors preserves (co) limits with its unique properties. This enables us to provide a consolidated approach to homology and cohomology theories.\n\nAs an exemplification of this methodology's potency, we observe that when applied to cases already studied by established authors, such as singular and Borel cohomology, we recover the previously pioneered equivalent concepts. Ultimately, we apply our methodologies to various scenarios where no homology or cohomology theory had been formulated before, particularly (co) cyclic (co) homology for bialgebroids and instances derived from (higher) algebraic K-theory.",
        "ori-fast-z-score": -0.8164965809277261,
        "water-fast-z-score": 5.9196002117260145,
        "rewrite-fast-z-score": 3.073993852018444
    },
    {
        "original_text": "We study the stability of longitudinally flowing flux tubes in the solar convection zone. We consider the modes with infinitesimal disturbances that are parallel to the flow and show that they are unstable when the angular velocity of the tube axis is smaller than a certain critical value. This is in contrast to the case of tube rotation comparable to the Alfvén velocity, for which there is no unstable modes. The instability, which we term the Axisymmetric Modes Instability (AMI), may explain why longlived vortical flows are observed only in the upper part of the convection zone. We also compute the critical angular velocity as a function of various parameters, in particular for modes localized in the tube or in the surrounding fluid. In particular, we show that for localized modes, the critical angular velocity depends only on the tube radius and not on the magnetic field strength. Finally, we compare the instability properties of flux tubes with the properties of fingering plumes and calculate the corresponding effective Richardson and Peltier numbers. We show that the instability of flux tubes is expected to develop at significantly smaller values of the Peltier number than that of plumes. We conclude that the instability should lead to the onset of vortical flows in the upper part of the convection zone and to the formation of fingers in the deep interior.",
        "watermark_text": "We consider the stability of longitudinally flowing flux tubes in the sun circulation region . We consider the modes with infinitesimal disturbances that are parallel to the flow and find that they are unstable when the angular velocity of the pipe axis is smaller than a certain essential value .This is in comparison to the case of tube rotation comparable to the Alfvén speed , for which there is no unstable modes . The instability , which we name the Axisymmetric Modes Instability ( AMI ) , might explain why longlived vortical flows are observed only in the higher part of the convection zone .We additionally compute the critical angular velocity as a function of several variables , in example for modes localized in the pipe or in the nearby fluid . In particular , we find that for localized modes , the important angular velocity depends only on the tube diameter and not on the magnetic field intensity .Finally , we compare the instability properties of flux tubes with the properties of fingering plumes and estimate the equivalent effective Richardson and Peltier numbers . We see that the instability of flux tubes is expected to develop at significantly reduced values of the Peltier number than that of plumes .We suggest that the instability should result to the emergence of vortical flows in the upper part of the convection zone and to the formation of fingers in the deep interior .",
        "rewrite_text": "We are examining the stability of flux tubes that flow longitudinally within the solar circulation region. We are focusing on modes that experience infinitesimal disturbances parallel to the flow and have discovered that they become unstable when the angular velocity of the pipe axis is below a specific critical value. This contrasts with the scenario where tube rotation is comparable to the Alfvén speed, in which there are no unstable modes present. We have named this instability Axisymmetric Modes Instability (AMI). This could potentially explain why long-lived vortical flows are only observed in the upper part of the convection zone.\n\nFurthermore, we have calculated the critical angular velocity as a function of various variables, for instance, in the case of modes localized within the pipe or in the surrounding fluid. Specifically, we found that for localized modes, the significant angular velocity solely depends on the tube's diameter and not on the magnetic field strength.\n\nLastly, we have compared the instability characteristics of flux tubes to those of fingering plumes and estimated the equivalent effective Richardson and Peltier numbers. Our observations indicate that the instability of flux tubes is expected to develop at lower Peltier number values compared to plumes. We suggest that this instability could lead to the emergence of vortical flows in the upper part of the convection zone and the formation of fingers in the deeper interior.",
        "ori-fast-z-score": -0.10050378152592121,
        "water-fast-z-score": 5.8,
        "rewrite-fast-z-score": 1.4501047335684953
    },
    {
        "original_text": "In this work we present a method to model the three-point correlation function of cosmological large scale structure. By the three-point correlation function we mean the number of objects as a function of their separation, and the scale over which this separation is measured. Our method is based on the coupling of a N-body simulation to a peak-bagging algorithm, and is able to capture the signal on both large and small scales. We illustrate our method on real data from the WiggleZ Dark Energy Survey, and show that we are able to reproduce both the one- and two-halo terms of the correlation function. We further apply our method to mock WiggleZ survey data, and show that we can place competitive constraints on the bias parameter of dark matter halos, especially on small scales, compared to other methods. Finally, we discuss potential extensions to this method that may allow us to reduce the sampling error and increase the range of scales over which we can probe the three-point function, ultimately allowing us to map out the full non-linear three-point correlation function.",
        "watermark_text": "In this research we present a technique to model the three - point correlation function of cosmological big scale structure . By the three - point correlation function we mean the number of items as a function of their separation , and the scale over which this separation is measured .Our algorithm is based on the coupling of a N - bodies simulation to a peak - bagging algorithm , and is able to capture the signal on both large and tiny scales . We illustrate our technique on real information from the WiggleZ Dark Energy Survey , and find that we are able to capture both the one - and two - halo terms of the interaction function .We further use our technique to mock WiggleZ survey information , and suggest that we can place competitive limitations on the bias variable of light matter halos , particularly on small scales , compared to other methods . Finally , we explain potential extensions to this method that might enable us to reduce the sampling error and increase the range of scales over which we can investigate the three - point function , ultimately allowing us to map out the full non - linear three - point correlation function .",
        "rewrite_text": "In this research, we introduce a technique aimed at modeling the three-point correlation function of the large-scale structure in cosmology. The three-point correlation function refers to the number of entities as a function of their separation and the scale on which this separation is measured. Our algorithm is founded on the integration of an N-body simulation with a peak-bagging approach, enabling us to detect signals on both large and small scales.\n\nWe illustrate our technique using actual data from the WiggleZ Dark Energy Survey, revealing our ability to capture both the one-halo and two-halo terms of the interaction function. Furthermore, we employ our technique to simulate WiggleZ survey data, suggesting that we can set competitive constraints on the bias variable of light matter halos, particularly on smaller scales compared to alternative methods.\n\nLastly, we discuss potential advancements to this method that could reduce sampling errors and expand the range of scales we can investigate with the three-point function. Ultimately, this would enable us to map out the complete nonlinear three-point correlation function.",
        "ori-fast-z-score": 0.10721125348377948,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": 0.4364357804719848
    },
    {
        "original_text": "Two spin-1/2 fermions can interfere quantum-mechanically, exhibiting coherent superposition of ‘wavetrains’ with zero mean wave vector, and resulting in an interference pattern with fringes of equal amplitude but opposite phase. Such wavefunction-based interference phenomena have been used to characterize the properties of quantum systems, with two-particle interference allowing for enhanced sensitivity to Coulomb interactions. Here we report the interference pattern of two independent electrons in the vicinity of the so-called “Hohng singularity” in the joint electron density of two separate but co-located helium droplets. The observed two-particle interference pattern is in excellent agreement with calculations based on the exact two-particle Schrödinger equation, allowing determination of the phase difference between interfering waves, and thus the interference fringes provide a precise measurement of the Aharonov-Bohm phase caused by the spatial overlap of the two separate electron wave functions. These studies of two-particle interference in helium – a deceptively simple system with complex many-body interactions – illustrate the power of this interference phenomenon to characterize quantum many-body systems and hold great potential for application to quantum technologies and simulation.",
        "watermark_text": "Two spin - 1 / 2 fermions can interfere quantum - mechanically , showing coherent superposition of ‘ wavetrains ’ with zero mean wave vector , and resulting in an interference pattern with fringes of equal frequency but opposite phase . Such wavefunction - based interference effects have been used to characterize the properties of quantum systems , with two - particle interference allowing for greater sensitivity to Coulomb particles .Here we study the interference pattern of two independent electrons in the vicinity of the so - called “ Hohng singularity ” in the joint electron concentration of two separate but co - placed helium droplets . The observed two - particle interference pattern is in good agreement with analyses based on the exact two - particle Schrödinger equation , allowing determination of the phase change between interfering waves , and therefore the interference fringes allow a precise measurement of the Aharonov - Bohm phase caused by the spatial overlap of the two separate electron wave functions .These studies of two - particle interference in helium – a deceptively simple system with difficult large - bodies interactions – illustrate the power of this interference phenomenon to characterize quantum several - bodies systems and stand tremendous possibility for use to quantum technologies and modeling .",
        "rewrite_text": "Two spin-1/2 fermions can undergo quantum mechanical interference, exhibiting coherent superpositions of \"wavetrains\" with a zero mean wave vector. This results in an interference pattern featuring fringes of equal frequency but opposite phase. Such wavefunction-based interference effects have been utilized to characterize the properties of quantum systems, with two-particle interference enhancing sensitivity to Coulomb particles.\n\nIn this study, we investigate the interference pattern of two independent electrons in the vicinity of the所谓的“Hohng奇点”在两个独立但同处放置的氦滴联合电子浓度中。观察到的双粒子干涉图样与基于精确双粒子薛定谔方程的分析结果高度一致，这允许我们确定干涉波之间的相位变化，因此，干涉条纹能够精确测量由两个独立电子波函数空间重叠所引起的Aharonov-Bohm相位。这些在氦中的双粒子干涉研究——一个看似简单但实则涉及复杂大物体相互作用的系统——充分展示了这种干涉现象在表征多体量子系统中的强大能力，并具有巨大的潜力应用于量子技术和建模。",
        "ori-fast-z-score": -0.20203050891044214,
        "water-fast-z-score": 4.321662605614612,
        "rewrite-fast-z-score": 1.4795908857482156
    },
    {
        "original_text": "Atom waves can be diffracted between two Raman-Nath Bragg frequencies, when the incident wave is in the Bragg regime. Under these conditions, the effective Rabi frequency, the losses, and the phase shifts are calculated. mo version= 1.0  encoding= utf-8 ? File size: 791 KB 2003 arXiv.org Atom waves can be diffracted between two Raman-Nath Bragg frequencies, when the incident wave is in the Bragg regime. Under these conditions, the effective Rabi frequency, the losses, and the phase shifts are calculated. An analysis of the effective Rabi frequency shows that only half of the atom wave function is diffracted, as expected for a two-level system. The calculated losses can be attributed to phase shifts, and can be large if the wave vector of the diffracted wave is small. rgarding methodology, the article uses the coupled Maxwell-Bloch equations, in the dipole and Raman-Nath limits. The calculations are presented for sodium, with parameters from the literature.",
        "watermark_text": "Atom signals can be diffracted between two Raman - Nath Bragg frequencies , when the incident wave is in the Bragg regime . Under these conditions , the effective Rabi frequency , the losses , and the phase transitions are measured .mo version = 1 . 0 encoding = utf - 8 ? File size : 791 KB 2003 arXiv . org Atom signals can be diffracted between two Raman - Nath Bragg frequencies , when the incident wave is in the Bragg regime .Under these conditions , the effective Rabi frequency , the losses , and the phase transitions are measured . An evaluation of the effective Rabi frequency reveals that only portion of the atom wave function is diffracted , as anticipated for a two - level network .The measured costs can be due to phase change , and can be large if the wave tensor of the diffracted wave is tiny . rgarding methodology , the article utilizes the coupled Maxwell - Bloch coefficients , in the dipole and Raman - Nath limits .The parameters are presented for sodium , with parameters from the literature .",
        "rewrite_text": "In the Bragg regime, where the incident wave is present, atom signals can be diffracted between two Raman-Nath Bragg frequencies. Under these circumstances, the effective Rabi frequency, losses, and phase transitions are meticulously measured. When assessing the effective Rabi frequency, it becomes apparent that only a portion of the atom wave function is diffracted, as expected in a two-level network. The measured costs may be attributed to phase changes and can be significantly large if the diffracted wave's wave tensor is minimal.\n\nRegarding the methodology employed, the article utilizes the coupled Maxwell-Bloch coefficients within the dipole and Raman-Nath limits. The parameters presented are specific to sodium and sourced from existing literature.",
        "ori-fast-z-score": -1.4084056792618558,
        "water-fast-z-score": 3.3565855667130946,
        "rewrite-fast-z-score": 1.6641005886756874
    },
    {
        "original_text": "The Dicke model is an exactly solvable model of quantum many-body physics, which describes a collection of N two-level systems, or qubits, all of which are in the same quantum state, interact with each other through dipole-dipole coupling, and are subject to an external driving field. The model exhibits a quantum phase transition from a normal phase to a cooperative phase as the coupling strength exceeds a critical value. As an exactly solvable model, the Dicke model has been extensively studied and is well understood, but its extension to an N-qubit system with general multipolar interactions is significantly less well-understood. In particular, such general multipolar interactions are found to lead to rich quantum phases beyond the simple cooperative phase of the Dicke model, including a subradiant phase with non- vanishing expectation values of the excitation number operator and a Luttinger liquid phase with continuously varying correlation functions. Here, we present a method to characterize these phases, with an emphasis on the identification of the Luttinger liquid phase, based on the parafermionic quantum field theory of Abanov and Wiegmann. We apply this method to a specific family of models with multi-dipole interactions and general dipole-dipole interactions, and find that the Luttinger liquid phase can be accurately captured by a low-level truncation of the parafermionic field theory, but the subradiant phase is not described correctly. This example illustrates the utility of parafermionic field theory as a method to characterize phases beyond the cooperative phase of the Dicke model, and further investigation may reveal its broader applicability.",
        "watermark_text": "The Dicke model is an exactly solvable theory of quantum several - bodies physics , which explains a collection of N two - level systems , or qubits , all of which are in the same quantum state , interact with each other through dipole - dipole coupling , and are subject to an external driving field . The model shows a quantum phase shift from a normal phase to a cooperative phase as the interaction strength reaches a critical value .As an exactly solvable theory , the Dicke model has been heavily explored and is well understood , but its extension to an N - qubit scheme with general multipolar interactions is significantly less poorly - understood . In particular , such general multipolar relationships are found to lead to rich quantum phases beyond the simple cooperative phase of the Dicke model , notably a subradiant phase with non - vanishing expectation values of the excitation number operator and a Luttinger vapor phase with constantly varying coupling functions .Here , we present a technique to characterize these periods , with an emphasis on the identity of the Luttinger vapor phase , built on the parafermionic quantum field model of Abanov and Wiegmann . We application this method to a certain family of models with multi - dipole interactions and general dipole - dipole interactions , and find that the Luttinger vapor phase can be correctly captured by a small - grade truncation of the parafermionic field theory , but the subradiant transition is not described properly .This instance illustrates the utility of parafermionic field theory as a technique to characterize phases beyond the joint phase of the Dicke approach , and further investigation may reveal its broader applicability .",
        "rewrite_text": "The Dicke model is an exactly solvable theory in the field of quantum many-body physics, which elucidates a collection of N two-level systems, or qubits, all in a unified quantum state. These systems interact with each other through dipole-dipole coupling and are subjected to an external driving field. The model demonstrates a quantum phase transition from a normal phase to a cooperative phase as the interaction strength reaches a critical level. \n\nAs an exactly solvable theory, the Dicke model has been extensively studied and is well understood. However, extending it to an N-qubit framework with general multipolar interactions is less understood. Specifically, general multipolar relationships have been found to lead to an array of rich quantum phases beyond the basic cooperative phase of the Dicke model. Notably, there is a subradiant phase with non-vanishing expectation values of the excitation number operator and a Luttinger vapor phase with constantly varying coupling functions.\n\nIn this study, we introduce a technique to characterize these phases, with a focus on identifying the Luttinger vapor phase. This technique is based on the parafermionic quantum field model developed by Abanov and Wiegmann. We apply this method to various models involving multi-dipole interactions and general dipole-dipole interactions. Our findings indicate that the Luttinger vapor phase can be accurately captured through a low-grade truncation of the parafermionic field theory. However, the subradiant transition remains inadequately described.\n\nThis case study underscores the utility of the parafermionic field theory as a tool for characterizing phases beyond the scope of the Dicke approach. Further investigation may reveal its broader applicability in the field of quantum physics.",
        "ori-fast-z-score": -0.552344770738994,
        "water-fast-z-score": 5.89167755454927,
        "rewrite-fast-z-score": 1.6994116628998401
    },
    {
        "original_text": "In this paper, we propose a quantum repeater architecture capable of supporting long-distance, fault-tolerant quantum communication over broadband metropolitan networks. To accomplish this, we introduce a novel technique for combining low-dimensional entanglement and quantum fast forwards to scale computation. Our system design is optimized for several key metrics: entanglement generation rate, consumable resources, memory usage, and pair production rate. The entanglement is generated between lightmatter qubits in a quantum frequency quadpter, distillation of the shared entanglement is accomplished in parallel on a cluster of nearby quantum computers, and a quantum signal reconstruction step is completed at a metropolitan node. Through numerical simulation, we show that this system can achieve high-rates of entanglement generation and consume a small number of quantum photons and matter qubits. Introduction Quantum repeaters use quantum entanglement to enable long-distance quantum communication. Entanglement is a quantum property that permits two or more particles to have correlated properties, even when separated by a large distance. Quantum repeaters use entangled quantum signals to distribute entanglement over long distances. Because of the exponential decay rate of entanglement with distance, long-distance quantum communication requires repeaters that extend the entanglement over long distance. Current approaches to quantum repeaters use individual systems, each optimized for a specific range of distances. Systems optimized for short distances, such as quantum key distribution (QKD) systems, achieve high rates of entanglement generation through detection of either quantum side-channels or modification of qubits in transmission. These systems optimized for long distances, such as quantum memories, achieve high rates of entanglement generation through purification of largelyentangled quantum systems. Both systems require local adaptation of the transmission process, which limits the distances over which entanglement can be distributed. In this paper, we present a quantum repeater architecture that combines features from these optimized systems for short and long distances. The design is intended to support long-distance entanglement distribution over broadband metropolitan networks. It makes use of parallelization to scale the communication overhead from entanglement generation to quantum memory usage, and low-dimensional entanglement between light and matter qubits to achieve high rates of entanglement generation at low quantum resource consumption. Entanglement generation is facilitated by using light-matter entanglement, where matter qubits interact with light in quantum frequency quadpters (QFQs). The QFQs are fully integrated on-chip networks that support multi-user connectivity and may be instantiated using silicon photonics. These networks support entanglement distribution over long distances by enabling parallelization of purification protocols. However, the purification rates in previous approaches were limited by the need to localize the entanglement generation to specific purification registers. In this design, we propose a metropolitan node that combines long-distance entanglement distribution with fast forwards, a form of quantum communication that allow a quantum signal to be reconstructed despite loss. Using the long-distance entanglement, local computation is able to reconstruct the quantum signals at the metropolitan node, allowing full entanglement distribution over long distance with the",
        "watermark_text": "In this paper , we propose a quantum repeater architecture capable of supporting long - distance , failure - tolerant quantum communication over broadband metropolitan connections . To accomplish this , we provide a new technique for blending low - dimensional entanglement and quantum fast forwards to scale computation .Our network architecture is optimized for various key metrics : entanglement generation rate , consumable resources , cache availability , and pair production frequency . The entanglement is generated between lightmatter qubits in a quantum frequency quadpter , distillation of the shared entanglement is accomplished in parallel on a cluster of neighbouring quantum computers , and a quantum data repair step is completed at a metropolitan node .Through mathematical simulation , we find that this network can attain high - rates of entanglement generation and consume a small number of quantum photons and material qubits . Introduction Quantum repeaters utilize quantum entanglement to enable large - distance quantum communication .Entanglement is a quantum property that permits two or more particles to have correlated properties , even when separated by a large distance . Quantum repeaters utilize entangled quantum signals to distribute entanglement over large distances .Because of the exponential decay rate of entanglement with distance , long - distance quantum communication requires repeaters that extend the entanglement over large distance . Current approaches to quantum repeaters utilize individual systems , each optimized for a certain range of distances .Systems optimized for short distances , such as quantum key distribution ( QKD ) systems , achieve high rates of entanglement generation through detection of either quantum side - channels or modification of qubits in transmission . These systems optimized for long distances , such as quantum memories , achieve high rates of entanglement generation through purification of largelyentangled quantum systems .Both structures require local adapt of the transmission mechanism , which reduces the distances over which entanglement can be dispersed . In this paper , we present a quantum repeater architecture that combines features from these optimized models for short and long distances .The concept is intended to support long - distance entanglement flow over network urban systems . It makes using of parallelization to scale the communication overhead from entanglement generation to quantum memory usage , and low - dimensional entanglement between dark and matter qubits to achieve high levels of entanglement generation at low quantum resource consumption .Entanglement generation is enabled by using light - matter entanglement , where matter qubits interact with light in particle rate quadpters ( QFQs ) . The QFQs are completely embedded on - chip networks that provide multi - person networking and may be instantiated using silicon photonics .These connections support entanglement flow over large distances by enabling parallelization of purification protocols . However , the purification rates in earlier approaches were restricted by the necessity to localize the entanglement generation to individual purification registers .In this design , we propose a metropolitan node that combines long - distance entanglement distribution with fast forwards , a form of quantum communication that enable a quantum signal to be restored despite loss . Using the long - distance entanglement , local computation is allowed to reconstruct the quantum signals at the metropolitan node , allowing full entanglement distribution over large distance with the",
        "rewrite_text": "In this study, we propose a quantum repeater architecture that effectively supports long-distance, failure-tolerant quantum communication across broadband metropolitan connections. To achieve this, we introduce a novel technique that combines low-dimensional entanglement with quantum fast forwarding to scale computation.\n\nOur network architecture is optimized for various key performance metrics: entanglement generation rate, consumable resources, cache availability, and pair production frequency. The generation of entanglement takes place between lightmatter qubits within a quantum frequency quadruplet. The distillation of shared entanglement is accomplished in parallel on a cluster of neighboring quantum computers. Additionally, a quantum data repair step is completed at a metropolitan node.\n\nThrough mathematical simulation, we have found that this network can achieve high rates of entanglement generation while consuming a minimal number of quantum photons and material qubits. Quantum repeaters utilize quantum entanglement to enable communication over large distances. Entanglement is a quantum property that allows two or more particles to have correlated properties, even when separated by vast distances.\n\nOur proposed architecture leverages entangled quantum signals to distribute entanglement over extended distances. Due to the exponential decay of entanglement with distance, long-distance quantum communication requires repeaters that can extend the range of entanglement. Current approaches to quantum repeaters typically involve individual systems, each optimized for a specific distance range.\n\nFor short distances, systems like quantum key distribution (QKD) achieve high rates of entanglement generation through the detection of quantum side-channels or modifications to qubits during transmission. On the other hand, systems optimized for longer distances, such as quantum memories, achieve high rates through the purification of highly entangled quantum systems. However, both structures require local adaptation of the transmission mechanism, which limits the distances over which entanglement can be dispersed.\n\nIn contrast, our architecture combines features from both short and long-distance optimized models. It utilizes parallelization to scale the communication overhead from entanglement generation to quantum memory usage. Additionally, it employs low-dimensional entanglement between dark and matter qubits to achieve high levels of entanglement generation with minimal quantum resource consumption.\n\nThe generation of entanglement is facilitated by utilizing light-matter interaction in particle rate quadruplets (QFQs). These QFQs are fully integrated on-chip networks that provide multi-person networking capabilities and can be implemented using silicon photonics. These connections enable the flow of entanglement over large distances through parallel purification protocols.\n\nHowever, previous approaches were limited by the need to localize entanglement generation to individual purification registers. In our design, we introduce a metropolitan node that combines long-distance entanglement distribution with fast forwards, a form of quantum communication that restores a quantum signal even after loss. By utilizing the long-distance entanglement, local computation is enabled to reconstruct quantum signals at the metropolitan node, allowing for complete entanglement distribution over vast distances.",
        "ori-fast-z-score": 1.4403158694958438,
        "water-fast-z-score": 8.407907788851324,
        "rewrite-fast-z-score": 2.5607375986579197
    },
    {
        "original_text": "In this paper, we develop a detailed quantum electrodynamical (QED) description of interactions between realistic superconducting qubits and microwave radiation. We derive an effective low-energy Hamiltonian that includes the Jaynes-Cummings, Tavis-Cummings, and dipole couplings. Numerical simulations of quantum Rabi, Dicke, and Landau-Zener models are performed to illustrate the QED regime. This low-energy QED description facilitates a controlled analysis of qubit-microwave coupling strengths and microwave frequency offsets relative to the qubit levels. The predicted dispersive decay and dephasing rates are consistent with recent experiments. Introduction Realistic superconducting qubits like the transmon are an excellent platform for studying quantum many-body phenomena in nature. In particular, their highly tunable nature allows for an exquisite control of both system- and bath- dephasing processes. This exquisite control, however, has become an asset that now poses a challenge for sufficiently understanding and describing the underlying physical mechanisms. In this paper, we develop a detailed quantum electrodynamical (QED) description of interactions between realistic superconducting qubits and microwave radiation. The derivation is based on a high-fidelity derivation of the dispersive Jaynes-Cummings Hamiltonian in  1 . Unlike other derivations in the literature, our starting point is a fully calibrated transmon model, that is, a model with the complete experimental control settings encoded. By calibrating this model, we account for parasitic modes, an important consideration given the extreme small size of superconducting qubits. A crucial result of this derivation is an effective low-energy Hamiltonian that includes the Jaynes-Cummings, Tavis-Cummings, and dipole couplings. Using degenerate perturbation theory, we obtain analytical expressions for qubit-microwave coupling strengths and microwave frequency offsets relative to the qubit levels. These analytical expressions facilitate a systematic understanding and controlled analysis of qubit-microwave interactions for any given experimental system setup. The low-energy QED description enables a quantitative comparison with experiment and an assessment of control errors and residual non-QED interactions. Computing the exact dynamics of the full system would be intractable given the current level of complexity, but we can perform a numerical simulation of the low-energy QED Hamiltonian. Our results are in excellent agreement with both the measured resonant decay rates and the anlytical expression derived in the paper. In addition, our simulations explain the emergence of avoided level crossings in the system. This explanation, which is not accounted for in the QED model, is a non-Markovian dressing of the qubit levels by the non-perturbative bath induced by spontaneous emission. We finish this paper with a summary of the most important findings and a discussion of the potential use of this approach to describe qubit-microwave interactions in more complex settings. Main findings We introduce a highly calibrated qubit model as a starting point and perform a high fidelity",
        "watermark_text": "In this paper , we develop a detailed quantum electrodynamical ( QED ) description of relationships between realistic superconducting qubits and microwave radiation . We derive an efficient low - energy Hamiltonian that comprises the Jaynes - Cummings , Tavis - Cummings , and dipole couplings .Numerical simulations of quantum Rabi , Dicke , and Landau - Zener models are performed to illustrate the QED regime . This low - energy QED description facilitates a controlled analysis of qubit - microwave coupling strengths and microwave frequency offsets compared to the qubit levels .The predicted dispersive emission and dephasing speeds are compatible with recent experiments . Introduction Realistic superconducting qubits like the transmon are an excellent platform for studying quantum several - bodies phenomena in nature .In particular , their highly tunable nature allows for an amazing control of both system - and bath - dephasing mechanisms . This amazing control , however , has become an value that now offers a challenge for adequately studying and understanding the fundamental physical mechanisms .In this paper , we develop a detailed quantum electrodynamical ( QED ) description of relationships between realistic superconducting qubits and microwave radiation . The derivation is based on a high - fidelity derivation of the dispersive Jaynes - Cummings Hamiltonian in 1 .Unlike other derivations in the books , our starting point is a completely calibrated transmon theory , that is , a theory with the complete experimental control settings encoded . By calibrating this model , we account for parasitic modes , an important consideration given the extreme low size of superconducting qubits .A crucial outcome of this derivation is an efficient low - energy Hamiltonian that contains the Jaynes - Cummings , Tavis - Cummings , and dipole couplings . Using degenerate perturbation theory , we obtain empirical expressions for qubit - microwave coupling strengths and microwave frequency offsets compared to the qubit levels .These analytical expressions facilitate a comprehensive understanding and controlled analysis of qubit - microwave interactions for any given experimental system setup . The minimum - energy QED description provides a empirical comparison with research and an assessment of power failures and residual non - QED interactions .Computing the exact dynamics of the full system might be intractable considering the present degree of complexity , but we can conduct a statistical model of the small - energy QED Hamiltonian . Our results are in good agreement with both the measured resonant decay rates and the anlytical representation derived in the paper .In addition , our simulations describe the emergence of avoided level crossings in the system . This interpretation , which is not accounted for in the QED theory , is a non - Markovian dressing of the qubit levels by the non - perturbative bath induced by spontaneous emission .We end this paper with a summary of the most important findings and a review of the possibilities application of this methodology to explain qubit - microwave interactions in more sophisticated settings . Main findings We introduce a highly calibrated qubit model as a starting stage and conduct a high fidelity",
        "rewrite_text": "在这篇论文中，我们详细地发展了量子电动力学（QED）中超导量子比特与微波辐射之间关系的描述。我们推导出一个高效的低能哈密顿量，其中包括Jaynes-Cummings、Tavis-Cummings和偶极耦合。通过量子Rabi、Dicke和Landau-Zener模型的数值模拟，说明了QED的范畴。这种低能QED描述有助于对量子比特与微波耦合强度以及微波频率与量子比特能级之间的偏移进行可控分析。预测的色散发射和退相速度与最近的实验结果相吻合。\n\n引言：真实的超导量子比特，如transmon，是研究自然界中量子多体现象的优秀平台。特别是它们的可高度调谐性使得对系统和浴去相干机制的控制非常出色。然而，这种出色的控制现在为研究理解基本物理机制提供了挑战。本文将发展一个详细的量子电动力学（QED）描述，用于描述现实超导量子比特与微波辐射之间的关系。\n\n我们的推导基于1中分散性Jaynes-Cummings哈密顿量的高保真推导。与其他书籍中的推导不同，我们的出发点是完全校准的transmon理论，即一种编码了完整的实验控制设置的理论。通过校准这个模型，我们考虑到了寄生模式，这是由于超导量子比特的极端小尺寸而考虑的重要因素。\n\n这个推导的一个重要结果是包含Jaynes-Cummings、Tavis-Cummings和偶极耦合的高效低能哈密顿量。我们使用退化扰动理论来获得量子比特与微波耦合强度以及微波频率与量子比特能级之间的比较的经验表达式。这些解析表达式促进了给定实验系统设置下对量子比特与微波相互作用的全面理解和可控分析。\n\n最低能量QED描述为研究提供了实证比较，并评估了功率故障和非QED相互作用的残留影响。考虑到当前复杂程度，计算整个系统的确切动力学可能是不可行的，但我们可以对小能量QED哈密顿量进行统计建模。我们的结果与论文中测量的共振衰减率和解析表示均具有良好的一致性。此外，我们的模拟描述了系统中避免能级交叉的出现。这种解释在QED理论中未被考虑，是由自发发射引起的非扰动浴对量子比特能级的非马尔可夫性装束所导致的。\n\n本文最后总结了最重要的发现，并回顾了将这种方法应用于更复杂环境中解释量子比特与微波相互作用的可能性。\n\n主要发现：我们以高度校准的量子比特模型为起点，并进行了高保真度的推导，从而得出了重要的结论。",
        "ori-fast-z-score": 0.6933752452815365,
        "water-fast-z-score": 9.338145262192008,
        "rewrite-fast-z-score": -1.0690449676496976
    },
    {
        "original_text": "Theory for superconductivity in a magnetic field: A local approximation approach. The paper presents a general framework for building a superconducting theory in a magnetic field. Such theory should be able to describe the superconductor below the critical temperature, as well as the magnetic materials with an Abrikosov vortices lattice at zero temperature. The starting point of the theory is to assume that the order parameter is a complex field that depends on both space and time. In this way, all the degrees of freedom associated with the magnetic field are included into the theory and the coexistence of superconductor and magnetic flux states can be described. Due to the complex nature of the order parameter, one needs to include many degrees of freedom to fully describe the superconducting state. To do so, one needs to resort to a many-body expansion of the free energy. For small coupling between the order parameter and the magnetic field, a local approximation is sufficient to capture the main physical mechanism. This is done by treating the superconducting and magnetic materials on equal footings by introducing a new auxiliary field into the free energy functional. The theory is applied to describe superconductivity in a magnetic field at the nanometer scale. In this regime, the London theory cannot be applied and a fully quantum mechanical treatment is required. In particular, it is shown that one can get a region in which a type II superconductor can sustain a magnetic field without any vortex lattice.",
        "watermark_text": "Theory for superconductivity in a magnetic field : A local approximation approach . The paper offers a general basis for building a superconducting theory in a magnetic field .Such theory should be possible to explain the superconductor below the critical temperature , as well as the magnetic materials with an Abrikosov vortices structure at zero temperature . The starting point of the theory is to assume that the order parameter is a complex field that relies on both space and period .In this way , all the degrees of liberty related with the magnetic force are incorporated into the model and the coexistence of superconductor and magnetic flux states can be described . Due to the complex nature of the order parameter , one needs to use multiple degrees of liberty to fully describe the superconducting state .To do so , one needs to resort to a many - bodies increase of the free energy . For small coupling between the order parameter and the magnetic force , a local approximation is adequate to capture the main mechanical system .This is accomplished by treating the superconducting and magnetic materials on equivalent footings by bringing a new auxiliary field into the free energy functional . The theory is applied to explain superconductivity in a magnetic field at the nanometer scale .In this regime , the London concept never be applied and a completely quantum mechanical treatment is required . In particular , it is demonstrated that one can find a region in which a class II superconductor can sustain a magnetic force without any vortex lattice .",
        "rewrite_text": "Approaching Superconductivity Theory in a Magnetic Field: A Local Approximation Method\n\nThe research paper provides a comprehensive foundation for developing a superconducting theory within a magnetic field. This theory aims to explain superconductivity below the critical temperature and the behavior of magnetic materials with Abrikosov vortex structures at zero temperature. The theory starts with the assumption that the order parameter is a complex field that depends on both spatial and temporal factors. By this approach, all degrees of freedom related to the magnetic force are integrated into the model, enabling the description of the coexistence of superconductor and magnetic flux states.\n\nGiven the complexity of the order parameter, multiple degrees of freedom are necessary to fully describe the superconducting state. To achieve this, a multibody increase in free energy is required. For cases where the coupling between the order parameter and magnetic force is weak, a local approximation is sufficient to capture the main mechanical system. This is achieved by treating superconducting and magnetic materials on an equal footing, introducing a new auxiliary field into the free energy functional.\n\nThe theory is applied to explain superconductivity in a magnetic field at the nanometer scale, where the London concept is no longer applicable and a fully quantum mechanical approach is necessary. Specifically, it is demonstrated that there exists a region in which a type II superconductor can sustain a magnetic force without forming a vortex lattice.",
        "ori-fast-z-score": 1.025755289064345,
        "water-fast-z-score": 6.127946159842712,
        "rewrite-fast-z-score": 2.5861309700971087
    },
    {
        "original_text": "The dwarf spheroidal galaxies (dSphs) are believed to be the most dark matter dominated objects in the universe. Their nature is still a matter of debate. Many of them are kinematically consistent with old population of stars but some of them exhibits evidence for recent star formation. The presence of significant populations of young stars in these galaxies has been debated. In this work, we present observations of 19 blue plumes in 16 dSphs. We find that 14 plumes are consistent with simple stellar population models with age less than 7-8 Gyr. The spectra of remaining five plumes can be fitted with combination of simple stellar population models with ages between 0.7-1.1 Gyr and 5-7 Gyr. We argue that the origin of the five blue plumes with intermediate age might be attributed to the blue straggler evolution and the star formation in these galaxies might not be recent as previously thought.",
        "watermark_text": "The dwarf spheroidal galaxies ( dSphs ) are considered to be the most dark matter dominated structures in the universe . Their existence is still a subject of dispute .Many of them are kinematically compatible with ancient population of stars but some of them exhibits evidence for recent star formation . The presence of notable populations of young galaxies in these galaxies has been discussed .In this research , we present observations of 19 blue plumes in 16 dSphs . We see that 14 plumes are compatible with simple stellar population models with age lower than 7 - 8 Gyr .The spectra of remaining five plumes can be fit with mixture of simple stellar community estimates with periods between 0 . 7 - 1 . 1 Gyr and 5 - 7 Gyr . We argue that the origin of the five blue plumes with intermediate age might be due to the blue straggler evolution and the star formation in these galaxies must not be early as previously thought .",
        "rewrite_text": "The dwarf spheroidal galaxies (dSphs) are regarded as the structures predominantly influenced by dark matter in the universe. Their existence remains a controversial topic. Many of these galaxies are dynamically consistent with an ancient population of stars, while others exhibit signs of recent star formation. Discussions have arisen regarding the presence of significant populations of young galaxies within these galaxies.\n\nIn this research, we present observations of 19 blue plumes found in 16 dSphs. Our findings indicate that 14 of these plumes align with simple stellar population models, with ages less than 7 to 8 billion years. The spectra of the remaining five plumes can be fitted with a mixture of estimates from simple stellar communities, spanning periods from 0.7 to 1.1 billion years and 5 to 7 billion years. We suggest that the origin of these five intermediate-age blue plumes may be attributed to the evolution of blue stragglers, and that the star formation in these galaxies may not have occurred as early as previously believed.",
        "ori-fast-z-score": -1.0,
        "water-fast-z-score": 4.341215710622296,
        "rewrite-fast-z-score": 0.819288030372914
    },
    {
        "original_text": "In this paper, we report on the performances of the MEGAPIE target, which is a central design for the GS2040E case (2 MW net power output). A substantial neutronics study was carried out for the first time for such a high temperature reactor configuration. Results were also compared with the simulations made for the actual design. Neutronic performances of the MEGAPIE target were found in good agreement with the actual design, with a DSA of 4.1% and a 3% level averagedteam pair space and momentum divergence. In addition, results showed that the hydrogen control rod have a strong impact on the reactivity and neutron economy of the core. The optimized configuration of the MEGAPIE target, called MEGAPIE+ is finally proposed. The MEGAPIE+ core configuration has been shown to perform close to the optimal one in terms of thermal efficiency, while improving the neutronics performances, thus leading to a significant reduction of the weight and the DSA.",
        "watermark_text": "In this paper , we paper on the performances of the MEGAPIE target , which is a central design for the GS2040E case ( 2 MW total power output ) . A extensive neutronics study was carried out for the first time for such a high heat reactor configuration .Results were also compared with the simulations made for the actual design . Neutronic performances of the MEGAPIE target were found in good agreement with the actual design , with a DSA of 4 . 1 % and a 3 % level averagedteam pair space and momentum divergence .In addition , results showed that the hydrogen control rod have a powerful impact on the reactivity and neutron economy of the core . The optimized structure of the MEGAPIE target , known MEGAPIE + is finally proposed .The MEGAPIE + core configuration has been shown to perform close to the ideal one in terms of thermal efficiency , while improving the neutronics abilities , thus resulting to a substantial decreased of the weight and the DSA .",
        "rewrite_text": "In this study, we present an analysis of the MEGAPIE target's performance, which is a key component in the GS2040E case (with a total power output of 2 MW). This is the first time an extensive neutronics study has been conducted for a high-heat reactor configuration of this kind. We compared our findings with simulations of the actual design.\n\nThe neutron performance of the MEGAPIE target was found to align well with the original design, with a DSA (Divergence of Space and Momentum) of 4.1% and an average team pair space divergence of 3%. Furthermore, our results indicated that the hydrogen control rod significantly affects the reactivity and neutron economy of the core.\n\nUltimately, we propose an optimized structure for the MEGAPIE target, known as MEGAPIE+. The MEGAPIE+ core configuration has demonstrated impressive thermal efficiency that closely mirrors the ideal performance while also enhancing neutronics capabilities. Consequently, it results in a substantial reduction in weight and DSA.",
        "ori-fast-z-score": 0.8551861104941365,
        "water-fast-z-score": 3.8805700005813275,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "The Bonn Lensing, Optical, and X-ray selected Galaxy Clusters (BLOX) is a sample of massive, X-ray emitting galaxy clusters constructed using data from X-ray, optical, and lensed Einstein ring surveys. BLOX clusters are detected in three steps. First, a HEC cluster finder identified groups and clusters in the Deep XMM-LSS (XMM-Newton Merged Serendipitous Survey) field. Next, a red-sequence cluster finder identified high-redshift galaxy clusters in the HEC clusters  redshift slices. Finally, a strong lensing cluster finder identified the most significant clusters from the red-sequence analysis. The final BLOX cluster catalog consists of 823 clusters, out of which 752 are at z>0.1 and 91 are at z>0.18, covering a sky area of 1.34 steradians and containing 18,314 cluster members. After applying various tests for sample contamination and cluster finding biases, BLOX stands as one of the largest and most reliable cluster samples constructed to date. The BLOX clusters  weak lensing mass and X-ray properties are presented in cosmological lensing papers associated with this release.",
        "watermark_text": "The Bonn Lensing , Optical , and X - ray selected Galaxy Clusters ( BLOX ) is a sample of large , X - ray emitting galaxy regions erected using data from X - ray , optical , and lensed Einstein ring studies . BLOX complexes are detected in three stages .First , a HEC cluster finder discovered groups and clusters in the Deep XMM - LSS ( XMM - Newton Merged Serendipitous Survey ) field . Next , a red - sequence cluster finder discovered high - redshift galaxy regions in the HEC galaxies redshift slices .Finally , a powerful lensing cluster finder found the most significant complexes from the red - sequence study . The final BLOX cluster catalog consists of 823 clusters , out of which 752 are at z > 0 . 1 and 91 are at z > 0 . 18 , covering a sky area of 1 . 34 steradians and featuring 18 , 314 cluster elements .After applying numerous tests for sample contamination and cluster finding biases , BLOX ranks as one of the largest and most accurate cluster specimens constructed to date . The BLOX clusters weak lensing mass and X - ray characteristics are presented in cosmological lensing articles associated with this release .",
        "rewrite_text": "The BLOX (Bonn Lensing, Optical, and X-ray selected Galaxy Clusters) is a collection of extensive X-ray emitting galaxy regions. This sample is constructed using data from X-ray, optical, and lensed Einstein ring studies. The BLOX complexes are identified in three stages. Initially, a HEC cluster finder identifies groups and clusters within the Deep XMM-LSS (XMM-Newton Merged Serendipitous Survey) field. Following this, a red-sequence cluster finder discovers high-redshift galaxy regions within the redshift slices of the HEC galaxies. Ultimately, a sophisticated lensing cluster finder pinpoints the most significant complexes from the red-sequence study. The final BLOX cluster catalog comprises 823 clusters, 752 of them at a redshift greater than 0.1, and 91 at a redshift exceeding 0.18. Spanning a sky area of 1.34 steradians, it features 18,314 cluster elements. After numerous tests for sample contamination and cluster finding biases, BLOX stands as one of the largest and most precise cluster specimens constructed so far. The weak lensing mass and X-ray characteristics of BLOX clusters are presented in associated cosmological lensing articles.",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 6.4372630957871815,
        "rewrite-fast-z-score": 3.1601109742955256
    },
    {
        "original_text": "In the study of gamma-ray bursts (GRBs), it is well known that the log-log correlation between gamma-ray fluence and GRB duration has a break at the shortest duration seen from Swift, about 2 s. The correlation between the fluence in two different energy bands and the break time is also non-linear. This has been explained by assuming that the observed energy bands sample different regions of the GRB relativistic jet, with higher energy bands sampling regions of faster-moving baryons. A physical origin for this break time has been missing, until now. It is demonstrated that the break time in the observed correlation is the result of a competition between the variability time scales of the engine that powers the burst and the resolution time scale of the detectors. The origin of this variability time scale is not a priori known, but is likely associated with internal gravity waves in the plasma outflow produced during the jet acceleration. A physical origin for the break time is demonstrated to be impossible. Thus, the previously-observed high-energy correlation between Swift GRBs no longer holds. A complete catalog of Swift GRB spectral parameters is presented. The catalog can be accessed online at https://observatory.gsfc.nasa.gov/ estimations/UXT11Y",
        "watermark_text": "In the study of gamma - ray flare ( GRBs ) , it is well established that the log - log correlation between gamma - ray fluence and GRB duration has a break at the longest duration viewed from Swift , about 2 s . The relationship between the fluence in two different energy bands and the broke time is also non - linear . This has been explained by assuming that the observed energy bands sample different regions of the GRB relativistic jet , with higher energy bands scanning regions of slower - changing baryons .A physical origin for this break time has been missing , until now . It is demonstrated that the broke time in the seen correlation is the result of a competition between the variability time ranges of the engine that powers the burst and the resolution time scale of the detectors .The origin of this variability time scale is not a priori known , but is probably associated with internal gravity currents in the plasma outflow generated during the jet acceleration . A physical origin for the broke time is demonstrated to be impossible .Thus , the previously - observed high - energy correlation between Swift GRBs no longer holds . A full catalog of Swift GRB spectral parameters is displayed .The collection can be downloaded online at https : / / observatory . gsfc . nasa . gov / estimations / UXT11Y",
        "rewrite_text": "In the investigation of gamma-ray flares (GRBs), it has been firmly established that there is a log-log correlation between gamma-ray fluence and GRB duration, which exhibits a break at approximately 2 seconds, the longest duration observed by the Swift satellite. The relationship between fluence in different energy bands and this break time is also nonlinear. This phenomenon has been explained by the assumption that the observed energy bands sample distinct regions of the GRB's relativistic jet, with higher energy bands scanning areas of slower-changing baryons.\n\nUntil now, a physical explanation for this break time has been elusive. However, it has been demonstrated that the observed break time in the correlation is a result of a competition between the variability time ranges of the engine powering the burst and the resolution time scale of the detectors. The origin of this variability time scale, which is not known in advance, is likely associated with internal gravity currents in the plasma outflow generated during jet acceleration. Proving a physical origin for the break time has been proven impossible. Consequently, the previously observed high-energy correlation among Swift GRBs no longer applies. A comprehensive catalog of Swift GRB spectral parameters is presented, and can be downloaded online at https://observatory.gsfc.nasa.gov/estimations/UXT11Y.",
        "ori-fast-z-score": 0.30460384954008574,
        "water-fast-z-score": 5.050762722761053,
        "rewrite-fast-z-score": 2.626396615835748
    },
    {
        "original_text": "The dark energy, which is interpreted as a component of the universe with negative pressure, is one of the biggest mysteries in modern physics. One of the most promising ways of measuring its properties is based on the distance based cosmology called ‘Cosmic Distance duality relations’ (DDGR), which connect the angular diameter distance to the last scattering surface, dA, with the luminosity distance, dL, to the observed galaxy cluster. So far, these relations have been tested only on small cluster samples (e.g. Sunyaev-Zeldovich (SZ) effect, gravitational lensing) and with low accuracy (mainly due to the poor calibration of the local distance ladder). I present the results of X-ray observations of the largest, most relaxed galaxy clusters with the Chandra X-ray observatory. These clusters follow the cosmic distance duality relation to an accuracy of ~8%, which is an improvement of a factor of 5 compared to earlier results. The derived precise X-ray masses, which are used to test the cosmological models, are also presented.",
        "watermark_text": "The black force , which is interpreted as a component of the universe with negative pressure , is one of the biggest mysteries in modern physics . One of the most attractive means of assessing its properties is based on the distance based cosmology named ‘ Cosmic Distance duality relations ’ ( DDGR ) , which link the angular length length to the last scattering surface , dA , with the luminosity distance , dL , to the seen galaxy cluster .So far , these relations have been tested only on small cluster specimens ( e . g . Sunyaev - Zeldovich ( SZ ) effect , gravity lensing ) and with poor precision ( mainly owing to the poor calibration of the local distance ladder ) .I present the results of X - ray observations of the largest , most relaxed galaxy galaxies with the Chandra X - ray observatory . These clusters follow the cosmic distance duality relation to an precision of ~ 8 % , which is an improvement of a factor of 5 compared to earlier findings .The derived exact X - ray masses , which are applied to test the cosmological models , are also presented .",
        "rewrite_text": "The black force, interpreted as a component of the universe with negative pressure, remains one of the foremost enigmas in modern physics. Among the most promising methods for exploring its properties is based on a distance-oriented cosmology known as 'Cosmic Distance Duality Relations' (DDGR). These relations connect the angular size with the final scattering surface, dA, to the luminosity distance, dL, of observed galaxy clusters. However, these relations have only been tested on small cluster samples (such as the Sunyaev-Zeldovich (SZ) effect and gravitational lensing) with limited precision, primarily due to the inadequate calibration of the local distance ladder.\n\nI present the results of X-ray observations conducted on the largest and most relaxed galaxies using the Chandra X-ray observatory. These clusters adhere to the cosmic distance duality relation with a precision of approximately 8%, which is a fivefold improvement compared to earlier findings. Additionally, the derived precise X-ray masses, which are utilized to test cosmological models, are also presented.",
        "ori-fast-z-score": 1.5652475842498528,
        "water-fast-z-score": 5.590169943749474,
        "rewrite-fast-z-score": 0.917662935482247
    },
    {
        "original_text": "The problem of counting colourings of the vertices of a graph, or equivalently, the problem of computing a count of the number of ways that a multinomial coefficient can be partitioned, has long been a problem of interest to combinatorists and probabilists. A significant advance was the introduction of the dimer / coloring / bijection / Rosetta Stone (as first described by Kasteleyn and McCoy, 1966) by statistical physicists Ken Ono, Chetan Nachbagauer and Chris Janjic, which allowed for the development of a bijective framework for more general partitioning problems (Araujo, 1980, 1981; Temperley, 1971). However, even this framework was unable to count certain multinomial coefficient partitions (e.g. partitions into squares) without extensive case-by-case analysis. In recent years, significant advances in computer algorithms have allowed for the study of these hard partitions via the Counting Principle (Dousse, Di Matteo, Rafieier and Cerf, 2013) and these algorithms have resulted in the enumeration of some hard partitions (e.g. all 2-partitions of a partition into squares) without case-by-case analysis. In this paper, we carry out a systematic enumeration of 2-colourings of all facets of the cubic lattice (a total of 3779 facets). We find 20 distinct2-colourings, each arising via a different bijective framework.",
        "watermark_text": "The question of counting colourings of the edges of a graph , or equivalently , the question of computing a count of the number of ways that a multinomial coefficient can be partitioned , has always been a problem of interest to combinatorists and probabilists . A significant progress was the introduction of the dimer / coloring / bijection / Rosetta Stone ( as initially known by Kasteleyn and McCoy , 1966 ) by statistical physicists Ken Ono , Chetan Nachbagauer and Chris Janjic , which allowed for the development of a bijective framework for more formal partitioning questions ( Araujo , 1980 , 1981 ; Temperley , 1971 ) .However , even this framework was impossible to count certain multinomial coefficient partitions ( e . g . partitions into squares ) without thorough case - by - case evaluation .In recent years , substantial advances in computer algorithms have permitted for the study of these hard partitions via the Counting Principle ( Dousse , Di Matteo , Rafieier and Cerf , 2013 ) and these algorithms have led in the enumeration of some hard partitions ( e . g . all 2 - partitions of a partition into squares ) without case - by - case evaluation .In this paper , we perform out a comprehensive enumeration of 2 - colourings of all facets of the cubic lattice ( a total of 3779 facets ) . We get 20 distinct2 - colourings , each resulting via a different bijective structure .",
        "rewrite_text": "The problem of counting the colorings of graph edges, or alternatively, the task of computing the number of ways a multinomial coefficient can be partitioned, has always been a subject of interest for both combinatorists and probabilists. A notable breakthrough was the introduction of the dimer/coloring/bijection/Rosetta Stone (originally known as such by Kasteleyn and McCoy in 1966) by statistical physicists Ken Ono, Chetan Nachbagauer, and Chris Janjic. This innovation facilitated the development of a bijective framework for more formal partitioning questions (Araujo, 1980, 1981; Temperley, 1971). Nevertheless, even this framework found it challenging to count certain multinomial coefficient partitions (e.g., partitions into squares) without a thorough case-by-case evaluation.\n\nIn recent years, significant advancements in computer algorithms have enabled the study of these complex partitions through the Counting Principle (Dousse, Di Matteo, Rafieier, and Cerf, 2013). These algorithms have led to the enumeration of some challenging partitions without the need for case-by-case evaluation. In this paper, we present a comprehensive enumeration of 2-colorings for all facets of the cubic lattice, which total 3779 facets. We have identified 20 distinct 2-colorings, each resulting from a unique bijective structure.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 4.185110693297313,
        "rewrite-fast-z-score": 2.215646837627989
    },
    {
        "original_text": "A population of Lyman alpha emitters (LAEs) at redshift ~ 4.5 was found to form over-densities by narrow-band Lyman alpha imaging, a sign of ongoing galaxy formation at early times. Subsequent spectroscopy of these over-densities revealed lower redshifts (~ 4.5), though, for the most part, the LAEs were observed to remain at z ~ 4.5. This apparent clustering was confirmed with numerical simulations and interpreted as evidence for the existence of substantial neutral hydrogen reserves in these early systems. Itai M. R惠and Andrea G。Crocce led a team that used imaging with the UltraVio survey to find this clustering and confirmed it with subsequent spectroscopic observations. The study is published in the journal Science. UltraVio (University of Vienna Observatory), an imaging survey using the robotic 2.2m telescope of the University of Vienna, is described at http://www.ifs.org.ua/usno/auto/auto.htm. The survey imaged eight square degrees of the sky in five passbands between 4000 and 9000 Å. The observations, from 2011 to 2016, were made with a wide-field camera designed by the University of Vienna and equipped with a InSb detector with a pixel size of 18 μm, covering a field of view of 1.76° × 1.76°. The survey is 95% complete to DM = 24.7. The data are available from the University of Vienna Science Archive (URL: http://tagc.ifac.tuwien.ac.at/ultravio/).",
        "watermark_text": "A community of Lyman alpha emitters ( LAEs ) at redshift ~ 4 . 5 was shown to form over - densities by narrow - band Lyman alpha imaging , a sign of ongoing galaxy formation at early years . Subsequent spectroscopy of these over - densities showed smaller redshifts ( ~ 4 . 5 ) , though , for the most part , the LAEs were found to remain at z ~ 4 . 5 .This evident clustering was confirmed with numerical simulations and interpreted as proof for the existence of large neutral hydrogen deposits in these first systems . Itai M . R [UNK] and Andrea G 。 Crocce led a team that used optical with the UltraVio study to find this clustering and confirmed it with subsequent spectroscopic observations .The survey is published in the journal Science . UltraVio ( University of Vienna Observatory ) , an imaging survey using the robotic 2 . 2m telescope of the University of Vienna , is documented at http : / / www . ifs . org . ua / usno / auto / auto . htm .The survey imaged eight square degrees of the heavens in five passbands between 4000 and 9000 Å . The surveys , from 2011 to 2016 , were made with a broad - field camera built by the University of Vienna and equipped with a InSb sensor with a pixel size of 18 μm , covering a field of view of 1 . 76° × 1 . 76° .The survey is 95 % complete to DM = 24 . 7 . The data are available from the University of Vienna Science Archive ( URL : www : / / tagc . ifac . tuwien . ac . at / ultravio / ) .",
        "rewrite_text": "A community of Lyman alpha emitters (LAEs) at a redshift of approximately 4.5 has been demonstrated to form over-densities through narrow-band Lyman alpha imaging, providing a sign of ongoing galaxy formation in the early universe. Subsequent spectroscopy of these over-densities revealed slightly lower redshifts (around 4.5), but for the most part, the LAEs were found to maintain a redshift of approximately 4.5. This evident clustering was further confirmed through numerical simulations and interpreted as evidence for the existence of large deposits of neutral hydrogen in these early systems.\n\nA team led by Itai M. R. and Andrea G. Crocce utilized optical instrumentation from the UltraVio study to detect this clustering, subsequently verifying it with subsequent spectroscopic observations. The survey, which is published in the journal Science, was conducted using the robotic 2.2m telescope of the University of Vienna Observatory (UltraVio). Documentation of the survey, which imaged eight square degrees of the sky in five passbands ranging from 4000 to 9000 Å, is available at http://www.ifs.org.ua/usno/auto/auto.htm.\n\nThe surveys, spanning from 2011 to 2016, were performed with a broad-field camera developed by the University of Vienna and equipped with an InSb sensor having a pixel size of 18 μm, providing a field of view of 1.76° x 1.76°. The survey is 95% complete up to a limiting magnitude of DM = 24.7. The data from this survey can be accessed through the University of Vienna Science Archive (URL: http://tagc.ifac.tuwien.ac.at/ultravio/).",
        "ori-fast-z-score": 1.0125791108334214,
        "water-fast-z-score": 6.039910552360811,
        "rewrite-fast-z-score": 1.5096588248481377
    },
    {
        "original_text": "Population III stars were the very first stars to form in the universe. Due to their high inward flux of Lyman-Werner radiation, the H<--->H2 reaction became rapidly inhibited, and subsequent H2 formation was suppressed. As a result, the lack of hydrogen caused these stars to become predominantly core-dominant, leading to the stellar dynamical collapse of the cores of these very first stars. Magnetorotational collapse, as the name suggests, involves a combination of magnetic and rotational forces that aid the gravitational force in collapsing a population III star. This process has never before been observed in such old, very first stars, and its implications for the early life of the universe are discussed. The very first stars to form in the universe, Population III, had no hydrogen due to the H<--->H2 reaction becoming inhibited by the suppression of subsequent H2 formation. These first stars were extremely massive, with typical masses of a few hundred solar masses. Population III stars, like our Sun, were initially dominated by their cores. With further contraction, radialGreg Anglin instability caused the formation of an iron core. This led to a dynamical collapse of the core, which is ultimately triggered when iron nuclei reach absolute zero pressure. At this point, electronuclear reactions supported by neutronization become so rapid that the infall velocity of the rest of the star becomes super-elastic. This results in a blast wave of nuclearfire, which completely encompasses the core. The stellar dynamics of these events have never before been observed in such very first stars, and their implications for the early life of the universe are discussed. The author(s) obtained funds from the European Research Council (ERC) under the European Union s Seventh Framework Programme (FP/2007-2013)/ERC Grant Agreement no. 319581( PLConfig ) and from the French National Research Agency (ANR) through the  Investissements d Avenir  programme (ANR-15-IDEX-02). Y. Jin is currently a Ph.D. student with Prof. Geoffrey E. Strand and Prof. Junhua Wang s group at the University of New Mexico.",
        "watermark_text": "Population III stars were the very first stars to form in the universe . Due to their high inward flux of Lyman - Werner light , the H < - - - > H2 reaction grew rapidly inhibited , and subsequent H2 development was suppressed .As a result , the lack of hydrogen caused these stars to become predominantly core - dominant , leading to the stellar dynamical sinking of the cores of these very first stars . Magnetorotational breakdown , as the naming suggests , involves a combination of magnetic and rotational pressures that aid the gravitational pressure in collapsing a population III star .This process has never before been observed in such old , very first stars , and its consequences for the early life of the universe are discussed . The very first stars to form in the universe , Population III , had no fuel owing to the H < - - - > H2 reaction getting inhibited by the destruction of later H2 form .These initial stars were extremely massive , with typical masses of a few hundred solar masses . Population III stars , like our Sun , were initially dominated by their cores .With further contraction , radialGreg Anglin instability caused the formation of an metal core . This turned to a dynamical collapse of the core , which is ultimately initiated when iron atoms reach absolute zero pressure .At this time , electronuclear effects supported by neutronization become so quick that the infall momentum of the rest of the star becomes super - elastic . This results in a explosion wave of nuclearfire , which completely covers the core .The stellar processes of these events have never before been observed in such very first stars , and their implications for the early life of the universe are discussed . The author ( s ) obtained funds from the European Research Council ( ERC ) under the European Union s Seventh Framework Programme ( FP / 2007 - 2013 ) / ERC Grant Agreement no .319581 ( PLConfig ) and from the French National Research Agency ( ANR ) through the Investissements d Avenir programme ( ANR - 15 - IDEX - 02 ) . Y . Jin is currently a Ph . D . pupil with Prof . Geoffrey E . Strand and Prof . Junhua Wang s group at the University of New Mexico .",
        "rewrite_text": "The initial stars in the universe, known as Population III, were the earliest star formations. Their high inward flux of Lyman-Werner light rapidly inhibited the H < - - - > H2 reaction, subsequently suppressing the development of H2. This resulted in a scarcity of hydrogen, leading these stars to predominantly feature core-dominant properties. Consequently, the cores of these pioneer stars sank dynamically. Magnetorotational breakdown, as its name suggests, involves a combination of magnetic and rotational pressures that aid in the gravitational collapse of a Population III star. This process has never been observed in such ancient, first-generation stars, and its impact on the early life of the universe is still under discussion.\n\nThese primary stars were exceptionally massive, with typical masses reaching several hundred solar masses. Like our Sun, Population III stars were initially governed by their cores. As they further contracted, the radial Greg Anglin instability gave rise to the formation of a metal core. This transitioned into a dynamic collapse of the core, ultimately initiated when iron atoms reached absolute zero pressure. At this point, neutronization-supported electronuclear effects became so rapid that the infalling momentum of the rest of the star became super-elastic. This led to a nuclearfire explosion wave that completely enveloped the core.\n\nThe authors received funding from the European Research Council (ERC) under the European Union's Seventh Framework Programme (FP/2007-2013)/ERC Grant Agreement no. 319581 (PLConfig) and from the French National Research Agency (ANR) through the Investissements d'Avenir programme (ANR-15-IDEX-02). Y. Jin is currently a Ph.D. student under the guidance of Prof. Geoffrey E. Strand and Prof. Junhua Wang at the University of New Mexico.",
        "ori-fast-z-score": 1.9445436482630056,
        "water-fast-z-score": 6.6551738206208535,
        "rewrite-fast-z-score": 2.011435198964418
    },
    {
        "original_text": "A detailed study of the suppression of cosmic ray flux above a sharp upper energy limit, known as the GZK cutoff, requires precise measurements of the altitude of the shower maximum, which can be achieved with surface detector arrays such as the Pierre Auger Observatory. In this work we present a new analysis technique based on the distribution of distances between consecutive surface detector elements, that allows the measurement of parameters that are more sensitive to the nature and the intensity of the primary particle than the traditional lateral distribution function. We apply this method to characterize the suppression of cosmic ray flux above the so-called GZK energy and compare the results with previous measurements using traditional parameterizations. We also study the sensitivity to changes in the content of primary cosmic rays and in the chemical composition by using a new parameter, the spectral rigidity, which is a measure of the energy separation between adjacent energy levels of a given atom or nucleus. We find that the behavior of the measured composition-sensitive parameters above the GZK cutoff is in agreement with the interpretation of this feature as a result of the interaction of high energy cosmic rays with the photon background, supporting the idea that the latter is composed mostly of atomic < C < /sub > 4 </sub> and < Si > nuclei.",
        "watermark_text": "A precise study of the suppression of cosmic ray flux above a sharp upper energy threshold , known as the GZK cutoff , requires careful observations of the altitude of the shower maximum , which can be obtained with surface detector arrays such as the Pierre Auger Observatory . In this project we present a new analysis technique focused on the distribution of distances between successive surface detector elements , that enables the observation of values that are more sensitive to the nature and the intensity of the primary object than the usual lateral distribution function .We use this method to characterize the suppression of cosmic ray flux above the so - called GZK energy and compare the results with previous measurements using traditional parameterizations . We also study the sensitivity to changes in the content of primary cosmic rays and in the chemical composition by using a new parameter , the spectral rigidity , which is a measure of the power separation between neighboring energy levels of a given molecule or nucleus .We see that the dynamics of the studied composition - sensitive parameters above the GZK cutoff is in agreement with the interpretation of this characteristic as a due of the interaction of high energy cosmic rays with the photon background , supporting the idea that the latter is composed primarily of atomic < C < / sub > 4 < / sub > and < Si > nuclei .",
        "rewrite_text": "A precise examination of the suppression of cosmic ray flux above the defined GZK cutoff, which marks a sharp upper energy threshold, necessitates careful monitoring of the peak altitude in cosmic ray showers. This can be achieved through the utilization of surface detector arrays, such as the Pierre Auger Observatory. In this project, we introduce an advanced analysis technique centered on the distribution of distances between consecutive surface detector elements. This technique enables the observation of values that are more responsive to the nature and intensity of the primary cosmic ray object than the traditional lateral distribution function.\n\nWe employ this method to characterize the suppression of cosmic ray flux above the GZK energy threshold and compare our findings with previous measurements utilizing traditional parameterization techniques. Furthermore, we investigate the sensitivity to alterations in the content of primary cosmic rays and their chemical composition by introducing a novel parameter: spectral rigidity. This measure quantifies the power difference between neighboring energy levels of a given molecule or nucleus.\n\nOur observations indicate that the dynamics of composition-sensitive parameters above the GZK cutoff align with the interpretation that this characteristic arises from the interaction of high-energy cosmic rays with the photon background. This supports the notion that the latter is primarily composed of <C sub>4</C> and <Si> nuclei.",
        "ori-fast-z-score": -1.165543034828717,
        "water-fast-z-score": 3.8851434494290564,
        "rewrite-fast-z-score": 0.4833682445228318
    },
    {
        "original_text": "We present a detailed analysis of the gamma-ray spectrum of the TeV source RX J1713.7-3946, based on 10 years of observations with the H.E.S.S. telescopes. We confirm the existence of two spectral components, previously reported by other experiments, with a break at approximately 600 GeV. The differential energy spectra of the two components and their integral fluxes are derived and the implications for the origin and physical properties of the gamma-ray emitting particles are discussed. The SNR RX J1713.7-3946 is one of the best candidates to study particles of extra-terrestrial origin (cosmic rays) interaction over a wide energy range from the radio to the TeV gamma-ray band. We performed a detailed study of the gamma-ray emission using data collected by the H.E.S.S. I and II telescopes. We confirm the existence of two spectral components, one with a spectral break at 600 GeV, the other one extending to at least 10 TeV. The differential energy spectra of the two components and their integral fluxes are derived and the implications for the origin and physical properties of the gamma-ray emitting particles are discussed.",
        "watermark_text": "We present a detailed analysis of the gamma - ray spectrum of the TeV source RX J1713 . 7 - 3946 , based on 10 years of measurements with the H . E . S . S . telescopes .We establish the existence of two spectral components , previously reported by other experiments , with a break at approximately 600 GeV . The differential energy spectra of the two parts and their integral fluxes are derived and the implications for the origin and physical properties of the beta - ray emitting objects are discussed .The SNR RX J1713 . 7 - 3946 is one of the best candidates to study particles of extra - terrestrial origin ( cosmic rays ) collision over a broad energy range from the radio to the TeV gamma - ray band . We conducted a detailed analysis of the gamma - ray radiation using data received by the H . E . S . S .I and II telescopes . We establish the existence of two spectral components , one with a spectral break at 600 GeV , the other one extending to at least 10 TeV .The differential energy spectra of the two parts and their integral fluxes are derived and the implications for the origin and physical properties of the gamma - ray emitting particles are discussed .",
        "rewrite_text": "We present a comprehensive analysis of the gamma-ray spectrum for the TeV source RX J1713.7-3946, utilizing a decade of measurements conducted by the H.E.S.S. telescopes. Our findings confirm the existence of two spectral components, which have been previously reported by other experiments, with a distinct break occurring at approximately 600 GeV. We have derived the differential energy spectra for both components and their corresponding integral fluxes. Additionally, we have discussed the potential implications for the origin and physical properties of the beta-ray emitting objects.\n\nThe supernova remnant, RX J1713.7-3946, stands as one of the most promising candidates for studying particles of extraterrestrial origin (cosmic rays) across a broad energy range, spanning from radio waves to TeV gamma-ray band. We conducted an in-depth analysis of gamma-ray radiation using data gathered by both H.E.S.S. I and II telescopes. Our results establish the presence of two distinct spectral components; one with a spectral break at 600 GeV, and the other extending to at least 10 TeV. We have further discussed the consequences of these findings for the origin and characteristics of the gamma-ray emitting particles.",
        "ori-fast-z-score": 1.1322770341445956,
        "water-fast-z-score": 4.302652729749464,
        "rewrite-fast-z-score": 1.7669044171975445
    },
    {
        "original_text": "Magnetism in the spiral galaxy NGC 6946 is investigated using high-resolution synchrotron radio continuum observations from the 64m telescope of the Max-Planck-Institute for Solar System Research (MPS). After a short general introduction, we describe the observations and data reduction, present magnetic field strength and configuration measurements, and discuss the regularity and origin of the observed magnetic fields. In particular, the newly introduced depolarization ring technique is employed to detect the weak, highly disordered, large-scale magnetic fields, and the detected ring-like magnetic structures are analyzed with respect to their azimuthal symmetries and embedded energy transport processes. Additionally, we introduce methods to characterize the regular small-scale magnetic fields, i.e. the spiral structure, dynamo modes and helical fields, by deducing vector fields of regularized and unsharp masked total magnetic field maps. We conclude by discussing consequences of the observed regular and random magnetic field configurations for the fueling and launching of the active galactic nucleus in the central region of the galaxy, and summarize the current research on NGC 6946 as a model galaxy for testing various aspects of interstellar and intergalactic magnetic field research using modern instrumentation.",
        "watermark_text": "Magnetism in the spiral galaxy NGC 6946 is investigated using high - resolution synchrotron radio continuum measurements from the 64m telescope of the Max - Planck - Institute for Solar System Research ( MPS ) . After a brief general introduction , we explain the experiments and information reduction , provide magnetic force weakness and configuration measurements , and consider the regularity and origin of the seen magnetic fields .In particular , the newly expanded depolarization ring technique is utilized to identify the stable , highly disordered , large - scale magnetic fields , and the detected circle - like magnetic structures are examined with regard to their azimuthal symmetries and embedded power transport cycles . Furthermore , we provide algorithms to characterize the ordinary small - scale magnetic fields , i . e .the spiral system , dynamo modes and helical fields , by deducing vector fields of regularized and unsharp masked total magnetic field maps . We conclude by exploring consequences of the reported regular and random magnetic field configurations for the fueling and launching of the active galactic nucleus in the central region of the galaxy , and summarize the present research on NGC 6946 as a prototype galaxy for studying several parts of interstellar and intergalactic magnetic field research utilizing modern instrumentation .",
        "rewrite_text": "Utilizing high-resolution synchrotron radio continuum measurements sourced from the 64m telescope of the Max-Planck-Institute for Solar System Research (MPS), the investigation of magnetism within the spiral galaxy NGC 6946 is conducted. After a general introduction, the study explains the experimental procedures and data reduction techniques. It presents measurements of magnetic force weakness and configuration, exploring the regularities and origins of the detected magnetic fields. Specifically, the newly expanded depolarization ring technique is utilized to identify stable, highly disordered, and large-scale magnetic fields. The observed circular magnetic structures are analyzed in terms of their azimuthal symmetries and embedded power transport cycles. Additionally, algorithms are provided to characterize ordinary small-scale magnetic fields, such as the spiral system, dynamo modes, and helical fields, through the deducing of vector fields from regularized and unsharp-masked total magnetic field maps.\n\nFinally, the research concludes by exploring the implications of the reported regular and random magnetic field configurations for fueling and launching the active galactic nucleus in the central region of the galaxy. It summarizes the current research on NGC 6946 as a prototype galaxy for studying various aspects of interstellar and intergalactic magnetic field research utilizing modern instrumentation.",
        "ori-fast-z-score": -2.8284271247461903,
        "water-fast-z-score": 3.9598500440211146,
        "rewrite-fast-z-score": 1.2935483472729858
    },
    {
        "original_text": "A semi-detached configuration is an hierarchical multiple system in which a star is orbiting a more massive companion (the white dwarf or subdwarf core) in a close, circularized orbit. These systems are of particular interest in astrophysics, being among the lowest-mass binary stars. SS Leporis is a semi-detached binary system whose components are respectively a late-type (mainly K) star and a white dwarf. Such a system is spatially resolved for the first time with the VLTI instrument VINCI at sub-mas resolution, and its integrated diameter has been measured for the first time. The resulting value of the binary system is dSi=3.35±0.14 mas, corresponding to a projected physical separation of 2.65±0.13 × 10-4 solar radii. The measured value is in very good agreement with theoretical predictions (2.94 solar radii).",
        "watermark_text": "A semi - detached configuration is an hierarchical multiple system in which a star is orbiting a more massive companion ( the dark dwarf or subdwarf core ) in a close , circularized orbit . These systems are of especially interest in astrophysics , being among the smallest - mass binary stars .SS Leporis is a semi - detached binary system whose components are respectively a late - class ( mainly K ) star and a white dwarf . Such a system is spatially resolved for the first time with the VLTI instrument VINCI at sub - mas resolution , and its integrated diameter has been measured for the first time .The resulting value of the binary system is dSi = 3 . 35±0 . 14 mas , equivalent to a projected physical separation of 2 . 65±0 . 13 × 10 - 4 solar radii . The measured value is in very best agreement with theoretical estimates ( 2 . 94 solar radii ) .",
        "rewrite_text": "A semi-detached configuration represents a hierarchical multiple system where a star orbits around a more massive companion, either a dark dwarf or subdwarf core, in a close, circularized orbit. These systems are of particular interest in astrophysics as they are among the smallest binary stars by mass. SS Leporis is an example of a semi-detached binary system, consisting of a late-class (primarily K-type) star and a white dwarf. For the first time, this system has been spatially resolved using the VLTI's VINCI instrument at sub-mas resolution, and its integrated diameter has been measured. The resulting value for the binary system's diameter is dSi = 3.35±0.14 mas, which translates to a projected physical separation of 2.65±0.13 x 10^-4 solar radii. The measured value aligns very well with theoretical estimates (2.94 solar radii).",
        "ori-fast-z-score": 0.6401843996644799,
        "water-fast-z-score": 2.581988897471611,
        "rewrite-fast-z-score": 0.8819171036881969
    },
    {
        "original_text": "Power-law distributions have been observed in a variety of empirical datasets, arising in a variety of contexts, such as the sizes of human settlements1, the masses of objects2, the income of individuals3, the stride lengths of individuals4, the cell diameters of plants5 or the bark widths of trees6. The broad applicability of power-law distributions has led to the suggestion that such distributions may arise naturally as the outcome of a number of simple generative processes, such as the product of independent random numbers7, the catastrophic failure of a single component in a random failure system8 or phase transitions in discrete models9. However, it has also been suggested that power-law distributions may result from the superposition of a large number of more common distributions10, the simple permuation of a single common distribution11 or a logarithmic distribution12. Indeed, there are a variety of processes capable of generating such distributions. For example, the superposition of a large number of common distributions may generate a distribution with a power-law tail13. In addition, heavy-tailed distributions with similar mathematical forms to power laws have been observed in a variety of contexts, arising from different generative processes. For example, the late stages of a logarithmic distribution may be similar in appearance to a power law14, and it has been suggested that the size distribution of computer files may be consistent with a power law, even though this is often attributed to finite size effects15. Similarly, one may speculate that certain types of networks may exhibit a power-law degree distribution, though this has not been demonstrated rigorously. In addition, there are processes which generate power-law distributions which are not generative processes. For example, the removal of nodes from a network with a power-law degree distribution may result in the removal of nodes with degree distributions similar to a power law16. It has also been suggested that heavy-tailed distributions may arise as a result of the sum of a small number of more common distributions17. For example, a truncated power law may arise as the result of a superposition of common distributions18. In this case, it is interesting to note that there are many examples of power-law distributions which arise as a result of a small number of generative processes. For example, the distribution of human settlement sizes appears to arise as the result of two generative processes - the growth of human populations through the coalescence of settlements and the natural variation in the size of settlements as dictated by geographical and environmental constraints19.",
        "watermark_text": "Power - law functions have been observed in a variety of statistical datasets , arising in a variety of contexts , such as the sizes of human settlements1 , the masses of objects2 , the income of individuals3 , the stride lengths of individuals4 , the cell diameters of plants5 or the bark widths of trees6 . The broad applicability of power - law distributions has led to the suggestion that such distributions may arise naturally as the result of a number of simple generative processes , such as the product of independent random numbers7 , the devastating failure of a single part in a random failure system8 or phase transitions in discrete models9 .However , it has additionally been proposed that power - law functions may come from the superposition of a large number of more common distributions10 , the simple permuation of a single common distribution11 or a logarithmic distribution12 . Indeed , there are a variety of processes capable of generating such distributions .For instance , the superposition of a large number of common distributions may generate a distribution with a power - law tail13 . In addition , heavy - tailed distributions with similar mathematical forms to power laws have been observed in a variety of contexts , arising from varying generative processes .For instance , the late stages of a logarithmic distribution might be analogous in appearance to a power law14 , and it has been proposed that the length distribution of machine files may be compatible with a power law , even though this is often attributed to finite diameter effects15 . Similarly , one may speculate that particular kinds of networks could display a power - law degree distribution , though this has not been shown rigorously .In addition , there are processes which generate power - law distributions which are not generative processes . For instance , the removal of vertices from a network with a power - law degree distribution might lead in the removal of nodes with degree distributions similar to a power law16 .It has additionally been proposed that heavy - tailed distributions may arise as a outcome of the sum of a small number of more common distributions17 . For instance , a truncated power law may arise as the result of a superposition of common distributions18 .In this instance , it is curious to note that there are many instance of power - law distributions which occur as a outcome of a small number of generative processes . For instance , the distribution of human village numbers seems to arise as the result of two generative processes - the development of human populations through the coalescence of settlements and the natural variation in the height of settlements as dictated by spatial and environmental constraints19 .",
        "rewrite_text": "Power-law functions have been observed in a wide range of statistical datasets, occurring in various contexts. For instance, they are found in the sizes of human settlements, the masses of objects, the incomes of individuals, the stride lengths of individuals, the cell diameters of plants, and the bark widths of trees. The widespread applicability of power-law distributions suggests that they may naturally arise from a number of simple generative processes. These include the product of independent random numbers, catastrophic failures in random failure systems, and phase transitions in discrete models.\n\nHowever, it has also been proposed that power-law functions could stem from the superposition of numerous more common distributions, simple permutations of a single common distribution, or even a logarithmic distribution. There are indeed various processes capable of generating such distributions. For instance, the superposition of multiple common distributions can create a distribution with a power-law tail.\n\nAdditionally, heavy-tailed distributions with mathematical similarities to power laws have been observed in various contexts, arising from different generative processes. For example, the late stages of a logarithmic distribution may resemble a power law in appearance. It has been suggested that the length distribution of machine files may align with a power law, despite often being attributed to finite diameter effects. Similarly, specific types of networks may exhibit power-law degree distributions, although this has not been rigorously proven.\n\nFurthermore, there are processes that generate power-law distributions that are not necessarily generative in nature. For instance, removing vertices from a network with a power-law degree distribution may result in the removal of nodes with degree distributions resembling a power law. It has also been proposed that heavy-tailed distributions can emerge as a result of the summation of a limited number of more common distributions. A truncated power law, for instance, can arise from the superposition of typical distributions.\n\nInterestingly, there are numerous examples of power-law distributions arising from a small number of generative processes. For instance, the distribution of human village numbers appears to stem from just two generative processes: the development of human populations through the merging of settlements and the natural variation in settlement heights constrained by spatial and environmental factors.",
        "ori-fast-z-score": 2.219956109522242,
        "water-fast-z-score": 8.041131376518667,
        "rewrite-fast-z-score": 1.7811762496977412
    },
    {
        "original_text": "Using data from the INTErnational Gamma-Ray Astrophysics Laboratory (INTEGRAL), we have studied the gamma-ray emission from the Galactic Centre region. The region is of interest due to its proximity and high concentration of massive black holes and pulsars. We performed a detailed spatial and spectral analysis of the 18 months of public observations covering the region. We find significant emission arising from the central 6 parsec, with spatial structures and spectral variations on both long and short time-scales. In particular, we identify a newly discovered MeV gamma-ray halo surrounding the central core. We investigate the possible origins of the emission, and find that both leptonic and hadronic processes are necessary to explain the data. In the former scenario, we consider both broad-line and narrow-line regions, and find that a combination of simple one-zone models are insufficient to explain the data. In the latter scenario, we consider the possibility of a population of fast transient sources. We find that a physical model in which distant supernova remnants collide with molecular clouds is consistent with the data, and require a dense wind of material from the central supermassive black hole to explain the gamma-ray halo. We discuss the testable implications of this model, and suggest future gamma-ray observations which may distinguish between the two scenarios.",
        "watermark_text": "Using results from the INTErnational Gamma - Ray Astrophysics Laboratory ( INTEGRAL ) , we have researched the gamma - ray radiation from the Galactic Centre region . The region is of interest due to its proximity and large concentration of large white holes and pulsars .We conducted a detailed spatial and spectral evaluation of the 18 months of public observations covering the region . We see considerable emission originating from the central 6 parsec , with temporal forms and spectral variations on both long and long time - scales .In particular , we identify a newly discovered MeV gamma - ray halo surrounding the main core . We explore the possible origins of the emission , and find that both leptonic and hadronic processes are necessary to explain the information .In the former scenario , we study both broad - line and wide - line regions , and find that a combination of simple one - zone models are insufficient to explain the information . In the latter scenario , we investigate the prospect of a population of rapid transient sources .We see that a physical theory in which distant supernova remnants collide with molecular clouds is compatible with the information , and require a dense breeze of debris from the main supermassive black hole to explain the gamma - ray halo . We discuss the testable implications of this model , and suggest current gamma - ray observations which would differentiate between the two scenarios .",
        "rewrite_text": "Utilizing data from the International Gamma-Ray Astrophysics Laboratory (INTEGRAL), we have conducted research on gamma-ray radiation emanating from the Galactic Centre region. This area is of great interest due to its proximity and the high concentration of large white dwarfs and pulsars within it. We have conducted a comprehensive spatial and spectral assessment based on 18 months of public observations covering this region.\n\nSignificant emissions are observed to originate from the central 6 parsecs, exhibiting temporal forms and spectral variations on both short and long-term scales. Specifically, a newly discovered MeV gamma-ray halo has been identified surrounding the main core. We explore potential sources of these emissions and find that both leptonic and hadronic processes are necessary to explain the data.\n\nIn the leptonic scenario, we investigate both broad-line and wide-line regions, concluding that a combination of simple one-zone models is insufficient to explain the observations. In the hadronic scenario, we explore the possibility of a population of rapidly transient sources. We observe that a physical theory in which distant supernova remnants collide with molecular clouds is compatible with the data, requiring a dense stream of debris from the primary supermassive black hole to explain the gamma-ray halo's formation.\n\nWe discuss the testable implications of this model and suggest current gamma-ray observations that could differentiate between the two scenarios.",
        "ori-fast-z-score": 0.769800358919501,
        "water-fast-z-score": 7.120653320005384,
        "rewrite-fast-z-score": 2.6943012562182536
    },
    {
        "original_text": "We study the topological and entanglement entropy of the toric code at finite temperature. The topological entropy is shown to coincide with the exponential growth or decay of the number of different energy eigenstates in the finite temperature regime. We characterize the topological entanglement entropy by considering a quantity proportional to the square of the mutual information, which we call the mutual information entanglement entropy (MIEE). We show that this MIEE also exhibits an exponential decay in the finite temperature regime. We evaluate the finite-size scaling of these topological and topological entanglement entropies using numerical exact diagonalization of small clusters. These results are used to compute the entanglement entropy of a version of the toric code at finite temperature, for comparison with experiments in photonic lattices, that could realistically simulate the model with current technology. We study the topological and entanglement entropy of the toric code at finite temperature. The topological entropy is shown to coincide with the exponential growth or decay of the number of different energy eigenstates in the finite temperature regime. We characterize the topological entanglement entropy by considering a quantity proportional to the square of the mutual information, which we call the mutual information entanglement entropy (MIEE). We show that this MIEE also exhibits an exponential decay in the finite temperature regime. We evaluate the finite-size scaling of these topological and topological entanglement entropies using numerical exact diagonalization of small clusters. These results are used to compute the entanglement entropy of a version of the toric code at finite temperature, for comparison with experiments in photonic lattices, that could realistically simulate the model with current technology. This work was primarily motivated by recent interest in the topological entropy of the toric code as a qubit noise threshold in quantum error correction. We calculate the topological entropy via the Loschmidt overlap transform, which expresses the number of distinct eigenstates as an exponential growth or decay in the number of Riemann zeros. We confirm the noise threshold by numerically evaluating the topological entropy of the toric code on small lattices. We also characterize the topological entropy via the mutual information entanglement entropy (MIEE), introduced to photonic lattice experiments recently. The MIEE is defined via a quantity proportional to the square of the mutual information, and can be estimated via the Loschmidt overlap transform. We numerically evaluate the finite-size scaling of topological entropy and MIEE for system sizes up to 15 x 15 with periodic boundary conditions. This calculation confirms that the topological entropy coincides with the exponential decay of the number of energy eigenstates in the finite temperature regime, and that the MIEE coincides with the exponential growth of this number. We also estimate the topological entropy and MIEE of the smallest non-trivial subsystem of the toric code on 15 x 15 lattices with open boundary conditions.",
        "watermark_text": "We consider the topological and entanglement entropy of the toric code at finite temperature . The topological entropy is demonstrated to coincide with the exponential growth or decay of the number of different energy eigenstates in the finite temperature regime .We characterize the topological entanglement entropy by examining a quantity proportional to the square of the mutual information , which we call the mutual information entanglement entropy ( MIEE ) . We see that this MIEE also exhibits an exponential decay in the finite heating regime .We evaluate the finite - length scaling of these topological and topological entanglement entropies using numerical precise diagonalization of tiny clusters . These results are using to compute the entanglement entropy of a version of the toric code at finite temperature , for comparison with experiments in photonic lattices , that might realistically simulate the model with current technology .We consider the topological and entanglement entropy of the toric code at finite temperature . The topological entropy is demonstrated to coincide with the exponential growth or decay of the number of different energy eigenstates in the finite temperature regime .We characterize the topological entanglement entropy by examining a quantity proportional to the square of the mutual information , which we call the mutual information entanglement entropy ( MIEE ) . We see that this MIEE also exhibits an exponential decay in the finite heating regime .We evaluate the finite - length scaling of these topological and topological entanglement entropies using numerical precise diagonalization of tiny clusters . These results are using to compute the entanglement entropy of a version of the toric code at finite temperature , for comparison with experiments in photonic lattices , that might realistically simulate the model with current technology .This research was specifically motivated by recent interest in the topological entropy of the toric coding as a qubit sound threshold in quantum error correction . We calculate the topological entropy via the Loschmidt overlap transform , which expresses the number of distinct eigenstates as an exponential growth or decay in the number of Riemann zeros .We determine the noise minimum by numerically evaluating the topological entropy of the toric code on small lattices . We additionally characterize the topological entropy via the mutual information entanglement entropy ( MIEE ) , introduced to photonic lattice experiments recently .The MIEE is characterized via a quantity proportional to the square of the mutual information , and can be estimated via the Loschmidt overlap transform . We numerically analyze the finite - length scaling of topological entropy and MIEE for scheme sizes up to 15 x 15 with periodic border conditions .This measurement verified that the topological entropy coincides with the exponential decay of the number of energy eigenstates in the finite temperature regime , and that the MIEE coincides with the exponential growth of this number . We additionally calculate the topological entropy and MIEE of the smallest non - trivial subsystem of the toric code on 15 x 15 lattices with open boundary constraints .",
        "rewrite_text": "We examine the topological and entanglement entropy of the toric code at finite temperature levels. The topological entropy is found to align with the exponential growth or decline in the multitude of energy eigenstates within the defined temperature range. To delineate the topological entanglement entropy, we investigate a metric proportional to the square of mutual information, which we term the Mutual Information Entanglement Entropy (MIEE). This MIEE also demonstrates an exponential decline within the confined heating regime. Utilizing precise numerical diagonalization of tiny clusters, we assess the finite-length scaling of both topological and entanglement entropies. These findings are applied to computing the entanglement entropy of a specific toric code version at various temperature points for comparison with photonic lattice experiments.\n\nThe driving force behind this research lies in the recent fascination with the topological entropy of toric coding as a potential qubit sound threshold in quantum error correction. We calculate the topological entropy through the Loschmidt overlap transform, which represents the diversity of eigenstates as an exponential growth or reduction linked to the count of Riemann zeros. We determine the noise threshold by numerically evaluating the topological entropy of the toric code on smaller lattices. Furthermore, we characterize the topological entropy via MIEE, a recently introduced metric for photonic lattice experiments. MIEE is defined by a metric directly related to the square of mutual information and can be estimated using the Loschmidt overlap transform. We perform a numerical analysis on the finite-length scaling of topological entropy and MIEE for scheme sizes extending up to 15x15 with periodic boundary conditions. Our measurements confirm that the topological entropy aligns with the exponential decline in energy eigenstates within the finite temperature range, while MIEE coincides with the exponential increase in this count. Additionally, we compute the topological entropy and MIEE of the smallest non-trivial subsystem within the toric code on 15x15 lattices with open boundary constraints.",
        "ori-fast-z-score": -1.3000224919331833,
        "water-fast-z-score": 4.664786588701422,
        "rewrite-fast-z-score": 0.552344770738994
    },
    {
        "original_text": "A detailed kinetic study of the ring opening of cycloalkanes byCBS-QB3 calculations is presented. The activation barriers for the chair, boat, and half-chair conformers of cyclohexane are predicted to be 22.4, 27.0, and 35.5 kcal mol-1, respectively. The corresponding barriers for cycloheptane and cyclooctane are predicted to be 31.8 and 43.2 kcal mol-1. These results are consistent with experiment and other CBS-QB3 calculations. The intrinsic barriers obtained for the chair conformer of norbornane and camphor are predicted to be 25.0 and 33.6 kcal mol-1, respectively, in good agreement with experiment. The large difference between the intrinsic barriers of the chair and half-chair conformers of cycloalkanes is discussed. The zero-point energy and thermal corrections are found to have a significant effect on the barrier heights. The rates for cyclohexane and norbornane at 200 degrees C are predicted to be 1.05 x 10-12 and 3.39 x 10-13 cm3 molecule-1 s-1, respectively, by using the modified Kamal equation. These rates are in good agreement with experiment and other CBS-QB3 calculations. The intrinsic barriers of cyclohexane and norbornane are predicted to be 36.0 and 39.1 kcal mol-1, respectively. The former is consistent with the difference between the barriers of the chair and half-chair conformers, and the latter is consistent with the experimental intrinsic barrier of cyclohexane of 39.7 kcal mol-1.",
        "watermark_text": "A full kinetic study of the ring opening of cycloalkanes byCBS - QB3 calculations is provided . The activation barriers for the seat , boat , and half - chair conformers of cyclohexane are expected to be 22 . 4 , 27 . 0 , and 35 . 5 kcal mol - 1 , respectively .The equivalent barriers for cycloheptane and cyclooctane are expected to be 31 . 8 and 43 . 2 kcal mol - 1 . These data are compatible with experiment and other CBS - QB3 calculations .The inherent barriers derived for the seat conformer of norbornane and camphor are expected to be 25 . 0 and 33 . 6 kcal mol - 1 , respectively , in good agreement with research . The large change between the intrinsic obstacles of the seat and half - chair conformers of cycloalkanes is mentioned .The zero - point energy and thermal corrections are found to have a substantial impact on the barrier heights . The rates for cyclohexane and norbornane at 200 degrees C are expected to be 1 . 05 x 10 - 12 and 3 . 39 x 10 - 13 cm3 molecule - 1 s - 1 , respectively , by using the modified Kamal equation .These rates are in good agreement with research and other CBS - QB3 calculations . The intrinsic barriers of cyclohexane and norbornane are expected to be 36 . 0 and 39 . 1 kcal mol - 1 , respectively .The first is compatible with the difference between the barriers of the seat and half - chair conformers , and the latter is consistent with the empirical intrinsic barrier of cyclohexane of 39 . 7 kcal mol - 1 .",
        "rewrite_text": "A comprehensive kinetic analysis of cycloalkane ring opening through CBS-QB3 calculations has been presented. The activation barriers for the seat, boat, and half-chair conformations of cyclohexane are predicted to be 22.4, 27.0, and 35.5 kcal/mol, respectively. Similarly, the corresponding barriers for cycloheptane and cyclooctane are anticipated to be 31.8 and 43.2 kcal/mol, respectively. These predicted values align well with experimental results and other CBS-QB3 calculations.\n\nFurthermore, the inherent barriers for the seat conformer of norbornane and camphor are anticipated to be 25.0 and 33.6 kcal/mol, respectively, which is in good agreement with previous research. The significant difference between the intrinsic obstacles of the seat and half-chair conformers of cycloalkanes has been highlighted. Additionally, it has been found that zero-point energy and thermal corrections significantly impact barrier heights.\n\nUsing the modified Kamal equation, the rate constants for cyclohexane and norbornane at 200 degrees Celsius are expected to be 1.05 x 10^-12 and 3.39 x 10^-13 cm3 molecule-1 s-1, respectively. These rate constants are in good agreement with previous research and other CBS-QB3 calculations. Lastly, the intrinsic barriers for cyclohexane and norbornane are anticipated to be 36.0 and 39.1 kcal/mol, respectively. The first value is consistent with the difference in barriers between the seat and half-chair conformers, while the latter is in line with the empirical intrinsic barrier of 39.7 kcal/mol for cyclohexane.",
        "ori-fast-z-score": -1.937329799813845,
        "water-fast-z-score": 3.9000674757995495,
        "rewrite-fast-z-score": 1.5460413650478515
    },
    {
        "original_text": "Interstellar dust is a trace component of matter in galaxies, which is mostly concentrated in dusty regions such as spiral arms and nuclei. There are two contradictory views on the origin of interstellar dust: one is that the dust is formed in stellar atmospheres and ejected via stellar winds, supernovae or planetary nebulae; the other is that the dust is continuously produced in regions of recent star formation. To solve this problem, a third model was proposed, according to which dust is formed in the intergalactic medium and is then brought into galaxies by collisions of large bodies, such as asteroids, comets, or dwarf galaxies. This theory explains the observed correlation between the abundance of interstellar dust and star formation. The presence of dust affects the development of new generations of stars and galaxies, and their properties. Interstellar dust comprises very small particles, typically a few hundred nanometers in size. Optical observations in the visual and infrared regions are the most suitable for studying the distribution and dynamics of interstellar dust. A promising tool for solving this problem is long-term monitoring of distant galaxies in the radio wavelength region. The evolution of the spectral indices of the radiation intensity in radio wavelengths depends strongly on the distribution of dust in galaxies. The analysis of the variation of the spectral index over a long period of time (several decades) makes it possible to determine the amount and distribution of interstellar dust in a galaxy. The results of long-term observations of the dust content in a number of galaxies, made using the 100-m and the 45-m radio telescopes of the MPIfR in Effelsberg, the 1.2-meter and the 0.7-meter telescopes of the Special Astrophysical Observatory in Russia, the Westerbork Synthesis Radio Telescope, the APEX Telescope, the IRAM 30-m and the SEST 20-m radio telescopes in Chile, the Metsähovi Radio Observatory in Finland, and other facilities have shown that the production and evolution of dust in a galaxy is associated with certain properties of this galaxy. It was established that in late-type spiral galaxies the intensity of radiation in radio wavelengths increases with time, which indicates the gradual accumulation of dust in the intergalactic medium, due to the merger of large bodies. In early-type spiral galaxies, the intensity of radiation remains almost unchanged, which indicates the small amount of dust produced in galaxies. In elliptical galaxies, the level of radio emission decreases with time, which may indicate the removal of dust from these galaxies, due to strong and continuous influence of hot gas.",
        "watermark_text": "Interstellar dust is a trace element of matter in galaxies , which is mainly concentrated in dusty regions such as spiral arms and nuclei . There are two contradictory perspectives on the origin of interstellar dust : one is that the dust is formed in stellar atmospheres and ejected via stellar winds , supernovae or planetary nebulae ; the other is that the dust is constantly produced in areas of recent star formation .To solve this question , a third model was suggested , according to which dust is formed in the intergalactic medium and is then sent into galaxies by collisions of large bodies , such as asteroids , comets , or dwarf galaxies . This theory presents the seen correlation between the quantity of interstellar dust and star formation .The presence of dust impacts the development of new generations of stars and galaxies , and their characteristics . Interstellar dust contains very small particles , generally a few hundred nanometers in length .Optical studies in the visual and infrared zones are the most suited for studying the distribution and dynamics of interstellar dust . A good tool for solving this question is long - term surveillance of distant galaxies in the radio spectral region .The evolution of the spectral indices of the radiation intensity in radio wavelengths relies highly on the distribution of dust in galaxies . The examination of the variation of the spectral index over a large period of time ( several decades ) makes it able to predict the extent and distribution of interstellar dust in a galaxy .The results of large - term observations of the dust content in a number of galaxies , made using the 100 - m and the 45 - m radio telescopes of the MPIfR in Effelsberg , the 1 . 2 - meter and the 0 . 7 - meter telescopes of the Special Astrophysical Observatory in Russia , the Westerbork Synthesis Radio Telescope , the APEX Telescope , the IRAM 30 - m and the SEST 20 - m radio telescopes in Chile , the Metsähovi Radio Observatory in Finland , and other facilities have shown that the production and evolution of dust in a galaxy is associated with certain characteristics of this galaxy . It was established that in late - class spiral galaxies the strength of radiation in radio wavelengths increases with time , which implies the slowly accumulation of dust in the intergalactic medium , owing to the merger of large bodies .In earliest - class spiral galaxies , the frequency of radiation remains virtually unchanged , which implies the small amount of dust created in galaxies . In elliptical galaxies , the degree of radio emission decreases with time , which would indicate the removal of dust from these galaxies , owing to powerful and continuous impact of hard gas .",
        "rewrite_text": "Interstellar dust represents a minor component of matter within galaxies, predominantly concentrated in dusty regions such as spiral arms and cores. There exist two contrasting viewpoints regarding its origin. One theory suggests that the dust is formed within stellar atmospheres and ejected via stellar winds, supernovae, or planetary nebulae. Alternatively, it is believed that dust is constantly generated in regions of recent star formation. To resolve this ambiguity, a third model has been proposed where dust is formed in the intergalactic medium and transported into galaxies through collisions with large bodies, including asteroids, comets, and dwarf galaxies.\n\nThis theory effectively explains the observed correlation between the amount of interstellar dust and star formation. The presence of dust significantly influences the development and characteristics of new generations of stars and galaxies. Interstellar dust comprises extremely small particles typically ranging from a few hundred nanometers in length. Optical studies in the visible and infrared regions are most effective for investigating the distribution and dynamics of this dust. Long-term surveillance of distant galaxies in the radio spectral region provides a valuable tool for addressing this question.\n\nThe evolution of spectral indices in radio wavelength intensity heavily relies on the distribution of dust in galaxies. By examining the variation of the spectral index over an extended period (several decades), it becomes possible to predict the extent and distribution of interstellar dust within a galaxy.\n\nComprehensive observations of dust content in numerous galaxies have been conducted using various radio telescopes, including the 100-m and 45-m radio telescopes at Effelsberg's MPIfR, 1.2-meter and 0.7-meter telescopes at the Special Astrophysical Observatory in Russia, the Westerbork Synthesis Radio Telescope, APEX Telescope, IRAM 30-m and SEST 20-m radio telescopes in Chile, Metsähovi Radio Observatory in Finland, and other facilities. These observations have revealed that the production and evolution of dust in a galaxy are associated with specific characteristics of that galaxy.\n\nIn late-type spiral galaxies, it has been established that the intensity of radiation at radio wavelengths increases over time, indicating a gradual accumulation of dust in the intergalactic medium due to the merging of large bodies. Conversely, in early-type spiral galaxies, the frequency of radiation remains relatively unchanged, suggesting a low amount of dust created within these galaxies. In elliptical galaxies, a decrease in radio emission with time suggests the removal of dust from these galaxies due to the continuous impact of dense gas.",
        "ori-fast-z-score": 1.1607620001760186,
        "water-fast-z-score": 8.270429251254134,
        "rewrite-fast-z-score": 1.503841235482809
    },
    {
        "original_text": "The Large Area Telescope (LAT) on the Gamma-ray Large Area Space Telescope (GLAST), when operating in the Diffuse Sciences mode, will study the isotropic gamma-ray emission of our Milky Way galaxy. This emission results from the high-energy interactions of Galactic Cosmic Rays (GCRs) with the interstellar gas and radiation fields. An adequate modeling of this component is needed in order to accurately study gamma-ray sources in our galaxy and perform comparisons with models of the Galactic diffuse emission. In this work we present the development of a GCR simulation that will be used to build the Galactic diffuse emission model for the GLAST. The simulation was developed using the GEANT4 toolkit, and includes a model for the gas and radiation field components, and for primary and secondary GCRs. We describe the simulation setup, the simulation results, the data analysis steps performed to build the GCR simulation output into 3D spatial maps of the spectrum, spatial distribution and time variation of the primary and secondary gamma-rays fluxes, and we discuss future improvements that we are considering to develop a complete GLAST Galactic diffuse emission model.",
        "watermark_text": "The Large Area Telescope ( LAT ) on the Gamma - ray Large Area Space Telescope ( GLAST ) , when operating in the Diffuse Sciences mode , will explore the isotropic γ - ray radiation of our Milky Way galaxy . This emission occurs from the high - energy interactions of Galactic Cosmic Rays ( GCRs ) with the interstellar gas and radiation fields .An appropriate modeling of this component is required in order to correctly survey gamma - ray sources in our universe and conduct comparisons with models of the Galactic diffuse emission . In this project we present the development of a GCR simulation that will be used to build the Galactic diffuse emission description for the GLAST .The model was developed using the GEANT4 toolkit , and comprises a simulation for the gas and radiation field components , and for secondary and secondary GCRs . We discuss the model setup , the model results , the information assessment stages performed to build the GCR simulation input into 3D spatial mapping of the spectrum , spatial spread and period change of the primary and secondary γ - radiation fluxes , and we explain potential improvements that we are considering to develop a complete GLAST Galactic diffuse emission model .",
        "rewrite_text": "The Large Area Telescope (LAT) aboard the Gamma-ray Large Area Space Telescope (GLAST), when operating in Diffuse Sciences mode, will undertake an exploration of the isotropic γ-ray radiation emitted by our Milky Way galaxy. This emission arises from high-energy interactions between Galactic Cosmic Rays (GCRs) and the interstellar gas and radiation fields. An accurate modeling of this component is essential for correctly surveying gamma-ray sources in the universe and comparing them with models of Galactic diffuse emission.\n\nIn this project, we present the development of a GCR simulation that will form the basis for describing Galactic diffuse emission for GLAST. This model was created using the GEANT4 toolkit, and includes simulations for both the gas and radiation field components, as well as for secondary and primary GCRs. We discuss the setup of the model, its results, and the various stages of information assessment involved in building the GCR simulation. This involves 3D spatial mapping of the spectrum, spatial spread, and period changes of both primary and secondary γ-radiation fluxes. We also explain potential improvements we are considering to develop a comprehensive GLAST Galactic diffuse emission model.",
        "ori-fast-z-score": -0.7777777777777778,
        "water-fast-z-score": 6.114295984380816,
        "rewrite-fast-z-score": 2.3626845919446504
    },
    {
        "original_text": "The study of the rho meson within the framework of the K-Matrix theory was performed for the first time in the framework of the Regge theory of fundamental particles. The role of the rho meson is important for a clear interpretation of pion electroproduction experiments at JLab (Mainz, Buhler, CExchange, and A2). In particular, it was shown that the inclusion of the rho meson in the analysis leads to a considerable improvement of the description of these experiments. The work was performed within the Regge theory, which is a fundamental theory of strong interactions. The Regge theory states that the total cross section of hadrons increases with the energy level in an approximate power-law dependence:. The approximation is good for hadrons with large masses, such as the pions. The Regge theory is based on the assumption that the exchanges between particles in the different reactions have a common pattern, which can be described by the exchange of fundamental particles called reggeons. The reggeons correspond to intermediate states in the quantum field theory between two hadrons. The first description of hadron interactions at high energies within the framework of quantum field theory was developed by G.V. Efimov in the framework of the simple Regge theory. A number of later theoretical and experimental studies of the high-energy interaction confirmed the main assumptions of the simple theory, in particular, the universal character of the exchanges of reggeons. Pions appear in the description of interactions at high energies due to the unique possibility of electromagnetic interaction of pseudoparticles. The SJC (Simple Isobar Model) describes the interaction of two pions with the maximum momentum of each particle limited by the values related to the physical decay channels of pions (soft pions). The first description of hadron interactions at high energies within the framework of quantum field theory was developed by G.V. Efimov in the framework of the simple Regge theory. A number of later theoretical and experimental studies of the high-energy interaction confirmed the main assumptions of the simple theory, in particular, the universal character of the exchanges of reggeons. Pions appear in the description of interactions at high energies due to the unique possibility of electromagnetic interaction of pseudoparticles. The SJC (Simple Isobar Model) describes the interaction of two pions with the maximum momentum of each particle limited by the values related to the physical decay channels of pions (soft pions). The simplest realization of the SJC, which takes into account the contribution of one exchanged meson, is the Born-Riley partial-wave representation. It was shown by G.P. Lepage that within the framework of the Born approximation and the high-energy approximation for the hadron interactions, the pomeron exchange can be realized as a sum of two reggeon exchanges. Thus, pions appear in the description of",
        "watermark_text": "The investigation of the rho meson within the framework of the K - Matrix concept was done for the first time in the framework of the Regge model of fundamental particles . The role of the rho meson is important for a clear interpretation of pion electroproduction experiments at JLab ( Mainz , Buhler , CExchange , and A2 ) .In particular , it was shown that the introduction of the rho meson in the analysis leads to a considerable advancement of the description of these experiments . The paper was done within the Regge principle , which is a basic theory of strong coupling .The Regge principle explains that the total cross section of hadrons increases with the power level in an approximate power - law dependence : . The approximation is good for hadrons with large masses , such as the pions .The Regge concept is based on the assumption that the exchanges between particles in the different processes have a common rhythm , which can be described by the transfer of fundamental particles termed reggeons . The reggeons relate to intermediate states in the quantum field theory between two hadrons .The first description of hadron interactions at high energies within the framework of quantum field theory was developed by G . V . Efimov in the framework of the simple Regge concept .A number of later conceptual and theoretical experiments of the high - energy coupling proved the main predictions of the simple theory , in notably , the universal character of the exchanges of reggeons . Pions feature in the description of coupling at high energies due to the unusual possibility of electromagnetic interaction of pseudoparticles .The SJC ( Simple Isobar Model ) describes the interaction of two pions with the maximum velocity of each particle limited by the values due to the physical decay pathways of pions ( soft pions ) . The first description of hadron interactions at high energies within the framework of quantum field theory was developed by G . V .Efimov in the framework of the simple Regge concept . A couple of later conceptual and experimental studies of the high - energy coupling proved the main predictions of the simple theory , in notably , the universal character of the exchanges of reggeons .Pions arise in the description of coupling at high energies due to the unusual possibility of electromagnetic interaction of pseudoparticles . The SJC ( Simple Isobar Model ) states the interaction of two pions with the maximum velocity of each particle limited by the values due to the physical decay pathways of pions ( sweet pions ) .The simplest realization of the SJC , which gives into consideration the contribution of one exchanged meson , is the Born - Riley partial - wave form . It was shown by G . P .Lepage that within the framework of the Born approximation and the high - energy approximation for the hadron interactions , the pomeron exchange can be realized as a sum of two reggeon exchanges . Thus , pions occur in the description of",
        "rewrite_text": "The initial investigation of the rho meson within the context of the K-Matrix theory was conducted initially within the Regge model for fundamental particles. The rho meson plays a crucial role in providing a clear interpretation for the experiments on pion electroproduction at JLab (Mainz, Buhler, CExchange, and A2). Specifically, the inclusion of the rho meson in the analysis has significantly enhanced the description of these experiments.\n\nThe research was carried out according to the Regge principle, which is a fundamental theory of strong coupling. This principle explains that the total cross-section of hadrons increases in an approximate power-law dependence on the power level. This approximation is particularly accurate for hadrons with large masses, such as pions.\n\nThe Regge concept is based on the assumption that particle exchanges in different processes share a common rhythm, which can be described through the transfer of fundamental particles known as reggeons. These reggeons are related to intermediate states in quantum field theory between two hadrons.\n\nG. V. Efimov was the first to describe hadron interactions at high energies within the framework of quantum field theory using the simple Regge concept. A series of subsequent conceptual and theoretical experiments on high-energy coupling have validated the main predictions of this theory, particularly the universal nature of reggeon exchanges.\n\nPions are crucial in describing coupling at high energies due to their unique ability for electromagnetic interaction with pseudoparticles. The Simple Isobar Model (SJC) outlines the interaction between two pions, with each particle's maximum velocity constrained by the physical decay paths of pions (referred to as \"sweet pions\" in this context).\n\nOne of the simplest implementations of the SJC, considering the contribution of a single exchanged meson, is the Born- Riley partial-wave form. G. P. Lepage has shown that within the framework of the Born approximation and high-energy approximation for hadron interactions, the pomeron exchange can be realized as a combination of two reggeon exchanges. Consequently, pions are frequently encountered in the descriptive context.",
        "ori-fast-z-score": 1.866691212406119,
        "water-fast-z-score": 8.853733734208943,
        "rewrite-fast-z-score": 2.799023467943904
    },
    {
        "original_text": "Neutral genetic drift, a form of genetic polymorphism occurring due to random genetic mutations, can aid protein sequence evolution. Such a scenario may occur frequently during early protein evolution, when long protein sequences are less likely to contain strongly deleterious mutations. Under this model, although most new mutations are deleterious, some are weakly beneficial or even neutral. These weakly beneficial mutations fix by chance in a population, because their slightly increased frequencies are due to genetic drift. Due to their small effect, these weakly beneficial mutations do not contribute to an organism s Darwinian fitness. However, they contribute to protein sequence evolution, by helping compensate for the negative effect of strongly deleterious mutations. This  fine-tuning  through neutral drift can increase the organism s protein functionality and Darwinian fitness, even though its average fitness is still close to zero. In contrast to previously proposed protein adaptation scenarios, this model explains how protein functionality can be maintained over long periods of time in the absence of positive Darwinian selection.",
        "watermark_text": "Neutral genetic drift , a form of genetic polymorphism arising owing to random genetic defects , can aid protein sequence evolution . Such a situation may happen widely during early protein development , when long protein strands are less likely to contain strongly deleterious variants .Under this model , although most new mutations are deleterious , some are weakly beneficial or even neutral . These weakly beneficial mutations fix by chance in a population , because their mildly increased frequencies are owing to mutation drift .Due to their tiny effect , these strongly effective mutations do not contribute to an organism s Darwinian fitness . However , they promote to protein sequence evolution , by helping compensate for the negative result of heavily deleterious mutations .This fine - tuned through neutral drift can increase the organism s gene complexity and Darwinian fitness , even though its average fitness is already nearly to zero . In comparison to earlier proposed protein adaptation models , this model describes how protein functionality can be maintained over large intervals of time in the absence of positive Darwinian selection .",
        "rewrite_text": "Neutral genetic drift, a type of genetic polymorphism arising from random genetic defects, can facilitate the evolution of protein sequences. Such a phenomenon frequently occurs during the early stages of protein development, especially when long protein chains are less likely to contain strongly detrimental variants. In this model, while the majority of new mutations are detrimental, some can be weakly beneficial or even neutral. These weakly beneficial mutations can randomly fix within a population due to mutation drift, leading to slight increases in their frequency. Although these highly effective mutations have minimal impacts on an organism's Darwinian fitness, they contribute to protein sequence evolution by compensating for the negative effects of heavily detrimental mutations. This fine-tuning through neutral drift can enhance the gene complexity and Darwinian fitness of an organism, even when its average fitness is already close to zero. In contrast to earlier proposed models of protein adaptation, this model explains how protein functionality can be sustained over extended periods of time without positive Darwinian selection.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 4.569057743101286,
        "rewrite-fast-z-score": 1.0314212462587933
    },
    {
        "original_text": "The deuteron electrodisintegration threshold cross section is one of the main doorway to study the interaction between the electromagnetic and the nuclear field. This process has been studied in the past at low and intermediate energies with various models and limited precision. In recent years the availability of powerful numerical methods to solve relativistic quantum mechanical problems allowed for the first time to perform calculations at higher energies, approaching the deuteron breakup threshold. We present here a simple approach to the calculation of the process at relativistic energies based on a distorted wave Born approximation. The formalism is applied to study the process at a laboratory energy of 1.2 GeV, which is relevant for the creation of hot dense matter in current and future accelerator facilities. We find good agreement with previous results from optical models and the expected deviation from them at backward angles. The manuscript is partly based on a talk presented at the INT workshop “Electroweak Astrophysics and Fundamental Interactions” (September 2024).",
        "watermark_text": "The deuteron electrodisintegration threshold cross area is one of the main doorway to study the interaction between the electromagnetic and the atomic field . This process has been studied in the past at low and intermediate energies with various models and minimal precision .In recent years the availability of powerful mathematical techniques to solve relativistic quantum mechanical problems led for the first time to conduct measurements at higher energies , approaching the deuteron breakup threshold . We consider here a simple technique to the calculation of the process at relativistic energies based on a distorted wave Born approximation .The formalism is applied to study the process at a lab power of 1 . 2 GeV , which is relevant for the creation of bright heavy material in current and future accelerator facilities . We get good agreement with previous findings from optical theories and the expected deviation from them at backward directions .The manuscript is partly based on a speech presented at the INT workshop “ Electroweak Astrophysics and Fundamental Interactions ” ( September 2024 ) .",
        "rewrite_text": "The cross-sectional area of the deuteron electrodisintegration threshold is a crucial entry point for exploring the interaction between electromagnetic and atomic fields. In the past, this process has been investigated at low and intermediate energies using various models with limited precision. Recently, the advent of powerful mathematical techniques to solve relativistic quantum mechanical problems has enabled measurements to be conducted at higher energies, approaching the deuteron breakup threshold for the first time.\n\nIn this study, we propose a straightforward method for calculating this process at relativistic energies based on the distorted wave Born approximation. This formalism is employed to investigate the process at a laboratory power of 1.2 GeV, which is significant for generating bright heavy materials in current and future accelerator facilities. Our findings show good agreement with previous optical theory predictions and expected deviations in backward directions. This manuscript is partially based on a presentation made at the INT workshop \"Electroweak Astrophysics and Fundamental Interactions\" in September 2023.",
        "ori-fast-z-score": 1.3587324409735149,
        "water-fast-z-score": 5.4349297638940595,
        "rewrite-fast-z-score": 1.118033988749895
    },
    {
        "original_text": "The Examples against The Generalized Jacobian Conjecture (Examples for GJC) by Brent Everly is an interesting paper showing that the Generalized Jacobian Conjecture (GJC) is not generally true. The GJC, proposed in 2001 by Luis Barreira, was a bold claim suggesting that generic topological chaos, a key feature of irregular dynamical systems, occurs in a large class of dynamical systems. The Examples against The Generalized Jacobian Conjecture demonstrates that a specific dynamical system, known to be in this large class of dynamical systems with no generic chaotic behaviour, does not satisfy the GJC. The Examples against The Generalized Jacobian Conjecture shows that the GJC is false in a generic sense, but leaves open the possibility that the GJC may be true for a smaller class of dynamical systems, for example those systems with some additional generic properties. The implications of this result are far-reaching and many resonances with diverse areas of science. For example, the Examples against The Generalized Jacobian Conjecture has applications to classical mechanics, the foundations of statistical mechanics, certain dimensions in coding theory, the statistical properties of weather and water clouds, and the combinatorics of words. The paper is well-written and very clear. The Examples against The Generalized Jacobian Conjecture has broad interest to a diverse audience of dynamical systems scientists and mathematicians.",
        "watermark_text": "The Examples against The Generalized Jacobian Conjecture ( Examples for GJC ) by Brent Everly is an interesting paper indicating that the Generalized Jacobian Conjecture ( GJC ) is not generally true . The GJC , suggested in 2001 by Luis Barreira , was a bold claim indicating that universal topological chaos , a key feature of irregular dynamical systems , exist in a large class of dynamical systems .The Examples against The Generalized Jacobian Conjecture demonstrates that a certain dynamical system , known to be in this big class of dynamical systems with no generic chaotic behaviour , does not satisfy the GJC . The Examples against The Generalized Jacobian Conjecture demonstrates that the GJC is false in a generic sense , but puts open the suggestion that the GJC may be true for a smaller class of dynamical systems , for example those systems with some additional generic properties .The implications of this effect are far - reaching and many resonances with varied fields of science . For instance , the Examples against The Generalized Jacobian Conjecture has applications to classical mechanics , the foundations of statistical mechanics , various dimensions in coding theory , the statistical characteristics of storms and sea dust , and the combinatorics of words .The paper is well - authored and very consistent . The Examples against The Generalized Jacobian Conjecture has broad interest to a diverse audience of dynamical systems scientists and mathematicians .",
        "rewrite_text": "Brent Everly的《针对广义雅可比猜想（GJC）的例子》是一篇引人入胜的论文，它表明广义雅可比猜想（GJC）并不普遍成立。该GJC是在2001年由路易斯·巴雷拉提出的，是一个大胆的断言，即普遍存在的拓扑混沌，作为不规则动力系统的一个关键特征，存在于一大类动力系统中。该论文通过实例证明了GJC在某些已知不属于通用混沌行为的大类动力系统中不成立。这表明GJC在一般意义上是不正确的，但同时也暗示GJC对于较小类别的动力系统可能是正确的，例如那些具有某些额外通用特性的系统。\n\n这一效应的深远影响与科学各领域的共鸣很多。例如，《针对广义雅可比猜想的例子》在经典力学、统计力学基础、编码理论中的各个维度、风暴和海尘的统计特征以及词汇组合学等方面都有应用。这篇论文的作者文笔流畅，内容一致性强。《针对广义雅可比猜想的例子》对于动力系统科学家和数学家等不同领域的读者群体具有广泛的吸引力。",
        "ori-fast-z-score": 2.429493573646624,
        "water-fast-z-score": 7.067617668790178,
        "rewrite-fast-z-score": 0
    },
    {
        "original_text": "The paper introduces the formalism to classify N=8 supergravity theories in five distinctive families and presents their bosonic fields content. It begins by reviewing the structure of N=8 supergravity theories and the embedding of its maximal SU(8) bosonic subgroup in the N=8 supersymmetric Poincaré group. Next, it describes a systematic classification of these theories in five families. The first family consists of the so-called maximally extended theories, which contains all N=8 supergravity theories as well as their consistent toroidal truncations. The second family consists of theories with SO(7, 1) invariance, which are based on the coset space G/H where G is the hidden symmetry group of maximal supergravity and H is one of its subgroup. The third family of theories is based on the supergroup E$_{7(7)}/$SU(8). The fourth family is based on the SO(p, 4) x SO(7−p) symmetry, where p is the number of space-time vector fields. The last family contains theories with SO(p, 3, C) symmetry, where C denotes the reality of the supergraviy C Becfore brane.",
        "watermark_text": "The paper introduces the formalism to classify N = 8 supergravity models in five distinctive families and provides their bosonic fields content . It start by reviewing the composition of N = 8 supergravity theories and the embedding of its maximal SU ( 8 ) bosonic subgroup in the N = 8 supersymmetric Poincaré group .Next , it explains a comprehensive classification of these theories in five families . The first class consists of the so - called maximally extended models , which contains all N = 8 supergravity models as well as their consistent toroidal truncations .The second family consists of theories with SO ( 7 , 1 ) invariance , which are based on the coset space G / H where G is the secret symmetry class of maximal supergravity and H is one of its subgroup . The third family of theories is based on the supergroup E $ _ { 7 ( 7 ) } / $ SU ( 8 ) .The fourth family is based on the SO ( p , 4 ) x SO ( 7−p ) symmetry , where p is the number of space - time vector fields . The last family contains ideas with SO ( p , 3 , C ) symmetry , where C denotes the reality of the supergraviy C Becfore brane .",
        "rewrite_text": "The study presents a method to categorize N = 8 supergravity models into five distinct families, and details their bosonic field content. It begins with a review of the composition of N = 8 supergravity theories and the integration of their maximal SU(8) bosonic subgroup into the N = 8 supersymmetric Poincaré group.\n\nProceeding further, it comprehensively classifies these theories into five families. The first class encompasses the so-called maximally extended models, which include all N = 8 supergravity models, as well as their consistent toroidal truncations. The second family consists of theories with SO(7, 1) invariance, which are based on the coset space G/H, where G represents the secret symmetry class of maximal supergravity and H is one of its subgroups.\n\nThe third family of theories is founded on the supergroup E7(7)/SU(8). The fourth family is based on the symmetry of SO(p, 4) x SO(7-p), where p represents the number of space-time vector fields. Lastly, the final family involves concepts with SO(p, 3, C) symmetry, where C signifies the reality of the supergravity C-brane.",
        "ori-fast-z-score": 0.629940788348712,
        "water-fast-z-score": 3.6536565724225296,
        "rewrite-fast-z-score": 1.3858697343671664
    },
    {
        "original_text": "Sandstone is one of the most important rock types in world with many significant economical and cultural implications. One of such implications are that of building and structure. Sandstone is used in the construction of several architectural structures such as bridges, buildings, and skyscrapers. Fontainebleau sandstone, which is one of the most well-known types of sandstone, is used in the construction of the exterior walls and staircases of the great Miami Penitentiary. Despite its significance in the construction industry, not much research has been carried out on Fontainebleau sandstone. This can largely be attributed to the complex hierarchical fracture network of the sandstone which makes it hard to study. This paper studies the fracture patterns of fractured Fontainebleau sandstone using optical microscopy and statistical analysis. The results show that the fractured sandstone follows Weibull statistics. Moreover, it was also shown that the failed surfaces are not smooth and possess non-Gaussian statistics which might have significant implications on the mechanics of fracture in Fontainebleau sandstone.",
        "watermark_text": "Sandstone is one of the most important stone types in world with many important economical and cultural implications . One of such consequences are that of building and shape .Sandstone is utilized in the built of several architectural buildings such as roads , structures , and skyscrapers . Fontainebleau sandstone , which is one of the most well - famous types of sandstone , is utilized in the built of the exterior walls and staircases of the great Miami Penitentiary .Despite its significance in the construction profession , not little research has been carried out on Fontainebleau sandstone . This can largely be due to the complex hierarchical cracking pattern of the sandstone which makes it difficult to study .This paper studies the fracture patterns of broken Fontainebleau sandstone using optical microscopy and statistical analysis . The results show that the fractured sandstone follows Weibull statistics .Moreover , it was also shown that the collapsed surfaces are not smooth and possess non - Gaussian statistics which would have important implications on the mechanics of fracture in Fontainebleau sandstone .",
        "rewrite_text": "Sandstone stands as a pivotal stone type worldwide, encompassing numerous economic and cultural ramifications. Its applications in construction and shaping are particularly significant. Numerous architectural structures, including roads, buildings, and skyscrapers, are constructed with sandstone. Notably, the renowned Fontainebleau sandstone, a highly acclaimed type, has been utilized in the construction of the exterior walls and staircases of the prominent Miami Penitentiary. However, despite its crucial role in the construction industry, there has been limited research conducted on Fontainebleau sandstone. This can largely be attributed to the intricate hierarchical cracking pattern that makes it challenging to study.\n\nThis paper delves into the fracture patterns of broken Fontainebleau sandstone through optical microscopy and statistical analysis. The findings reveal that the fractured sandstone adheres to Weibull statistics. Furthermore, it has been demonstrated that the collapsed surfaces are not smooth and exhibit non-Gaussian statistics, which could have significant implications for the mechanics of fracture in Fontainebleau sandstone.",
        "ori-fast-z-score": 0.808290376865476,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 1.5215349135496974
    },
    {
        "original_text": "A real-time renormalization group (RT-RG) is applied to a quantum dot system in the Coulomb blockade regime. The conductance through the dot is computed as a function of the system’s parameters, namely the gate and bias voltages, as well as the plunger (i.e., orbital) number of the dot. A separation between low- and high-energy scales is established by a smallness parameter which is assumed to be the bias voltage scaled by a large energy, e.g., the plunger energy. Exact solution is obtained by truncating the hierarchy of integro-differential equations at the second member. In this approximation the conductance is parameterized by the solution of a closed set of differential equations. Our solution method allows us to study a wide range of the system’s parameters. In particular, we compute a crossover from tunneling to charge-transfer regimes as the system is tuned from an initial almost empty dot to a filled dot. Furthermore, we compute a cutoff energy and a cutoff scale that separate slow (i.e., flowing with the system’s parameters) from fast (i.e., independent) scales in the system. The former dependence is captured by the effective theory, whereas the latter is absorbed into therunning (i.e., evolving) cutoff scale. This provides a powerful method to explore a nonequilibrium quantum dot problem as one varies the plunger (orbital) number and gate voltage. In particular, this allows one to locate a parameter regime where a given nonequilibrium steady state was not reached during the system’s evolution from some initial conditions. For example, we find that if the initial state of the quantum dot is almost empty (filled), then the conductance through the dot is minimal (maximinal) at the Coulomb blockade peak of the dot’s conductance.",
        "watermark_text": "A real - time renormalization group ( RT - RG ) is applied to a quantum dot network in the Coulomb blockade regime . The conductance through the dot is computed as a function of the device ’ s parameters , notably the gate and bias voltages , as also as the plunger ( i . e . , orbital ) number of the dot .A separation between small - and large - energy scales is maintained by a smallness parameter which is expected to be the bias voltage scaled by a large energy , e . g . , the plunger power . Exact solve is found by truncating the hierarchy of integro - differential coefficients at the second member .In this equation the conductance is parameterized by the solve of a closed collection of differential equations . Our solution technique lets us to study a broad variety of the process ’ s components .In particular , we compute a crossover from tunneling to charge - transfer regimes as the system is tuned from an initial almost empty dot to a filled dot . Furthermore , we compute a cutoff energy and a cutoff scale that distinguish slow ( i . e . , flowing with the system ’ s parameters ) from slow ( i . e . , independent ) scales in the system .The former dependence is captured by the effective theory , whereas the second is absorbed into therunning ( i . e . , evolving ) cutoff scale . This offers a powerful method to examine a nonequilibrium quantum dot problem as one varies the plunger ( orbital ) number and gate voltage .In particular , this enables one to locate a parameter regime where a given nonequilibrium steady state was not achieved during the system ’ s evolution from some early conditions . For instance , we find that if the initial state of the quantum dot is almost empty ( filled ) , then the conductance through the dot is minimal ( maximinal ) at the Coulomb blockade peak of the dot ’ s conductance .",
        "rewrite_text": "A real-time renormalization group (RT-RG) method is employed on a quantum dot network within the Coulomb blockade regime. The conductance across the dot is calculated as a function of the device parameters, notably the gate and bias voltages, as well as the plunger (i.e., orbital) number of the dot. A smallness parameter maintains a distinction between small and large energy scales, which is anticipated to be the bias voltage scaled by a large energy, such as the plunger power. An exact solution is achieved by truncating the hierarchy of integro-differential coefficients at the second member in the equation, where the conductance is parameterized by the solution of a closed set of differential equations.\n\nOur solution technique allows us to explore a wide range of process components. Specifically, we compute a transition from tunneling to charge-transfer regimes as the system shifts from an initially nearly empty dot to a fully loaded dot. Additionally, we calculate a cutoff energy and a cutoff scale that differentiate between slow (i.e., influenced by system parameters) and independent scales within the system. The former dependency is captured by the effective theory, while the latter is absorbed into the evolving (i.e., changing) cutoff scale.\n\nThis offers a powerful approach to investigate nonequilibrium quantum dot problems as the plunger (orbital) number and gate voltage vary. In particular, it enables the identification of parameter regimes where a given nonequilibrium steady state was not achieved during the system's evolution from initial conditions. For instance, we discover that when the initial state of the quantum dot is nearly empty (or filled), the conductance through the dot is minimal (or maximal) at the Coulomb blockade peak of the dot's conductance.",
        "ori-fast-z-score": -1.5666989036012806,
        "water-fast-z-score": 3.7859388972001824,
        "rewrite-fast-z-score": 1.1358152736593492
    },
    {
        "original_text": "Sigma Orionis is a small rich cluster some 1,200 light-years from the Earth. It lies at the heart of the Orion Molecular Cloud, a massive star-forming complex located in the constellation Orion some 420 light-years distant. Located at a fairly young estimated age of some 7 million years, Sigma Orionis is still deeply embedded in its natal cloud, and consequently provides an excellent laboratory to study the substellar content of very young clusters. The first comprehensive census of brown dwarfs and very low-mass stars in the cluster was presented in Belikov et al. (2012). Here we present optical, near-infrared and IRAC/Spitzer photometry of over 130 additional cluster members, increasing the total membership count to well over 200. This allows us to more precisely characterize the the substellar initial mass function (IMF), shown here to closely follow a power-law with exponent -1.3 in the substellar mass range 0.1–0.015 M⊙. The mass function is substantially lower than that of older clusters like the Trapezium in the Orion Nebula (e.g. Kroupa et al. 2007) and comparable to that of much younger (tens of millions of years) clusters, like the Nice model for the formation of the Solar System (Boss & Bernstein 2012). It is also substantially flatter than recent field determinations from near-IR imaging (e.g. N TOP algorithms, McElwain et al. 2014) and significantly steeper than determinations from IRAC/Spitzer data, which may be affected by significant sample incompleteness at low masses. We have used this new cluster mass function to update the projected cluster core and core accretion rates, obtaining values of 3.4 × 10–5 M⊙yr–1 and 8.6 × 10–5 M⊙yr–1, respectively. These are substantially higher than previous estimates but consistent with theoretical core accretion models for very low-mass stars and brown dwarfs at the young age of Sigma Orionis.",
        "watermark_text": "Sigma Orionis is a small rich cluster some 1 , 200 light - years from the Earth . It lies at the heart of the Orion Molecular Cloud , a huge star - creating complex located in the constellation Orion some 420 light - years distant .Located at a fairly young projected age of some 7 million years , Sigma Orionis is already heavily embedded in its natal cloud , and consequently gives an excellent laboratory to study the substellar concentration of very young galaxies . The first comprehensive census of brown dwarfs and very low - weight stars in the cluster was presented in Belikov et al .( 2012 ) . Here we present imaging , near - infrared and IRAC / Spitzer photometry of over 130 additional cluster participants , increasing the total membership count to well over 200 .This enables us to more properly characterize the the substellar initial mass function ( IMF ) , shown here to closely follow a power - law with exponent - 1 . 3 in the substellar mass spectrum 0 . 1 – 0 . 015 [UNK] . The mass distribution is greatly smaller than that of older clusters like the Trapezium in the Orion Nebula ( e . g .Kroupa et al . 2007 ) and comparable to that of far younger ( tens of millions of years ) clusters , like the Nice model for the formation of the Solar System ( Boss & Bernstein 2012 ) .It is also substantially flatter than latest field determinations from near - IR scanning ( e . g . N TOP algorithms , McElwain et al .2014 ) and substantially steeper than determinations from IRAC / Spitzer images , which may be caused by significant sample incompleteness at low masses . We have utilized this new cluster mass distribution to update the projected cluster core and core accretion speeds , obtaining values of 3 . 4 × 10 – 5 [UNK] – 1 and 8 . 6 × 10 – 5 [UNK] – 1 , respectively .These are greatly higher than prior estimates but consistent with theoretical core accretion scenarios for very low - weight stars and dark dwarfs at the early age of Sigma Orionis .",
        "rewrite_text": "Sigma Orionis is a small yet wealthy cluster located approximately 1,200 light years away from Earth. It rests at the center of the Orion Molecular Cloud, a vast star-forming complex situated in the Orion constellation, which is roughly 420 light years distant. With an estimated young age of around 7 million years, Sigma Orionis is already deeply embedded within its birth cloud, providing an exceptional laboratory for studying the substellar concentration of young galaxies.\n\nIn 2012, Belikov et al. presented the first comprehensive census of brown dwarfs and very low-mass stars in this cluster. Here, we present imaging, near-infrared, and IRAC/Spitzer photometry data of over 130 additional cluster members, increasing the total membership count to over 200. This allows us to more accurately characterize the substellar initial mass function (IMF), which is found to closely follow a power-law with an exponent of -1.3 in the substellar mass spectrum ranging from 0.1 to 0.015 units.\n\nThe mass distribution in Sigma Orionis is notably smaller compared to older clusters like the Trapezium in the Orion Nebula (e.g., Kroupa et al. 2007) and is comparable to that of much younger clusters (tens of millions of years old) such as the Nice model for Solar System formation (Boss & Bernstein 2012). It is also notably flatter than recent field determinations from near-IR scanning (e.g., NTOP algorithms, McElwain et al. 2014) and steeper than determinations from IRAC/Spitzer images, which may be due to significant sample incompleteness at low masses.\n\nWe have utilized this new cluster mass distribution to update the projected cluster core and core accretion speeds, obtaining values of 3.4 x 10^-5 to 1 and 8.6 x 10^-5 to 1, respectively. These values are significantly higher than previous estimates but are consistent with theoretical core accretion scenarios for very low-mass stars and dark dwarfs at the early age of Sigma Orionis.",
        "ori-fast-z-score": -1.3522468075656264,
        "water-fast-z-score": 3.718678720805473,
        "rewrite-fast-z-score": 2.3664319132398464
    },
    {
        "original_text": "A new comprehensive set of elemental abundances in damped Lyman alpha absorbers (DLAs) is presented. The dataset is based on high-resolution spectroscopy of 19 quasars, aimed at the Zn and Y lines, which together provide 18$<$Z/H$<$27 and 26$<$Y/H$<$0.5 and cover almost 4 orders of magnitude in DLA  N$_2$H$^+$ (+) H$_2$ {}abundance. In addition to these Zn and Y lines, we also analyze other Zn and Y abundances from the previously published data of B13 and HS15. The paper includes re-analyses of the data from B13 and HS15, as well as new results for the previously analyzed elements O, Si, Ti, V, Cr, Co, and Ni. It also includes the first results for Al, Cu, and Mn. The full dataset allows, for the first time, detailed studies of the star formation histories (SFHs) in DLA galaxies. The new Y/Zn atomic ratios as a function of metallicity allow us to exclude models with a constant SFH with time (including exponentially declining ones). This significantly increases the number of SFHs that can be allowed by the data. We find that a two-burst model with a 5 Gyr-old secondary burst is the best model for the majority of the galaxies, with an additional very small fraction of galaxies best described by a constant SFH. The new dataset will be valuable for studies of the early evolution of galaxies, understanding of the chemical evolution of galaxies, and the testing of models of the formation and evolution of galaxies.",
        "watermark_text": "A new comprehensive collection of elemental abundances in damped Lyman alpha absorbers ( DLAs ) is provided . The dataset is based on high - resolution spectroscopy of 19 quasars , aiming at the Zn and Y lines , which together provide 18 $ < $ Z / H $ < $ 27 and 26 $ < $ Y / H $ < $ 0 . 5 and cover nearly 4 orders of magnitude in DLA N $ _ 2 $ H $ ^ + $ ( + ) H $ _ 2 $ { } abundance .In addition to these Zn and Y lines , we also analyze other Zn and Y abundances from the previously written data of B13 and HS15 . The paper includes re - studies of the information from B13 and HS15 , as well as fresh results for the previously analyzed elements O , Si , Ti , V , Cr , Co , and Ni .It additionally contains the first findings for Al , Cu , and Mn . The full dataset provides , for the first time , detailed analyses of the star formation histories ( SFHs ) in DLA galaxies .The revised Y / Zn atomic ratios as a function of metallicity allow us to eliminate scenarios with a constant SFH with time ( especially exponentially declining ones ) . This substantially increases the quantity of SFHs that can be allowed by the information .We see that a two - burst model with a 5 Gyr - old secondary burst is the best model for the majority of the galaxies , with an additional very small fraction of stars best described by a constant SFH . The updated dataset will be valuable for research of the early evolution of galaxies , awareness of the chemical evolution of galaxies , and the testing of models of the formation and evolution of galaxies .",
        "rewrite_text": "A novel compilation of elemental abundances has been presented for damped Lyman alpha absorbers (DLAs). This dataset is derived from high-resolution spectroscopy of 19 quasars, focusing on the Zn and Y lines. These lines offer a comprehensive range of abundance ratios, covering a nearly four-order magnitude scale in DLA N$_2$H$^+$H$_2$ abundance, with values of 18 < Z/H < 27 and 26 < Y/H < 0.5.\n\nBeyond the Zn and Y lines, our analysis also encompasses Zn and Y abundances from previous data sets such as B13 and HS15. This paper re-examines information from B13 and HS15, while also presenting fresh results for previously analyzed elements including O, Si, Ti, V, Cr, Co, and Ni. Additionally, it includes the first findings for Al, Cu, and Mn.\n\nThis comprehensive dataset offers unprecedented insights into the star formation histories (SFHs) of DLA galaxies. The revised Y/Zn atomic ratios, as a function of metallicity, enable us to rule out scenarios with a constant SFH over time, especially those with exponentially declining rates. This significantly expands the range of SFHs that can be supported by the data.\n\nOur observations indicate that a two-burst model with a 5 Gyr old secondary burst is the most suitable for the majority of galaxies, with a small fraction of stars best described by a constant SFH. This updated dataset will be invaluable for researching the early evolution of galaxies, understanding the chemical evolution of galaxies, and testing models of galaxy formation and evolution.",
        "ori-fast-z-score": 0.20203050891044214,
        "water-fast-z-score": 5.858884758402822,
        "rewrite-fast-z-score": 1.4
    },
    {
        "original_text": "In the standard model (SM) of particle physics, the gauge fields of the electroweak force and the Higgs field underpin the particles and interactions we see in our world. In particular, the weak force is mediated by the W and Z bosons, and the Higgs boson, yet the interactions of these particles are described by a single coupling constant, coupling the strong, weak and electromagnetic forces together. The renormalization group (RG) has told us that this single coupling constant must evolve from a high energy value ofunity at high energy scales. By introducing an additional symmetry between the strong and electroweak forces, and a singlet scalar field under both, we can naturally give the coupling constant of the weak force a value less than that of the strong coupling constant, without the presence of any new particles. This symmetry breaking is achieved through the vev of the Higgs field, which is what gives the particles their masses. This scalar field, and the corresponding symmetry breaking, is called the Higgs mechanism. Because the Higgs has no particular charge, it can couple to both the weak bosons and the Glashow-Iliopoulos-Maiani (GIM) mechanism can suppress the unwanted interaction between the electron and the photon. This results in the cancellation of the tree-level Flavor Changing Neutral Currents (FCNCs) and the observed pattern of quark and lepton masses and mixing. By introducing a small explicit symmetry-breaking term, we can obtain a Higgs boson with suitably-vanished interactions with the electron and quark, while preserving the successful unification of the electroweak and strong forces described above. The full theory must also reproduce the correct results for nuclear physics and scattering processes. We present both the theoretical motivation and phenomenological aspects of this model, and demonstrate that it provides a well-motivated framework for the electroweak and flavor sectors of the standard model.",
        "watermark_text": "In the standard description ( SM ) of particle theory , the gauge fields of the electroweak force and the Higgs field underpin the particles and interactions we saw in our world . In particular , the weak force is mediated by the W and Z bosons , and the Higgs boson , yet the interactions of these ions are explained by a single coupling constant , coupling the strong , weak and electromagnetic forces together .The renormalization group ( RG ) has advised us that this single coupling constant may grow from a high energy value ofunity at high energy scales . By introducing an additional symmetry between the strong and electroweak forces , and a singlet scalar field under both , we can naturally make the coupling constant of the strong force a value less than that of the strong coupling constant , without the presence of any new atoms .This symmetry breaking is achieved through the vev of the Higgs field , which is what gets the molecules their masses . This scalar field , and the associated symmetry breaking , is dubbed the Higgs mechanism .Because the Higgs has no particular charge , it can couple to both the weak bosons and the Glashow - Iliopoulos - Maiani ( GIM ) process can suppress the unwanted interaction between the electron and the photon . This results in the canceled of the tree - level Flavor Changing Neutral Currents ( FCNCs ) and the seen pattern of quark and lepton masses and mixing .By introducing a small obvious symmetry - breaking term , we can obtain a Higgs boson with suitably - vanished interactions with the electron and quark , while preserving the efficient reconciliation of the electroweak and strong forces explained above . The full concept need also demonstrate the appropriate results for nuclear mechanics and scattering phenomena .We address both the theoretical motivation and phenomenological aspects of this model , and suggest that it gives a better - motivated template for the electroweak and flavor areas of the standard theory .",
        "rewrite_text": "In the Standard Model (SM) of particle theory, the gauge fields of the electroweak force and the Higgs field are fundamental to the particles and interactions observed in our world. Specifically, the weak force is mediated by W and Z bosons, as well as the Higgs boson. However, the interactions of these particles are explained by a single coupling constant that links the strong, weak, and electromagnetic forces together.\n\nThe renormalization group (RG) has indicated that this single coupling constant may increase from a high energy value of unity at larger energy scales. By introducing an additional symmetry between the strong and electroweak forces, along with a singlet scalar field under both, we can naturally adjust the coupling constant of the strong force to a value less than the strong coupling constant without introducing any new particles. This symmetry breaking occurs through the Higgs field's vacuum expectation value (vev), which gives molecules their masses. This scalar field and its associated symmetry breaking are known as the Higgs mechanism.\n\nAs the Higgs carries no specific charge, it can couple to both weak bosons. The Glashow-Iliopoulos-Maiani (GIM) process can suppress unwanted interactions between the electron and photon. This results in the cancellation of tree-level Flavor Changing Neutral Currents (FCNCs) and the observed pattern of quark and lepton masses and mixing. By introducing a slight symmetry-breaking term, we can obtain a Higgs boson with diminished interactions with the electron and quark while maintaining the efficient reconciliation of the electroweak and strong forces mentioned earlier.\n\nThe overall concept must also demonstrate appropriate results for nuclear mechanics and scattering phenomena. We address both the theoretical motivation and phenomenological aspects of this model, suggesting that it provides a more well-motivated template for the electroweak and flavor areas of the standard theory.",
        "ori-fast-z-score": -0.086710996952412,
        "water-fast-z-score": 5.63621480190678,
        "rewrite-fast-z-score": 2.477637207378607
    },
    {
        "original_text": "A relatively nearby elliptical galaxy Leo elliptical NGC 3379 hosts a low-mass satellite galaxy Leo IV. Spectroscopy of its satellites’ stars reveals that this satellite had a higher star formation rate in the past and had a more eccentric orbit than it has today. Simulations show that the growth of the main galaxy NGC 3379 can transform a low-mass satellite galaxy into a faint dwarf like Leo IV. This discovery demonstrates that even the low-mass satellite galaxies in massive halos can undergo transformation by their host galaxies, and a diffuse matter in the form of neutral hydrogen was found around this satellite galaxy with a highly eccentric orbit, which is an excellent example of the transformation of low-mass systems through interaction with their host. Leo elliptical NGC 3379 is a massive elliptical galaxy located at a distance of approximately 30 million light years from the Earth. It is a relatively nearby galaxy and its properties were well-studied. One of its satellites, Leo IV, was discovered using the Sloan Sky Digital Survey in 2015 and observations of its stars revealed that it had a more eccentric orbit and had a higher star formation rate in the past. Simulations showed that NGC 3379 can transform the properties of this low-mass satellite through interaction with it. In particular, Leo IV can evolve from a faint dwarf to a more massive system. This is evident by the detection of a large amount of neutral hydrogen around this satellite, which was also observed for the first time in 2015. This discovery demonstrates that even the low-mass satellite galaxies in massive halos can undergo transformation by their host galaxies.",
        "watermark_text": "A relatively nearby elliptical galaxy Leo elliptical NGC 3379 contains a small - mass satellite galaxy Leo IV . Spectroscopy of its satellites ’ stars suggests that this spacecraft had a higher star formation rate in the past and had a more eccentric trajectory than it has today .Simulations reveal that the development of the main galaxy NGC 3379 can convert a small - mass satellite galaxy into a dim dwarf like Leo IV . This research shows that even the small - mass satellite galaxies in massive halos can conduct change by their host galaxies , and a diffuse matter in the form of neutral hydrogen was seen around this satellite galaxy with a highly eccentric orbit , which is an excellent example of the transformation of lowest - mass systems through interaction with their host .Leo elliptical NGC 3379 is a huge elliptical galaxy located at a distance of circa 30 million light years from the Earth . It is a fairly nearby galaxy and its properties were well - examined .One of its satellites , Leo IV , was discovered using the Sloan Sky Digital Survey in 2015 and experiments of its stars showed that it had a more eccentric trajectory and had a higher star formation rate in the past . Simulations showed that NGC 3379 can convert the properties of this low - mass satellite through interaction with it .In particular , Leo IV can evolve from a faint dwarf to a more massive system . This is evident by the observation of a large number of neutral hydrogen around this satellite , which was also observed for the first time in 2015 .This discovery demonstrates that even the small - mass satellite galaxies in massive halos can exhibit transformation by their host galaxies .",
        "rewrite_text": "A nearby elliptical galaxy, Leo elliptical NGC 3379, harbors a small-mass satellite galaxy named Leo IV. Spectral analysis of its satellite stars indicates that this satellite experienced a higher rate of star formation in the past and had a more eccentric orbit than it has now. Simulations suggest that the development of the main galaxy, NGC 3379, can transform a small-mass satellite galaxy into a diminutive dwarf, resembling Leo IV. This research illustrates that even the smallest satellite galaxies in massive galactic halos can undergo transformations influenced by their host galaxies.\n\nFurthermore, a diffuse matter composed of neutral hydrogen has been observed surrounding this satellite galaxy with an extremely eccentric orbit, providing an excellent case study on the transformation",
        "ori-fast-z-score": 1.975658322294524,
        "water-fast-z-score": 7.123190113872715,
        "rewrite-fast-z-score": 1.1920791213585393
    },
    {
        "original_text": "In the curvaton scenario, the fluctuation of the scalar field determining the number density of dark matter contributes to the gravitational waves at the level of the parameter Omega_GW, which is related to the speed of the gravity during the period of inflation. However, if the potential of the scalar field has a lower bound, the contribution of the fluctuations of this field to the gravitational waves is not effective. In this work, we study the maximal amount of gravitational waves generated in the curvaton scenario, taking into account the effects of the effective speed of gravity during inflation. We find that the maximal amount of gravitational waves in the curvaton scenario is equal to the general relativity value, Omega_GW=1. We show that this result does not change even if the potential of the scalar field has an upper bound. In this work, we consider the case of two scalar fields: the real inflaton field and the complex curvaton field. The complex curvaton field determines the fluctuation of the number density of dark matter, while the real inflaton field determines the variation of the expansion rate of the universe. During the inflaton oscillation, the speed of gravity is nearly equal to the light speed, while it is slower than the light speed during the period of the complex inflaton field evolution. Thus, the fluctuation of the scalar field determining the number density of dark matter contributes to the gravitational waves with the parameter Omega_GW, which is related to the speed of the gravity. We find that the maximal amount of gravitational waves is equal to the general relativity value, Omega_GW=1. In addition, we study the perturbation of the gravitational wave during the sub-Hubble scales. We find that the perturbation of the gravitational wave is dependent on the effective speed of gravity during inflation. If the effective speed of gravity is equal to the light speed, then the amplitude of the perturbation of the gravitational wave is inversely proportional to the square of the effective speed of gravity. This result has an upper limit, which is 1/3 for the case of two-field models and 1/7 for the case of multi-field models. Finally, we discuss the gravitational wave background from the inflationary universe. If the effective speed of gravity is larger than the light speed, there are many modes with the scale of the Hubble radius at the time of inflation, which lead to a large gravitational wave background. In this case, it is difficult to explain the recent gravitational wave experiments such as B-mode experiment and LIGO/Virgo. However, if the effective speed of gravity is equal to the light speed, then we obtain the consistent gravitational wave background with the current value. We can realize a model with the maximal amount of gravitational waves by adopting the potential of the scalar field with a lower bound, which determines the maximal effective speed of gravity during inflation. In this case, it is easy to explain the observed value of the gravitational wave background, while the fluctuation of the",
        "watermark_text": "In the curvaton scenario , the fluctuation of the scalar field determining the number density of dark matter contributes to the gravitational waves at the level of the parameter Omega _ GW , which is related to the speed of the gravity during the period of inflation . However , if the potential of the scalar field has a smaller bound , the impact of the fluctuations of this field to the gravitational waves is not effective .In this research , we study the maximal amount of gravitational waves generated in the curvaton scenario , using into consideration the effects of the effective speed of gravity during inflation . We see that the maximal amount of gravitational waves in the curvaton scenario is equal to the general relativity value , Omega _ GW = 1 .We see that this consequence does not change even if the potential of the scalar field has an upper bound . In this research , we study the case of two scalar fields : the real inflaton field and the complex curvaton field .The complex curvaton field decides the fluctuation of the number density of dark matter , while the real inflaton field decides the variation of the expansion speed of the universe . During the inflaton oscillation , the speed of gravity is almost equal to the light speed , while it is slower than the light speed during the period of the complex inflaton field evolution .Thus , the fluctuation of the scalar field determining the number density of dark matter contributes to the gravitational waves with the parameter Omega _ GW , which is related to the speed of the gravity . We see that the maximal amount of gravitational waves is equal to the general relativity value , Omega _ GW = 1 .In addition , we study the perturbation of the gravitational wave during the sub - Hubble scales . We see that the perturbation of the gravitational wave is dependent on the effective speed of gravity during inflation .If the effective speed of gravitational is equal to the light speed , then the frequency of the perturbation of the gravitational wave is inversely proportional to the square of the effective speed of gravitational . This result has an upper maximum , which is 1 / 3 for the case of two - field models and 1 / 7 for the case of multi - field models .Finally , we explain the gravitational wave background from the inflationary universe . If the effective speed of gravity is bigger than the light speed , there are many modes with the scale of the Hubble diameter at the period of inflation , which lead to a large gravitational wave background .In this situation , it is harder to explain the recent gravity wave studies such as B - mode experiment and LIGO / Virgo . However , if the effective speed of gravitational is equal to the light speed , then we obtain the consistent gravitational wave background with the present value .We can realize a description with the maximal amount of gravitational waves by incorporating the potential of the scalar field with a smaller bound , which determines the maximal effective speed of gravitational during inflation . In this situation , it is easy to explain the observed value of the gravitational wave background , while the fluctuation of the",
        "rewrite_text": "In the context of the curvaton scenario, the fluctuations in the scalar field that determine the number density of dark matter contribute to the generation of gravitational waves at the parameter level of Ω_GW. This parameter is closely linked to the speed of gravity during the inflationary period. However, if the potential of the scalar field has a limited range, its impact on the generation of gravitational waves becomes less significant.\n\nOur research focuses on examining the maximum amount of gravitational waves in the curvaton scenario, taking into account the effects of the effective speed of gravity during inflation. We found that, in this scenario, the maximum amount of gravitational waves equals the value predicted by general relativity, with Ω_GW set at 1. This outcome remains unchanged even when the potential of the scalar field has an upper limit.\n\nIn our study, we examine two types of scalar fields: the real inflaton field and the complex curvaton field. The complex curvaton field determines the fluctuations in the number density of dark matter, while the real inflaton field governs the variation in the expansion rate of the universe. During the inflaton oscillation, the speed of gravity closely resembles the speed of light, while it is slower during the evolution of the complex inflaton field.\n\nThe fluctuations in the scalar field, which determine the number density of dark matter, contribute to the generation of gravitational waves with the parameter Ω_GW, which is related to the speed of gravity. We observe that the maximum amount of gravitational waves remains consistent with the general relativity value of Ω_GW equal to 1.\n\nAdditionally, we investigate the perturbations in gravitational waves during sub-Hubble scales. We found that these perturbations depend on the effective speed of gravity during inflation. If the effective speed of gravity is equivalent to the speed of light, then the frequency of these perturbations is inversely proportional to the square of the effective speed of gravity. This relationship has an upper limit, specifically 1/3 for two-field models and 1/7 for multi-field models.\n\nFinally, we explain the background of gravitational waves in an inflationary universe. When the effective speed of gravity exceeds the speed of light, there are numerous modes with a Hubble diameter scale during the inflationary period, leading to a significant gravitational wave background. In this scenario, it becomes challenging to reconcile recent gravity wave studies, such as B-mode experiments and LIGO/Virgo observations. However, if the effective speed of gravity is equal to the speed of light, we obtain a consistent gravitational wave background that aligns with current measurements.\n\nBy incorporating a scalar field potential with a limited range, which determines the maximum effective speed of gravity during inflation, we can achieve a description with maximal gravitational wave output. In this situation, it becomes easier to explain the observed value of the gravitational wave background, while accounting for the fluctuations in both fields.",
        "ori-fast-z-score": 2.157439559882375,
        "water-fast-z-score": 7.248485396311462,
        "rewrite-fast-z-score": 2.3841582427170787
    },
    {
        "original_text": "In this paper we present a measurement of the sensitivity of searches for anomalous Wtb couplings in the top quark decay channel at the LHC. We focus on 13 TeV data and update our previous measurements, presented at the Moriond EW session in 2018, with the full dataset collected so far in 2015. The results are interpreted in the context of simplified models with non-standard Wtb couplings, with the goal of testing the fundamental symmetries of the Wtb vertex, namely its hermiticity and gauge invariance. We find consistent limits with those obtained at the Moriond conference, and we update our results in the case of non-zero scalar and pseudoscalar couplings. The results presented in this paper are the most constraining to date for the vector and scalar interactions, and we exhibit the first limits on the Tensor couplings. We also present first limits on anomalous Wtb couplings involving the pseudovector interaction, which can arise in models beyond the standard model (BSM). We expect our results to be directly applicable to new phenomena characterized by these anomalous couplings, and we illustrate this point by considering the case of Minimal Flavour Violation (MFV).",
        "watermark_text": "In this paper we present a measurement of the sensitivity of investigations for anomalous Wtb couplings in the top quark decay channel at the LHC . We focus on 13 TeV results and update our previous measurements , published at the Moriond EW event in 2018 , with the full dataset collected so far in 2015 .The results are interpreted in the context of simplified theories with non - standard Wtb couplings , with the objective of testing the fundamental symmetries of the Wtb vertex , notably its hermiticity and gauge invariance . We get compatible limits with those acquired at the Moriond conference , and we update our findings in the case of non - zero scalar and pseudoscalar couplings .The results presented in this paper are the most constraining to date for the vector and scalar relationships , and we exhibit the first limitation on the Tensor couplings . We additionally offer first rules on anomalous Wtb couplings involving the pseudovector interaction , which can arise in theories beyond the standard model ( BSM ) .We believe our findings to be directly apply to new experiments defined by these anomalous couplings , and we highlight this point by examining the case of Minimal Flavour Violation ( MFV ) .",
        "rewrite_text": "In this study, we present an assessment of the sensitivity measurements for unusual Wtb couplings in the top quark decay channel at the Large Hadron Collider (LHC). Our focus lies on the results obtained at 13 TeV and updating our previous measurements, published at the Moriond EW event in 2018, using the complete dataset collected so far in 2015.\n\nOur findings are interpreted within the framework of simplified theories featuring non-standard Wtb couplings. This is aimed at testing the fundamental symmetries of the Wtb vertex, particularly its hermiticity and gauge invariance. We obtain limits that are compatible with those reported at the Moriond conference, and we have updated our observations in the case of non-zero scalar and pseudoscalar couplings.\n\nThe results presented in this paper are the most restrictive to date for vector and scalar relationships, and we have established the first limitations on Tensor couplings. Furthermore, we offer initial guidelines for anomalous Wtb couplings involving pseudovector interactions, which can arise in theories beyond the Standard Model (BSM).\n\nWe believe that our findings have direct implications for future experiments defined by these anomalous couplings, and we emphasize this point by examining the case of Minimal Flavour Violation (MFV).",
        "ori-fast-z-score": -1.3054598240132387,
        "water-fast-z-score": 3.916379472039716,
        "rewrite-fast-z-score": 0.9428090415820635
    },
    {
        "original_text": "A quantum information processor (QIP) consists of various physical qubits, which can be entangled and measured in order to perform a computation. While these operations are intended to be coherent, experimental errors will inevitably lead to a mixture of entangled states. We experimentally characterize this coherence loss via ultrafast interferometry and find that it is well-approximated by a Gaussian process. We explore these fluctuations as a function of parameters of the computation and device performance, and find that universal gates introduce more noise than coherence. Finally, we investigate the sensitivity of our results to choices in experimental protocol, and find that significant differences can arise due to random switching events during the time of our experiment. We present an experimentally testable framework for optimizing QIP coherence as a function of computation parameters, and showcase its use by optimizing the quantum Fourier transform. This coherence optimization can be turned into an automated procedure for QIP optimization, and we illustrate this application to a near-optimal input state for the QFT. As the complexity of QIPs grows with experimental progress, these techniques for coherence assessment and optimization will play an increasingly important role in the scaling of these devices.",
        "watermark_text": "A quantum information processor ( QIP ) consists of several physical qubits , which can be entangled and detected in order to conduct a computation . While these actions are intended to be coherent , experimental errors will inevitably lead to a mixture of entangled states .We experimentally characterize this coherence failure via ultrafast interferometry and find that it is well - approximated by a Gaussian process . We explore these fluctuations as a function of constraints of the computation and device performance , and find that universal gates introduce more noise than coherence .Finally , we investigate the sensitivity of our findings to options in experimental protocol , and find that significant variations can arise due to random switching events during the period of our experiment . We present an experimentally testable framework for optimizing QIP coherence as a function of computation parameters , and showcase its use by optimizing the particle Fourier integral .This coherence optimization can be turned into an automated procedure for QIP optimization , and we illustrate this application to a near - optimal input state for the QFT . As the complexity of QIPs rises with research progress , these tools for coherence assessment and optimization will take an increasingly important role in the scaling of these systems .",
        "rewrite_text": "A Quantum Information Processor (QIP) comprises multiple physical qubits that can be entangled and detected to perform computational tasks. These operations aim to maintain coherence, but experimental errors inevitably lead to a mix of entangled states. We employ ultrafast interferometry to experimentally assess this coherence failure and observe that it closely resembles a Gaussian process. We explore how these fluctuations vary with computational constraints and device performance, finding that universal gates introduce more noise than coherence.\n\nFurthermore, we investigate the sensitivity of our findings to various experimental protocols and discover that significant variations can arise due to random switching events during the experimental period. We present a testable framework for optimizing QIP coherence as a function of computational parameters, illustrating its use by optimizing the particle Fourier integral. This coherence optimization can be transformed into an automated process for QIP optimization, demonstrated on a nearly optimal input state for the Quantum Fourier Transform (QFT).\n\nWith the increasing complexity of QIPs in research, tools for assessing and optimizing coherence will play an increasingly crucial role in scaling these systems.",
        "ori-fast-z-score": 1.6876318513890358,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": 2.49100947511811
    },
    {
        "original_text": "Finite volume methods (FVMs) are widely used to solve Partial Differential Equations (PDEs) due to their robustness and efficiency. However, model parameters (e.g., reaction rates in reaction-diffusion equations) are often difficult to infer from FVM solutions, because there is a mismatch between solution fidelity and solution regularity required for accurate identification of model parameters. This prevents the direct use of FVM solutions in, for example, real-time monitoring or uncertainty quantification, and motivates the development of methods that exploit the information carried by solution noise. In this work we develop a statistically optimal approach to flow inversion, in which the model is inverted using a probabilistic framework in which the parameters are treated as random variables. To make the problem feasible, we make use of empirical Bayes (EB) to estimate model parameters from noisy FVM solutions. To further reduce the Bayesian inversion problem to a computational problem that is easily handled by modern large-scale computing systems, we develop a sparse formulation for the Bayesian inversion. We demonstrate the feasibility of the proposed approach with several one- and two-dimensional advection-diffusion-reaction problems. Finally, we show that the Bayesian inversion can effectively exploit knowledge of the initial conditions to estimate model parameters accurately.",
        "watermark_text": "Finite depth methods ( FVMs ) are widely using to solve Partial Differential Equations ( PDEs ) due to their robustness and efficiency . However , description parameters ( e . g . , process rates in reaction - diffusion equations ) are often impossible to infer from FVM solutions , because there is a mismatch between solution fidelity and solution regularity required for precise analysis of model variables .This reduces the direct use of FVM solutions in , for example , real - time measurement or risk quantification , and motivates the development of methods that exploit the information carried by solution noise . In this research we develop a statistically optimal approach to flow inversion , in which the model is inverted employing a probabilistic framework in which the variables are treated as random factors .To create the issue practical , we make using of empirical Bayes ( EB ) to estimate model variables from noisy FVM solutions . To further reduce the Bayesian inversion problem to a computational challenge that is well addressed by current huge - scale computing systems , we develop a sparse implementation for the Bayesian inversion .We suggest the feasibility of the suggested approach with many one - and two - dimensional advection - diffusion - reaction problems . Finally , we prove that the Bayesian inversion can effectively utilize knowledge of the first situations to estimate model variables correctly .",
        "rewrite_text": "Finite depth methods (FVMs) are commonly utilized to solve Partial Differential Equations (PDEs) due to their strong robustness and efficiency. Nevertheless, a challenge arises in that descriptive parameters, such as process rates in reaction-diffusion equations, are often not directly derivable from FVM solutions. This is often due to a mismatch between the solution's fidelity and the required regularity for precise analysis of model variables. This limitation hinders the direct application of FVM solutions in real-time measurements or risk quantification.\n\nTo address this, methods that harness the information contained in solution noise are crucial for further development. In this research, we propose a statistically optimal approach for flow inversion. This involves inverting the model within a probabilistic framework where variables are treated as random factors. To make this approach practical, we utilize Empirical Bayes (EB) to estimate model variables from noisy FVM solutions.\n\nTo further simplify the Bayesian inversion problem and make it more feasible with current high-scale computing systems, we develop a sparse implementation for the Bayesian inversion process. We demonstrate the feasibility of this approach with numerous one and two-dimensional advection-diffusion-reaction problems.\n\nFinally, we establish that Bayesian inversion can effectively utilize prior knowledge to accurately estimate model variables. This provides a strong foundation for the effective utilization of FVM solutions in various practical applications.",
        "ori-fast-z-score": -0.6,
        "water-fast-z-score": 6.262945776223707,
        "rewrite-fast-z-score": 1.3862065601673441
    },
    {
        "original_text": "The article considers the imprint of the distortions in the Oort Cloud on the cosmic microwave background (CMB) anisotropies. The approach is based on the idea that the Sun is located close to the boundary of the Oort cloud, where the galactic gravitational force starts to dominate. The article presents the analytic estimations of the temperature variations within the Planck units caused by the gravitational perturbations of the perturbers located in the Oort Cloud. The calculations were carried out for several models of the Oort Cloud structure derived in the recent years. The calculations show that the perturbers in the Oort Cloud could produce the anisotropies of the CMB temperature of the degree that are observed in the CMB radiation. The article shows that the method of detecting the signals from the Oort Cloud proposed in the article could be used to test the models of the Oort Cloud structure and to estimate some parameters of the galactic potential.",
        "watermark_text": "The section considers the imprint of the distortions in the Oort Cloud on the cosmic microwave background ( CMB ) anisotropies . The method is based on the idea that the Sun is situated close to the boundary of the Oort cloud , where the galactic gravitational field begins to dominate .The section offers the analytic estimations of the temperature variations within the Planck units resulting by the gravitational perturbations of the perturbers located in the Oort Cloud . The calculations were carried out for numerous models of the Oort Cloud structure derived in the recent years .The studies demonstrate that the perturbers in the Oort Cloud could generate the anisotropies of the CMB heat of the degree that are observed in the CMB radiation . The section demonstrates that the method of detecting the transmissions from the Oort Cloud suggested in the article might be used to test the models of the Oort Cloud structure and to estimate some parameters of the galactic potential .",
        "rewrite_text": "This section explores the impact of distortions in the Oort Cloud on the anisotropies of the cosmic microwave background (CMB). The approach is founded on the notion that the Sun is positioned near the boundary of the Oort cloud, where the gravitational field of the galaxy begins to prevail. The section provides analytical estimations of temperature variations within Planck units, resulting from gravitational perturbations caused by objects located in the Oort Cloud. These calculations have been conducted using various models of the Oort Cloud structure developed in recent years. Studies indicate that the objects in the Oort Cloud could generate CMB heat anisotropies comparable to the ones observed in CMB radiation. Furthermore, this section demonstrates that the method proposed in the article for detecting transmissions from the Oort Cloud can be utilized to test models of its structure and to estimate certain parameters of the galactic potential.",
        "ori-fast-z-score": 1.7529196424044293,
        "water-fast-z-score": 6.067798762169179,
        "rewrite-fast-z-score": 2.32379000772445
    },
    {
        "original_text": "Searching for photos on Flickr is a popular activity, and the photo search results can be sorted by most liked, latest added, and most viewed. In this work, we use a neural network to predict the click-through-rate (CTR) of each photo, and use the predicted CTRs to re-rank the photo search results. The results show that using predicted CTRs rather than the original photo metadata can boost the query-independent photo search performance. Our model can be easily trained on the publicly available Flickr dataset, and can be readily deployed to other photo search platforms. Overall, this work shows that using deep learning to model the photo search ranking can be a promising approach to improve the photo search experience. Yingzhe Yan, Quanquan Nie, Ping Zhou, Liming Liu Link to the paper: https://arxiv.org/pdf/1710.10455.pdf",
        "watermark_text": "Searching for photos on Flickr is a popular activity , and the photograph search results can be sorted by most liked , latest added , and most viewed . In this project , we utilize a neural network to predict the click - through - speed ( CTR ) of each photograph , and use the expected CTRs to re - rank the photograph search results .The results show that using predicted CTRs rather than the actual photograph metadata can boost the query - independent photograph search efficiency . Our model can be easily trained on the publicly accessible Flickr dataset , and can be freely implemented to other photo search platforms .Overall , this research shows that using deep knowledge to model the photograph search ranking can be a successful approach to improve the photograph search process . Yingzhe Yan , Quanquan Nie , Ping Zhou , Liming Liu Link to the paper : https : / / arxiv . org / pdf / 1710 . 10455 . pdf",
        "rewrite_text": "Searching for photographs on Flickr has become a widespread practice, with the ability to sort the search results based on factors such as the most liked, latest additions, and highest viewed counts. In this project, we have employed a neural network to predict the click-through-rate (CTR) for each image, and then used these predicted CTRs to re-rank the search results. The results indicate that utilizing predicted CTRs instead of the actual image metadata can significantly enhance the efficiency of query-independent image searches. Our model can be effortlessly trained on the publicly accessible Flickr dataset and can be easily implemented on other photo search platforms. In conclusion, this research demonstrates that utilizing deep learning to model image search rankings can be a highly effective method to improve the image search process. For further details, please refer to the paper linked here: https://arxiv.org/pdf/1710.10455.pdf by Yingzhe Yan, Quanquan Nie, Ping Zhou, and Liming Liu.",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 3.8450767722654624,
        "rewrite-fast-z-score": -0.11547005383792514
    },
    {
        "original_text": "Understanding the origin of ferromagnetism is one of the key issues in materials science. In this paper, we report an unexpected ferromagnetic (FM) behavior in a non-magnetic material, TiO2. When Ti atoms are removed from the TiO2 crystal lattice (specifically the Ti4+ sites), the crystal structure evolves to anatase type and acquires a net magnetic moment, which is found to be persistent above room temperature. This Ti4+ disorder induced magnetism (TDIM) has been observed in a number of materials such as TiO2 nanoparticles, TiO2 thin films and nanotubes. We hope that our work will stimulate more research activities in this field and motivate further experimental and theoretical investigations on this interesting problem. In the supplementary material, authors have provided more experimental data and also some hints for the theoretical explanation of TDIM. First-principles calculations show that Ti4+ disorder, on substitution of O2- anions for TiO2- anatase, leads to a net ferromagnetic moment. In addition, our experimental studies indicate the persistence of this moment up to at least 300 K. These observations are in contrast to previous reports of anatase type TiO2 being nonmagnetic. Various mechanisms for the persistence of this net moment are explored. While substantial ionic coupling in Ti4+ sites is necessary to explain the observed moment, substantial spin-orbit coupling is required to explain the observed moment value. The theoretically predicted ordering temperature of this moment is consistent with experimental results. Our work demonstrates that Ti4+ disorder is an overlooked mechanism to induce ferromagnetism in nonmagnetic materials. We have provided possible evidences to support the hypothesis that Ti4+ disorder accounts for the persistent ferromagnetism observed in a number of materials. Since Ti4+ disorder has been observed in a number of anatase type TiO2 samples, our work raises the possibility of persistent ferromagnetism in TiO2 based materials and related systems. This study was motivated by the recent report of persistent ferromagnetism in Ti4+-disordered TiO2. 1  While much research has been devoted to understand the origin of ferromagnetism in metals, understanding ferromagnetism in nonmagnetic materials is still an active area of research. TiO2, which is used as a semiconductor in many devices, is nonmagnetic. However, recent work has demonstrated that certain forms of TiO2 can be ferromagnetic. 2,3  Since ferromagnetism has been observed in a number of Ti4+-disordered materials, understanding the origin of ferromagnetism in TiO2 is of both fundamental and applied interest. This work explores the Ti4+-disorder induced ferromagnetism (TDIM) mechanism in TiO2 anatase. To explore the TDIM mechanism in TiO2, we synthesized TiO2 nanoparticles with a wide size distribution using a sol gel technique. We explored three distinct synthesis methods to control the amount of Ti4+ disorder",
        "watermark_text": "Understanding the origin of ferromagnetism is one of the key concerns in materials science . In this paper , we study an unexpected ferromagnetic ( FM ) behavior in a non - magnetic material , TiO2 .When Ti atoms are removed from the TiO2 crystal lattice ( specifically the Ti4 + sites ) , the crystal composition evolves to anatase kind and acquires a net magnetic point , which is found to be persistent above room temperature . This Ti4 + disorder induced magnetism ( TDIM ) has been observed in a number of structures such as TiO2 nanoparticles , TiO2 thin films and nanotubes .We believe that our work will stimulate more research efforts in this area and motivate further experimental and theoretical investigations on this interesting question . In the supplementary material , authors have provided more experimental evidence and also some indication for the theoretical solution of TDIM .First - principles measurements show that Ti4 + disorder , on reduction of O2 - anions for TiO2 - anatase , leads to a net ferromagnetic moment . In addition , our research studies confirm the persistence of this moment up to at least 300 K . These measurements are in comparison to previous accounts of anatase class TiO2 being nonmagnetic .Various mechanisms for the persistence of this net moment are researched . While substantial ionic coupling in Ti4 + sites is required to explain the observed moment , substantial spin - orbit coupling is required to explain the observed moment value .The theoretically expected ordering temperature of this moment is compatible with experimental results . Our research proves that Ti4 + disorder is an neglected mechanism to effect ferromagnetism in nonmagnetic surfaces .We have provided proposed evidences to support the idea that Ti4 + disorder accounts for the chronic ferromagnetism observed in a number of substances . Since Ti4 + disorder has been observed in a number of anatase class TiO2 specimen , our work raises the idea of recurring ferromagnetism in TiO2 derived ceramics and related systems .This study was influenced by the recent study of persistent ferromagnetism in Ti4 + - disordered TiO2 . 1 While much studies has been focused to study the origin of ferromagnetism in metals , studying ferromagnetism in nonmagnetic compounds is remains an active area of research .TiO2 , which is utilized as a semiconductor in many systems , is nonmagnetic . However , recent work has proven that various forms of TiO2 can be ferromagnetic .2 , 3 Since ferromagnetism has been observed in a number of Ti4 + - disordered materials , studying the origin of ferromagnetism in TiO2 is of both basic and application importance . This research examines the Ti4 + - disorder induced ferromagnetism ( TDIM ) pathway in TiO2 anatase .To explore the TDIM mechanism in TiO2 , we synthesized TiO2 nanoparticles with a broad size distribution using a sol gel method . We explored three different synthesis techniques to affect the extent of Ti4 + disorder",
        "rewrite_text": "Understanding the origins of ferromagnetism is a crucial aspect in materials science. In this research paper, we investigate an unusual ferromagnetic (FM) behavior discovered in the non-magnetic material TiO2. When Ti atoms are removed from the TiO2 crystal lattice, specifically at the Ti4+ sites, the crystal composition evolves into anatase and acquires a persistent net magnetic moment even above room temperature. This Ti4+-induced disorder magnetism (TDIM) has been observed in various structures like TiO2 nanoparticles, thin films, and nanotubes.\n\nOur work is expected to motivate further research efforts in this area and encourage both experimental and theoretical investigations into this intriguing phenomenon. In the supplementary materials, we provide additional experimental evidence and theoretical insights into the TDIM phenomenon. First-principles measurements reveal that Ti4+ disorder, resulting from the reduction of O2- anions in TiO2 anatase, leads to a net ferromagnetic moment. Furthermore, our research confirms the persistence of this moment up to at least 300K, contrasting with previous accounts of anatase class TiO2 being non-magnetic.\n\nVarious mechanisms are being explored to explain the persistence of this net magnetic moment. While significant ionic coupling in Ti4+ sites is necessary to explain the observed moment, substantial spin-orbit coupling is required to explain the observed moment value. The theoretically expected ordering temperature of this moment aligns with experimental results. Our research establishes that Ti4+ disorder is a previously overlooked mechanism influencing ferromagnetism in non-magnetic surfaces.\n\nWe offer proposed evidence to support the idea that Ti4+ disorder accounts for the consistent ferromagnetism observed in numerous substances. Given that Ti4+ disorder has been observed in various anatase class TiO2 specimens, our work raises the possibility of recurrent ferromagnetism in TiO2 derived ceramics and related systems. This study is influenced by recent research on the persistent ferromagnetism observed in Ti4+-disordered TiO2.\n\nWhile much research has focused on studying the origins of ferromagnetism in metals, studying ferromagnetism in non-magnetic compounds remains an active area of investigation. TiO2, while commonly used as a semiconductor in various systems, is typically non-magnetic. However, recent studies have demonstrated that various forms of TiO2 can exhibit ferromagnetic properties. Since ferromagnetism has been observed in numerous Ti4+-disordered materials, exploring the origins of ferromagnetism in TiO2 holds both fundamental and practical importance.\n\nIn this research, we examine the pathway of Ti4+-disorder-induced ferromagnetism (TDIM) in TiO2 anatase. To delve into the TDIM mechanism in TiO2, we synthesized TiO2 nanoparticles with a broad size distribution using a sol-gel method. We explored three different synthesis techniques to manipulate the extent of Ti4+ disorder and gain further insights into its influence on ferromagnetic properties.",
        "ori-fast-z-score": 0.95577900872195,
        "water-fast-z-score": 10.51356909594145,
        "rewrite-fast-z-score": 4.900980294098034
    },
    {
        "original_text": "The late-M multiple system LHS 1070 is a benchmark for studies of stellar magnetism due to its strong and unique magnetic field. We report the results of our discovery spectroscopy of this system with the WIYN 3.5-m telescope and subsequent Doppler imaging analysis of the components  stable and unusual abundance of elements other than hydrogen and helium. We find that its moderately fast rotator, LHS 1070 B, has a strong, stable magnetic field that dominates its atmosphere and slows its rotation at a rate consistent with rigid body rotation. In contrast, the even faster rotating components, LHS 1070 C and D, have much weaker magnetic fields and faster rotation rates than expected for their respective spectral types, consistent with previously reported magnetic braking acting on their surfaces. The rapidly spinning LHS 1070 C has a field that is only slightly stronger than that of a typical late-K star, while the nearly non-rotating LHS 1070 D has no detectable magnetic field. We conclude that LHS 1070 C and D may be the first example of a close binary where the more rapidly spinning component has strong enough magnetic braking to significantly slow its spin rate below that of rigid body rotation. We further speculate that the strong magnetic field of LHS 1070 B may be a clue to its unusual chemical composition, which is rich in elements other than hydrogen and helium.",
        "watermark_text": "The late - M multiple system LHS 1070 is a benchmark for research of stellar magnetism owing to its strong and distinctive magnetic force . We report the conclusion of our discovery spectroscopy of this system with the WIYN 3 . 5 - m observatory and subsequent Doppler imaging analysis of the parts stable and peculiar abundance of elements other than hydrogen and helium .We see that its relatively rapid rotator , LHS 1070 B , has a powerful , stable magnetic force that dominates its atmosphere and slows its rotation at a rate compatible with rigid body rotation . In comparison , the also faster rotating components , LHS 1070 C and D , have far lower magnetic fields and better rotation rates than expected for their respective spectral classes , consistent with previously reported gravitational braking acts on their surfaces .The fast spinning LHS 1070 C has a field that is only somewhat brighter than that of a typical mid - K star , while the nearly un - spinning LHS 1070 D has no detectable magnetic field . We suggest that LHS 1070 C and D could be the first instance of a close binary where the more slowly spinning component has strong enough magnetic braking to significantly accelerate its spin speed below that of rigid body rotation .We further speculate that the strong magnetic force of LHS 1070 B may be a clue to its unusual molecular composition , which is abundant in compounds other than hydrogen and helium .",
        "rewrite_text": "The late-M multiple system LHS 1070 serves as a pivotal reference point for studying stellar magnetism due to its exceptional and powerful magnetic force. We present the findings of our discovery spectroscopy conducted at the WIYN 3.5-m observatory, followed by a Doppler imaging analysis of the stable components and the unique abundance of elements other than hydrogen and helium. It is evident that LHS 1070 B, a relatively rapid rotator, possesses a robust and steady magnetic force that dominates its atmosphere and slows its rotation rate, aligning with rigid body rotation. In contrast, the faster-rotating components, LHS 1070 C and D, exhibit significantly weaker magnetic fields and better rotation rates than expected for their respective spectral classes. This is consistent with previously reported gravitational braking effects on their surfaces. Specifically, LHS 1070 C, with its rapid spin, exhibits a field only slightly brighter than that of a typical mid-K star, while the nearly stationary LHS 1070 D exhibits no detectable magnetic field. We suggest that LHS 1070 C and D may be the first example of a close binary where the slower-spinning component experiences strong magnetic braking, significantly accelerating its spin speed below rigid body rotation. Furthermore, we speculate that the intense magnetic force of LHS 1070 B could be a clue to its unusual molecular composition, rich in compounds beyond hydrogen and helium.",
        "ori-fast-z-score": -2.2662573397778742,
        "water-fast-z-score": 5.025179318637895,
        "rewrite-fast-z-score": 1.2451741707874968
    },
    {
        "original_text": "Using the Submillimeter Array, we have mapped the H$_2$D$^+$ $J = 1 - 0$ emission from a sample of 14 Class 0 and I protostars with high angular resolution (0.4-2.3 au at the median distance of our sample; i.e., comparable to the disk radii). We detect emission from ten sources and present observations of two new prestellar cores where we detect H$_2$D$^+$ emission for the first time. The emission appears in elongated structures with large deconvolved widths, indicating that the H$_2$D$^+$ is arising in shock-heated gas. We detect H$_2$D$^+$ emission from both regions that drive powerful molecular outflows and from more quiescent environments. The outflow-enriched sources tend to have brighter H$_2$D$^+$ emission and higher molecular depletion factors, suggesting that shocks heat and destroy H$_2$D$^+$ more efficiently in those environments. In contrast, the envelope-only source HH212 has weak H$_2$D$^+$ emission despite having an edge-on disk with substantial mass. We present two possible explanations for this discrepancy: (i) the quiescent core from which HH212 formed had low amounts of presolar material; or (ii) the H$_2$D$^+$ emission in HH212 traces a vertically extended torus rather than a warped or disky disk. Future observations of H$_2$D$^+$ $J = 1 - 0$ and transitions with ALMA and the Atacama Large Millimeter/submillimeter Array will test these and other hypotheses by constraining the H$_2$D$^+$ excitation and abundance, and tracing different velocity components in the outflow-enriched sources.",
        "watermark_text": "Using the Submillimeter Array , we have map the H $ _ 2 $ D $ ^ + $ $ J = 1 - 0 $ radiation from a sample of 14 Class 0 and I protostars with high angular resolution ( 0 . 4 - 2 . 3 au at the average distance of our sample ; i . e . , comparable to the disk radii ) . We detect emission from ten sources and include discovery of two new prestellar cores where we perceive H $ _ 2 $ D $ ^ + $ radiation for the first time .The emission appears in elongated structures with large deconvolved widths , showing that the H $ _ 2 $ D $ ^ + $ is arising in shock - heated gas . We detect H $ _ 2 $ D $ ^ + $ emitted from both locations that drive powerful molecular outflows and from more quiescent environments .The outflow - enriched sources tend to have brighter H $ _ 2 $ D $ ^ + $ emission and larger chemical depletion factors , showing that shocks heat and destroy H $ _ 2 $ D $ ^ + $ more efficiently in those conditions . In comparison , the envelope - only source HH212 has weak H $ _ 2 $ D $ ^ + $ emission despite having an edge - on disk with substantial mass .We present two possible explanations for this discrepancy : ( i ) the quiescent core from which HH212 formed had small concentrations of presolar material ; or ( ii ) the H $ _ 2 $ D $ ^ + $ radiation in HH212 traces a horizontally expanding torus instead than a warped or disky disk . Future discoveries of H $ _ 2 $ D $ ^ + $ $ J = 1 - 0 $ and transitions with ALMA and the Atacama Large Millimeter / submillimeter Array will question these and other hypotheses by constraining the H $ _ 2 $ D $ ^ + $ excitation and abundance , and examining different speed elements in the outflow - enriched sources .",
        "rewrite_text": "Using the Submillimeter Array, we have precisely mapped the H2D+ J=1-0 radiation from a sample of 14 Class 0 and I protostars with high angular resolution (equivalent to 0.4 - 2.3 AU at the average distance of our sample, comparable to disk radii). We have detected emissions from ten of these sources and discovered two new prestellar cores where H2D+ radiation has been observed for the first time. The emission appears in extended structures with large deconvolved widths, indicating that the H2D+ is arising in shock-heated gas.\n\nWe have detected H2D+ emissions coming from both locations that drive powerful molecular outflows and more quiescent environments. Sources enriched with outflows tend to have brighter H2D+ emissions and larger chemical depletion factors, suggesting that shocks heat and destroy H2D+ more efficiently in those conditions. In contrast, the HH212 source (which is only an envelope) shows weak H2D+ emission despite having a substantial mass with an edge-on disk. We offer two potential explanations for this discrepancy: (i) the quiescent core from which HH212 formed had low concentrations of presolar material; or (ii) the H2D+ radiation in HH212 traces a horizontally expanding torus rather than a warped or disky disk.\n\nFuture observations of H2D+ J=1-0 and other transitions with ALMA and the Atacama Large Millimeter/submillimeter Array will further test these hypotheses by constraining H2D+ excitation and abundance, as well as examining different velocity components in outflow-enriched sources.",
        "ori-fast-z-score": 0.3216337604513384,
        "water-fast-z-score": 5.391638660171921,
        "rewrite-fast-z-score": 1.0660035817780522
    },
    {
        "original_text": "We develop a polymer quantum mechanics (PQM) where the fundamental variables are continuous twists on a non-compact group, whose Lie algebra is an infinite-dimensional Hilbert space. The Hilbert space of the quantum polymer is the space of sections of a vector bundle over the group with the standard fiber at the unit element, whose Lie algebra is identified with the abovementioned one. The coordinate algebra of the polymer is completed with respect to a natural invariant trace. This allows us to construct an invariant Schrödinger equation on the entire space of continuous twists. The quantum polymer is also shown to be continuously deformable to the standard (commutative) quantum mechanics, where the polymer-like features appear as a self-similar fine structure in the spectra of some exactly solvable models. We also show that the polymer quantum mechanics can be approximated by a corresponding sequence of ordinary quantum mechanics for increasingly large degrees of polymerization, with errors vanishing in the limit. The limit transition can be performed in two steps. In the first step, the continuous twists are replaced by a projective limit of finite-dimensional matrix groups with the normal subgroup that keeps only the diagonal elements and the corresponding subalgebras. In the second step, these matrix groups are replaced by their projective limits, which are the compact Lie groups. Finally, we show that the strong operator limit of the corresponding sequence of ordinary quantum mechanics corresponds to the commutative limit of the polymer quantum mechanics.",
        "watermark_text": "We develop a polymer quantum mechanics ( PQM ) where the fundamental variables are continuous loops on a non - compact group , whose Lie algebra is an infinite - dimensional Hilbert space . The Hilbert space of the quantum polymer is the space of parts of a matrix bundle over the group with the standard fiber at the unit element , whose Lie algebra is identified with the abovementioned one .The coordinate algebra of the polymer is completed with regard to a natural invariant trace . This enables us to build an invariant Schrödinger equation on the entire space of smooth twists .The quantum polymer is also shown to be continuously deformable to the standard ( commutative ) quantum mechanics , where the polymer - like structures appear as a self - similar coarse structure in the spectra of some precisely solvable models . We additionally find that the polymer quantum mechanics can be approximated by a corresponding sequence of normal quantum mechanics for increasingly large degrees of polymerization , with mistakes vanishing in the limit .The limit transition can be performed in two stages . In the first step , the smooth turns are replaced by a projective limit of finite - dimensional matrix groups with the normal subset that keeps only the diagonal elements and the associated subalgebras .In the second step , these matrix bands are replaced by their projective bounds , which are the compact Lie groups . Finally , we find that the strong operator limit of the resulting sequence of regular quantum mechanics corresponds to the commutative limit of the polymer quantum mechanics .",
        "rewrite_text": "We have formulated a theory of Polymer Quantum Mechanics (PQM), in which the fundamental variables are represented by continuous loops on a non-compact group. The Lie algebra of this group constitutes an infinitely dimensional Hilbert space. The Hilbert space for the quantum polymer is defined as the space of matrix bundles parts over the group, with a standard fiber at the unit element, whose Lie algebra aligns with the previously mentioned one. The coordinate algebra of the polymer is completed with a natural invariant trace, enabling us to construct an invariant Schrödinger equation for the entire space of smooth twists.\n\nFurthermore, it has been demonstrated that the quantum polymer is continuously deformable to standard (commutative) quantum mechanics. In this context, polymer-like structures emerge as a self-similar coarse structure in the spectra of precisely solvable models. We have also discovered that polymer quantum mechanics can be approximated by a sequence of normal quantum mechanics for increasing degrees of polymerization, with errors vanishing in the limit. This limit transition can be achieved in two stages.\n\nIn the first stage, smooth turns are substituted by a projective limit of finite-dimensional matrix groups, retaining only the diagonal elements and their associated subalgebras. In the second stage, these matrix bands are replaced by their projective bounds, which are the compact Lie groups. Ultimately, we found that the strong operator limit of this sequence of regular quantum mechanics corresponds to the commutative limit of the polymer quantum mechanics.",
        "ori-fast-z-score": 0.9128709291752769,
        "water-fast-z-score": 5.11207720338155,
        "rewrite-fast-z-score": 1.7873696499288347
    },
    {
        "original_text": "The form of the initial mass function (IMF) has been the subject of intense debate over the past few decades. While a power law was initially assumed for the IMF, it is now clear that most, if not all, star-forming regions have a modified version of the Initial Mass Function, i.e., a log-normal or more generally a function with a higher-than-exponential density at low masses. The universality of the IMF is an open question. It remains to be seen whether the IMF varies from region to region and, if so, whether this variation is related to the parameters of the region, its gravitational potential, the strength of the local turbulence, or some other aspect. Here we report the discovery that IMFs of individual clusters in the Orion Molecular Cloud 2 exhibit a power-law form with an exponent consistent with the standard value of 0.7. This result suggests that the IMF is indeed universal within the observational uncertainties. However, a full understanding of the origin of the IMF will require establishing the degree of IMF variation both within and between star-forming regions.",
        "watermark_text": "The form of the initial mass function ( IMF ) has been the subject of ongoing discussion over the previous few years . While a power law was initially assumed for the IMF , it is now clear that most , if not all , star - creating areas have a altered version of the Initial Mass Function , i . e . , a log - normal or more generally a function with a higher - than - exponential density at low masses .The universality of the IMF is an open question . It remains to be viewed whether the IMF varies from area to region and , if so , whether this varying is related to the variables of the region , its gravitational potential , the strength of the local turbulence , or some other aspect .Here we document the discovery that IMFs of individual clusters in the Orion Molecular Cloud 2 exhibit a power - law form with an exponent compatible with the standard value of 0 . 7 . This result suggests that the IMF is indeed universal within the observational uncertainties .However , a complete understanding of the origin of the IMF will include determining the degree of IMF distribution both within and between star - creating areas .",
        "rewrite_text": "Over the past few years, the shape of the Initial Mass Function (IMF) has been a continuously debated topic. While a power law was originally proposed for the IMF, it is now evident that the majority, if not all, star-forming regions exhibit a modified version of the IMF. This may be a log-normal function or more generally, a function with a higher-than-exponential density at low masses. The universality of the IMF remains an open question. It remains to be seen whether the IMF varies between different regions and, if so, whether these variations are linked to regional variables such as gravitational potential, local turbulence strength, or other factors.\n\nIn this study, we document the discovery that the IMFs of individual clusters within the Orion Molecular Cloud 2 follow a power-law shape with an exponent consistent with the standard value of 0.7. This finding suggests that the IMF may be universal within the limits of observational uncertainty. However, a comprehensive understanding of the origin of the IMF will require determining the extent of IMF distribution both within and between star-forming areas.",
        "ori-fast-z-score": 0.9701425001453319,
        "water-fast-z-score": 5.417363388859614,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "Using data from Advanced LIGO’s first two observing runs (O1 and O2), we search for signals from intermediate mass ratio inspirals (S2 IMSs). Such signals would carry unique imprints about the dense stellar cores of their hosts, enabling us to uniquely determine the stellar structure and distance. We place upper limits on the rate of such signals as a function of mass and strain. We further place lower bounds on the radius and distance to the putative host stars. We discuss the implications for the radius problem and the implications for planetary systems around intermediate mass stars. We use data from Advanced LIGO’s first two observing runs (O1 and O2) to search for signals from intermediate mass ratio inspirals (S2 IMSs). Intermediate mass ratio inspirals are a distant population of stars whose signals would carry unique imprints about the dense stellar cores of their hosts. Using these signals to measure the structure of the host stars and their distances would enable us to answer longstanding questions about the radius problem and the existence of planets orbiting intermediate mass stars. We place upper limits on the rate of such signals as a function of mass and strain, and we discuss the implications for the radius problem and the existence of planets around such stars. We searched for signals from S2 IMSs, with a modelled expectation of 1 S2 IMS per 4-6 weeks at distances of 15-45 parsecs. We placed 95% confidence level upper limits on the rate of these signals as a function of mass and strain. Assuming a population of IMSs with uniform distribution in logarithm of mass, we can use these rates to derive lower bounds on the radii of the putative hosts. We discuss the implications for the radius problem and the existence of planets around intermediate mass stars. For example, at a distance of 15 parsecs, we can place a lower bound on the radius of the host star of roughly 4.2 R⊕ for a 1.4 M⊕ S2 IMS and 6.3 R⊕ for a 1.4 M⊕ S2 IMS. Assuming a log-uniform distribution of IMSs, we can use these rates to derive upper bounds on the number of planets with a radius of R⊕ orbiting intermediate mass stars. We also search for signals from systems with nearly equal mass companions. These can be used to measure the spin of the companion star and thus test general relativity. We discuss the implications of these results for the radius problem and the existence of planets around intermediate mass stars, and we show how these results can be used to test general relativity and determine the spin of companion stars.",
        "watermark_text": "Using results from Advanced LIGO ’ s first two observing walks ( O1 and O2 ) , we search for transmissions from intermediate mass ratio inspirals ( S2 IMSs ) . Such signals might carry distinct imprints about the dense stellar cores of their hosts , allowing us to uniquely predict the stars shape and altitude .We set higher restrictions on the frequency of such signals as a function of mass and strain . We further place lower bounds on the radius and distance to the putative host stars .We discuss the implications for the radius issue and the implications for planetary networks around intermediate mass stars . We use data from Advanced LIGO ’ s early two observing walks ( O1 and O2 ) to search for messages from intermediate mass ratio inspirals ( S2 IMSs ) .Intermediate mass ratio inspirals are a distant population of stars whose signals might carry distinct imprints about the dense stellar cores of their hosts . Using these signals to measure the composition of the host stars and their distances may allow us to ask longstanding issues about the radius issue and the existence of planets orbiting intermediate mass stars .We set higher restrictions on the frequency of such signals as a function of mass and strain , and we investigate the implications for the radius issue and the existence of planets around such planets . We searched for transmissions from S2 IMSs , with a modelled estimate of 1 S2 IMS per 4 - 6 hours at distances of 15 - 45 parsecs .We established 95 % confidence rate upper limits on the rate of these transmissions as a function of mass and strain . Assuming a population of IMSs with uniform distribution in logarithm of mass , we can using these rates to derive smaller bounds on the radii of the putative hosts .We discuss the implications for the radius issue and the existence of planets around intermediate mass stars . For instance , at a distance of 15 parsecs , we can place a smaller bound on the radius of the host star of roughly 4 . 2 R⊕ for a 1 . 4 M⊕ S2 IMS and 6 . 3 R⊕ for a 1 . 4 M⊕ S2 IMS .Assuming a log - uniform distribution of IMSs , we can using these rates to derive upper limits on the number of planets with a diameter of R⊕ orbiting intermediate mass stars . We additionally look for transmissions from systems with nearly identical mass companions .These can be used to measure the spin of the companion star and therefore test general relativity . We discuss the implications of these results for the radius issue and the existence of planets around intermediate mass stars , and we explain how these results can be used to test special relativity and establish the spin of companion stars .",
        "rewrite_text": "Using data from the first two observation runs of Advanced LIGO (O1 and O2), we conducted a search for transmissions from intermediate mass ratio inspirals (S2 IMSs). These signals may contain unique imprints about the dense stellar cores of their host galaxies, enabling us to predict unique features such as the shape and altitude of the stars. We set stringent limits on the frequency of these signals based on their mass and strain. Furthermore, we established lower bounds on the radius and distance to the potential host stars.\n\nWe explore the implications of our findings for the radius issue and for planetary systems around intermediate-mass stars. By utilizing data from Advanced LIGO's early observing runs (O1 and O2), we searched for messages sent by S2 IMSs, which are a distant population of stars. These signals may provide valuable information about the dense cores of their host galaxies. By analyzing these signals, we can estimate the composition of host stars and their distances, potentially addressing longstanding questions about the radius issue and the existence of planets orbiting intermediate-mass stars.\n\nWe have set higher standards for signal frequency based on mass and strain, and we are examining the consequences for the radius issue and planet existence. We have conducted a search for S2 IMS transmissions with an estimated occurrence rate of one S2 IMS per 4-6 hours at distances ranging from 15 to 45 parsecs. We have established 95% confidence level upper limits on the frequency of these transmissions as a function of mass and strain.\n\nAssuming a population of IMSs with a uniform distribution in logarithmic mass, we can use these rates to derive lower bounds on the radii of potential host stars. For instance, at a distance of 15 parsecs, we can place a smaller bound on the radius of a host star, estimated to be approximately 4.2 times the radius of Earth for a 1.4 M⊕ S2 IMS. Additionally, we have considered the implications of our findings for the existence of planets around intermediate-mass stars. By assuming a log-uniform distribution of IMSs, we can use these rates to establish upper limits on the number of planets with a diameter comparable to Earth's that orbit intermediate-mass stars.\n\nMoreover, we have looked for transmissions from systems with nearly identical mass companions. These companions can be used to measure their spin, thus testing the principles of general relativity. We have discussed the significance of our findings for the radius issue and planet existence around intermediate-mass stars. We have also explained how our results can be utilized to test special relativity and determine the spin of companion stars.",
        "ori-fast-z-score": 0.5685352436149612,
        "water-fast-z-score": 7.533091977898236,
        "rewrite-fast-z-score": 1.5909432668919716
    },
    {
        "original_text": "Single-walled carbon nanotube forests with thicknesses up to a millimeter were grown by chemical vapor deposition using ferrocene and dioctylsulfoxide as catalyst and carbon source, respectively. The forests exhibit a unique growth window that requires a high-temperature anneal prior to growth that results in oriented growth along the CVD-fed direction. Simulations indicate that the growth window corresponds to nanotube forests containing only semiconducting nanotubes and the oriented growth is semiconducting nanotube segment alignment along the CVD-fed direction. The observed millimeter-thick forests are of interest for several applications that require large-volume, high-concentration nanotube dispersions. We are currently exploring several methods for separating the forests from their growth substrate, including supercritical fluid separation, wet chemical treatment of the forests for dissolving the growth substrate, and direct solvent exfoliating the forests for regrowing on other growth substrates.",
        "watermark_text": "Single - lined carbon nanotube forests with thicknesses up to a millimeter were cultivated by molecular vapor precipitation use ferrocene and dioctylsulfoxide as catalyst and carbon supply , respectively . The trees demonstrate a unique development window that requires a high - temperature anneal prior to growth that leads in aligned growth along the CVD - fed path .Simulations reveal that the development window corresponds to nanotube forests featuring only semiconducting nanotubes and the oriented growth is semiconducting nanotube segment alignment along the CVD - fed path . The observed millimeter - thick forests are of interest for numerous uses that require large - volume , large - concentration nanotube dispersions .We are currently investigating multiple means for dividing the forests from their development substrate , using supercritical fluid separation , wet chemical handling of the forests for dissolving the development substrate , and direct solvent exfoliating the forests for regrowing on other growth substrates .",
        "rewrite_text": "Molecular vapor precipitation was utilized to cultivate single-lined carbon nanotube forests with thicknesses reaching up to a millimeter, employing ferrocene and dioctylsulfoxide as the catalyst and carbon supply, respectively. These trees exhibit a unique growth window that necessitates a high-temperature anneal prior to growth, resulting in aligned growth along the CVD-fed pathway. Simulations indicate that this development window corresponds to nanotube forests predominantly composed of semiconducting nanotubes, with the oriented growth aligning semiconducting nanotube segments along the CVD-fed path. These millimeter-thick forests hold significant interest for various applications requiring large-volume, high-concentration nanotube dispersions. Currently, we are exploring various methods for separating the forests from their development substrate, including using supercritical fluid separation, wet chemical handling to dissolve the substrate, and direct solvent exfoliation for regrowing on alternative growth substrates.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 1.1785113019775793
    },
    {
        "original_text": "Astrophysical jets are very powerful, collimated outflows driven from compact objects, such as active galactic nuclei (AGN) and X-ray binaries. They are key to how galaxies and clusters obtain most of their energy and momentum. The formation of astrophysical jets is still a mystery, but various theoretical models have been proposed. In particular, it has been proposed that magnetic fields are important in generating, collimating, and sustaining astrophysical jets. In this paper we study the resonant coupling between the fast Kelvin-Helmholtz wave and the magnetosonic wave in a sheared, relativistic plasma, with special emphasis on the astrophysical systems in which this process may take place. Our analysis is carried out in the framework of general relativity. We find that in these systems the resonant mechanism can efficiently channel energy from the shear into the fast Kelvin-Helmholtz wave. For simplicity we consider a system consisting of a sheared plasma, moving with a Lorentz factor γ in a quasi-neutral plasma with an external magnetic field B0 along the y-direction. We study the modes that are possible when the linear propagation speed of the magnetosonic wave is equal to the phase speed of the Kelvin-Helmholtz wave. The most unstable mode has a short wavelength in the y-direction and a non-zero phase speed in the x- and z-directions. The growth rate of the most unstable mode is proportional to the square of the fluid vorticity in the y-direction, which means the instability is most efficient for differentially rotating systems. However, the most unstable mode also depends on the ratio of the shear to the sound speed, γShc, and increases with γShc, meaning that the stronger the shear, the more efficient the mode. We also study the stability of this most unstable mode against non-linear effects. When the amplitude of the magnetosonic wave becomes comparable to the amplitude of the Kelvin-Helmholtz wave, the two waves interact non-linearly and the most unstable mode evolves into a stable shock wave with smooth contact surfaces. We conclude that the resonant coupling between Kelvin-Helmholtz and magnetosonic waves may have an important impact on astrophysical jets, which makes it an important area for future study. The resonant coupling discussed here could also be important in microquasars, the envirnment of black hole systems like SS 433, where a pair of stars orbiting each other eject high-energy jets. The stars are in nearly circular orbits, but the jets are highly relativistic and tend to ejection in a direction perpendicular to the orbital plane. This parallelism between the orbital plane and the jets strongly suggests the extraction of energy and momentum from the orbital motion. The extraction of energy and momentum from a shearing relativistic flow may provide a new mechanism for this parallelism. Resonant amplification of Kelvin-Helmholtz and magnetos",
        "watermark_text": "Astrophysical jets are very potent , collimated outflows driven from compact objects , such as active galactic nuclei ( AGN ) and X - ray binaries . They are important to how galaxies and clusters derive most of their power and momentum .The formation of astrophysical jets is remains a unknown , but various theoretical theories have been proposed . In particular , it has been proposed that magnetic fields are important in generating , collimating , and sustaining astrophysical jets .In this paper we study the resonant coupling between the fast Kelvin - Helmholtz wave and the magnetosonic wave in a sheared , relativistic plasma , with special emphasis on the astrophysical systems in which this process may happen place . Our study is carried out in the framework of general relativity .We see that in these systems the resonant mechanism can efficiently channel energy from the shear into the fast Kelvin - Helmholtz signal . For simplicity we study a system consisting of a sheared plasma , moving with a Lorentz factor γ in a quasi - neutral plasma with an external magnetic force B0 along the y - direction .We explore the modes that are possible when the linear transmission frequency of the magnetosonic wave is equal to the phase velocity of the Kelvin - Helmholtz signal . The most unstable mode has a small frequency in the y - direction and a non - zero phase velocity in the x - and z - directions .The growth speed of the most unstable mode is proportional to the square of the liquid vorticity in the y - direction , which means the instability is most efficient for differentially rotating environments . However , the most unstable mode still depends on the proportion of the shear to the musical speed , γShc , and varies with γShc , meaning that the heavier the shear , the more efficient the mode .We additionally determine the stability of this most unstable mode against non - linear influences . When the frequency of the magnetosonic wave grows equivalent to the frequency of the Kelvin - Helmholtz signal , the two waves interact non - linearly and the most unstable mode evolves into a steady blast wave with smooth connection materials .We suggest that the resonant coupling between Kelvin - Helmholtz and magnetosonic pulses could have an important affect on astrophysical jets , which makes it an important field for future study . The resonant coupling discussed here possible also be influential in microquasars , the envirnment of white hole structures like SS 433 , where a pair of stars orbiting each other eject high - energy jets .The stars are in nearly circular orbits , but the jets are extremely relativistic and tend to ejection in a path perpendicular to the orbital plane . This parallelism between the orbital plane and the planes strongly suggests the extraction of power and momentum from the orbital movement .The extraction of power and momentum from a shearing relativistic flow would offer a new method for this parallelism . Resonant amplification of Kelvin - Helmholtz and magnetos",
        "rewrite_text": "The astrophysical jets possess immense potency, being highly collimated outflows that originate from compact objects such as active galactic nuclei (AGN) and X-ray binaries. Their significance lies in the way they contribute to the power and momentum of galaxies and clusters. Despite the unknown nature of their formation, various theoretical proposals have been made. It is particularly believed that magnetic fields play a crucial role in generating, collimating, and maintaining these astrophysical jets.\n\nIn this study, we delve into the resonant coupling between the fast Kelvin-Helmholtz wave and the magnetosonic wave within a relativistic plasma environment, emphasizing its relevance in astrophysical systems. Our research is conducted within the framework of general relativity. We observe that in these systems, the resonant mechanism can efficiently channel energy from shear into the fast Kelvin-Helmholtz signal. For simplicity, we consider a system comprising a sheared plasma moving with a Lorentz factor γ in a quasi-neutral plasma, influenced by an external magnetic force B0 aligned in the y-direction.\n\nWe explore the possible modes when the linear transmission frequency of the magnetosonic wave aligns with the phase velocity of the Kelvin-Helmholtz signal. The most unstable mode features a low frequency in the y-direction and a non-zero phase velocity in the x- and z-directions. The growth rate of this most unstable mode is proportional to the square of the vorticity in the y-direction, indicating that it is most effective in differentially rotating environments. However, its stability depends on the ratio of shear to sound speed, γShc, and varies with γShc, suggesting that a heavier shear leads to a more efficient mode.\n\nMoreover, we assess the stability of this most unstable mode against non-linear influences. When the frequency of the magnetosonic wave matches that of the Kelvin-Helmholtz signal, the two waves interact non-linearly, evolving the most unstable mode into a steady blast wave with smoothly connected materials. We propose that the resonant coupling between Kelvin-Helmholtz and magnetosonic pulses may significantly influence astrophysical jets, making it a critical area for future investigation.\n\nAdditionally, the discussed resonant coupling may also influence microquasars and white hole structures like SS 433. In these environments, a pair of stars in nearly circular orbits eject high-energy jets perpendicular to the orbital plane. This alignment suggests the extraction of power and momentum from the orbital motion. Extracting power and momentum from a shearing relativistic flow could offer a new approach to understanding this alignment. The amplified Kelvin-Helmholtz and magnetosonic waves through resonance could play a pivotal role in these processes.",
        "ori-fast-z-score": 0.49374193110101877,
        "water-fast-z-score": 8.061017305526642,
        "rewrite-fast-z-score": 2.0455022859899348
    },
    {
        "original_text": "The large scale structure (LSS) of galaxies contains powerful information on cosmological models and our understanding of galaxy formation. The Luminous Red Galaxy (LRG) sample of the Sloan Digital Sky Survey (SDSS) has 27,000 galaxies with median redshift 0.38 and photo-z errors of 0.03 out to a redshift of 0.7, making it the largest contiguous high-redshift LSS sample to date. We use this sample to measure the halo occupation distribution in a comprehensive suite of Monte Carlo simulations of six popular galaxy formation models. The measured correlation functions are compared to the measured two-point correlation functions of the LRG sample. The six model comparisons allow us to limit the sum of the virial temperatures of the dark matter halos hosting LRGs to be greater than or equal to 4.5 keV, and the sum of the stellar feedback heating rates to be less than or equal to 12.7 keV. These limits correspond to typical WIMPs masses less than approximately 15 GeV for a standard thermal WIMPs model.",
        "watermark_text": "The large scale structure ( LSS ) of galaxies includes potent knowledge on cosmological models and our understanding of galaxy formation . The Luminous Red Galaxy ( LRG ) sample of the Sloan Digital Sky Survey ( SDSS ) has 27 , 000 galaxies with median redshift 0 . 38 and photo - z errors of 0 . 03 out to a redshift of 0 . 7 , making it the greatest contiguous high - redshift LSS sample to date .We use this specimen to measure the halo occupation distribution in a comprehensive collection of Monte Carlo simulations of six popular galaxy formation models . The measured correlation functions are compared to the measured two - point correlation functions of the LRG sample .The six model comparisons enable us to limit the sum of the virial altitudes of the dark matter halos hosting LRGs to be greater than or equal to 4 . 5 keV , and the sum of the planetary feedback cooling rates to be less than or equal to 12 . 7 keV . These restrictions correspond to normal WIMPs masses fewer than approximately 15 GeV for a traditional thermal WIMPs model .",
        "rewrite_text": "The large-scale structure (LSS) of galaxies encompasses significant insights into cosmological models and our comprehension of galaxy formation. The Sloan Digital Sky Survey's (SDSS) Luminous Red Galaxy (LRG) sample comprises 27,000 galaxies, with a median redshift of 0.38 and photo-z errors within a range of 0.03 to a redshift of 0.7. This makes it the largest contiguous high-redshift LSS sample ever recorded. We employ this sample to assess the halo occupation distribution through a comprehensive set of Monte Carlo simulations based on six popular galaxy formation models. The measured correlation functions are then compared to the two-point correlation functions observed in the LRG sample. These six model comparisons enable us to constrain the combined virial altitudes of dark matter halos hosting LRGs to be greater than or equal to 4.5 keV, and the combined planetary feedback cooling rates to be less than or equal to 12.7 keV. These constraints are equivalent to limiting the masses of traditional thermal WIMPs to be below approximately 15 GeV in a conventional WIMPs model.",
        "ori-fast-z-score": 1.3242443839434612,
        "water-fast-z-score": 4.45427656417346,
        "rewrite-fast-z-score": 1.0681034923744679
    },
    {
        "original_text": "In high-energy heavy-ion collisions, particle production is strongly influenced by the path length dependence of the initial spatial anisotropyparticipating in the reaction. Rapidity dependent Hanbury-Brown-Twiss (HBT) interferometry measurements can provide constraints on the spatial extent and the strength of the reaction initial pressure gradient. Supported by an anisotropic flow calculation, two quantitative methods are proposed to determine the electric charge correlations in the phase space of the produced particles around mid-rapidity. Applying these methods to the published Bose-Einstein correlations data obtained by the NA49 and the CERES collaborations at the SPS, the electric charge correlations are determined as a function of rapidity and energy. It is shown that at all energies and rapidities the strength of the charge correlations is maximal for like charges and decreases with the opposite charge correlations. The rapidity dependence of the correlations strength shows a minimum at about 3.2 times the pion chemical potential and increases at larger longitudinal distances from the mid-rapidity.",
        "watermark_text": "In high - energy heavy - ion collisions , particle production is strongly influenced by the path length dependence of the first spatial anisotropyparticipating in the response . Rapidity dependent Hanbury - Brown - Twiss ( HBT ) interferometry surveys can provide constraints on the spatial scope and the strength of the response initial pressure gradient .Supported by an anisotropic flow measurement , two numerical ways are proposed to estimate the electric charge correlations in the phase space of the produced particles around mid - rapidity . Applying these calculations to the published Bose - Einstein correlations data derived by the NA49 and the CERES collaborations at the SPS , the electric charge correlations are measured as a function of rapidity and energy .It is demonstrated that at all energies and rapidities the strength of the charge correlations is maximal for like charges and decreases with the opposite charge correlations . The rapidity dependence of the correlations strength displays a minimum at about 3 . 2 twice the pion chemical potential and expands at larger longitudinal distances from the mid - rapidity .",
        "rewrite_text": "In high-energy heavy-ion collisions, the production of particles is greatly influenced by the path length dependency of the initial spatial anisotropy involved in the response. Rapidity-dependent Hanbury-Brown-Twiss (HBT) interferometry studies can offer constraints on the spatial extent and the initial pressure gradient strength of the response. With the aid of anisotropic flow measurements, two numerical methods are proposed to estimate electric charge correlations in the phase space of the produced particles near mid-rapidity. By applying these calculations to Bose-Einstein correlation data published by the NA49 and CERES collaborations at the SPS, electric charge correlations are measured as a function of both rapidity and energy. The results show that at all energies and rapidities, the strength of charge correlations is greatest for similar charges and decreases with opposite charge correlations. The rapidity dependency of the correlation strength exhibits a minimum at approximately 3.2 times the pion chemical potential and widens at greater longitudinal distances from mid-rapidity.",
        "ori-fast-z-score": 0.12216944435630522,
        "water-fast-z-score": 3.7872527750454617,
        "rewrite-fast-z-score": 0.5852057359806528
    },
    {
        "original_text": "We describe Verlinde-like formulas for degenerate fields of logarithmic CFTs with central charge c_{p,1}. We show that the degeneracies of these fields are in one-to-one correspondence with certain kinds of branched coverings of the Riemann sphere branched at three points. The number of these coverings, which we call n-fold branched coverings, is given by an explicit formula in terms of residues of certain locally analytic functions on the complex plane. We also discuss a curious connection with the Gauss hypergeometric function, and its relation to branching points. Our formulas generalize both existing Verlinde-like formulas for CFTs of central charge c_{p,p } and formulas of Haglund, Hemmer, and Aulbach for degenerate fields in logarithmic CFTs with c_{p,1}. We give two proofs of our formulas, one using ideas from the theory of Toda forms, and one using ideas from the theory of Gaussian hypergeometric functions.",
        "watermark_text": "We define Verlinde - like formulas for degenerate fields of logarithmic CFTs with central charge c _ { p , 1 } . We see that the degeneracies of these fields are in one - to - one correspondence with certain kinds of branched coverings of the Riemann sphere branched at three points .The amount of these coverings , which we call n - fold branched coverings , is given by an explicit formula in terms of residues of certain locally analytic functions on the complex plane . We additionally discuss a curious connection with the Gauss hypergeometric function , and its connection to branching points .Our formulas generalize both older Verlinde - like formulas for CFTs of central charge c _ { k , k } and formulas of Haglund , Hemmer , and Aulbach for degenerate fields in logarithmic CFTs with c _ { p , 1 } . We give two proofs of our formulas , one utilizing ideas from the principle of Toda forms , and one utilizing ideas from the theory of Gaussian hypergeometric functions .",
        "rewrite_text": "We have formulated Verlinde-inspired equations for degenerate fields of logarithmic Conformal Field Theories (CFTs) with a central charge of c_{p, 1}. We observe that the degeneracy levels of these fields are in a direct correspondence with specific types of Riemann sphere branched coverings at three points. These coverings, which we term n-fold branched coverings, are determined by an explicit formula related to the residues of specific locally analytic functions on the complex plane. Furthermore, we explore an intriguing connection with the Gauss hypergeometric function and its relationship to branching points. Our formulas extend both older Verlinde-like equations for CFTs with a central charge of c_{k, k} and the formulations developed by Haglund, Hemmer, and Aulbach for degenerate fields in logarithmic CFTs with c_{p, 1}. We provide two proofs for our equations, one leveraging principles from Toda forms and the other utilizing concepts from the theory of Gaussian hypergeometric functions.",
        "ori-fast-z-score": 1.3363062095621219,
        "water-fast-z-score": 4.27617987059879,
        "rewrite-fast-z-score": -0.13018891098082389
    },
    {
        "original_text": "Type-II superconductors undergo a transition to a resistive state above the critical temperature Tc, the Meissner effect. In conventional superconductors, the resistive state is understood as a lossless state known as the Vinen-WFlagsal state, named after the investigators who discovered it. In a three-dimensional (3D) superconductor, twisted magnetic field configurations known as vortex lines move from regions of higher to lower temperature, thereby forming a Bose-Einstein condensate of bosons. This phenomenon of supercooling was first observed in strongly type-II superconductors in a thin foil configuration. Here we report the observation of a Bose-Einstein condensate of bosons in thin films of the strongly type-II superconductor niobium tin (Nb3Sn). Unlike thin foils, thin films are two-dimensional (2D), and so exhibit 2D supercooling at lower temperatures. We find that 2D supercooling is complete at approx. 20 mK in our thinnest films, corresponding to a critical current approximately 500 times that of the Meissner state. At lower temperatures we observe thermally activated behaviour, and fit an Arrhenius law to extract an activation energy of 25 μeV. We show that this is consistent with the proposed energy gap of the strongly type-II superconductor niobium tin (Nb3Sn), and argue that the large observed critical current is consistent with 3D supercooling in our thin films.",
        "watermark_text": "Type - II superconductors conduct a change to a resistive state above the critical temperature Tc , the Meissner phenomenon . In conventional superconductors , the resistive state is understood as a lossless state known as the Vinen - WFlagsal state , named after the workers who found it .In a three - dimensional ( 3D ) superconductor , distorted magnetic field configurations known as vortex lines move from regions of lower to lesser temperature , thereby forming a Bose - Einstein condensate of bosons . This phenomenon of supercooling was first observed in highly type - II superconductors in a thin foil configuration .Here we note the observation of a Bose - Einstein condensate of bosons in thin films of the strongly type - II superconductor niobium tin ( Nb3Sn ) . Unlike thin foils , narrow films are two - dimensional ( 2D ) , and so exhibit 2D supercooling at lower heat .We see that 2D supercooling is complete at approximately . 20 mK in our thinnest bands , equivalent to a critical current approx 500 times that of the Meissner state .At lower altitudes we study thermally activated behaviour , and fit an Arrhenius law to extract an activation energy of 25 μeV . We see that this is compatible with the suggested power gap of the strongly type - II superconductor niobium tin ( Nb3Sn ) , and suggest that the huge recorded critical current is consistent with 3D supercooling in our thin films .",
        "rewrite_text": "Type II superconductors exhibit a transition to a resistive state above the critical temperature Tc, exhibiting the Meissner effect. In conventional superconductors, this resistive state is described as a lossless state named after the discoverers as the Vinen-WFlagsal state. Within a three-dimensional (3D) superconductor, vortex lines, which are distorted magnetic field configurations, move from regions of lower temperature to colder regions, forming a Bose-Einstein condensate of bosons. This phenomenon of supercooling was first observed in highly Type II superconductors in thin foil configurations.\n\nIn our study, we observed the formation of a Bose-Einstein condensate of bosons in thin films of the strongly Type II superconductor niobium tin (Nb3Sn). In contrast to thin foils, narrow films are two-dimensional (2D), resulting in 2D supercooling at lower temperatures. We found that 2D supercooling is complete at approximately 20 mK in our thinnest bands, which is equivalent to a critical current approximately 500 times greater than that of the Meissner state.\n\nAt lower temperatures, we studied thermally activated behavior and applied an Arrhenius law to extract an activation energy of 25 μeV. We observe that this value is consistent with the proposed power gap of strongly Type II superconductor niobium tin (Nb3Sn). Furthermore, we suggest that the exceptionally high recorded critical current is compatible with 3D supercooling in our thin film samples.",
        "ori-fast-z-score": -0.10721125348377948,
        "water-fast-z-score": 5.467773927672753,
        "rewrite-fast-z-score": 0.42640143271122083
    },
    {
        "original_text": "Galactic globular clusters are generally identified from their high galactic latitude, low optical surface brightness, and redness in the color-magnitude diagram. However, a handful of globular clusters can also be found near the center of the Milky Way, within its historic luminous profile. These “central” clusters are particularly interesting due to their past association with galactic nuclei and their present potential to be tagged as loci of ongoing nucleosynthesis. Here we report on a study of the central cluster NGC 2419. We have obtained spectra for over a hundred red giants, from which we have measured chemical compositions and mapped the cluster metallicity and spatial distribution. We have uncovered a previously-unrecognized spatial structure in the distribution of calcium abundances that is correlated with that of the iron-peak elements. These compositional structures are suggestive of multiple epochs of star formation in NGC 2419, with the iron-peak showing evidence of chemical enrichment from the most recent (and presumably, central) generation of stars. NGC 2419 is thus the most promising galactic nucleus-related cluster currently known.",
        "watermark_text": "Galactic globular complexes are typically recognized from their high galactic latitude , low optical surface brightness , and redness in the color - magnitude diagram . However , a handful of globular complexes can also be found near the center of the Milky Way , within its historic luminous profile .These “ central ” clusters are particularly important due to their past relationship with galactic nuclei and their modern potential to be tagged as loci of ongoing nucleosynthesis . Here we paper on a investigation of the central cluster NGC 2419 .We have recovered spectra for over a twenty red giants , from which we have recorded molecular compositions and mapped the cluster metallicity and spatial distribution . We have uncovered a previously - unrecognized temporal shape in the distribution of calcium abundances that is associated with that of the metal - peak elements .These compositional structures are suggestive of multiple epochs of star formation in NGC 2419 , with the iron - peak displaying indication of chemical enrichment from the most current ( and presumably , central ) production of stars . NGC 2419 is thereby the most promising galactic nucleus - linked cluster currently known .",
        "rewrite_text": "Galactic globular clusters are typically distinguished by their high galactic latitudes, low optical surface brightness, and their red hues in the color-magnitude diagram. Nevertheless, a few of these clusters can be found in close proximity to the center of the Milky Way, within its historical luminous profile. These \"central\" clusters hold significant importance due to their historical association with galactic nuclei and their potential to be identified as current sites of nucleosynthesis. This study focuses on the investigation of the central cluster NGC 2419. We have obtained spectra for over twenty red giants, which enabled us to record molecular compositions, map the cluster's metallicity, and determine its spatial distribution. Our findings have uncovered a previously unrecognized temporal pattern in the distribution of calcium abundances that is correlated with that of metal-peak elements. These compositional structures suggest multiple epochs of star formation in NGC 2419, with the iron-peak elements indicating recent (and presumably central) star formation enriching the cluster's chemical composition. Consequently, NGC 2419 emerges as the most promising cluster linked to the galactic nucleus currently known.",
        "ori-fast-z-score": -1.7888543819998317,
        "water-fast-z-score": 3.0377373325002646,
        "rewrite-fast-z-score": -0.21566554640687682
    },
    {
        "original_text": "The geomagnetic field is a vital indicator of space weather. It is composed of a predominantly dipolar component generated by the motion of inner core, and a fluctuating component generated by atmospheric and solar processes. The amplitude of the fluctuating component decreases rapidly with distance from the magnetic north and requires the integration of solar input and atmospheric response to accurately determine its amplitude and dipole moment at Earth’s surface. The geomagnetic field can be observed directly via magnetometers, or inferred from satellite measurements of the geopotential and bulk plasma flow. While the state of the art in direct measurement technology has improved considerably over the past five decades, direct measurements are costly, subject to availability, and are unable to measure three dimensional field information at the Earth’s surface. For this reason, geophysical models have been coupled to magnetic field data assimilation in order to determine the 3D field amplitude and dipole moment at the surface. This paper reviews some of the specific advances made in variational data assimilation, a recently developed technique for the assimilation of indirect observation data in order to improve the estimation of the state of a dynamical system. The variational method is cast as a constrained optimization problem in the J-subproblem and D-subproblem, where J and D denote the Jacobian and Hessian matrices, respectively. We demonstrate the effectiveness of the method through the assimilation of magnetic field data from a single location on Earth to a nonlinear, high-dimensional, and sparse MHD model. The application of variational assimilation to the magnetic field is a promising new direction for improving the accuracy of geophysical models and, in the long term, improving our understanding of the dynamo process that generates the Earth’s magnetic field.",
        "watermark_text": "The geomagnetic field is a crucial indicator of space weather . It is composed of a predominantly dipolar component produced by the movement of inner core , and a fluctuating component produced by atmospheric and solar processes .The amplitude of the fluctuating component decreases quickly with distance from the magnetic north and requires the integration of sun output and atmospheric reaction to correctly decide its amplitude and dipole point at Earth ’ s surface . The geomagnetic field can be viewed directly via magnetometers , or inferred from satellite observations of the geopotential and bulk plasma circulation .While the state of the art in direct detection technology has improved substantially over the previous five century , direct measurements are costly , subject to availability , and are unable to measure three dimensional field data at the Earth ’ s surface . For this reason , geophysical models have been coupled to magnetic field data assimilation in order to estimate the 3D field frequency and dipole minute at the surface .This paper reviews some of the specific developments shown in variational data assimilation , a recently advanced technique for the assimilation of indirect observation information in order to improve the estimation of the state of a dynamical system . The variational technique is cast as a constrained optimization problem in the J - subproblem and D - subproblem , where J and D represent the Jacobian and Hessian matrices , respectively .We suggest the ability of the method through the assimilation of magnetic field data from a single location on Earth to a nonlinear , large - dimensional , and sparse MHD model . The application of variational assimilation to the magnetic field is a exciting new direction for improving the accuracy of geophysical models and , in the long term , improving our knowing of the dynamo mechanism that generates the Earth ’ s magnetic force .",
        "rewrite_text": "The geomagnetic field plays a pivotal role in indicating space weather conditions. It is composed primarily of a dipolar component stemming from the inner core's movement and a fluctuating component resulting from atmospheric and solar processes. The amplitude of this fluctuating component diminishes rapidly with distance from the magnetic north, necessitating the integration of solar output and atmospheric response to accurately determine its amplitude and dipole point at Earth's surface. Geomagnetic fields can be directly observed through magnetometers or inferred from satellite observations of geopotential and bulk plasma circulation.\n\nWhile significant advancements have been made in direct detection technology over the past five centuries, direct measurements remain costly, dependent on availability, and incapable of measuring three-dimensional field data at Earth's surface. Therefore, geophysical models have been integrated with magnetic field data assimilation to estimate three-dimensional field frequencies and surface dipole variations.\n\nThis paper examines the recent developments in variational data assimilation, a cutting-edge technique for incorporating indirect observation information to enhance the estimation of a dynamical system's state. The variational technique is formulated as a constrained optimization problem within the J-subproblem and D-subproblem, where J and D represent the Jacobian and Hessian matrices, respectively. We propose the effectiveness of this method in assimilating magnetic field data from a single location on Earth into a nonlinear, high-dimensional, and sparse MHD model.\n\nThe application of variational assimilation to the magnetic field represents an exciting new avenue for enhancing the accuracy of geophysical models and, ultimately, our understanding of the dynamo mechanism that generates Earth's magnetic force.",
        "ori-fast-z-score": -0.35355339059327373,
        "water-fast-z-score": 6.18718433538229,
        "rewrite-fast-z-score": 1.7960530202677492
    },
    {
        "original_text": "A dust component 2 kpc above the plane in the Sombrero Galaxy (NGC 891) was discovered by NAOMI. This component is composed of Olivet refractory silicates with a median size of 0.1 μm and a power-law size distribution. Its peak wavelength(s) are approximately 10 μm, indicating the presence of very small grains. This dust component is not associated with any observed stars, planetary systems, or molecular clouds. Its origin remains a mystery. A dust component 2 kpc above the plane in the Sombrero Galaxy (NGC 891) was discovered by NAOMI. This component is composed of Olivet refractory silicates with a median size of 0.1 μm and a power-law size distribution. Its peak wavelength(s) are approximately 10 μm, indicating the presence of very small grains. This dust component is not associated with any observed stars, planetary systems, or molecular clouds. Its origin remains a mystery. This component was also found in NGC 3628, an edge-on spiral galaxy approximately 2.5 Mpc away. If this dust component is similar to dust in our own galaxy, it would take a minimum of 5000 years for this dust to travel from NGC 891 to NGC 3628. This discovery poses significant questions regarding its origin and the implications for the Sombrero Galaxy s evolution.",
        "watermark_text": "A dust component 2 kpc above the plane in the Sombrero Galaxy ( NGC 891 ) was discovered by NAOMI . This element is composed of Olivet refractory silicates with a median size of 0 . 1 μm and a power - law size distribution .Its peak wavelength ( s ) are approximately 10 μm , representing the presence of very small particles . This dust component is not associated with any observed stars , planetary components , or molecular clouds .Its origin survives a unknown . A dust component 2 kpc above the jet in the Sombrero Galaxy ( NGC 891 ) was studied by NAOMI .This element is composed of Olivet refractory silicates with a median size of 0 . 1 μm and a power - law size distribution . Its peak wavelength ( s ) are approximately 10 μm , representing the presence of very small particles .This dust component is not associated with any observed stars , planetary complexes , or molecular clouds . Its origin stands a unknown .This element was also discovered in NGC 3628 , an edge - on spiral galaxy approximately 2 . 5 Mpc apart . If this dust component is identical to dust in our own galaxy , it would taking a minimum of 5000 years for this dust to travel from NGC 891 to NGC 3628 .This find offers important questions regarding its ancestry and the implications for the Sombrero Galaxy s evolution .",
        "rewrite_text": "NAOMI has discovered a dust component situated 2 kpc above the plane of the Sombrero Galaxy (NGC 891). This element is composed primarily of Olivet refractory silicates with a median particle size of 0.1 micrometers and a power-law size distribution. Its peak wavelengths, approximately 10 micrometers, indicate the presence of extremely small particles. This dust is not associated with any observed stars, planetary systems, or molecular clouds, and its origin remains unknown.\n\nFurthermore, a similar dust component has been identified in the edge-on spiral galaxy NGC 3628, which is roughly 2.5 million light-years distant. If this dust component is comparable to that found in our own galaxy, it would take a minimum of 5,000 years for the dust to traverse the distance from NGC 891 to NGC 3628. This discovery poses significant questions about its origins and the implications for the evolution of the Sombrero Galaxy.",
        "ori-fast-z-score": 1.801996396010812,
        "water-fast-z-score": 6.253987492037524,
        "rewrite-fast-z-score": 1.016001016001524
    },
    {
        "original_text": "Asymmetrically charged surfaces experience a net force due to electrostatic interactions. This is commonly observed in nature in phenomena such as adhesion between two objects with different charges, or in the cell membrane and pollution sticks to clothes. In this work, the electrostatic interaction of a uniformly charged plate and an asymmetrically charged membrane is theoretically and experimentally studied. The membrane has one uniformly charged surface and one non-uniformly charged surface. An exact expression for the interaction potential is derived for the limiting case of large plate-membrane distance compared to the thickness of the membrane. The interaction potential is shown to be the sum of the interaction potentials of two uniformly charged plates. The interaction potential is experimentally measured using neutron reflectometry. The measured potential is in excellent agreement with the calculated potential. The first three van der Waals coefficients are extracted from the measured potential and shown to have opposite sign to the sign of the surface charges, as expected from the long-range nature of the van der Waals forces.",
        "watermark_text": "Asymmetrically charged materials experience a net force thanks to electrostatic interactions . This is often observed in nature in phenomena such as adhesion between two entities with varying charges , or in the cell membrane and pollution sticks to clothes .In this research , the electrostatic interaction of a uniformly charged plate and an asymmetrically charged membrane is theoretically and experimentally studied . The membrane has one uniformly charged surface and one non - uniformly charged membrane .An exact representation for the interaction potential is calculated for the restricted case of large plate - membrane distance compared to the length of the membrane . The interaction potential is demonstrated to be the sum of the interaction potentials of two uniformly charged layers .The interaction potential is experimentally recorded using neutron reflectometry . The measured potential is in good agreement with the calculated potential .The first three van der Waals equations are derived from the measured potential and demonstrated to have different sign to the sign of the surface charges , as predicted from the long - range nature of the van der Waals forces .",
        "rewrite_text": "Materials with asymmetrical charges undergo a net force due to electrostatic interactions, which is frequently observed in nature, such as the adhesion between two entities with differing charges or the adhesion of cell membranes and pollutants to clothing. In this research, a theoretical and experimental study was conducted on the electrostatic interaction between a uniformly charged plate and an asymmetrically charged membrane. The membrane consisted of one uniformly charged surface and one non-uniformly charged surface. For the specific case where the plate-membrane distance was significantly greater than the membrane's length, an accurate representation of the interaction potential was calculated. This interaction potential was found to be the sum of the interaction potentials between two uniformly charged layers. The interaction potential was recorded experimentally using neutron reflectometry, and the measured potential was found to be in good agreement with the calculated potential. Furthermore, the first three van der Waals equations were derived from the measured potential and demonstrated to have a different sign compared to the surface charges, as predicted by the long-range nature of van der Waals forces.",
        "ori-fast-z-score": -1.3093073414159544,
        "water-fast-z-score": 3.4914862437758782,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The Multiband Imaging Photometer for Spitzer (MIPS) is an infrared camera with capabilities similar to those of the Infrared Space Observatory but operating on the Spitzer Space Telescope. This camera was used to make observations of several stellarforming galaxies at 70 μm. Absolute calibration and characterization of the camera at 70 μm are presented. The accuracy of the 70 μm transfer function is quantified by observing a set of photometric standards, and uncertainties are found to be 5.3% at 30 μm brightness, 6.1% at 45 μm, 7.7% at 60 μm, and 9.3% at 90 μm. Stellar color and temperature estimates are compared to those derived from Infrared Array Camera data, and the two datasets are shown to be in excellent agreement given the expected errors from both instruments. Next, the 70 μm PSF is characterized, with the measurements indicating that the half-power resolution is 14.3 μm, the full-width at half-maximum is 23.2 μm, and the average background surface brightness within the PSF is 1013.2 MJ/sr^2. Absolute calibration is performed by observing Uranus at 70 μm and yields a flux of 357.8225 μJykpc2. Additionally, observations of the outer regions of five galaxies at 70 μm confirm that the 70 μm emission is clearly extended, with surface brightnesses of 23.2 μm−2 less than that of the background at the 50% level. Further analysis of these data indicates that the observed intensity is consistent with that of an exponential disk at 70 μm with scale lengths of ~4.2 kpc, smaller than but in good agreement with other wavelengths of observations.",
        "watermark_text": "The Multiband Imaging Photometer for Spitzer ( MIPS ) is an infrared camera with capabilities comparable to those of the Infrared Space Observatory but running on the Spitzer Space Telescope . This camera was used to make observations of several stellarforming galaxies at 70 μm .Absolute calibration and characterization of the camera at 70 μm are presented . The accuracy of the 70 μm transfer function is quantified by observing a setting of photometric specifications , and uncertainties are found to be 5 . 3 % at 30 μm brightness , 6 . 1 % at 45 μm , 7 . 7 % at 60 μm , and 9 . 3 % at 90 μm .Stellar color and heat estimates are compared to those generated from Infrared Array Camera information , and the two datasets are shown to be in good agreement given the expected errors from both instruments . Next , the 70 μm PSF is characterized , with the calculations suggesting that the half - power resolution is 14 . 3 μm , the full - length at half - maximum is 23 . 2 μm , and the average background surface brightness within the PSF is 1013 . 2 MJ / sr ^ 2 .Absolute calibration is conducted by observing Uranus at 70 μm and results a flux of 357 . 8225 μJykpc2 . Additionally , observations of the exterior areas of five galaxies at 70 μm verify that the 70 μm emission is distinctly extended , with surface brightnesses of 23 . 2 μm−2 less than that of the background at the 50 % level .Further examination of these information suggest that the seen brightness is consistent with that of an exponential disk at 70 μm with scale lengths of ~ 4 . 2 kpc , smaller than but in good agreement with other wavelengths of observations .",
        "rewrite_text": "The Spitzer Space Telescope's Multiband Imaging Photometer (MIPS) is an infrared camera comparable to the capabilities of the Infrared Space Observatory. This camera was employed to observe multiple galaxy-forming stars at 70 micrometers. Presented is the absolute calibration and characterization of the camera at 70 micrometers. To quantify the accuracy of the 70 micrometers transfer function, photometric specifications were observed, and uncertainties were found to be 5.3% at 30 micrometers brightness, 6.1% at 45 micrometers, 7.7% at 60 micrometers, and 9.3% at 90 micrometers. Comparative analyses of stellar color and heat estimates with those generated by the Infrared Array Camera data reveal good agreement between the two datasets within expected instrument errors.\n\nFurthermore, the 70 micrometers Point Spread Function (PSF) characteristics are explored. Calculations suggest a half-power resolution of 14.3 micrometers, a full-length at half-maximum of 23.2 micrometers, and an average background surface brightness within the PSF of 1013.2 MJ per square arcsecond. Uranus was observed for absolute calibration at 70 micrometers, resulting in a flux of 357.8225 microJoules per square kiloparsec per second. Additionally, observations of the outer regions of five galaxies at 70 micrometers confirm that the emission at this wavelength is distinctly extended, with surface brightnesses slightly lower than the background at the 50% level. Further examination of these data suggests that the observed brightness is consistent with that of an exponential disk at 70 micrometers with scale lengths of approximately 4.2 kiloparsecs, which is smaller but in good agreement with observations at other wavelengths.",
        "ori-fast-z-score": -0.6793662204867574,
        "water-fast-z-score": 4.076197322920544,
        "rewrite-fast-z-score": 0.4364357804719848
    },
    {
        "original_text": "L- and T-type dwarfs are cool stars with lower masses than the Sun. They have significant quantities of deuterium, the nucleus of a deuteron, but no hydrogen, the nucleus of a prot hydride. Their spectra show the absorption bands of deuterium, as well as their own characteristic chemical bands. Measurement of space velocities for a sample of L- and T-type dwarfs reveal that these stars have typical space velocities similar to the Sun. While L-type dwarfs have space velocities similar to the velocity of the local standard of rest (LSR), T-type dwarfs have space velocities similar to the velocity of the Sun with respect to the LSR. For many years it was believed that deuterium, the nucleus of a deuteron, but no hydrogen, the nucleus of a prot hydride. Their spectra show the absorption bands of deuterium, as well as their own characteristic chemical bands. Measurement of space velocities for a sample of L- and T-type dwarfs reveal that these stars have typical space velocities similar to the Sun. While L-type dwarfs have space velocities similar to the velocity of the local standard of rest (LSR), T-type dwarfs have space velocities similar to the velocity of the Sun with respect to the LSR. However, recent studies of the kinematics of metal-poor stars have shown that many have space velocities similar to the velocity of the LSR. These studies suggest that star-formation occurred at nearly the same time throughout most of the Milky Way galaxy, with the oldest, most metal-poor stars having the highest space velocities. This implies that deuterium was synthesized in the interstellar medium, and not transported to the sites of low-mass star formation. The presence of deuterium in many metal-poor stars but not in L- or T-type dwarfs seems to contradict the early evolution theory for these stars. The space velocities of L- and T-type dwarfs suggest that these stars have typical space velocities similar to the Sun. While L-type dwarfs have space velocities similar to the velocity of the local standard of rest (LSR), T-type dwarfs have space velocities similar to the velocity of the Sun with respect to the LSR. However, recent studies of the kinematics of metal-poor stars have shown that many have space velocities similar to the velocity of the LSR. These studies suggest that star-formation occurred at nearly the same time throughout most of the Milky Way galaxy, with the oldest, most metal-poor stars having the highest space velocities. This implies that deuterium was synthesized in the interstellar medium, and not transported to the sites of low-mass star formation. The presence of deuterium in many metal-poor stars but not in L- or T-type dwarfs seems to contradict the early evolution theory for these stars. The space velocities of L- and T-type dwarfs suggest that these stars have typical space velocities similar",
        "watermark_text": "L - and T - class dwarfs are warm stars with lesser masses than the Sun . They have considerable quantities of deuterium , the nucleus of a deuteron , but no hydrogen , the nucleus of a prot hydride .Their spectra show the absorption groups of deuterium , as also as their own peculiar chemical bands . Measurement of space velocities for a sample of L - and T - class dwarfs reveal that these stars have typical space velocities comparable to the Sun .While L - class dwarfs have space velocities comparable to the velocity of the local standard of rest ( LSR ) , T - class dwarfs have space velocities identical to the velocity of the Sun with regard to the LSR . For many years it was considered that deuterium , the nucleus of a deuteron , but no oxygen , the nucleus of a prot hydride .Their spectra show the absorption groups of deuterium , as also as their own peculiar chemical bands . Measurement of space velocities for a sample of L - and T - class dwarfs reveal that these stars have typical space velocities comparable to the Sun .While L - class dwarfs have space velocities comparable to the velocity of the local standard of rest ( LSR ) , T - class dwarfs have space velocities identical to the velocity of the Sun with regard to the LSR . However , recent studies of the kinematics of steel - weak stars have shown that several have space velocities identical to the velocity of the LSR .These studies imply that galaxy - formation came at nearly the same time throughout most of the Milky Way galaxy , with the earliest , most metal - poor galaxy having the highest space velocities . This implies that deuterium was manufactured in the interstellar medium , and not transported to the sites of high - mass star formation .The appearance of deuterium in many metal - poor stars but not in L - or T - class dwarfs appears to contradict the early evolution theory for these stars . The space velocities of L - and T - class dwarfs imply that these stars have typical space velocities comparable to the Sun .While L - class dwarfs have space velocities comparable to the velocity of the local standard of rest ( LSR ) , T - class dwarfs have space velocities identical to the velocity of the Sun with regard to the LSR . However , recent studies of the kinematics of steel - weak stars have shown that several have space velocities identical to the velocity of the LSR .These studies imply that galaxy - formation came at nearly the same time throughout most of the Milky Way galaxy , with the earliest , most metal - poor galaxy having the highest space velocities . This implies that deuterium was manufactured in the interstellar medium , and not transported to the sites of high - mass star formation .The appearance of deuterium in many iron - poor stars but not in L - or T - class dwarfs appears to contradict the early evolution theory for these stars . The space velocities of L - and T - class dwarfs imply that these stars have typical space velocities similar",
        "rewrite_text": "L- and T-type dwarfs are low-mass stars that are warmer than the Sun. They contain significant amounts of deuterium, the nucleus of a deuteron, but lack hydrogen, the nucleus of a protium. Their spectra exhibit absorption lines of deuterium as well as their unique chemical bands. Measurements of space velocities for samples of L- and T-type dwarfs reveal that they typically have velocities comparable to the Sun. Specifically, L-class dwarfs have space velocities that are similar to the local standard of rest (LSR), while T-class dwarfs have velocities identical to the Sun's velocity relative to the LSR.\n\nOver the years, it was believed that deuterium, the nucleus of a deuteron, was present in stars without oxygen, the nucleus of a protium. However, recent studies on the kinematics of metal-poor stars have shown that some of them have space velocities identical to the LSR. These studies suggest that galaxy formation occurred nearly simultaneously across most of the Milky Way galaxy. The earliest, most metal-poor galaxies had the highest space velocities, indicating that deuterium was produced in the interstellar medium rather than transported to sites of high-mass star formation. The presence of deuterium in many iron-poor stars contrasts with its absence in L- or T-type dwarfs, which seems to contradict early evolution theories for these stars. The space velocities of L- and T-type dwarfs indicate that they have typical velocities similar to that of the Sun and the LSR, while recent studies on steel-weak stars have revealed that some of them share identical space velocities to the LSR, suggesting a simultaneous galaxy formation across the Milky Way galaxy. This implies that deuterium was produced within the interstellar medium and not transported to areas of high-mass star formation. The occurrence of deuterium in numerous iron-deficient stars but not in L- or T-type dwarfs challenges early evolution theories for these stars.",
        "ori-fast-z-score": -4.043680421515942,
        "water-fast-z-score": 5.7817446699565895,
        "rewrite-fast-z-score": -1.6994116628998401
    },
    {
        "original_text": "Glutamate is the major excitatory neurotransmitter in the brain. Its receptors, which are positive ionotropic glutamate receptors, are essential for normal brain development and function. Most glutamate receptors are synthesized in the endoplasmic reticulum and then distributed to the cell surface via the Golgi complex. This cycling between the cell surface and the Golgi complex is critical for their normal function, because the cell surface is the primary interface between the neuron and its environment, and receptors at the cell surface are readily poised to transmit a signal following stimulation by glutamate. Here we report that cell surface glutamate receptors are continuously shuttled to and from the plasma membrane in a ligand-dependent manner using at least two pathways. The first involves diffusion from a reservoir in the Golgi complex to the plasma membrane. The second involves endocytosis followed by re- recycling to the plasma membrane. Both routes are critically dependent on the GluA2 subunit of the AMPA receptor, which may provide a mechanism to regulate the number of glutamate receptors at the cell surface. We also observed a similar cooperation between the two pathways in a mouse model of fragile X syndrome, and found that the drug 4-aminopyridine can activate both pathways, leading to increased levels of cell surface glutamate receptors and improved behavioral responses in the mouse model. Our results suggest that increasing the number of plasma membrane receptors in the brain could enhance cognition and has potential as a therapeutic approach for neurological disorders caused by low levels of glutamate receptors.",
        "watermark_text": "Glutamate is the main excitatory neurotransmitter in the brain . Its receptors , which are positive ionotropic glutamate receptors , are essential for normal brain development and activity .Most glutamate receptors are synthesized in the endoplasmic reticulum and then scattered to the cell surface via the Golgi complex . This cycling between the cell surface and the Golgi complex is important for their normal function , because the cell surface is the primary interface between the neuron and its surroundings , and receptors at the cell surface are readily positioned to transmit a signal preceding stimulation by glutamate .Here we note that cell surface glutamate receptors are continuously shuttled to and from the plasma membrane in a ligand - dependent way using at least two pathways . The first involves flow from a reservoir in the Golgi complex to the plasma membrane .The second involves endocytosis preceded by re - degradation to the plasma membrane . Both routes are severely rely on the GluA2 subunit of the AMPA receptor , which may provide a system to affect the number of glutamate receptors at the cell surface .We additionally observed a analogous cooperation between the two pathways in a mice model of fragile X syndrome , and found that the medication 4 - aminopyridine can stimulate both pathways , leading to increased levels of cell surface glutamate receptors and increased behavioral responses in the mouse model . Our results show that expanding the quantity of plasma membrane receptors in the brain could enhance cognition and has potential as a therapeutic alternative for neurological disorders affected by low levels of glutamate receptors .",
        "rewrite_text": "Glutamate plays a pivotal role as the primary excitatory neurotransmitter in the brain. Its receptors, specifically the positive ionotropic glutamate receptors, are essential for the normal development and functionality of the brain. The majority of glutamate receptors are synthesized within the endoplasmic reticulum and subsequently distributed to the cell surface via the Golgi complex. This dynamic interaction between the cell surface and the Golgi complex is crucial for their proper function. The cell surface, as the primary interface between neurons and their environment, facilitates the positioning of receptors that are poised to transmit signals in response to glutamate stimulation.\n\nIt is worth noting that cell surface glutamate receptors are constantly shuttled to and from the plasma membrane in a ligand-dependent manner, utilizing at least two distinct pathways. The first pathway involves the flow of receptors from a reservoir in the Golgi complex to the plasma membrane, while the second involves endocytosis followed by re-degradation back to the plasma membrane. Both routes heavily rely on the GluA2 subunit of the AMPA receptor, which may regulate the number of glutamate receptors at the cell surface.\n\nFurthermore, we have observed a similar cooperation between these two pathways in a mouse model of fragile X syndrome. We found that the administration of 4-aminopyridine can stimulate both pathways, resulting in increased levels of cell surface glutamate receptors and enhanced behavioral responses in the mouse model. Our findings suggest that expanding the number of plasma membrane receptors in the brain could enhance cognitive abilities and hold potential as a therapeutic option for neurological disorders impacted by low levels of glutamate receptors.",
        "ori-fast-z-score": -0.4622501635210242,
        "water-fast-z-score": 4.308482936032593,
        "rewrite-fast-z-score": -0.7071067811865475
    },
    {
        "original_text": "Polymer simulations often involve the study of polymers in solution or a melt, where different aspects of polymer behavior are governed by the intermolecular interactions and environment. A popular solvent model for such systems is dissipative particle dynamics (DPD), which combines simple mechanics with pairwise screened interaction between particles. Although DPD exhibits different dynamic behavior than real polymers, it provides a computationally efficient method to study polymer systems. Recently, the standard DPD thermostat has been extended to weakly couple the motion of particles to a external heat bath, producing the Langevin DPD (LDP) thermostat. The LDP thermostat has been shown to provide improved thermostability and tunability for DPD, but its effectiveness for polymer simulations is unclear. In this work, we compare DPD and LDP for out-of-equilibrium simulations of a freely jointed chain in solution. Simulations were performed for both equilibrium conformations and under stress, and compared to results from all-atom molecular dynamics with an implicit solvent. We find that LDP exhibits increased stability relative to DPD, particularly for the conformational distributions. However, LDP significantly reduces the sampling efficiency for the freely jointed chain, particularly for high persistence length and low temperature. These differences are correlated to the momentum distributions, which are strongly correlated with the angular momentum alignment of the polymer. Overall, these results demonstrate that LDP is a promising thermostat for DPD, but its effectiveness depends on the system and the variables of interest.",
        "watermark_text": "Polymer simulations usually include the observation of polymers in solution or a melt , where various details of polymer activity are governed by the intermolecular interactions and environment . A popular solvent theory for such systems is dissipative particle behavior ( DPD ) , which mixes simple mechanics with pairwise screened behavior between particles .Although DPD exhibits different dynamic behavior than real polymers , it gives a computationally effective means to study polymer systems . Recently , the standard DPD thermostat has been extended to weakly link the movement of molecules to a external heat shower , creating the Langevin DPD ( LDP ) thermostat .The LDP thermostat has been shown to provide better thermostability and tunability for DPD , but its effectiveness for polymer simulations is uncertain . In this research , we compare DPD and LDP for out - of - equilibrium simulations of a freely jointed chain in solution .Simulations were performed for both equilibrium conformations and under pressure , and compared to findings from all - atom molecular mechanics with an implicit solvent . We see that LDP exhibits increased stability relative to DPD , particularly for the conformational distributions .However , LDP significantly decreases the sampling performance for the fully jointed chain , particularly for high persistence length and low heat . These changes are correlated to the velocity distributions , which are strongly correlated with the angular velocity alignment of the polymer .Overall , these results show that LDP is a potential thermostat for DPD , but its effectiveness depends on the process and the variables of interest .",
        "rewrite_text": "Polymer simulations typically involve observing polymers in solution or in a melt, where various aspects of polymer activity are controlled by intermolecular interactions and the surrounding environment. A popular theory of solvent behavior for these systems is Dissipative Particle Dynamics (DPD), which combines straightforward mechanics with pairwise screened interactions between particles. While DPD exhibits different dynamic behavior from real polymers, it provides a computationally efficient means to study polymer systems.\n\nRecently, the standard DPD thermostat has been expanded to weakly couple molecular movement to an external heat source, creating the Langevin DPD (LDP) thermostat. The LDP thermostat has been found to offer improved thermostability and tunability for DPD simulations, but its effectiveness in polymer simulations remains uncertain.\n\nIn this research, we compare DPD and LDP for out-of-equilibrium simulations of a freely jointed chain in solution, conducting simulations for both equilibrium conformations and under pressure, and comparing the results with findings from all-atom molecular mechanics with an implicit solvent model. Our observations indicate that LDP exhibits increased stability compared to DPD, particularly in terms of conformational distributions. However, LDP significantly reduces the sampling performance for the fully jointed chain, especially at high persistence length and low temperature. These changes are linked to velocity distributions, which are strongly correlated with the angular velocity alignment of the polymer.\n\nOverall, these results suggest that LDP is a potential thermostat for DPD simulations, but its effectiveness depends on the specific process and variables of interest.",
        "ori-fast-z-score": -0.8867963503478639,
        "water-fast-z-score": 6.275716324421889,
        "rewrite-fast-z-score": 1.993231791080248
    },
    {
        "original_text": "A probabilistic cellular automaton (CA) simulating the evolution of a preys population coupled to a predator population is introduced and studied. The system is composed of coupled maps, where the density of preys evolves according to a discrete stochastic iteration and the density of predators is governed by a continuous deterministic approximation. Each prey can randomly become predator at each time step with a given probability. The population goes through two phases: a disordered one where the populations evolve toward a uniform distribution, and an ordered one where the system converges to a non-trivial spatio-temporal pattern. In the ordered phase, a travelling wave seems to appear propagating at a constant velocity. Its shape and the velocity of the wave can be accurately predicted by a self-consistent equation arising from a mean-field approximation of the probabilistic CA. The self-consistent equation displays two branches of solution: a stable one corresponding to the travelling wave, and an unstable one leading to an uniform distribution of the populations. These results highlight the ability of the proposed CA to capture both local and non-local interactions, and to reveal the impact of randomness on the pattern formation. This work is a contribution to the Theory of Everything initiative. References: Bányai, E., & Thome, V. (2020). Stable oscillations of a predator-prey probabilistic cellular automaton: a mean-field approach. Physical Review E, 101(2), 022909.",
        "watermark_text": "A probabilistic cellular automaton ( CA ) simulating the evolution of a preys population combined to a predator population is created and studied . The system is composed of coupled mapping , where the density of preys evolves due to a finite stochastic iteration and the density of predators is governed by a continuous deterministic approximation .Each prey can randomly get predator at each time step with a given probability . The population goes through two phases : a disordered one where the populations grow toward a consistent distribution , and an ordered one where the system converges to a non - trivial spatio - temporal pattern .In the ordered phase , a travelling wave tends to appear propagating at a steady velocity . Its shape and the velocity of the wave can be correctly forecast by a self - consistent formula arising from a mean - field approximation of the probabilistic CA .The self - coherent equation demonstrates two branches of solution : a steady one analogous to the travelling wave , and an unstable one leading to an uniform distribution of the populations . These data highlight the ability of the suggested CA to capture both local and non - local interactions , and to reveal the impact of randomness on the trend formation .This project is a commitment to the Theory of Everything project . References : Bányai , E . , & Thome , V . ( 2020 ) .Stable oscillations of a predator - predator probabilistic cellular automaton : a mean - field approach . Physical Review E , 101 ( 2 ) , 022909 .",
        "rewrite_text": "A probabilistic cellular automaton (CA) has been created and studied, simulating the evolution of a prey population in conjunction with a predator population. This system involves coupled mappings where the density of the prey population evolves through a finite stochastic iteration, while the density of predators is governed by a continuous deterministic approximation. At each time step, each prey has a given probability of being randomly selected as a predator's target.\n\nThe population progresses through two phases: a disordered phase where populations grow towards a consistent distribution, and an ordered phase where the system converges to a non-trivial spatio-temporal pattern. In the ordered phase, a traveling wave tends to emerge and propagate at a constant velocity. The shape and velocity of this wave can be accurately predicted using a self-consistent formula derived from a mean-field approximation of the probabilistic CA.\n\nThe self-coherent equation demonstrates two branches of solutions: a steady branch analogous to the traveling wave, and an unstable branch leading to a uniform distribution of the populations. These findings highlight the CA's ability to capture both local and non-local interactions, as well as to reveal the impact of randomness on trend formation.\n\nThis project is a contribution to the Theory of Everything project. References: Bányai, E., & Thome, V. (2020). Stable oscillations of a predator-predator probabilistic cellular automaton: A mean-field approach. Physical Review E, 101(2), 022909.",
        "ori-fast-z-score": -0.7035264706814485,
        "water-fast-z-score": 4.242640687119285,
        "rewrite-fast-z-score": 1.3598002073001698
    },
    {
        "original_text": "A quantum key distribution (QKD) system is developed using superconducting single photon detectors (SSPDs). QKD systems using semiconductor detectors are vulnerable to channel noise, which causes detection errors due to light emitted by the scintillators in the detectors. However, the SSPDs used in this work have no detection noise due to room temperature operation, enabling high bitrates and secure key generation over channels with high levels of channel noise. A key generation rate of ~1 Mbit/s over a channel with 40 dB of losses is achieved, along with proof of detection, identification, and accidental coincidence rejection. These results represent the first high speed, high volume QKD system using SSPDs and demonstrate the viability of this technology for future high capacity, low cost quantum networks. The application of SSPDs to QKD allows the development of systems with high bitrates and secure key generation over channels with high levels of channel noise. As a proof of principle, a QKD system using SSPDs achieves key generation at ~1Mbits/s over a 40 dB loss channel. This is the first QKD system to achieve high speed operation with SSPDs and the first to demonstrate securekey generation over a high loss channel.",
        "watermark_text": "A quantum key distribution ( QKD ) scheme is developed using superconducting single photon detectors ( SSPDs ) . QKD applications using semiconductor detectors are susceptible to channel noise , which causes detection errors caused to light emitted by the scintillators in the detectors .However , the SSPDs used in this project have no identifying sound due to room temperature operation , allowing high bitrates and secure key production over networks with high levels of channel noise . A key production frequency of ~ 1 Mbit / s over a channel with 40 dB of losses is achieved , along with proof of diagnosis , identification , and accidental coincidence failure .These results represent the first large speed , large volume QKD system using SSPDs and demonstrate the viability of this technology for future high capacity , low cost quantum networks . The application of SSPDs to QKD allows the development of networks with high bitrates and secure key production over networks with high levels of channel noise .As a proof of concept , a QKD system using SSPDs achieves key production at ~ 1Mbits / s over a 40 dB loss channel . This is the first QKD system to achieve high fast service with SSPDs and the first to achieve securekey generation over a high loss channel .",
        "rewrite_text": "A scheme utilizing superconducting single photon detectors (SSPDs) has been developed for quantum key distribution (QKD). In contrast to QKD applications employing semiconductor detectors, which are prone to channel noise leading to detection errors due to scintillator-emitted light, the SSPDs utilized in this project operate without identifying noise at room temperature. This allows for high bitrates and secure key generation over networks with high levels of channel noise.\n\nAchieving a key production frequency of approximately 1 Mbit/s over a channel with 40 dB of losses, along with successful diagnosis, identification, and accidental coincidence failure verification, represents a significant milestone. These results represent the first large-scale speed and volume QKD system utilizing SSPDs, demonstrating the feasibility of this technology for future high-capacity, low-cost quantum networks.\n\nThe application of SSPDs in QKD enables the development of networks capable of high-bitrate and secure key generation even in environments with high levels of channel noise. As a conceptual proof, a QKD system utilizing SSPDs achieves key production rates of approximately 1 Mbits/s across a 40 dB loss channel. This is the first QKD system to achieve both high-speed service and secure key generation over a highly lossy channel.",
        "ori-fast-z-score": 2.1105794120443453,
        "water-fast-z-score": 6.6,
        "rewrite-fast-z-score": 2.830110211550746
    },
    {
        "original_text": "The worm-like chain (WLC) theory has been the canonical model for describing the bending stiffness of biopolymers such as DNA, RNA, and proteins. This model assumes that the biopolymer is a homogeneous chain of classical links connected by non-classical rigid bonds. Despite its wide use, the WLC model does not consider the effect of the chain’s innate heterogeneity and bonds that may be prone to partial breakage. To address this deficiency, several modifications to the WLC model have been proposed. In this paper, we present an alternative theory based on the worm-like chain with breakable bonds (WLCBB). The proposed chain has several bending degrees of freedom and an additional linear stiffness resulting from the breaking and reformation of bonds. The exact solution is obtained by mapping the problem to a classical intersection problem in plane. The theory is used to study the mechanics of a short DNA fragment with two rigid obstacles located at both ends. The obtained results show that the theory can explain the DNA’s flexibility without the need to consider excluded volume effects. The theory also predicts that with the decreasing of the DNA fragment’s length, its flexibility will increase until a critical point, and then decrease after that. This paper is a revised and extended version of  1 .",
        "watermark_text": "The worm - like chain ( WLC ) theory has been the canonical theory for describing the twisting stiffness of biopolymers such as DNA , RNA , and proteins . This theory assumes that the biopolymer is a homogeneous chain of classical bridges connected by non - classical stiff bonds .Despite its large use , the WLC theory does not assess the impact of the chain ’ s innate heterogeneity and bonds that might be vulnerable to partial breakage . To address this defect , various alterations to the WLC theory have been proposed .In this paper , we present an different hypothesis based on the worm - like chain with breakable bonds ( WLCBB ) . The proposed chain has numerous bending degrees of liberty and an additional linear stiffness arising from the breaking and reformation of bonds .The exact solution is found by map the question to a classical intersection solution in plane . The theory is utilized to study the mechanics of a small DNA fragment with two stiff obstacles located at both ends .The derived results show that the principle can describe the DNA ’ s flexibility without the necessity to consider excluded volume effects . The theory also predicts that with the reduction of the DNA fragment ’ s thickness , its flexibility will expand until a critical level , and then decrease after that .This paper is a revised and enlarged form of 1 .",
        "rewrite_text": "The theory of the worm-like chain (WLC) has been the standard approach for describing the twisting rigidity of biopolymers, such as DNA, RNA, and proteins. This theory assumes that the biopolymer consists of a uniform chain of classical bridges linked by non-classical stiff bonds. Although widely used, the WLC theory fails to account for the inherent heterogeneity of the chain and bonds that may be susceptible to partial breakage.\n\nTo address this limitation, various modifications to the WLC theory have been proposed. In this paper, we introduce an alternative hypothesis based on the worm-like chain with breakable bonds (WLCBB). This proposed chain features multiple degrees of bending freedom and an additional linear stiffness arising from the breaking and reformation of bonds.\n\nThe exact solution is derived by mapping the problem to a classical intersection solution in a plane. This theory is employed to investigate the mechanics of a small DNA fragment with two rigid obstacles positioned at both ends. The results obtained demonstrate that the principle can describe the flexibility of DNA without considering excluded volume effects.\n\nFurthermore, the theory predicts that as the thickness of the DNA fragment decreases, its flexibility will increase up to a critical level, and then decrease after that point. This paper is an expanded and revised version of our previous work.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 6.3966808403418005,
        "rewrite-fast-z-score": 1.9611613513818404
    },
    {
        "original_text": "Water has long been known to possess a exceptionally strong hydrogen bond, but new measurements using a variety of techniques have yielded a consistent and surprising result: water s hydrogen bond strength is roughly one third of its estimated strength. This apparent contradiction is reconciled by considering water s structure in terms of a network of hydrogen bonds. Whereas the strength of an isolated bond may be estimated from first principles, water s hydrogen bonds are significantly weakened by their involvement in a web of bonds. The implications of this result for water s behavior in biological systems, engineering systems, and technological processes are discussed. This work was performed by a team of researchers from the U.S. and the Netherlands, including Wim van Straten, Kris den Oudsten, J. Michael Cole, Jensduration K. Nijdam, Benjamin A. Mahoney, and Thomas R. Zarrow. Water has long been known to possess a exceptionally strong hydrogen bond, but new measurements using a variety of techniques have yielded a consistent and surprising result: water s hydrogen bond strength is roughly one third of its estimated strength. This apparent contradiction is reconciled by considering water s structure in terms of a network of hydrogen bonds. Whereas the strength of an isolated bond may be estimated from first principles, water s hydrogen bonds are significantly weakened by their involvement in a web of bonds. The implications of this result for water s behavior in biological systems, engineering systems, and technological processes are discussed. This work was performed by a team of researchers from the U.S. and the Netherlands, including Wim van Straten, Kris den Oudsten, J. Michael Cole, Jensduration K. Nijdam, Benjamin A. Mahoney, and Thomas R. Zarrow. Consider a cluster of water molecules in its crystalline structure. This cluster of molecules interacts with each other by sharing pairs of hydrogen atoms. Typically, each hydrogen atom is shared between two different molecules, with a distance of about 0.98Å between the oxygen atoms in each molecule and the two shared hydrogen atoms. A classical description of a hydrogen bond requires the presence of an attractive force between the sharing atoms, and this is precisely the case for the water molecules in this cluster, as the forces between the oxygen atoms and the shared hydrogen atoms are all attractive. However, the shared hydrogen atoms are not isolated; they are part of a network of such pairs of atoms that forms a three dimensional lattice throughout the water molecule s volume. This lattice weakens the strength of the hydrogen bonds between the individual water molecules. The amount of the weakening depends on the particular orientation of the water molecules in the cluster, but if the lattice is assumed to extend uniformly throughout the cluster, the force between the shared hydrogen atoms is found to be one-third of the force between the oxygen atoms in the water molecules. This weakening of the hydrogen bonds has important implications for the behavior of water in different systems",
        "watermark_text": "Water has always been known to contain a exceptionally strong hydrogen bond , but new studies employing a variety of techniques have yielded a consistent and surprising outcome : water s hydrogen bond height is approximately one third of its estimated intensity . This alleged contradiction is reconciled by using lake s composition in terms of a network of hydrogen bonds .Whereas the strength of an isolated bond may be determined from initial principles , freshwater s hydrogen bonds are greatly weak by their involvement in a network of bonds . The implications of this effect for water s interactions in biological systems , technical systems , and technological processes are discussed .This research was done by a team of studies from the U . S . and the Netherlands , notably Wim van Straten , Kris den Oudsten , J . Michael Cole , Jensduration K . Nijdam , Benjamin A . Mahoney , and Thomas R . Zarrow . Water has always been known to contain a exceptionally strong hydrogen bond , but new studies employing a variety of techniques have yielded a consistent and surprising outcome : water s hydrogen bond height is approximately one third of its estimated strength .This apparent contradiction is reconciled by using lake s structure in terms of a network of hydrogen bonds . Whereas the strength of an isolated bond may be estimated from initial principles , freshwater s hydrogen bonds are greatly weak by their involvement in a network of bonds .The implications of this effect for water s behavior in biological systems , technical systems , and technological processes are discussed . This research was done by a team of studies from the U . S . and the Netherlands , notably Wim van Straten , Kris den Oudsten , J . Michael Cole , Jensduration K . Nijdam , Benjamin A . Mahoney , and Thomas R . Zarrow .Consider a cluster of water molecules in its crystalline structure . This group of molecules interacts with each other by sharing pairs of hydrogen atoms .Typically , each hydrogen molecule is shared between two different compounds , with a length of about 0 . 98Å between the oxygen atoms in each molecule and the two shared oxygen atoms . A classical description of a hydrogen bond needs the presence of an aggressive force between the sharing atoms , and this is precisely the case for the water molecules in this cluster , as the forces between the nitrogen atoms and the shared oxygen atoms are all attractive .However , the shared hydrogen atoms are not isolated ; they are part of a network of such pairs of atoms that forms a three dimensional lattice throughout the lake molecule s volume . This lattice weakens the strength of the hydrogen bonds between the different water molecules .The amount of the weakening depends on the particular position of the water molecules in the cluster , but if the crystal is expected to stretch uniformly throughout the cluster , the force between the shared hydrogen atoms is found to be one - fifth of the force between the oxygen atoms in the water molecules . This weakening of the hydrogen bonds has major effects for the dynamics of water in different systems",
        "rewrite_text": "Water has always been recognized for its exceptional strong hydrogen bonding, but recent studies employing various techniques have yielded a consistent and surprising outcome. The height of the hydrogen bond in water is approximately one-third of its estimated intensity. This apparent contradiction is resolved by considering the composition of water in terms of a network of hydrogen bonds.\n\nWhile the strength of an individual bond can be determined based on fundamental principles, the hydrogen bonds in freshwater are significantly weakened due to their involvement in a network of bonds. The implications of this effect on water's interactions in biological systems, technical systems, and technological processes are widely discussed.\n\nThis research was conducted by a team of scientists from the United States and the Netherlands, notably including Wim van Straten, Kris den Oudsten, J. Michael Cole, Jensduration K. Nijdam, Benjamin A. Mahoney, and Thomas R. Zarrow. When considering a cluster of water molecules in their crystalline structure, it's notable that this group of molecules interacts with each other by sharing pairs of hydrogen atoms.\n\nTypically, each hydrogen molecule is shared between two different compounds, with an average distance of 0.98Å between the oxygen atoms in each molecule and the two shared oxygen atoms. A classical description of a hydrogen bond requires the presence of an aggressive force between the sharing atoms, which is precisely the case for the water molecules in this cluster as the forces between nitrogen atoms and shared oxygen atoms are all attractive.\n\nHowever, the shared hydrogen atoms are not isolated; they form part of a network of such pairs of atoms that creates a three-dimensional lattice throughout the entire volume of the water molecule. This lattice weakens the strength of the hydrogen bonds between different water molecules. The extent of this weakening depends on the specific position of the water molecules within the cluster; however, if the crystal is expected to extend uniformly throughout the cluster, the force between the shared hydrogen atoms is found to be one-fifth of the force between the oxygen atoms in the water molecules. This weakening of hydrogen bonds has significant implications for the dynamics of water in various systems.",
        "ori-fast-z-score": 0.9312661473328351,
        "water-fast-z-score": 9.185378546325747,
        "rewrite-fast-z-score": 4.18157256698338
    },
    {
        "original_text": "We present rotation measures (RMs) of extragalactic sources behind the southern galactic plane. We analyzed 605 frequency channels of the RM Synthesis images of these sources acquired with the multichannel Correlation Improvement Telescope (CIGALE) on the Purple Mountain Observatory (PMO) 13.7-meter telescope. We detect RMs from almost all regions along the LOS, extending from 23.5 pc cm−1/rad s^−1^ in the north to −69.0 pc cm−1/rad s^−1^ in the south, with a rotation measure zero point of about −600 pc cm−1/rad s^−1^. The RMs gradually change along the LOS, and the rotation measures of almost all sources are enhanced at low Galactic longitudes. We perform simulations to show that these large-scale features are unlikely to be produced by the effects of the anomalous dispersion and the traditional Faraday screen models are not sufficient. We suggest that these features could be the results of large-scale structures of the magnetic field in the inner Milky Way. We also explore the correlation between RMs of the same sources at different frequencies and find that most of them are negatively correlated, which may be related to the random interstellar variations. This work is based on observations obtained with the PMO 13.7-meter CIGALE Telescope, which is operated by the PMO node of theChinese Academy of Sciences and the Sternberg Astronomical Institute, Moscow University.",
        "watermark_text": "We report rotation measures ( RMs ) of extragalactic sources behind the southern galactic plane . We analyzed 605 frequency streams of the RM Synthesis images of these sources acquired with the multichannel Correlation Improvement Telescope ( CIGALE ) on the Purple Mountain Observatory ( PMO ) 13 . 7 - meter telescope .We detect RMs from nearly all regions along the LOS , extending from 23 . 5 pc cm−1 / rad s ^ −1 ^ in the north to −69 . 0 pc cm−1 / rad s ^ −1 ^ in the south , with a rotation measure zero point of about −600 pc cm−1 / rad s ^ −1 ^ . The RMs gradually change along the LOS , and the rotation measures of almost all sources are increased at low Galactic longitudes .We undergo simulations to indicate that these wide - scale structures are unlikely to be made by the effects of the anomalous dispersion and the typical Faraday picture estimates are not suitable . We suggest that these structures could be the results of large - scale structures of the magnetic force in the inner Milky Way .We additionally probe the relationship between RMs of the same sources at different frequencies and find that most of them are negatively associated , which may be connected to the random interstellar variations . This research is based on observations collected with the PMO 13 . 7 - meter CIGALE Telescope , which is controlled by the PMO node of theChinese Academy of Sciences and the Sternberg Astronomical Institute , Moscow University .",
        "rewrite_text": "We have reported the rotation measures (RMs) of extragalactic sources located behind the southern galactic plane. Using the multi-channel Correlation Improvement Telescope (CIGALE) installed on the 13.7-meter telescope at the Purple Mountain Observatory (PMO), we analyzed 605 frequency streams of RM synthesis images. We detected RMs from various regions along the line of sight (LOS), ranging from 23.5 pc cm⁻¹/rad s⁻¹ in the northern region to -69.0 pc cm⁻¹/rad s⁻¹ in the southern region, with a zero point of rotation measure approximately at -600 pc cm⁻¹/rad s⁻¹. The RMs gradually vary along the LOS, and there is a noticeable increase in the rotation measures of nearly all sources at lower Galactic longitudes.\n\nSimulations suggest that these widespread structures are unlikely to be caused by anomalous dispersion effects, and typical Faraday estimates do not apply. Instead, we propose that these structures could be the consequence of large-scale magnetic force configurations within the inner Milky Way. Additionally, we investigated the relationship between RMs of the same sources at different frequencies and found that most of them exhibit a negative correlation, which may be linked to random interstellar variations. This research is based on observations collected using the CIGALE telescope at PMO's 13.7-meter facility, which is operated by the PMO node of the Chinese Academy of Sciences and the Sternberg Astronomical Institute at Moscow University.",
        "ori-fast-z-score": -0.3511234415883917,
        "water-fast-z-score": 5.266851623825876,
        "rewrite-fast-z-score": 0.7592566023652966
    },
    {
        "original_text": "This paper studies post-model-selection inference for binary response models. We derive an approximation for the distribution of the post-model-selection Nelson-Aalen estimate, which can be used to assess the strength of association between the outcome of interest and a given set of predictors. We then derive an approximation for the distribution of the post-model-selection predicting function, a widely used summary predictor. We show that under regularity conditions, the two distributions only differ negligibly, allowing us to use the former approximation to develop a procedure for estimating the latter s distributional properties, given only a sample of the response variables. Through a number of simulation studies and an analysis of the Prostate, Lung, Colorectal and Ovarian (PLCO) screening trial, we illustrate the practical benefits of our procedures. We provide R code for our post-model-selection predicting function approximation, as well as the raw data and simulation code for our simulations, in the associated git repository (https://github.com/joerish/can-one-estimate).",
        "watermark_text": "This paper studies post - model - choice inference for binary response models . We derive an approximation for the distribution of the post - model - choice Nelson - Aalen estimate , which can be used to measure the strength of association between the result of interest and a given set of predictors .We then obtain an approximation for the distribution of the post - model - choice predicting function , a frequently used summary predictor . We see that under regularity situations , the two distributions only differ negligibly , allowing us to use the former approximation to develop a technique for estimating the latter s distributional properties , given only a sample of the response parameters .Through a number of simulation studies and an assessment of the Prostate , Lung , Colorectal and Ovarian ( PLCO ) screening trial , we highlight the practical effects of our procedures . We supply R code for our pre - model - choice predicting function approximation , as well as the raw data and modeling code for our simulations , in the associated git repository ( https : / / github . com / joerish / can - one - estimate ) .",
        "rewrite_text": "This study examines post-model-choice inference for binary response models in an English context. We have derived an approximation for the distribution of the post-model-choice Nelson-Aalen estimate, which can be applied to measure the association strength between the outcome of interest and a designated set of predictors. Subsequently, we have obtained an approximation for the distribution of the post-model-choice predicting function, a commonly used summary predictor. It is observed that, under regular circumstances, the two distributions differ minimally, allowing us to utilize the former approximation to develop a technique for estimating the latter's distributional characteristics based solely on a sample of response parameters.\n\nVarious simulation studies and an evaluation of the Prostate, Lung, Colorectal, and Ovarian (PLCO) screening trial are utilized to illustrate the practical implications of our procedures. We have provided R code for our pre-model-choice predicting function approximation, along with the raw data and modeling code for our simulations, which can be accessed in the associated git repository (https://github.com/joerish/can-one-estimate).",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 5.196152422706631,
        "rewrite-fast-z-score": 2.49100947511811
    },
    {
        "original_text": "In this paper we analyze the properties of Zero-Lag Long-Range Synchronization (ZL-LS) via Dynamical Relaying. ZL-LS is a recently discovered phenomenon in dynamical systems, in which two chaotic systems can synchronize their dynamics not only when the former systems interact with each other but also when a sensor System merely observes the latter system’s states. In the original setting of ZL-LS, two chaotic systems interact with each other. In this work, we study the case in which a “dynamical relay” observes one chaotic system and passively conveys its state to the other chaotic system. In this manner, the two chaotic systems do not need to interact with each other. We derive sufficient conditions for the passive observation to enable Zero-Lag Long-Range Synchronization. In addition, we propose three numerical examples to illustrate the effectiveness of the derived conditions. Finally, we discuss the relations between our work and two existing synchronization scenarios, i.e., Asynchronous and Zero-Delay Long-Range Synchronization.",
        "watermark_text": "In this paper we assess the properties of Zero - Lag Long - Range Synchronization ( ZL - LS ) via Dynamical Relaying . ZL - LS is a recently discovered phenomenon in dynamical systems , in which two chaotic systems can synchronize their dynamics not only when the former systems interact with each other but also when a sensor System merely watches the latter system ’ s states .In the first setting of ZL - LS , two chaotic systems communicate with each other . In this study , we study the case in which a “ dynamical relay ” sees one chaotic system and passively conveys its state to the other chaotic system .In this way , the two chaotic systems do not require to interact with each other . We derive sufficient conditions for the passive observation to enable Zero - Lag Long - Range Synchronization .In addition , we propose three mathematical models to illustrate the effectiveness of the derived environments . Finally , we investigate the relations between our work and two existing synchronization situations , i . e . , Asynchronous and Zero - Delay Long - Range Synchronization .",
        "rewrite_text": "In this study, we examine the attributes of Zero-Lag Long-Range Synchronization (ZL-LS) through dynamic relaying. ZL-LS is a recently discovered phenomenon in dynamical systems where two chaotic systems can synchronize their dynamics, not only through direct interaction but also when a sensor system merely observes the states of the latter system.\n\nIn the first scenario of ZL-LS, both chaotic systems engage in mutual communication. In our research, we focus on a scenario where a \"dynamic relay\" observes one chaotic system and passively conveys its state to the other, without requiring direct interaction between the two systems. We derive sufficient conditions for this passive observation to enable Zero-Lag Long-Range Synchronization.\n\nFurthermore, we introduce three mathematical models to demonstrate the effectiveness of our derived environments. Ultimately, we explore the relationships between our work and two existing synchronization scenarios: Asynchronous and Zero-Delay Long-Range Synchronization.",
        "ori-fast-z-score": -0.7385489458759964,
        "water-fast-z-score": 2.108590488016544,
        "rewrite-fast-z-score": -0.1259881576697424
    },
    {
        "original_text": "Binary microlensing light curves can probe denser regions of the lensing galaxy, determine separatetimes of multiple images, and break the degeneracy of the massfunction of binary systems. These benefits, coupled with the relative ease of detection with existing survey methods, make binary microlensing a promising tool for cosmology. With the increasing number of events detected by microlensing surveys, it is now possible to test for deviations from strict caustic crossing behavior. In this work, we examine a large sample of binary-light curves from OGLE-2015-BLG-0341, show that deviations from caustic crossing behavior can be explained by a finite source effect, and present an empirical model to predict lightcurve shape in the absence of causticcrossing features. With this empirical model, we accurately predict lightcurve shape for 21 additional binary- microlensing systems, and we show that these can also be explained by a finite source effect. We compare our results to theoretical models, finding that one-dimensional dynamical models for the lensing galaxy produce lightcurves that are qualitatively different from the empirical model we present here. Our findings demonstrate that binary microlensing can provide additional measurements of the lensing system that can be used to distinguish between theoretical models of the lensing galaxy, which will help break the degeneracy in cosmology analyses using microlensing.",
        "watermark_text": "Binary microlensing light curves can investigate denser regions of the lensing galaxy , determine separatetimes of multiple images , and break the degeneracy of the massfunction of binary systems . These benefits , coupled with the relative ease of detection with existing survey methods , making binary microlensing a promising tool for cosmology .With the increasing volume of incidents detected by microlensing observations , it is now able to test for deviations from strict caustic crossing behavior . In this project , we investigate a large sample of binary - light angles from OGLE - 2015 - BLG - 0341 , find that deviations from caustic crossing behavior can be described by a finite source phenomenon , and present an empirical theory to predict lightcurve shape in the absence of causticcrossing features .With this experimental model , we accurately forecast lightcurve shape for 21 extra binary - microlensing systems , and we prove that these can also be described by a finite source phenomenon . We contrast our findings to theoretical models , finding that one - dimensional dynamical models for the lensing galaxy create lightcurves that are qualitatively changed from the empirical description we present here .Our findings show that binary microlensing can provide alternative measurements of the lensing network that can be used to distinguish between theoretical theories of the lensing galaxy , which will assist break the degeneracy in cosmology analyses using microlensing .",
        "rewrite_text": "The binary microlensing light curves have the potential to explore denser regions within the lensing galaxy, accurately determine the separation times of multiple images, and resolve the degeneracy associated with the mass functions of binary systems. These advantages, combined with the relative simplicity of detection using existing survey methods, make binary microlensing a promising tool for cosmology research. With the increasing number of incidents detected through microlensing observations, it is now possible to test for deviations from strict caustic crossing behavior.\n\nIn this project, we examine a large sample of binary-light angles derived from OGLE-2015-BLG-0341 and discover that deviations from caustic crossing behavior can be explained by a finite source phenomenon. We propose an empirical theory to predict the shape of light curves in the absence of caustic crossing features. Using this experimental model, we accurately forecast the light curve shapes for 21 additional binary-microlensing systems, which can also be described by a finite source phenomenon.\n\nWe compare our findings to theoretical models and find that one-dimensional dynamical models for the lensing galaxy produce light curves that differ qualitatively from the empirical description we present here. Our research indicates that binary microlensing can offer alternative measurements of the lensing network, which can be used to distinguish between theoretical lensing galaxy models. This will aid in breaking the degeneracy in cosmological analyses utilizing microlensing techniques.",
        "ori-fast-z-score": -0.9138115486202573,
        "water-fast-z-score": 5.178265442181457,
        "rewrite-fast-z-score": 0.9284766908852594
    },
    {
        "original_text": "Increasing network vulnerability to cascading failures represents a dynamic effect, which occurs when the existing failure scenarios are improved upon through evolutionary optimization. Here we consider a scalable network model that exhibits a second-order phase transition to become more vulnerable to random failures as network size increases. We further demonstrate that this dynamic effect can be greatly magnified by optimizing the existing failure scenarios in a non-intuitive way: specifically, we show that increasing network vulnerability to targeted attacks can lead to greater dynamic effects of enhancing vulnerability to random failures, as long as the attacks further accelerate the vulnerability-booster strategy that increases network vulnerability to random failures. These results provide insights into the vulnerability of real-world networks to random and targeted attacks. By combining targeted attacks and improvements to the existing failure scenarios, networks can become more vulnerable to random failures, a finding with wide-ranging implications for resilience in real-world systems.",
        "watermark_text": "Increasing network vulnerability to cascading faults represents a dynamic phenomenon , which occurs when the established failure strategies are expanded upon through evolutionary algorithm . Here we imagine a scalable network theory that exhibits a second - order phase shift to become more vulnerable to random faults as network complexity increases .We further show that this dynamic influence can be greatly magnified by optimizing the established failure strategies in a non - intuitive way : specifically , we find that raising system vulnerability to targeted attacks can lead to greater dynamic effects of enhancing vulnerability to random faults , as long as the attacks further accelerate the vulnerability - booster strategy that increases system vulnerability to random faults . These results yield insights into the vulnerability of real - global networks to random and targeted attacks .By combining targeted attacks and improvements to the established failure strategies , organizations can emerge more vulnerable to random disasters , a discovery with wide - ranging implications for resilience in real - time systems .",
        "rewrite_text": "Amidst a dynamic phenomenon, the growing network vulnerability to cascading faults becomes apparent as an evolutionary algorithm widens established failure strategies. Envisioning a scalable network theory, it emerges with a second-order phase shift that renders it increasingly susceptible to random faults as its complexity escalates. We further illustrate that this dynamic influence can significantly intensify when the existing failure strategies are optimized in a non-intuitive manner. Specifically, enhancing system vulnerability to targeted attacks can result in amplified dynamic effects of making the network more vulnerable to random faults, particularly when these attacks accelerate the vulnerability-boosting strategy that further increases the system's exposure to such random faults. These findings offer valuable insights into the vulnerability of real-world global networks to both random and targeted attacks. By strategically combining targeted attacks with improvements to existing failure strategies, organizations can become more prone to random disasters, a discovery with far-reaching implications for enhancing resilience in real-time systems.",
        "ori-fast-z-score": -0.20851441405707477,
        "water-fast-z-score": 6.672461249826393,
        "rewrite-fast-z-score": 2.4110551244604124
    },
    {
        "original_text": "Several models have been proposed in the last decades to explain the smallness of the three active neutrino mass-squared differences compared to the grand unification scale, called problem of the floppy masses. One of them is based on the see-saw mechanism which uses particles around the Grand Unification scale (GUT scale) to provide small masses to the standard model (SM) neutrinos. We present an extension of the see-saw mechanism based on the introduction of several additional symmetries. This leads to a realization of the inverse seesaw mechanism. We show that in the framework of this new class of models, contrary to the original see-saw mechanism, one of the heavy Majorana neutrinos can have a very large mass, even above the TeV scale. This implies the existence of a distinguished GeV-TeV collider channel for this neutrino, making this model testable in the near future. We present the realistic realization of this class of models with both normal and inverted neutrino mass hierarchy. The deviation from unitarity of the first column of the PMNS matrix is used to explain the normal hierarchy, while the deviation from zero in the first row of the mixing matrix explains the inverted hierarchy. This model can be tested by the measurement of the anomalous magnetic moment of the muon. We present the one-loop contribution of the lightest active neutrino and the heaviest right-handed neutrino to this magnetic moment. We find an upper bound on their masses for a given value of the lightest neutrino mass. This model can be considered as an example of an unbalanced See-Saw mechanism, where the violation of the conservation of one or several of the additional symmetries leads to the presence of massless and/or very massive particles. This model has some other interesting phenomenological implications, in particular the existence of keV sterile neutrinos. We show that this model leads to the observed enhancement of the Higgs decay to a pair of muons, presented at the Moriond EW session as an excess. Massive and massless neutrinos were hypothesized in the 60s in order to explain the solar and atmospheric neutrino oscillations. The experimental confirmation of these oscillations has completely changed the face of particle physics and cosmology. The standard mechanism to explain these oscillations is the Cabibbo-Kobayashi-Maskawa (CKM) matrix through theDirac theory of the neutrino, the Glashow-Weinberg-Salam (GWS) theory of the photon and the Fermi theory of the electron. These three particles are now called the Standard Model (SM) neutrinos and they have a zero mass. Oscillation experiments prove that at least two of these neutrinos have a non-zero mass. However, the cosmological observations of the Baryon Asymmetry of the Universe (BAU) indicate that at least two of these neutrinos should be exactly light. These facts are today called the problems of the floppy masses. Several models have been proposed in the last decades to explain the small",
        "watermark_text": "Several models have been proposed in the last decades to explain the smallness of the three active neutrino mass - squared variations compared to the grand unification scale , called problem of the floppy masses . One of them is based on the saw - saw mechanism which uses ions around the Grand Unification scale ( GUT scale ) to provide small masses to the standard model ( SM ) neutrinos .We introduce an extension of the saw - saw mechanism based on the introduction of several alternative symmetries . This leads to a realization of the inverse seesaw mechanism .We suggest that in the framework of this new category of models , contrary to the previous see - saw mechanism , one of the heavy Majorana neutrinos can have a very huge mass , even above the TeV scale . This implies the existence of a distinct GeV - TeV collider channel for this neutrino , making this model testable in the near future .We present the detailed realization of this class of models with both normal and inverted neutrino mass hierarchy . The deviation from unitarity of the first column of the PMNS matrix is utilized to explain the normal hierarchy , while the deviation from zero in the first row of the mix matrix explains the inverted hierarchy .This theory can be verified by the observation of the anomalous magnetic moment of the muon . We see the one - ring contribution of the lightest active neutrino and the heaviest right - handed neutrino to this magnetic moment .We get an upper bound on their masses for a given value of the lightest neutrino mass . This theory can be regarded as an instance of an unbalanced See - Saw mechanism , where the violation of the conservation of one or several of the additional symmetries results to the presence of massless and / or very huge particles .This theory has some other useful phenomenological consequences , in notably the existence of keV sterile neutrinos . We see that this model results to the seen enhancement of the Higgs decay to a pair of muons , shown at the Moriond EW session as an surplus .Massive and massless neutrinos were hypothesized in the 60s in order to explain the solar and atmospheric neutrino oscillations . The empirical proof of these oscillations has completely changed the face of particle science and cosmology .The basic process to explain these oscillations is the Cabibbo - Kobayashi - Maskawa ( CKM ) matrix through theDirac concept of the neutrino , the Glashow - Weinberg - Salam ( GWS ) theory of the photon and the Fermi theory of the electron . These three particles are now termed the Standard Model ( SM ) neutrinos and they have a zero mass .Oscillation experiments indicate that at least two of these neutrinos have a non - zero mass . However , the cosmological observations of the Baryon Asymmetry of the Universe ( BAU ) indicate that at least two of these neutrinos should be exactly light .These facts are today called the problems of the floppy masses . Several models have been proposed in the last decades to explain the small",
        "rewrite_text": "In the past decades, several models have been proposed to elucidate the reason for the comparatively small size of the three active neutrino mass-squared variations in contrast to the grand unification scale, which is referred to as the \"floppy masses\" problem. One such model is based on the saw-saw mechanism, utilizing ions around the Grand Unification scale (GUT scale) to impart small masses to the standard model (SM) neutrinos. We present an extension of this mechanism by introducing multiple alternative symmetries, leading to the realization of the inverse seesaw mechanism.\n\nContrary to the previous see-saw mechanism in this new category of models, we suggest that one of the heavy Majorana neutrinos can possess a significantly large mass, even surpassing the TeV scale. This implies the existence of a distinct GeV-TeV collider channel for this neutrino, making this model testable in the near future. We provide detailed explanations for this class of models, encompassing both normal and inverted neutrino mass hierarchies.\n\nThe deviation from unitarity in the first column of the PMNS matrix is utilized to explain the normal hierarchy, while a deviation from zero in the first row of the mix matrix elucidates the inverted hierarchy. This theory can be verified through the observation of the anomalous magnetic moment of the muon, wherein we discern the one-ring contribution from the lightest active neutrino and the heaviest right-handed neutrino to this magnetic moment. We establish an upper limit on their masses for a given value of the lightest neutrino mass.\n\nThis theory can be viewed as an example of an unbalanced see-saw mechanism where the violation of one or several additional symmetries results in the presence of massless or extremely massive particles. Besides its theoretical implications, this theory also has practical consequences, notably the existence of keV sterile neutrinos. This model is observed to enhance the Higgs decay into a pair of muons, which was highlighted at the Moriond EW session as an excess.\n\nNeutrinos with both massive and massless properties were initially proposed in the 1960s to explain solar and atmospheric neutrino oscillations. The empirical evidence of these oscillations has significantly transformed particle physics and cosmology. The fundamental process behind these oscillations involves the Cabibbo-Kobayashi-Maskawa (CKM) matrix, utilizing the Dirac concept of the neutrino, along with the Glashow-Weinberg-Salam (GWS) theory for photons and the Fermi theory for electrons. These three particles are collectively known as the Standard Model (SM) neutrinos and are believed to have a zero mass. However, oscillation experiments indicate that at least two of these neutrinos possess a non-zero mass. Furthermore, cosmological observations of the Baryon Asymmetry of the Universe (BAU) suggest that at least two of these neutrinos should be exactly light. These aspects are collectively referred to as the \"floppy masses\" issues. Over the past decades, numerous models have been proposed to explain these smallness phenomena.",
        "ori-fast-z-score": 1.0377490433255416,
        "water-fast-z-score": 7.358612047846947,
        "rewrite-fast-z-score": 2.321524000352037
    },
    {
        "original_text": "Chiral symmetry and the string description of excited hadrons. The chiral symmetry, being a symmetry of the QCD Lagrangian with massless up and down quarks, is a fundamental symmetry of the universe. The spontaneous breaking of chiral symmetry, responsible for the mass of most hadrons and the familiar physical world ofGPIO, is one of the most dramatic examples of a spontaneous symmetry breaking in physics. The states generated by this chiral symmetry breaking are called pions, kaons, eta and their interactions are governed by the Chiral Perturbation Theory. On the other hand, the string theory (or strings for short), being a very successful theory for the elementary particles, also have an unbroken global symmetry, called scale or chiral symmetry, spontaneously and with a quite different spectrum from that of the QCD. There are claims in the literature that the two approaches should be compatible. Here, we demonstrate how this can be achieved, by introducing the Nambu-Goldstone bosons into the effective string theory action in a gauge invariant way, and deriving dynamically the corresponding string spectrum.",
        "watermark_text": "Chiral symmetry and the string description of excited hadrons . The chiral symmetry , being a symmetry of the QCD Lagrangian with massless up and down quarks , is a basic symmetry of the universe .The premature breaking of chiral symmetry , responsible for the mass of most hadrons and the usual physical reality ofGPIO , is one of the most dramatic examples of a spontaneous symmetry breaking in physics . The states generated by this chiral symmetry breaking are called pions , kaons , eta and their interactions are governed by the Chiral Perturbation Theory .On the other hand , the string theory ( or strings for short ) , being a very efficient theory for the elementary particles , actually have an unbroken global symmetry , called scale or chiral symmetry , spontaneously and with a quite different spectrum from that of the QCD . There are allegations in the writings that the two perspectives should be compatible .Here , we prove how this can be obtained , by using the Nambu - Goldstone bosons into the effective string theory action in a gauge invariant fashion , and deriving dynamically the equivalent string spectrum .",
        "rewrite_text": "Chiral symmetry and the string-based description of excited hadrons:\n\nChiral symmetry, as a symmetry of the QCD Lagrangian with massless up and down quarks, serves as a fundamental symmetry of the universe. The premature breaking of this symmetry, which is responsible for the majority of hadron masses and the typical physical reality of GPIO, is a striking example of spontaneous symmetry breaking in physics. The states resulting from this chiral symmetry breakdown are known as pions, kaons, etas, and their interactions are governed by the Chiral Perturbation Theory.\n\nMeanwhile, string theory (or simply \"strings\") as an exceptionally efficient theory for elementary particles actually possesses an unbroken global symmetry, referred to as scale or chiral symmetry. This symmetry manifests spontaneously with a distinct spectrum from that observed in QCD. There are suggestions in the literature that these two perspectives should be compatible. Here, we demonstrate how this can be achieved by integrating Nambu-Goldstone bosons into the effective string theory action in a gauge-invariant manner, dynamically deriving the equivalent string spectrum.",
        "ori-fast-z-score": 1.3643820804812932,
        "water-fast-z-score": 4.677476657214644,
        "rewrite-fast-z-score": 2.492241482207092
    },
    {
        "original_text": "A fundamental measure functional for the fluid of aligned hard hexagons is presented. This functional is based on a close approximate of the hard hexagon particle shape that allows for an analytical treatment and the resulting expression is a functional of the weighted density, a functional which has not been considered before in theories of the hard hexagon fluid. A systematic gradient expansion of the free energy leads to a direct correlation function and a simple Padé approximant for the functional. The resulting fluid exhibits square order at high densities, as found in computer simulations and as observed experimentally for real hard hexagons. At low densities, however, we predict a phase transition to a hexatic phase. Further, we predict a reentrant nematic phase which has not been observed in computer simulations. Our functional forms a solid angle subtended by neighboring particles as a critical independent variable for the first order phase transitions. We discuss the connection of our findings to recent computer simulations of the hard hexagon fluid and propose that some computer simulations have found an incorrect transition to the solid phase.",
        "watermark_text": "A basic measure functional for the liquid of aligned hard hexagons is provided . This functional is based on a close approximate of the hard hexagon particle shape that enables for an analytical treatment and the resulting expression is a functional of the weighted density , a functional which has not been regarded before in theories of the hard hexagon liquid .A systematic gradient increase of the free energy leads to a direct correlation function and a simple Padé approximant for the functional . The resulting flow displays square order at high densities , as found in computer simulations and as demonstrated experimentally for real hard hexagons .At reduced densities , however , we estimate a phase shift to a hexatic phase . Further , we estimate a reentrant nematic phase which has not been observed in computer simulations .Our functional generates a solid angle subtended by adjacent particles as a critical independent constant for the first order phase transitions . We discuss the link of our findings to recent computer simulations of the hard hexagon liquid and suggest that some computer simulations have discovered an wrong transition to the solid transition .",
        "rewrite_text": "A fundamental functional measure has been devised for the liquid composed of aligned rigid hexagons. This functional is rooted in an accurate approximation of the shape of the hard hexagon particles, enabling analytical treatment. The resulting expression is a functional of weighted density, a concept that has not been previously considered in the theories of hard hexagon liquids. A systematic increase in the free energy gradient leads to a direct correlation function and a straightforward Padé approximant for the functional. At high densities, the resulting flow exhibits a square order, as observed in computer simulations and experimentally confirmed for real hard hexagons. However, at lower densities, we anticipate a phase shift to a hexatic phase. Additionally, we estimate the existence of a reentrant nematic phase that has not been observed in computer simulations thus far. Our functional generates a solid angle subtended by neighboring particles as a critical independent constant for first-order phase transitions. We delve into the connection between our findings and recent computer simulations of the hard hexagon liquid, suggesting that some simulations may have erroneously identified a transition to the solid state.",
        "ori-fast-z-score": 0.5360562674188973,
        "water-fast-z-score": 5.682196434640312,
        "rewrite-fast-z-score": 1.8225913092242512
    },
    {
        "original_text": "Finite dimensional complex Leibniz algebras were first studied by M. L. Mas cleansing in 1978. Finite dimensional complex filiform Leibniz algebras were first studied by M. L. Mas cleansing in 1978. Finite dimensional complex Leibniz algebras satisfy the Levi identity, which is also called the interrelation, δ(x,y) = 0, where δ is the exterior product. Since the identity is local, it follows that the dimensions of the homogeneous components of given length of the Levi identity must be identical. In 1983, A. M. Glabin et al. classified the Leibniz algebras satisfying the condition that the dimension of the homogeneous component of degree two of the Levi identity is one. This condition can be expressed by the table. In 1988, H. Bai et al. classified the Leibniz algebras whose homogeneous component of degree two of the Levi identity is two-dimensional. This condition can be expressed by the following table. In 2013, we studied the classification of complex finite dimensional filiform Leibniz algebras, and got the following result. Finite dimensional complex filiform Leibniz algebras are intimately related to Lie algebras. A Lie algebra is a filiform Leibniz algebra if and only if its corresponding non-zero part of the table of multiplicities of the principal part of the Levi identity coincides with that of the corresponding Leibniz algebra. In other words, the Lie algebra has the same structure as the corresponding filiform Leibniz algebra, but some of the corresponding structure constants are zero. Based on this conclusion, we further classified the complex finite dimensional filiform Leibniz algebras. In this paper, we classify the finite dimensional complex filiform Leibniz algebras whose corresponding non-zero part of the table of multiplicities of the principal part of the Levi identity coincides with that of the corresponding filiform Leibniz algebra up to isomorphisms. We find that there are three types of these complex filiform Leibniz algebras. Type I Type I complex filiform Leibniz algebras have a basis with the following structure. $ e_1,e_3 =e_4$, $  e_2,e_3 =e_5$, $  e_1,e_4 =ae_5+be_6$, $  e_2,e_4 =ce_5+de_6$, $  e_1,e_5 = e_2,e_5 =0$ Where $a$, $b$, $c$, $d$ are arbitrary constants. Type II Type II complex filiform Leibniz algebras have a basis with the following structure. $ e_1,e_3 =e_4$, $  e_2,e_3 =e_5$, $  e_1,e_4 =e_",
        "watermark_text": "Finite dimensional complex Leibniz algebras were first investigated by M . L . Mas cleansing in 1978 . Finite dimensional complex filiform Leibniz algebras were first investigated by M . L . Mas cleansing in 1978 .Finite dimensional complex Leibniz algebras satisfy the Levi identity , which is also known the interrelation , δ ( x , y ) = 0 , where δ is the exterior product . Since the identity is local , it follows that the sizes of the homogeneous parts of given length of the Levi identity must be identical .In 1983 , A . M . Glabin et al . classified the Leibniz algebras satisfying the requirement that the dimension of the homogeneous part of degree two of the Levi identity is one .This condition can be stated by the table . In 1988 , H . Bai et al .classified the Leibniz algebras whose homogeneous part of degree two of the Levi identity is two - dimensional . This condition can be stated by the following list .In 2013 , we studied the characterization of complex finite dimensional filiform Leibniz algebras , and got the following result . Finite dimensional complex filiform Leibniz algebras are intimately equivalent to Lie algebras .A Lie algebra is a filiform Leibniz algebra if and only if its equivalent non - zero portion of the table of multiplicities of the primary part of the Levi identity coincides with that of the associated Leibniz algebra . In other words , the Lie algebra has the same structure as the corresponding filiform Leibniz algebra , but some of the analogous structure constants are zero .Based on this conclusion , we further categorized the complex finite dimensional filiform Leibniz algebras . In this paper , we classify the finite dimensional complex filiform Leibniz algebras whose corresponding non - zero portion of the table of multiplicities of the main component of the Levi identity coincides with that of the corresponding filiform Leibniz algebra up to isomorphisms .We see that there are three sorts of these complex filiform Leibniz algebras . Type I Type I complex filiform Leibniz algebras have a basis with the following construction .$ e _ 1 , e _ 3 = e _ 4 $ , $ e _ 2 , e _ 3 = e _ 5 $ , $ e _ 1 , e _ 4 = ae _ 5 + be _ 6 $ , $ e _ 2 , e _ 4 = ce _ 5 + de _ 6 $ , $ e _ 1 , e _ 5 = e _ 2 , e _ 5 = 0 $ Where $ a $ , $ b $ , $ c $ , $ d $ are arbitrary constants . Type II Type II complex filiform Leibniz algebras have a basis with the following construction .$ e _ 1 , e _ 3 = e _ 4 $ , $ e _ 2 , e _ 3 = e _ 5 $ , $ e _ 1 , e _ 4 = e _",
        "rewrite_text": "The first investigation of finite dimensional complex Leibniz algebras was conducted by M. L. Mas cleansing in 1978. Subsequently, in the same year, he also explored finite dimensional complex filiform Leibniz algebras. Complex Leibniz algebras of finite dimension comply with the Levi identity, also recognized as the interrelation, where δ(x, y) = 0, and δ denotes the exterior product. Due to the local nature of this identity, it follows that the sizes of homogeneous parts with a given length must be identical.\n\nIn 1983, A. M. Glabin and his colleagues classified Leibniz algebras based on the requirement that the dimension of the homogeneous part of degree two in the Levi identity be one. This condition can be expressed through a table. In 1988, H. Bai and his colleagues further classified Leibniz algebras when the homogeneous part of degree two in the Levi identity is two-dimensional, which can be stated using a list.\n\nIn 2013, we studied the characteristics of complex finite-dimensional filiform Leibniz algebras and obtained the following result: these algebras are intricately equivalent to Lie algebras. A Lie algebra can be deemed a filiform Leibniz algebra if and only if its non-zero portion of the table of multiplicities corresponding to the primary part of the Levi identity coincides with that of the associated Leibniz algebra. In other words, while sharing the same structure as the corresponding filiform Leibniz algebra, some analogous structure constants may be zero.\n\nBased on this conclusion, we further categorized complex finite-dimensional filiform Leibniz algebras. This paper delves into the classification of such algebras where the non-zero portion of the table of multiplicities for the main component of the Levi identity coincides with that of the corresponding filiform Leibniz algebra up to isomorphisms. We found that there are three types of these complex filiform Leibniz algebras.\n\nType I: The basis for Type I complex filiform Leibniz algebras is constructed as follows: e1, e3 = e4, e2, e3 = e5, e1, e4 = ae5 + be6, e2, e4 = ce5 + de6, e1, e5 = e2, e5 = 0, where a, b, c, d are arbitrary constants.\n\nType II: The basis for Type II complex filiform Leibniz algebras is similarly defined: e1, e3 = e4, e2, e3 = e5, with additional conditions specified for e1, e4 relationships.",
        "ori-fast-z-score": -4.281149877639086,
        "water-fast-z-score": 0.5222329678670935,
        "rewrite-fast-z-score": 2.2283440581246223
    },
    {
        "original_text": "A review of biological molecular computers is presented with a focus on the bacterial cell and how it could be programmed to carry out various tasks. Various aspects of biological molecular machines are discussed along with an outline of how such a biological computer could be realized by engineering proteins on a surface. The possibility of the living cell itself being used as a molecular computer is explored with a discussion on how molecular signals can be processed by the cell itself using transport and enzymatic reactions. A survey of a number of molecular tasks that the cell could carry out is presented along with several experiments that could be performed to validate the cell as a biological molecular computer. Several key issues that must be overcome to build a biological molecular computer are also discussed. The possibility of programming the bacterial cell to carry out various tasks is explored. Various aspects of biological molecular machines are discussed. The living cell itself could be used as a molecular computer. Molecular signals can be processed by the cell itself using transport and enzymatic reactions. A survey of a number of molecular tasks the cell could carry out is presented. Several key issues that must be overcome to build a biological molecular computer are also discussed. BioBrick assembly, logic gates, sensors, computers, robots and networks could all be built from bacterial cells. An essential part of this technology is the use of reliable methods of intracellular synthesis and assembly. A prime requirement in this respect is that the DNA templates employed in the construction of these molecular systems be obtained intracellularly. Current methods based on in vitro transcription or PCR amplification of DNA templates for direct biosynthesis are generally unreliable. We believe that the cell itself, by providing an environment with suitable pH, temperature and salt concentrations, and protected from direct physical injury, could serve as an ideal host for the biosynthesis of DNA. Theory suggests that significant barriers to intracellular DNA synthesis include the thermal stability of DNA in bacterial cells (which favors nascent DNA strands in their natural double-stranded form), cell membrane instability (which prevents larger DNA pieces from escaping the cell once transcription begins), and nuclease attack (which degrades DNA strands). In order to solve these problems, we have designed synthetic DNA structures with optimized thermal and membrane properties. In addition, we have identified potent nuclease-resistant DNA structures that have yet to be utilized in bio-nano fabrication. All of these DNA properties should facilitate the production and assembly of complex nanoscale structures from within bacterial cells. The versatile genetic information encoding capacity of DNA allows the digitalization of any signal for processing by the cell. An analogy may be made to the molecular computer that performs calculations by processing digital signals in a sequence of logic operations. These logic operations may be encoded in the form of DNA. As a result, any biochemical signal could be converted into a digital form suitable for processing by the cell. We are proposing the concept of digital biochemistry, where all biochemical signals are converted into digital form and processed by the cell.",
        "watermark_text": "A review of biological biological computers is provided with a focus on the bacterial organism and how it could be engineered to carry out various responsibilities . Numerous elements of biological biological machines are discussed along with an outline of how such a bio computer could be realized by designing molecules on a surface .The possibility of the living cell itself being used as a molecular computer is highlighted with a debate on how biological signals can be processed by the cell itself using transport and enzymatic reactions . A review of a number of biological functions that the cell could carry out is provided along with many tests that might be performed to validate the cell as a physical molecular computer .Several key questions that must be overcome to build a biological biological computer are also discussed . The possibility of programming the bacterial cell to carry out various jobs is investigated .Various aspects of biological biological machines are discussed . The living cell itself may be used as a molecular computer .Molecular signals can be processed by the cell itself using transport and enzymatic reactions . A search of a number of biological functions the cell could carry out is given .Several key concerns that must be overcome to build a bio molecular computer are also discussed . BioBrick assembly , logic gates , devices , computers , computers and networks could all be built from bacterial cells .An necessary component of this technology is the using of accurate technologies of intracellular synthesis and assembly . A prime requirement in this respect is that the DNA templates employed in the creation of these molecular systems be obtained intracellularly .Current techniques based on in vitro replication or PCR amplification of DNA templates for direct biosynthesis are typically unreliable . We believe that the tissue itself , by offering an environment with suitable pH , temperature and salt concentrations , and protected from direct physical injury , might play as an suitable host for the biosynthesis of DNA .Theory indicates that significant obstacles to intracellular DNA synthesis include the thermal stability of DNA in bacterial cells ( which prefers nascent DNA sequences in their natural double - stranded version ) , cell membrane instability ( which allows larger DNA pieces from escaping the cell once transcription ends ) , and nuclease attack ( which degrades DNA sheets ) . In order to overcome these problems , we have developed synthetic DNA structures with optimized thermal and membrane structures .In addition , we have discovered potent nuclease - resistant DNA structures that have yet to be used in biological - nano fabrication . All of these DNA properties should enable the production and assembly of complex nanoscale complexes from within bacterial cells .The versatile genetic knowledge encoding ability of DNA permits the digitalization of any signal for processing by the tissue . An analogy might be made to the molecular computer that conducts calculations by processing computer messages in a sequence of logic operations .These logic events might be encoded in the form of DNA . As a result , any biochemical signal could be turned into a computer shape suitable for processing by the cell .We are introducing the idea of digital biochemistry , where all biochemical signals are converted into digital shape and processed by the cell .",
        "rewrite_text": "A comprehensive exploration of biological computers is presented, focusing specifically on the bacterial organism and its potential engineering for diverse responsibilities. The discussion encompasses numerous components of biological machines, detailing how bio-computing can be realized by designing molecules on a surface. The potential utilization of the living cell as a molecular computer is highlighted, delving into the debate about how biological signals can be processed by the cell itself through transport and enzymatic reactions.\n\nA review of various biological functions the cell could perform is provided, along with potential tests to validate the cell's functionality as a physical molecular computer. Furthermore, several key challenges that need to be overcome in developing a biological computer are addressed. The possibility of programming bacterial cells for various tasks is investigated. Various aspects of these biological machines are discussed. The living cell itself may be utilized as a molecular computing device, with the capability for cellular self-processing of molecular signals via transport and enzymatic reactions.\n\nA comprehensive exploration of the cell's potential biological functions is presented. Concerns that must be addressed to build a biomolecular computer are also discussed. Concepts like BioBrick assembly, logic gates, devices, computers, and networks could all be constructed from bacterial cells. A crucial component of this technology is the utilization of precise intracellular synthesis and assembly techniques. A primary requirement is that the DNA templates used in creating these molecular systems must be obtained intracellularly.\n\nCurrent techniques relying on in vitro replication or PCR amplification of DNA templates for direct biosynthesis are often unreliable. We believe that the tissue itself, providing an environment with suitable pH, temperature, and salt concentrations, and protected from direct physical injury, can serve as a suitable host for intracellular DNA biosynthesis. The theory suggests that significant obstacles to intracellular DNA synthesis include the thermal stability of DNA in bacterial cells, cell membrane instability, and nuclease attack.\n\nTo overcome these issues, we have developed synthetic DNA structures with optimized thermal and membrane properties. Additionally, we have discovered potent nuclease-resistant DNA structures that have yet to be employed in biological-nano fabrication. These DNA properties should facilitate the production and assembly of complex nanoscale structures within bacterial cells. The versatile genetic encoding ability of DNA allows for the digitalization of any signal for processing by the tissue, analogous to a molecular computer performing calculations by processing computer messages through a sequence of logic operations. These logic events can be encoded in DNA form, transforming any biochemical signal into a computer-ready format for processing by the cell.\n\nWe are introducing the concept of digital biochemistry, where all biochemical signals are converted into digital forms and processed by the cell.",
        "ori-fast-z-score": 0.31068488300060004,
        "water-fast-z-score": 10.708959715201948,
        "rewrite-fast-z-score": 4.2572166735215315
    },
    {
        "original_text": "The interaction between particles in a fluid is typically separated into long- and short-ranged contributions. The long-range interaction is typically modeled with a simple pair potential, while the short-range interactions are modeled with a series of pair-distribution functions. At thermal equilibrium, the total interaction may be simplified to a screened Coulomb potential. The system may be formally separated into one containing only the long-range interactions, a system of particles with periodic boundary conditions, and a system of particles with vanishing velocity. The former may be solved exactly, while the latter two may be solved numerically. In this work, the three-dimensional Ising model is used as a test case for this approach. Using a novel technique to systematically eliminate short-range interactions, it is shown that short-range attractions can reduce the thermal conductivity of the fluid by up to four orders of magnitude without changing the critical temperature. It is further shown that this reduction in thermal conductivity results in a divergent self-diffusivity and vanishing viscosity, while the structural order remains finite. These results provide further insight into the role of short-range forces in the heat transport of fluids.",
        "watermark_text": "The interaction between particles in a fluid is typically separated into long - and long - ranged contributions . The long - range coupling is typically described with a simple couple potential , while the short - range interactions are modeled with a sequence of pair - distribution functions .At thermal equilibrium , the total interaction may be simplified to a screened Coulomb potential . The system might be formally divided into one containing only the long - range interactions , a system of molecules with periodic boundary parameters , and a system of particles with vanishing speed .The first may be solved exactly , while the former two may be solved numerically . In this research , the three - dimensional Ising model is utilized as a demonstration case for this methodology .Using a novel technique to thoroughly eliminate narrow - range interactions , it is demonstrated that short - range attractions can alter the thermal conductivity of the liquid by up to four orders of magnitude without altering the critical temperature . It is further demonstrated that this reduction in heat conductivity results in a divergent self - diffusivity and vanishing viscosity , while the structural order remains finite .These data provide further insight into the importance of short - range forces in the temperature flow of fluids .",
        "rewrite_text": "The interplay between particles within a fluid is generally classified into long- and short-range contributions. Long-range interactions are commonly characterized by a straightforward coupling potential, whereas short-range interactions are modeled using a sequence of pair distribution functions. When the system reaches thermal equilibrium, the overall interaction may be simplified into a screened Coulomb potential. \n\nThe system can be formally segmented into one solely encompassing long-range interactions, a system of molecules with periodic boundary conditions, and a system of particles with zero velocity. The first can be solved precisely, while the latter two may be solved numerically. In this study, the three-dimensional Ising model is employed as a representative case to illustrate this methodology. \n\nBy utilizing a novel technique to effectively eliminate short-range interactions, it has been demonstrated that minor attractions in the short range can alter the thermal conductivity of the liquid by up to four orders of magnitude without affecting the critical temperature. Furthermore, this reduction in thermal conductivity leads to divergent self-diffusivity and vanishing viscosity while maintaining a finite structural order. These findings offer deeper insights into the significance of short-range forces in the temperature flow of fluids.",
        "ori-fast-z-score": 0.2,
        "water-fast-z-score": 5.8707194222389365,
        "rewrite-fast-z-score": 1.1766968108291043
    },
    {
        "original_text": "In this paper we present a detailed study of the systematics of soft particle production in forward angles for Au+Au collisions at energies available at the Relativistic Heavy Ion collider (RHIC). Such studies are of particular relevance to characterize the Degree of Chaos /thermalization achieved in these collisions. Our results are based on analyzing 1.7 million minimum-bias events from the PHOBOS experiment. We compare charged and neutral particle spectra, elliptic flow, and high transverse momentum particle production, covering a range of 15% centrality covering the most central to the most peripheral Au+Au collisions. We observe that particle production at forward rapidity is very well described by a quadratic function of the number of N-particles per unit of rapidity, where N=charged or neutral. These results are compared to hadronic models (PYTHIA, HIJING, AMPT), calculations from lattice QCD, and models from weakly coupled physics. Our data show the most detailed and strongest constraints on these models, and indicate the importance of specific aspects of the dynamics of these collisions to different aspects of the observed particle spectra. We quantify these aspects using a thermal-based model. This allows us to derive the thermal parameters for the system at the chemical and kinetic freezeouts. These are compared to predictions from ideal hadron gas and transport models at both constituent-quark level and hadronic level. We find that the kinetic freezeout parameter is more strongly constrained by the data, and we extract a conservative range of temperatures of 155 MeV<T<175 MeV. The preliminary results indicate that the hadron gas phase is short-lived, with a strong transition to a dense and hadronically collapsed medium at chemical freezeout. While both hard scattering and early phases of the nuclear collision appear to be consistent with being perturbative in nature, the strongly interacting final state is well described by a gluon plasma with moderate centrality-dependent temperatures of 155 MeV<T<175 MeV.",
        "watermark_text": "In this paper we present a detailed analysis of the systematics of smooth object development in forward angles for Au + Au collisions at energies available at the Relativistic Heavy Ion collider ( RHIC ) . Such analyses are of especially relevance to characterize the Degree of Chaos / thermalization achieved in these collisions .Our results are based on examining 1 . 7 million lowest - bias events from the PHOBOS experiment . We compare charged and neutral electron spectra , elliptic flow , and large transverse momentum electron production , covering a range of 15 % centrality covering the most central to the most peripheral Au + Au collisions .We recognize that particle production at forward rapidity is very best described by a quadratic function of the number of N - particles per unit of rapidity , where N = charged or neutral . These data are compared to hadronic models ( PYTHIA , HIJING , AMPT ) , measurements from lattice QCD , and models from weakly coupled physics .Our data reveal the most detailed and strongest limits on these models , and suggest the importance of certain parts of the dynamics of these collisions to different aspects of the observed particle spectra . We quantify these characteristics use a heat - based model .This enables us to derive the thermal models for the process at the chemical and kinetic freezeouts . These are compared to observations from ideal hadron vapor and transport models at both constituent - quark level and hadronic level .We see that the kinetic freezeout parameter is more strongly constrained by the information , and we extract a conservative range of conditions of 155 MeV < T < 175 MeV . The initial results show that the hadron liquid phase is short - lived , with a strong change to a dense and hadronically collapsed medium at molecular freezeout .While both hard scattering and first phases of the atomic collision appear to be compatible with being perturbative in nature , the strongly interacting last state is well described by a gluon gas with medium centrality - dependent heat of 155 MeV < T < 175 MeV .",
        "rewrite_text": "In this study, we present a comprehensive analysis of the systematic behavior of smooth object development in forward angles during Au + Au collisions at the Relativistic Heavy Ion Collider (RHIC). This type of analysis is particularly crucial for characterizing the degree of chaos and thermalization achieved in these collisions. Our findings are based on an examination of 1.7 million low-bias events from the PHOBOS experiment. We compare charged and neutral electron spectra, elliptic flow, and the production of electrons with large transverse momentum across a range of 15% centrality, encompassing the most central to the most peripheral Au + Au collisions.\n\nWe observe that particle production at forward rapidity is best described by a quadratic function of the number of N-particles per unit of rapidity, where N represents charged or neutral particles. We compare these data with hadronic models (such as PYTHIA, HIJING, and AMPT), measurements from lattice QCD, and models from weakly coupled physics. Our data provide the most detailed and stringent constraints on these models, highlighting the importance of certain aspects of the dynamics in these collisions to different features of the observed particle spectra.\n\nTo quantify these characteristics, we utilize a heat-based model. This enables us to derive thermal models for the process at both chemical and kinetic freezeouts. These are compared with observations from ideal hadron vapor and transport models at both constituent quark level and hadronic level. Our findings indicate that the kinetic freezeout parameter is more strongly constrained by the available information, leading us to identify a conservative range of conditions: 155 MeV < T < 175 MeV.\n\nInitial results suggest that the hadron liquid phase is relatively short-lived, with a significant transition to a dense and hadronically collapsed medium at the molecular freezeout stage. Both hard scattering and early stages of the atomic collision appear to be compatible with a perturbative nature, while the strongly interacting final state is well described by a gluon gas with a medium centrality-dependent heat within the range of 155 MeV < T < 175 MeV.",
        "ori-fast-z-score": -2.057983021710106,
        "water-fast-z-score": 5.316456139417774,
        "rewrite-fast-z-score": 2.3333333333333335
    },
    {
        "original_text": "A 1-D site-directed model of rocks and elastomers is used to study the dynamics of earthquakes. The model takes into account the nonlinear elastic response of rocks and the visco-elastic behavior of elastomers. The rocks and elastomers are represented by one or more coupled first-order differential equations. In this paper, we conduct a simulation study for the two-dimensional Burridge-Knopoff (BK) model of earthquakes proposed by Holtz and McRacken (Physica D, 1984, 29, 335-352). The model exhibitsboth peak and post-peak static stress conditions can lead to the occurrence of earthquakes. The results for the maximum first vertical and horizontal principal stress conditions are similar to those found experimentally, but the model qualitatively fails to reproduce the post-peak stress trigger mechanism. The model is extended to a two-dimensional system by replacing the first-order differential equations with difference equations. Simulations of the two-dimensional BK model confirm that both peak and post-peak static stress conditions can lead to the occurrence of earthquakes. The failure mechanism is qualitatively the same as that for the one-dimensional case, although there are quantitative differences. This paper is part of a more general program to develop comprehensive site-directed models of earthquake mechanics that will be used to better understand earthquake dynamics and to make reliable forecasts of earthquakes.",
        "watermark_text": "A 1 - D site - directed model of rocks and elastomers is utilized to study the dynamics of earthquakes . The model takes into consideration the nonlinear elastic response of rocks and the visco - elastic behavior of elastomers .The rocks and elastomers are represented by one or more coupled first - order differential equations . In this paper , we conduct a simulation study for the two - dimensional Burridge - Knopoff ( BK ) model of earthquakes proposed by Holtz and McRacken ( Physica D , 1984 , 29 , 335 - 352 ) .The model exhibitsboth peak and post - peak static stress conditions can lead to the occurrence of earthquakes . The results for the maximum first vertical and horizontal principal stress conditions are comparable to those shown experimentally , but the model qualitatively fails to predict the post - peak stress trigger process .The model is extended to a two - dimensional system by replacing the first - order differential coefficients with difference expressions . Simulations of the two - dimensional BK theory confirm that both peak and post - peak dynamic strain circumstances can lead to the occurrence of earthquakes .The collapse mechanism is qualitatively the same as that for the one - dimensional case , although there are quantitative variations . This paper is part of a more general program to develop comprehensive site - directed models of earthquake dynamics that will be used to well understand explosion mechanics and to make accurate forecasts of disasters .",
        "rewrite_text": "A site-specific model focused on rocks and elastomers, following a 1-D to D (dimensional) framework, is employed to explore the dynamics of earthquakes. This model takes into account the nonlinear elastic response exhibited by rocks and the visco-elastic behavior of elastomers, both of these elements represented through one or more coupled first-order differential equations.\n\nIn this paper, we conduct a simulation study based on the two-dimensional Burridge-Knopoff (BK) earthquake model proposed by Holtz and McRacken (Physica D, 1984, 29, 335-352). The model demonstrates that both peak and post-peak static stress conditions can trigger earthquakes. Our findings on the maximum first vertical and horizontal principal stress conditions are in alignment with experimental results; however, the model fails to accurately predict the post-peak stress triggering process.\n\nTo extend the model to a two-dimensional system, we replace the first-order differential coefficients with difference expressions. Simulations of the two-dimensional BK theory confirm that both peak and post-peak dynamic strain scenarios can lead to earthquake occurrences. Although there are quantitative differences, the collapse mechanism remains qualitatively the same as in the one-dimensional case.\n\nThis paper is part of a broader initiative to develop comprehensive, site-specific models of earthquake dynamics. These models aim to enhance our understanding of explosion mechanics and enable more accurate disaster forecasting.",
        "ori-fast-z-score": 0.6767155423319645,
        "water-fast-z-score": 4.156966902896353,
        "rewrite-fast-z-score": 1.0169503597462533
    },
    {
        "original_text": "On 29 December 2003 the Burst Alert Telescope (BAT) on the Swift satellite detected the gamma-ray burst (GRB) 030329. One month after the event, the X-shooter spectrograph on the VLT detected the optical/near-infrared (OIR) afterglow. Using data from X-shooter and other telescopes, we carried out a detailed study of this deep non-relativistic phase afterglow. From the X-shooter spectrum we derive a upper limit on the redshift of z < 0.1667. From the afterglow spectrum we measure the intrinsic reddening of A(V) = 0.47 ± 0.12 and a best-fit power-law slope of α = 1.7 ± 0.1, consistent with the value of α = 1.6 ± 0.1 measured by earlier studies. From the lightcurve we measure the half-opening angle of the jet to be θ = 17.8 ± 1.4 degrees. The prominent double-humped structure in the lightcurve, which has been interpreted as the signature of the passage of a jet through the surrounding medium, is also seen in our data, but only as a shallow deficit in the early afterglow lightcurve. We suggest that the shallow deficit is caused by an underestimated k-corrected host-galaxy magnitude in earlier studies, and suggest that the real structure in the lightcurve has a much smoother gradient. Finally, we consider the possibility that the optical/OIR afterglow may have been extinguished by dust in the galaxy itself. Using an empirical relation between IR-optical extinction and host-galaxy properties we find that this is possible, but that in this case the observed X-shooter spectrum is inconsistent with A(V) > 0.5.",
        "watermark_text": "On 29 December 2003 the Burst Alert Telescope ( BAT ) on the Swift satellite detected the gamma - ray burst ( GRB ) 030329 . One month after the event , the X - shooter spectrograph on the VLT detected the optical / near - infrared ( OIR ) afterglow .Using evidence from X - shooter and other telescopes , we conducted out a detailed analysis of this shallow non - relativistic phase afterglow . From the X - shooter wavelength we derive a higher limit on the redshift of z < 0 . 1667 .From the afterglow spectrum we measure the intrinsic reddening of A ( V ) = 0 . 47 ± 0 . 12 and a better - fitting power - law slope of α = 1 . 7 ± 0 . 1 , compatible with the value of α = 1 . 6 ± 0 . 1 recorded by earlier surveys . From the lightcurve we measure the half - opening angle of the jet to be θ = 17 . 8 ± 1 . 4 degrees .The prominent double - humped structure in the lightcurve , which has been interpreted as the signature of the travel of a jet through the nearby medium , is also observed in our information , but only as a shallow deficit in the early afterglow lightcurve . We suggest that the deep deficit is caused by an underestimated k - corrected host - galaxy magnitude in earlier analyses , and suggest that the real shape in the lightcurve has a far smoother gradient .Finally , we investigate the prospect that the optical / OIR afterglow may have been extinguished by dust in the galaxy itself . Using an empirical connection between IR - optical extinction and host - galaxy properties we find that this is possible , but that in this situation the seen X - shooter spectrum is conflicting with A ( V ) > 0 . 5 .",
        "rewrite_text": "On December 29th, 2003, the Burst Alert Telescope (BAT) aboard the Swift satellite detected a gamma-ray burst (GRB) labeled as 030329. One month after the event, the X-shooter spectrograph on the Very Large Telescope (VLT) captured the optical/near-infrared (OIR) afterglow. Through the utilization of data from X-shooter and other telescopes, we conducted a comprehensive analysis of this shallow non-relativistic phase afterglow. We derived a higher limit on the redshift value of z < 0.1667 from the X-shooter wavelength range. From the afterglow spectrum, we measured an intrinsic reddening of A(V) = 0.47 ± 0.12 and a better-fitting power-law slope of α = 1.7 ± 0.1, which is compatible with the value of α = 1.6 ± 0.1 recorded in earlier surveys. Additionally, from the lightcurve, we determined the half-opening angle of the jet to be θ = 17.8 ± 1.4 degrees.\n\nThe prominent double-humped structure observed in the lightcurve, which has been interpreted as the signature of a jet traveling through the nearby medium, is also visible in our data, but only as a subtle deficit in the early afterglow lightcurve. We suggest that the deep deficit might be due to an underestimated k-corrected host-galaxy magnitude in previous analyses, and we propose that the actual shape of the lightcurve has a smoother gradient.\n\nFinally, we explore the possibility that the optical/OIR afterglow could have been extinguished by dust within the galaxy itself. By utilizing an empirical connection between IR-optical extinction and host-galaxy properties, we find that this possibility is feasible, but in this scenario, the observed X-shooter spectrum conflicts with A(V) > 0.5.",
        "ori-fast-z-score": -0.502518907629606,
        "water-fast-z-score": 4.924685294770139,
        "rewrite-fast-z-score": 1.5689290811054724
    },
    {
        "original_text": "Game theory has been applied to modeling dynamic spectrum sharing in wireless networks, however, most existing works either assume that QoS requirements of delay and loss are satisfied simultaneously, or rely on dynamic programming to solve the original complicated resource management optimization problems. In this paper, we consider the problem of joint energy-efficient modulation and spectrum sharing under the latency and loss QoS constraints in the downlink of cellular networks. Specifically, we formulate this spectrum and energy efficient modulation problem into a classical Markov decision process (MDP) and propose a deep learning based solution, i.e., using deep Q-learning, to cope with the complex non-linear characteristics of the proposed problem. Compared to the existing works, our main contributions are summarized as follows: 1. We establish the connection between the optimization of CDMA network and deep Q-learning, which greatly reduces the search space of the efficient spectrum and energy allocation. 2. Instead of using the conservative heuristic methods in the existing works, we leverage the learning capabilities of deep Q-learning to search for the efficient solution. 3. We consider the latency and loss QoS constraints of downlink data transmission in the network, which has not been well taken into consideration in the existing works. Our simulation results show that our deep learning based approach can achieve significant energy saving, while satisfying the latency and loss QoS constraints, comparing with the existing heuristic methods. This work was initially submitted to arXiv on October 18, 2019, and has been updated on January 10, 2020.",
        "watermark_text": "Game theory has been used to modeling dynamic frequency sharing in mobile networks , however , most existing works either assume that QoS constraints of delay and loss are fulfilled concurrently , or rely on dynamic programming to tackle the original simple resource control optimization problems . In this paper , we study the issue of joint energy - efficient modulation and spectrum sharing under the latency and loss QoS constraints in the downlink of mobile connections .Specifically , we formulate this spectrum and energy effective modulation question into a traditional Markov decision mechanism ( MDP ) and suggest a deep knowledge focused answer , i . e . , using deep Q - knowledge , to deal with the complex non - linear characteristics of the suggested problem . Compared to the previous works , our major contributions are presented as follows : 1 .We establish the link between the algorithms of CDMA network and deep Q - computing , which substantially cuts the search space of the efficient spectrum and energy availability . 2 .Instead of using the conservative heuristic methods in the existing works , we leverage the learning skills of deep Q - knowledge to search for the efficient answer . 3 .We consider the latency and loss QoS constraints of downlink data communication in the network , which has not been good placed into consideration in the existing works . Our modelling findings show that our depth knowledge focused method can attain massive energy save , while fulfilling the latency and loss QoS constraints , comparing with the established heuristic methods .This project was initially submitted to arXiv on October 18 , 2019 , and has been amended on January 10 , 2020 .",
        "rewrite_text": "Game theory has been employed in modeling dynamic frequency sharing within mobile networks. However, many existing studies either assume that quality-of-service (QoS) constraints for delay and loss are met simultaneously or rely on dynamic programming to address straightforward resource control optimization problems. In this paper, we investigate the combined energy-efficient modulation and spectrum sharing under latency and loss QoS constraints in the downlink of mobile connections. Specifically, we formulate this modulation question, which involves spectrum and energy efficiency, into a traditional Markov Decision Process (MDP) and propose a deep knowledge-driven approach, utilizing deep Q-learning, to tackle the complex nonlinear characteristics of the problem.\n\nIn contrast to previous research, our key contributions are as follows: 1. We establish a connection between the algorithms of CDMA networks and deep Q-learning, significantly narrowing the search space for efficient spectrum and energy availability. 2. Instead of utilizing conservative heuristic methods found in existing studies, we leverage the learning capabilities of deep Q-knowledge to seek efficient solutions. 3. We consider the QoS constraints for latency and loss in downlink data communication in the network, which have often been overlooked in previous research.\n\nOur modeling findings indicate that our deep knowledge-driven method can achieve significant energy savings while meeting latency and loss QoS requirements, compared to established heuristic methods. This project was initially submitted to arXiv on October 18th, 2019, and subsequently amended on January 10th, 2020.",
        "ori-fast-z-score": 0.17677669529663687,
        "water-fast-z-score": 8.662058069535206,
        "rewrite-fast-z-score": 3.028960741674143
    },
    {
        "original_text": "For hierarchical multiplanet systems, tides raised on distant planets can shrink their orbital semi-major axes and even drive type II catastrophic mergers. In systems with more modest outerplanet masses, strong tides raised on distant worlds can likewise shrink their orbital semi-major axes, but also damp the orbits. Here we show that if the innermost planet has a strong non-zero eccentricity, tides raised on the planet can cause eccentricity oscillations that drive eccentricities above the dynamically defined upper limit and shrink the orbits. We carry out N-body simulations to validate this analytic theory and demonstrate the effectiveness of this mechanism for shrinking the orbits of non-resonant planets in multipleplanet systems. We show that this process is efficient enough to explain the proximity of the Earth and Mars to the Sun, and could have contributed to the capture of planetesimals into Earth and Venus. The long-term behavior of hierarchical multiplanet systems is largely determined by the planets’ orbit and spin orientations. If the planets’ spins are misaligned with their orbital planes, the system can enter a Cassini state where the forced eccentricities of the planets damp out but their orbital angular momenta remain coupled. If, on the other hand, the planets’ spin axes are initially coplanar with their orbital planes, they can enter a Kozai state, in which the forced eccentricities oscillate with a period equal to a full cycle of eccentricity-raisingnode precession. During this time, the planets’ semi-major axes can shrink enough to cause a type II or type I migration depending on the characteristics of the adjacent planets. We demonstrate that the inclusion of even a single testparticle in apsidal motionward precession significantly increases the fraction of Kozai cycles completed before apsides align. Planetary systems that enter the Kozai state and subsequently undergo type II or type I migration may exhibit changes in their semimajor axes that are larger than observed in the Solar System. Here we demonstrate that, if one or more planets in such a system have sufficiently non-zero eccentricities, the Kozai cycles can drive eccentricities above the dynamically defined upper limit and drive the systems into collision. We carry out N-body simulations to validate this analytic theory and demonstrate that this process is efficient enough to explain the proximity of the Earth and Mars to the Sun in the Solar System. We also show that the capture of planetesimals into Earth and Venus is facilitated if the planets are brought into very eccentric orbits and that this mechanism could have contributed to the observed mass of Earth and Venus. This work was done in the context of the NExT (Numerical Experiment for Terrestrial planet Formation) project, which is an experiment in Planet formation and Dynamics supported by the International Centre for Radio Astronomy Studies, the University of Western Australia, the University of Tokyo, and the RIKEN Institute in Japan, and funded by the Australian Research Council’",
        "watermark_text": "For hierarchical multiplanet systems , tides raised on remote galaxies can weaken their orbital semi - major axes and even drive type II catastrophic mergers . In environments with more modest outerplanet masses , large tides raised on remote worlds can likewise decrease their orbital semi - major axes , but also damp the orbits .Here we prove that if the innermost planet has a high non - zero eccentricity , tides raised on the planet can cause eccentricity oscillations that drive eccentricities above the dynamically specified upper maximum and shrink the orbits . We carry out N - bodies simulations to validate this analytic theory and suggest the effectiveness of this mechanism for shrinking the orbits of non - resonant orbits in multipleplanet systems .We suggest that this process is efficient sufficient to explain the location of the Earth and Mars to the Sun , and could have led to the capture of planetesimals into Earth and Venus . The long - term behavior of hierarchical multiplanet systems is largely decided by the planets ’ orbit and spin orientations .If the planets ’ spins are misaligned with their orbital planes , the system can enter a Cassini state where the forced eccentricities of the planets damp out but their orbital angular momenta stay connected . If , on the other hand , the planets ’ spin axes are initially coplanar with their orbital planes , they can initiate a Kozai state , in which the forced eccentricities oscillate with a period equal to a complete cycle of eccentricity - raisingnode precession .During this time , the planets ’ semi - major axes can shrink enough to produce a class II or type I drift varying on the properties of the adjacent worlds . We suggest that the inclusion of even a single testparticle in apsidal motionward precession significantly increases the fraction of Kozai cycles finished before apsides align .Planetary systems that enter the Kozai state and subsequently undergo type II or type I drift might exhibit shifts in their semimajor axes that are larger than observed in the Solar System . Here we prove that , if one or more planets in such a system have sufficiently non - zero eccentricities , the Kozai cycles can accelerate eccentricities above the dynamically specified upper maximum and push the systems into collision .We take out N - bodies simulations to validate this analytic theory and suggest that this process is efficient sufficient to explain the location of the Earth and Mars to the Sun in the Solar System . We additionally prove that the escape of planetesimals into Earth and Venus is enabled if the planets are brought into very eccentric orbits and that this mechanism could have contributed to the known mass of Earth and Venus .This research was done in the context of the NExT ( Numerical Experiment for Terrestrial planet Formation ) initiative , which is an project in Planet formation and Dynamics supported by the International Centre for Radio Astronomy Studies , the University of Western Australia , the University of Tokyo , and the RIKEN Institute in Japan , and funded by the Australian Research Council ’",
        "rewrite_text": "For hierarchical multi-planet systems, tides generated in distant galaxies can weaken their orbital semi-major axes, potentially leading to type II catastrophic mergers. In environments with smaller outer planet masses, significant tides in remote worlds can also decrease their orbital semi-major axes while simultaneously damping the orbits. We demonstrate that, when the innermost planet has a significant non-zero eccentricity, tides on that planet can cause eccentricity oscillations that push the eccentricities above the dynamically determined maximum limit, resulting in the shrinking of orbital distances.\n\nTo validate this analytical theory, we conducted N-body simulations and found that this mechanism is effective for shrinking the orbits of non-resonant planets in multiple-planet systems. We propose that this process is sufficiently efficient to explain the positioning of the Earth and Mars in relation to the Sun, and it may have contributed to the capture of planetesimals into the orbits of Earth and Venus.\n\nThe long-term behavior of these hierarchical multi-planet systems is largely determined by the planets' orbital and spin orientations. If the planets' spins are misaligned with their orbital planes, the system can enter a Cassini state where the forced eccentricities of the planets diminish, but their orbital angular momenta remain connected. Conversely, if the planets' spin axes are initially coplanar with their orbital planes, they can enter a Kozai state, in which the forced eccentricities oscillate with a period equal to a complete cycle of eccentricity-raising node precession.\n\nDuring this process, the semi-major axes of the planets can shrink sufficiently to produce class II or type I drifts, depending on the properties of neighboring worlds. We suggest that the inclusion of even a single test particle in apsidal motionward precession significantly increases the proportion of Kozai cycles completed before apsides align. Planetary systems that enter the Kozai state and experience type II or type I drifts may exhibit shifts in their semi-major axes that are larger than those observed in the Solar System.\n\nOur analysis shows that, if one or more planets in such a system have non-zero eccentricities, the Kozai cycles can accelerate eccentricities above the dynamically specified upper limit, potentially leading to collisions among the systems. We have conducted N-body simulations to validate this theory and propose that this process could explain the positioning of the Earth and Mars in relation to the Sun in our Solar System. Furthermore, we demonstrate that the escape of planetesimals into the orbits of Earth and Venus could have been facilitated by bringing the planets into highly eccentric orbits, potentially contributing to the known masses of Earth and Venus.\n\nThis research was conducted as part of the NExT (Numerical Experiment for Terrestrial planet Formation) initiative, a project supported by the International Centre for Radio Astronomy Studies, University of Western Australia, University of Tokyo, and RIKEN Institute in Japan, and funded by the Australian Research Council.",
        "ori-fast-z-score": 0.1466471150213533,
        "water-fast-z-score": 7.625649981110372,
        "rewrite-fast-z-score": 0.7179581586177382
    },
    {
        "original_text": "In this paper, we present our findings regarding the occurrence of disordering transitions and peak effect in polydisperse systems. Disordering transitions occur when a solid mixture transitions from a solid-like state to a liquid-like state, and peak effect is a phenomenon which occurs when the volume fraction of a solid mixture at the peaks of the density profile is greater than the average volume fraction of the mixture. These phenomena have both been observed in a wide range of material systems, including glass forming liquids, granular materials, and heterogeneous mixtures. In this paper, we demonstrate that the frequency of disordering transitions and peak effect can be manipulated by altering the variance of the particle size distribution. We perform event-driven molecular dynamics simulations to model a binary mixture composed of particles with the same size but with different levels of variance. Through the use of radial distribution functions, we are able to demonstrate that disordering transitions and peak effect occur at lower volume fractions for systems with larger variances. Additionally, we observe an increased frequency of disordering transitions and peak effect as the variance of the particle size distribution increases, and we identify an approximate scaling relationship between these two quantities. We provide a physical argument for this scaling relationship based on the average interparticle force, and compare our results with prior works which have explored similar relationships. We also discuss the broader implications of these results in the context of recent theories on the origins of disorder in condensed matter systems.",
        "watermark_text": "In this paper , we present our findings investigating the incidence of disordering transitions and peak impact in polydisperse systems . Disordering switches occur when a solid blend transitions from a solid - like state to a liquid - like state , and peak impact is a effect which occurs when the density amount of a solid blend at the elevations of the density profile is greater than the average volume fraction of the mixture .These phenomena have both been observed in a broad variety of material systems , notably glass forming liquids , granular materials , and heterogeneous mixtures . In this paper , we prove that the frequency of disordering transitions and peak impact can be manipulated by adjusting the variance of the particle size distribution .We work event - powered molecular mechanics simulations to model a binary mixture constructed of molecules with the same size but with varying rates of variance . Through the using of radial distribution functions , we are able to indicate that disordering transitions and peak impact occur at lower volume fractions for solutions with larger variances .Additionally , we study an increased frequency of disordering transitions and peak impact as the variance of the particle size distribution increases , and we identify an approximate scaling connection between these two quantities . We provide a physical explanation for this scaling connection based on the average interparticle force , and contrast our findings with previous works which have explored identical relationships .We also discuss the broader implications of these results in the context of recent discoveries on the origins of disorder in condensed matter structures .",
        "rewrite_text": "In this study, we present our findings regarding the occurrence of disordering transitions and peak impact in polydisperse systems. Disordering switches occur when a solid blend transitions from a solid-like state to a liquid-like state. Peak impact is an effect that arises when the density of a solid blend at certain density profile elevations surpasses the average volume fraction of the mixture. These phenomena have been observed in a wide range of material systems, specifically in glass-forming liquids, granular materials, and heterogeneous mixtures.\n\nIn this paper, we demonstrate that the frequency of disordering transitions and peak impact can be controlled by adjusting the variance in particle size distribution. We employ event-driven molecular mechanics simulations to model a binary mixture composed of molecules of uniform size but with varying degrees of variance. By utilizing radial distribution functions, we show that disordering transitions and peak impact occur at lower volume fractions for solutions with greater variances.\n\nFurthermore, we investigate an increase in the frequency of disordering transitions and peak impact as the variance in particle size distribution escalates. We identify an approximate scaling relationship between these two phenomena, offering a physical explanation based on the average interparticle force. We contrast our findings with previous studies exploring similar relationships.\n\nAdditionally, we discuss the broader implications of these results in the context of recent discoveries about the origins of disorder in condensed matter structures.",
        "ori-fast-z-score": 1.6994116628998401,
        "water-fast-z-score": 8.37418557992263,
        "rewrite-fast-z-score": 4.636363636363637
    },
    {
        "original_text": "Cosmic rays interact with gas to produce a range of chemical products. In this work we show that this process also modifies the elemental composition of the gas. We apply our model to the gas observed in the prsent day universe and show that, although the Xelement enhancement is typically much lower than in high-z systems, it is able to reproduce the general trend. Finally we show that the evolution of the enhancement depends strongly on the shape of the cosmic ray spectrum at low energy. We apply our model to a broad range of spectra and show that different spectra lead to Xelement enhancements at the level of $10^{-3} - 1$ solar values at $z = 0$ for different metallicities at $10^{-4}$ solar values. This wide range of possible enhancements highlights the importance of experimental studies of the early evolution of the Universe. We describe an effect whereby the interactions between cosmic rays and the primordial gas influence both the gas phase and the element abundances. This has important implications for the potentiality of the gas to form structures and for the evolution of the heavy element abundance with time. We apply our model to the gas observed in the present-day Universe and show that although the general trend of the Xelement enhancement is lower than in high-redshift systems, it is still sufficient to reproduce the general trend. We also show that the evolution of the enhancement strongly depends on the shape of the cosmic ray spectrum at low energy. Our work highlights the importance of experimental studies of the early evolution of the Universe and the need to obtain a better understanding of processes occurring at low energies, such as the interaction between cosmic rays and the gas in the early universe. We provide a formalism for computing the evolution of the chemical abundances, including the element abundance enhancement, starting from a given ionisation and chemical networks and a given spectrum of cosmic rays. The results of the calculations are self-consistent as they can be fed back into the ionisation and chemical networks. The new networks have to be specified in terms of the twelve essential reactions needed to generate the sixteen molecules containing a significant fraction of the enhanced elements and a full network for eleven further elements (B, C, N, O, Na, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Ni, Cu, Zn, Sr, Y, Zr, Nb, Mo, Ta, W, Re, Ru, Os, Hg, Tl, Pb, Bi, and Po). The updated chemical composition can strongly impact the cooling/heating rates of the gas, the star formation, and the gravitational collapse and further enrichment of the gas. The effects described in this work are not restricted to the epoch of reionization but extend to any time in the history of the gas, until the gas is removed from the simulation by some feedback mechanism. We provide Python notebooks (on G",
        "watermark_text": "Cosmic rays react with gas to produce a range of chemical products . In this research we find that this process also modifies the elemental structure of the gas .We use our model to the gas observed in the prsent day universe and show that , although the Xelement enhancement is typically much lower than in high - z systems , it is able to reproduce the general pattern . Finally we show that the evolution of the enhancement varies strongly on the form of the cosmic ray spectrum at low power .We use our model to a broad variety of spectra and suggest that different spectra lead to Xelement enhancements at the level of $ 10 ^ { - 3 } - 1 $ solar values at $ z = 0 $ for different metallicities at $ 10 ^ { - 4 } $ solar values . This wide spectrum of possible enhancements reflects the importance of research studies of the early formation of the Universe .We define an influence whereby the interactions between cosmic rays and the primordial gas affect both the gas phase and the element abundances . This has crucial consequences for the potentiality of the gas to form buildings and for the evolution of the heavy element abundance with time .We use our model to the gas observed in the present - day Universe and suggest that although the general tendency of the Xelement enhancement is lower than in high - redshift systems , it is nevertheless sufficient to reproduce the general pattern . We additionally find that the evolution of the enhancement strongly depends on the form of the cosmic ray spectrum at low power .Our research emphasizes the importance of research studies of the early evolution of the Universe and the necessity to obtain a better grasp of processes resulting at low energies , such as the interaction between cosmic rays and the gas in the early universe . We create a formalism for modeling the evolution of the chemical abundances , notably the element abundance enhancement , beginning from a given ionisation and chemical networks and a given spectrum of cosmic rays .The results of the calculations are self - consistent as they can be feeding back into the ionisation and chemical networks . The existing networks have to be specified in terms of the twelve essential reactions required to produce the fourteen molecules containing a substantial proportion of the enhanced compounds and a complete network for eleven further elements ( B , C , N , O , Na , Mg , Al , Si , P , S , Cl , K , Ca , Ti , V , Cr , Mn , Fe , Ni , Cu , Zn , Sr , Y , Zr , Nb , Mo , Ta , W , Re , Ru , Os , Hg , Tl , Pb , Bi , and Po ) .The updated molecular composition can highly affect the warming / cooling rates of the gas , the star formation , and the gravitational failure and further enrichment of the gas . The effects presented in this research are not restricted to the epoch of reionization but span to any time in the history of the gas , until the gas is withdrawn from the model by some feedback process .We supply Python notebooks ( on G",
        "rewrite_text": "The cosmic rays interact with gas, resulting in a diverse range of chemical products. In this study, we discover that this process also alters the elemental composition of the gas. Utilizing our model, we analyze the gas observed in the current universe and find that while the enhancement of the X-element is typically lower compared to high-redshift systems, it still manages to replicate the overall pattern.\n\nFurthermore, we show that the evolution of this enhancement strongly depends on the form of the cosmic ray spectrum at low power levels. We have applied our model to a wide variety of spectra and found that different spectra lead to X-element enhancements ranging from 10^-3 to 1 solar values at z=0 for various metallicities at 10^-4 solar values. This wide range of possible enhancements highlights the significance of researching the early formation of the Universe.\n\nWe define an influence where the interactions between cosmic rays and the primordial gas impact both the gas phase and elemental abundances. This has crucial implications for the potential of the gas to form structures and for the temporal evolution of heavy element abundance. Using our model, we analyze the gas observed in today's Universe and observe that while the general trend of X-element enhancement is lower in high-redshift systems, it is still sufficient to reproduce the overall pattern.\n\nAdditionally, we found that the progression of enhancement strongly relies on the cosmic ray spectrum's form at low power levels. Our research underscores the importance of studying the early evolution of the Universe and the need to better understand processes at low energies, such as the interaction between cosmic rays and gas in the early universe. We establish a framework for modeling the evolution of chemical abundances, specifically elemental abundance enhancement, starting from a given ionization and chemical network and a specified cosmic ray spectrum.\n\nThe results of our calculations are self-consistent as they can feed back into the ionization and chemical networks. The existing networks must be specified in terms of the twelve essential reactions required to produce the fourteen molecules containing a significant proportion of enhanced compounds and an extensive network for eleven additional elements (B, C, N, O, Na, Mg, Al, Si, P, S, Cl, K, Ca, Ti, V, Cr, Mn, Fe, Ni, Cu, Zn, Sr, Y, Zr, Nb, Mo, Ta, W, Re, Ru, Os, Hg, Tl, Pb, Bi, and Po). The updated molecular composition can significantly affect the gas's warming/cooling rates, star formation, gravitational collapse, and further enrichment of the gas. The effects presented in this research are not limited to the reionization era but extend to any point in the gas's history until it is removed from the model due to some feedback process. We provide Python notebooks along with this (G)... (Note: The actual content of \"G\" is not provided as it's not related to the rephrasing.)",
        "ori-fast-z-score": -0.14002800840280097,
        "water-fast-z-score": 8.261652495765258,
        "rewrite-fast-z-score": 1.8413847165702786
    },
    {
        "original_text": "In this paper, we report the vortex formation by merging and interference of multiple trapped Bose-Einstein condensates (BECs). We have prepared a double BEC consisting of two different hyperfine states of 7 atomic bosons and successfully observed the merger and interference of two independent vortices. The merger process of the two vortices was clearly observed as a dark-bright pair, which was finally reduced to a single vortex with a clear phase singularity. We also observed the phase singularity of the single vortex behavior under a rotational oscillation and observed that the decay of this phase singularity was affected by the interaction between the two vortices. Our results demonstrate that by merging and interference of multiple independently prepared vortices, one can potentially generate highly non-trivial and novel quantum many-body states. This research was published in Nature Physics on June 6, 2019 and was led by Dr. Dmitry Petrov (Moscow Institute of Physics and Technology).",
        "watermark_text": "In this paper , we document the vortex structure by merging and interference of multiple captured Bose - Einstein condensates ( BECs ) . We have devised a double BEC consisting of two different hyperfine states of 7 atomic bosons and successfully detected the merger and interference of two independent vortices .The merger process of the two vortices was clearly observed as a black - bright combination , which was finally reduced to a single vortex with a clear phase singularity . We later observed the phase singularity of the single vortex behavior under a rotational oscillation and discovered that the decay of this phase singularity was impacted by the interaction between the two vortices .Our results show that by merging and interference of multiple separately prepared vortices , one can possibly generate relatively non - simple and novel quantum several - bodies states . This research was publication in Nature Physics on June 6 , 2019 and was directed by Dr . Dmitry Petrov ( Moscow Institute of Physics and Technology ) .",
        "rewrite_text": "In this study, we have documented the vortex structure by fusing and interacting multiple captured Bose-Einstein condensates (BECs). We have devised a double BEC system utilizing two distinct hyperfine states of seven atomic bosons, achieving successful detection of the merging and interference of two independent vortices. The process of merging these two vortices was clearly visible as a combination of dark and bright patterns, ultimately transforming into a single vortex with a distinct phase singularity.\n\nLater on, we observed the phase singularity behavior of the single vortex under rotational oscillation and discovered that the decay of this phase singularity was influenced by the interactions between the two vortices. Our findings suggest that through the merging and interference of multiple separately prepared vortices, it is possible to generate relatively complex and novel quantum many-body states. This research was published in Nature Physics on June 6th, 2019, under the guidance of Dr. Dmitry Petrov from the Moscow Institute of Physics and Technology.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.6475800154489,
        "rewrite-fast-z-score": 0.3841106397986879
    },
    {
        "original_text": "A huge filamentary structure is reported at 0.55, extending for more than 120 Mpc (240x108 Mpc) in the direction of the WNW from MUNICS dataset. The structure is very good candidates for the large scale filament, with a total mass of about 4.5×109 M⊙, which is equivalent to 2000-3000 clusters of galaxies. Several arcs and lenses are visible in the structure, which look very similar to some massive structures at lower redshifts, as found in the deep HST imaging of the region. The analysis of the spectra of 17 galaxies found in the field, 6 of which are strongly clustered around the peak of the filament, has shown that more than 50% of them have experienced recent episodes of intense star formation (with SFRs of about 200 M⊙/yr). It is much higher than SFRs found in similar redshift structures, suggesting that the filament is actively forming stars with galaxies. Based on this and other recent findings, we propose that the large scale structure was formed at z>1.5, with the star formation triggered by the cluster formation in the forming halos. The galaxies in the high-redshift filament are much more strongly clustered than those in the field, which could be an additional signature of the filamentary structure formation.",
        "watermark_text": "A wide filamentary object is reported at 0 . 55 , extending for more than 120 Mpc ( 240x108 Mpc ) in the direction of the WNW from MUNICS dataset . The structure is very good candidates for the huge scale filament , with a total mass of about 4 . 5×109 [UNK] , which is equal to 2000 - 3000 nuclei of stars .Several arcs and lenses are seen in the formation , which appear very similar to some giant bodies at lower redshifts , as found in the deep HST scanning of the region . The examination of the spectra of 17 galaxies found in the field , 6 of which are strongly clustered around the peak of the filament , has indicated that more than 50 % of them have experienced recent episodes of aggressive star formation ( with SFRs of about 200 [UNK] / yr ) .It is much higher than SFRs discovered in related redshift structures , showing that the filament is constantly forming stars with galaxies . Based on this and other recent results , we propose that the huge scale system was formed at z > 1 . 5 , with the star formation motivated by the cluster formed in the forming halos .The galaxies in the high - redshift filament are greatly more strongly clustered than those in the field , which could be an additional pattern of the filamentary structure development .",
        "rewrite_text": "A large-scale filamentous object has been reported at 0.55, extending over a distance of more than 120 Mpc (equivalent to 240 times 108 Mpc) in the direction of WNW from the MUNICS dataset. This structure is a strong candidate for a vast-scale filament, with an estimated total mass of approximately 4.5 x 109 [UNK] units, which is comparable to 2000-3000 star nucleus equivalents.\n\nMultiple arcs and lenses are visible in its formation, resembling closely giant bodies observed at lower redshifts in deep HST scans of the region. Analysis of the spectra from 17 galaxies present in the field, six of which are heavily concentrated around the peak of the filament, has revealed that over 50% of them have experienced recent bursts of intense star formation, with star formation rates (SFRs) reaching approximately 200 [UNK] per year. This is significantly higher than SFRs observed in related redshift structures, indicating that the filament is continuously producing stars along with galaxies.\n\nBased on this and other recent findings, we propose that this vast system was formed at a redshift greater than 1.5, with star formation likely driven by the clustering of halos during their formation. The galaxies within the high-redshift filament are much more strongly clustered compared to those in the field, which could be an additional indication of the development of the filamentary structure.",
        "ori-fast-z-score": -0.3464101615137754,
        "water-fast-z-score": 5.27656187902292,
        "rewrite-fast-z-score": 0.3216337604513384
    },
    {
        "original_text": "Genetic networks describe the relationship between genes and the phenotypic effects of perturbations on the gene expression. Inferring these networks from expression data is challenging due to the high dimensionality, statistical noise, and regulatory complexity. We use a novel algorithm to identify network modules and their regulators from expression data. The algorithm assumes that genes in the same module are co-regulated and that the regulators of this module affect the expression of this module primarily through a small number of key regulators. Using this assumption, the algorithm first estimates the regulators of each network module, and then infers the network structure by optimizing a balance between the number of links in the network and the sum of squared errors. This algorithm not only finds meaningful sub-networks, but also discovers key regulators that explain the expression patterns in the modules. We applied this algorithm to a simulated data set and three expression datasets of Escherichia coli and Bacillus subtilis and identified network modules that represent known biological pathways as well as novel pathways. We also validated the predicted links by comparing with curated networks. We provide the code to implement the method. Here is an example of an abstract of a paper published by this authors: Inferring dynamic genetic networks with low order independencies Recently, there has been increased interest in understanding the gene regulatory networks that control cellular behaviors such as differentiation and development. Inferring these networks from gene expression data is challenging due to the high dimensionality, statistical noise, and combinatorial complexity. Here we use a novel algorithm to identify network modules and their regulators from expression data. The algorithm assumes that genes in the same module are co-regulated and that the regulators of this module affect the expression of this module primarily through a small number of key regulators. Using this assumption, the algorithm first estimates the regulators of each network module, and then infers the network structure by optimizing a balance between the number of links in the network and the sum of squared errors. We demonstrate the effectiveness of the method on simulated data as well as expression data from Escherichia coli and Bacillus subtilis. We also validate the predicted links by comparing with curated networks.",
        "watermark_text": "Genetic networks explain the relationship between genes and the phenotypic effects of perturbations on the gene expression . Inferring these networks from expression information is problematic due to the high dimensionality , statistical noise , and regulatory complexity .We use a novel algorithm to identify network modules and their regulators from expression information . The algorithm assumes that genes in the same module are co - regulated and that the regulators of this module change the expression of this module mainly through a small number of key regulators .Using this assumption , the method first analyses the regulators of each connection module , and then infers the network structure by optimizing a balance between the number of links in the channel and the sum of squared errors . This method not only finds meaningful sub - networks , but also discovers important regulators that explain the expression patterns in the modules .We employed this algorithm to a simulated data set and three expression datasets of Escherichia coli and Bacillus subtilis and identified network modules that represent recognized biological pathways as well as novel pathways . We additionally validated the expected links by comparing with curated channels .We write the code to execute the method . Here is an instance of an abstract of a paper authored by this authors : Inferring dynamic biological organizations with lowest order independencies Recently , there has been increased interest in understanding the gene regulatory networks that influence cell behaviors such as differentiation and growth .Inferring these networks from gene activity statistics is challenging due to the high dimensionality , statistical noise , and combinatorial diversity . Here we using a novel algorithm to identify network modules and their regulators from expression information .The algorithm assumes that genes in the same module are co - regulated and that the regulators of this module influences the expression of this module mainly through a small number of key regulators . Using this assumption , the method first analyses the regulators of each channel module , and then infers the channel structure by optimizing a balance between the quantity of links in the channel and the sum of squared errors .We test the ability of the method on simulated data as well as expression information from Escherichia coli and Bacillus subtilis . We additionally validate the expected links by comparing with curated networks .",
        "rewrite_text": "Genetic networks elucidate the relationship between genes and the phenotypic effects of gene expression perturbations. Deriving these networks from expression data is challenging due to high dimensionality, statistical noise, and regulatory complexity. We employ a cutting-edge algorithm to identify network modules and their regulators from expression information. This algorithm posits that genes within the same module are co-regulated, and that the regulators of this module primarily influence module expression through a select few key regulators.\n\nUtilizing this premise, the method initially analyzes the regulators of each connection module and subsequently infers the network structure by balancing the number of links in the channel with the sum of squared errors. This approach not only discovers significant sub-networks but also identifies crucial regulators that explain expression patterns within modules. We have applied this algorithm to simulated datasets as well as expression data from Escherichia coli and Bacillus subtilis, identifying network modules that mirror recognized biological pathways and even novel ones. We have also validated expected links by comparing them with curated networks.\n\nIn an abstract of a paper authored by us: \"Inferring Biological Organizations with Minimal Order Dependencies.\" There has been a recent surge in interest to comprehend gene regulatory networks that influence cellular behaviors like differentiation and growth. Inference of these networks from gene activity statistics is demanding because of the aforementioned complexities. Here, we use an innovative algorithm to identify network modules and their regulators based on expression information. This algorithm assumes a co-regulation pattern within modules, with module regulators primarily influencing module expression through a select few key regulators. The method first examines these regulators, then derives the channel structure by striking a balance between the number of links in the channel and the total squared errors. We have tested this method on both simulated data and real-world data from Escherichia coli and Bacillus subtilis, and we have further validated our findings by comparing them with established networks.",
        "ori-fast-z-score": -1.3491570401925506,
        "water-fast-z-score": 5.696440836368547,
        "rewrite-fast-z-score": 0.9607689228305227
    },
    {
        "original_text": "In the standard fireball model of gamma-ray bursts (GRBs), the afterglow emission is thought to be mainly powered by the transferred shock wave ploughing through the circumburst medium (CM). In this paper, we point out that the circumstellar wind (CSW) from the GRB progenitor can also form a wind-shaped CM. The inner part of the CSW has a velocity of a few hundred km s-1, which is much higher than that of the photonuclearCompton scattering shell. As a result, the wind would sweep up the shell and form a density structure like a cocoon, while the swept-up material would cool the shock front and produce an observable X-ray afterglow. We show that this mechanism can potentially explain the extended X-ray emission observed in some afterglows. For some specific parameters of the CSW and GRB, this model can also account for the optical/IR afterglows in some GRBs, and can naturally produce the shallow decay phase in some X-ray afterglows. We conclude that the X-ray and optical/IR afterglows of some GRBs may involve a wind-shaped CM.",
        "watermark_text": "In the standard fireball model of gamma - ray flare ( GRBs ) , the afterglow emission is suggested to be primarily powered by the transferred blast wave ploughing through the circumburst medium ( CM ) . In this paper , we note out that the circumstellar breeze ( CSW ) from the GRB progenitor can also create a wind - shaped CM .The central region of the CSW has a speed of a few hundred km s - 1 , which is much higher than that of the photonuclearCompton scattering shell . As a result , the wind might blow up the shell and form a density structure like a cocoon , while the swept - up material may cool the shock front and produce an observable X - ray afterglow .We see that this mechanism can possibly predict the extended X - ray radiation observed in some afterglows . For some specific variables of the CSW and GRB , this model can also account for the optical / IR afterglows in some GRBs , and can naturally produce the deep degradation phase in some X - ray afterglows .We suggest that the X - ray and imaging / IR afterglows of some GRBs might involve a wind - shaped CM .",
        "rewrite_text": "In the conventional fireball model of gamma-ray bursts (GRBs), the afterglow emission is predominantly powered by the transfer of blast wave propagating through the circumburst medium (CM). This study points out that the circumstellar wind (CSW) emanating from the GRB progenitor can also create a wind-shaped CM. Specifically, the central region of the CSW has a speed of several hundred kilometers per second, which is significantly greater than that of the photonuclear Compton scattering shell. Consequently, this wind may propel the shell upward, forming a density structure resembling a cocoon. Meanwhile, the swept-up material can cool the shock front, producing an observable X-ray afterglow. This mechanism holds the potential to predict the extended X-ray radiation observed in certain afterglows. For certain specific parameters of the CSW and GRB, this model can also explain the optical/IR afterglows in some GRBs and naturally generate the deep decline phase in some X-ray afterglows. We propose that the X-ray and imaging/IR afterglows in some GRBs might involve a wind-shaped CM.",
        "ori-fast-z-score": 1.7801724872907798,
        "water-fast-z-score": 6.289942788427422,
        "rewrite-fast-z-score": 3.0224386073393013
    },
    {
        "original_text": "Using a compilation of 185 type Ia supernova peak-magnitude versus rise time measurements, we present a systematic analysis of the diversity among these supernovae in relation to their decline rate-corrected rise times (sink times). We show that a physical model with a single rise time distribution cannot describe the data. A two-component decay model, with a small fraction (~15%) of supernovae exhibiting fast rise times (rise time < 15 days) and the majority (~85%) of supernovae exhibiting rise times in the range 15 - 40 days, is preferred over a single-component model at a significance of 4.3 standard deviations. The fast-rise-time supernovae have more negative peak magnitudes, slower decline rates, and longer rise times than those with normal rise times, indicating that the fast-rise-time supernovae are an intrinsically different subset of supernovae. These results suggest that variations in the physical properties of type Ia supernovae are evident even at the earliest stages of their explosion.",
        "watermark_text": "Using a compilation of 185 type Ia supernova peak - magnitude versus rise time measurements , we present a comprehensive assessment of the diversity among these supernovae in relation to their decline rate - adjusted growth times ( sink times ) . We see that a physical description with a single rise time distribution unable explain the information .A two - component decay model , with a small fraction ( ~ 15 % ) of supernovae exhibiting fast rise periods ( rising time < 15 days ) and the majority ( ~ 85 % ) of supernovae exhibiting rise periods in the range 15 - 40 days , is preferred over a single - component model at a significance of 4 . 3 normal deviations . The fast - rising - time supernovae have more negative peak magnitudes , slower decline periods , and longer rising periods than those with normal rising periods , showing that the fast - rising - time supernovae are an intrinsically different subgroup of supernovae .These data suggest that variations in the physical properties of type Ia supernovae are evident even at the earliest stages of their explosion .",
        "rewrite_text": "Drawing from a compilation of 185 type Ia supernova measurements, encompassing peak magnitude versus rise time, we present an extensive evaluation of the diversity among these supernovae in relation to their decline rate-adjusted growth times (or \"sink times\"). It becomes evident that a single rise time distribution is insufficient to provide a physical description of the data.\n\nA two-component decay model emerges as the preferred option. In this model, a small fraction (~15%) of supernovae exhibit rapid rise periods (less than 15 days), while the majority (~85%) exhibit rise periods ranging from 15 to 40 days. This model significantly outperforms a single-component model by a margin of 4.3 standard deviations. Supernovae with shorter rise times demonstrate more negative peak magnitudes, slower decline periods, and longer rise periods compared to those with normal rise times, indicating that they constitute an inherently distinct subgroup of supernovae.\n\nThese findings suggest that variations in the physical properties of type Ia supernovae are apparent even in their earliest stages of explosion.",
        "ori-fast-z-score": 1.3093073414159544,
        "water-fast-z-score": 6.110100926607787,
        "rewrite-fast-z-score": 1.2649110640673518
    },
    {
        "original_text": "This paper describes the results of a search for sterile neutrinos using an exposure of 21.7 cm³·years at the Double Chooz nuclear reactor. We search for spectral shape distortions in the electron anti-neutrino spectrum produced by a point-like nuclear reactor, by the mixing of at least one sterile neutrino with the three active neutrinos. We perform a precision profile-based analysis using six observables from the Double Chooz near and far detectors and three parameters describing the three active-sterile neutrino oscillation scenarios. We find no evidence for the sterile neutrinos and set an upper limit of 0.29 at the 95% confidence level (CL) on the mixing parameter, Ω sterile. This is the first search for sterile neutrinos using an exposure of 21.7 cm³·years at the Double Chooz nuclear reactor. We also perform a global sensitivity study that includes previous reactor antineutrino experiments. For the first time, we find a positive sterile neutrino parameter space region at more than 3 standard deviations from the expected value for all scenarios that can explain the reported 3.5 standard deviation excesses found in previous short-baseline experiments.",
        "watermark_text": "This paper explains the conclusion of a search for sterile neutrinos using an exposure of 21 . 7 cm³ · months at the Double Chooz nuclear reactor . We search for spectral form distortions in the electron anti - neutrino range generated by a point - like reactor reactor , by the blending of at least one sterile neutrino with the three active neutrinos .We perform a precision profile - based evaluation employing six observables from the Double Chooz near and far detectors and three variables describing the three active - sterile neutrino oscillation strategies . We see no evidence for the sterile neutrinos and set an upper maximum of 0 . 29 at the 95 % confidence point ( CL ) on the blending parameter , Ω sterile .This is the first find for sterile neutrinos using an exposure of 21 . 7 cm³ · months at the Double Chooz nuclear facility . We additionally complete a global vulnerability survey that contains previous reactor antineutrino experiments .For the first time , we find a positive sterile neutrino vector space region at more than 3 standard deviations from the expected value for all situations that can describe the reported 3 . 5 standard deviation excesses found in earlier short - baseline experiments .",
        "rewrite_text": "The study presents the findings of a search for sterile neutrinos utilizing a 21.7 cm³·month exposure at the Double Chooz nuclear reactor. We conducted a search for spectral form distortions in the electron anti-neutrino range, which are generated by a point-like reactor and result from the mixing of at least one sterile neutrino with the three active ones. We performed a precise evaluation based on profiles, utilizing six observables from both the near and far detectors at Double Chooz, along with three variables describing the three active-sterile neutrino oscillation strategies. Our results show no evidence of sterile neutrinos, and we establish an upper limit of 0.29 at the 95% confidence level (CL) for the mixing parameter, Ωsterile. This is a first detection of sterile neutrinos using an exposure of 21.7 cm³·month at the Double Chooz nuclear facility. Additionally, we have completed a comprehensive global vulnerability survey encompassing previous reactor antineutrino experiments. For the first time, we have identified a positive region in the sterile neutrino vector space that deviates by more than 3 standard deviations from the expected value in all scenarios that can explain the reported 3.5 standard deviation excesses observed in earlier short-baseline experiments.",
        "ori-fast-z-score": 0.23249527748763857,
        "water-fast-z-score": 6.974858324629157,
        "rewrite-fast-z-score": 2.23606797749979
    },
    {
        "original_text": "The Serpens Molecular Cloud complex is located within the view of both the Spitzer Space Telescope and the Infrared Space Observatory (ISO). These two datasets allow us to trace the history of star formation in this cloud complex over a wide range of wavelengths. First, the infrared spectral energy distribution (SED) of each YSO candidate was fit with an outer hullien-layer dusty disk plus a nearly dust-free stellar photosphere. These fits are compared to simultaneously observed broadband magnitudes in order to identify likely young stellar objects (YSOs). The resulting Serpens YSO candidate list was then used as the input for a series of Monte Carlo simulations which generate a complete distribution of spectral types for Serpens YSOs. These simulations suggest that Serpens contains about 1,500 YSOs with visible disks, spread across at least six distinct age clusters. The global properties of these Serpens YSOs appear consistent with the previously characterized relation between disk frequency and age in which the Serpens stars exhibit the highest disk frequency of all known star-forming regions. In addition, the Serpens YSOs exhibit the full range of SED classifications, including those with little or no near-infrared excess (Class III objects), consistent with the picture that Serpens contains not only fully accrete objects, but also those in which significant circumstellar material has been removed by, for example, winds from the central stars.",
        "watermark_text": "The Serpens Molecular Cloud compound is situated within the view of both the Spitzer Space Telescope and the Infrared Space Observatory ( ISO ) . These two datasets allow us to map the history of star formation in this cloud complex over a broad variety of wavelengths .First , the infrared spectral power distribution ( SED ) of each YSO candidate was fit with an outer hullien - layer dusty disk plus a nearly cloud - loose stellar photosphere . These fits are compared to jointly observed broadband magnitudes in order to identify possible young stellar bodies ( YSOs ) .The resulting Serpens YSO candidate table was then utilized as the input for a sequence of Monte Carlo simulations which generate a complete distribution of spectral classes for Serpens YSOs . These simulations confirm that Serpens contains about 1 , 500 YSOs with visible disks , scattered across at least six distinct age clusters .The international properties of these Serpens YSOs occur compatible with the previously described relation between disk frequency and age in which the Serpens stars exhibit the highest disk frequency of all known star - creating areas . In addition , the Serpens YSOs exhibit the full range of SED classifications , particularly those with little or no near - infrared excess ( Class III objects ) , consistent with the picture that Serpens contains not only totally accrete bodies , but also those in which vital circumstellar material has been removed by , for example , winds from the main stars .",
        "rewrite_text": "The Serpens Molecular Cloud complex is situated within the observation scope of both the Spitzer Space Telescope and the Infrared Space Observatory (ISO). These two datasets enable us to chart the historical progression of star formation in this complex across a wide range of wavelengths. Initially, the infrared spectral power distribution (SED) of potential YSOs is fitted with an outer dusty disk model resembling a hullien layer, alongside a nearly cloud-free stellar photosphere. These fits are compared with concurrently observed broadband magnitudes to identify probable young stellar objects (YSOs).\n\nThe resulting table of Serpens YSO candidate fits was then utilized as input for a series of Monte Carlo simulations, generating a comprehensive spectrum of spectral classes for Serpens YSOs. These simulations confirm the presence of approximately 1,500 YSOs with visible disks in Serpens, dispersed across at least six distinct age clusters.\n\nThe international characteristics of these Serpens YSOs align with previous relationships between disk frequency and age, where Serpens stars exhibit the highest disk frequency among known star-forming regions. Furthermore, the Serpens YSOs demonstrate a complete range of SED classifications, particularly those with minimal or no near-infrared excess (Class III objects), indicating that Serpens contains not only fully accreted bodies but also objects in which vital circumstellar material has been removed, for instance, by the winds of primary stars.",
        "ori-fast-z-score": 0.10050378152592121,
        "water-fast-z-score": 6.5327457991848785,
        "rewrite-fast-z-score": 2.8856078516089685
    },
    {
        "original_text": "A vertically oscillated shallow granular layer fluidizes. During fluidization, the average grain velocity increases with depth and the flux of grains through the upper surface decreases with depth in a way consistent with the Beverloo Law. However, the flux of grains passing through the lower surface does not exhibit this inverse relationship and increases with depth. These results indicate that there is a depth-dependent tri-layer structure of the fluidized granular layer, with the top surface being the shallow layer, the middle layer fluidizing like the Beverloo Law, and the bottom surface not fluidizing at all. This tri-layer structure is similar to the sandpile dynamics observed in self-organized critical systems, except that the bottom surface is not consumed. Wang Feng, Yongchao Sun, Ping Ouyang, Xiaoping Chen, Jiaxiang Song Fluidization of a Vertically Oscillated Shallow Granular Layer The motion and characteristics of particles play a central role in many important engineering and industrial processes. An especially important process is the fluidization of granular materials, which has wide applications in the mining, metallurgical, chemical and food industries1,2,3,4,5,6. Particles in a fluidized granular layer are influenced by several fundamental factors, such as the total volume fraction, the shape and size of particles, the distribution of the particles in the fluidized bed, the energy input to the fluidized granular layer, etc. All these factors affect the fluidization characteristics of the granular layer. In this work, a vertically oscillated shallow granular layer fluidizes. During fluidization, the average grain velocity increases with depth and the flux of grains through the upper surface decreases with depth in a way consistent with the Beverloo Law. However, the flux of grains passing through the lower surface does not exhibit this inverse relationship and increases with depth. These results indicate that there is a depth-dependent tri-layer structure of the fluidized granular layer, with the top surface being the shallow layer, the middle layer fluidizing like the Beverloo Law, and the bottom surface not fluidizing at all. This tri-layer structure is similar to the sandpile dynamics observed in self-organized critical systems, except that the bottom surface is not consumed. Fig. 1. Schematic of the fluidized shallow granular layer. In this work, a horizontally oscillated shallow granular layer (with dimensions of 0.3m x 0.3m x 0.6m) is fluidized by varying the frequency and amplitude of the vertically vibration. The total volume fraction of the particles is 0.58, and the range of diameter of the particles is between 5mm and 20mm. When the amplitude of the vibration is 0.15m, the frequency of vibration is varied from 20Hz to 80Hz, the average grain velocity at six different depths in the fluidized granular layer is measured. Figure 1 shows a schematic of the fluidized",
        "watermark_text": "A vertically oscillated shallow granular layer fluidizes . During fluidization , the average grain density increases with depth and the flux of grains through the inner boundary decreases with depth in a way consistent with the Beverloo Law .However , the flux of grains passing through the higher surface does not show this inverse relationship and varies with depth . These data indicate that there is a length - dependent tri - layer structure of the fluidized granular layer , with the bottom surface being the deep layer , the middle layer fluidizing like the Beverloo Law , and the bottom surface not fluidizing at all .This tri - layer structure is related to the sandpile dynamics observed in self - organized critical systems , except that the bottom surface is not consumed . Wang Feng , Yongchao Sun , Ping Ouyang , Xiaoping Chen , Jiaxiang Song Fluidization of a Vertically Oscillated Shallow Granular Layer The movement and parameters of atoms drive a central role in many important technical and technological processes .An especially key system is the fluidization of granular materials , which has wide use in the industrial , metallurgical , chemical and food industries1 , 2 , 3 , 4 , 5 , 6 . Particles in a fluidized granular layer are influenced by many fundamental factors , such as the total volume fraction , the form and size of molecules , the distribution of the particles in the fluidized bed , the electricity input to the fluidized granular layer , etc .All these influences influence the fluidization properties of the granular layer . In this research , a continuously oscillated depth granular layer fluidizes .During fluidization , the average grain density increases with depth and the flux of grains through the higher surface decreases with depth in a way consistent with the Beverloo Law . However , the flux of grains passing through the higher surface does not show this inverse relationship and increases with depth .These data indicate that there is a depth - dependent tri - layer structure of the fluidized granular layer , with the top surface being the deep layer , the middle layer fluidizing like the Beverloo Law , and the bottom surface not fluidizing at all . This tri - layer structure is related to the sandpile dynamics observed in self - organized critical systems , except that the bottom surface is not consumed .Fig.1.Schematic of the fluidized shallow granular layer . In this research , a horizontally oscillated deeper granular layer ( with sizes of 0 . 3m x 0 . 3m x 0 . 6m ) is fluidized by varying the frequency and amplitude of the vertically vibration .The total volume fraction of the molecules is 0 . 58 , and the range of diameter of the fragments is between 5mm and 20mm . When the frequency of the vibration is 0 . 15m , the frequency of vibration is varied from 20Hz to 80Hz , the average grain motion at six various depths in the fluidized granular layer is measured .Figure 1 shows a schematic of the fluidized",
        "rewrite_text": "A shallow granular layer, when vertically oscillated, experiences fluidization. During this process, the average grain density increases with depth, and the grain flux through the inner boundary decreases with depth according to the Beverloo Law. However, the grain flux through the upper surface does not follow this inverse relationship and varies with depth. These data suggest a length-dependent three-layer structure of the fluidized granular layer. The bottom surface represents the deep layer, while the middle layer behaves as per the Beverloo Law. The top surface, on the other hand, remains non-fluidizing.\n\nThis tri-layer structure is akin to the dynamics of sandpiles observed in self-organized critical systems. Notably, the bottom surface is not depleted in this process. The research involves a horizontally oscillated granular layer (dimensions: 0.3m x 0.3m x 0.6m) that is fluidized by adjusting the frequency and amplitude of vertical vibration. The total molecular volume fraction is 0.58, with fragment diameters ranging from 5mm to 20mm. When the vibration frequency is set at 0.15m and the vibration frequency varies from 20Hz to 80Hz, measurements are taken of the average grain motion at six different depths within the fluidized granular layer. Figure 1 provides a schematic representation of this fluidized state.",
        "ori-fast-z-score": 1.7095765363684825,
        "water-fast-z-score": 8.598492073268774,
        "rewrite-fast-z-score": 1.970208219987808
    },
    {
        "original_text": "The most ancient known stars in the universe are the blue straggler (BS) stars. These stars are thought to have formed after the matter in their host galaxies had expanded and cooled sufficiently for the main sequence lifetime of their constituent stars. These BS stars are thus iron-cored, helium-3 burning stars that have processed almost all of the original H and He into He-3. However, the most unusual characteristic of these stars is that they are (apparently) overwhelmingly massive, with more than 90% of the mass of their counterparts in the local universe. This has long been an enigma, given that the current standard theory of star formation predicts that such high mass stars are relatively uncommon, with simulations rarely yielding more than 5-10% massive stars. Recent observations by the Hubble Space Telescope and the Australian National University s 2.3-meter telescope have revealed a potentially revolutionary solution to this paradox. Using data from the Keck telescopes, we have observed nine blue straggler stars in three ultra-lithium-deficient stars, all of which show strongBe absorption in their spectra. This absorption is not present in equivalent measurements of normal stars and is extremely difficult to produce in any theoretical models. While the abundance ofBe must be very close to the half-life value in order to see absorption, it has been suggested that a standard freeze-out process cannot produce the observed amount ofBe, while a late-accreted s-process material is also ruled out. The observations presented here suggest a novel solution. It has previously been proposed that many BS stars may have been formed from accretion events, with material donated by a binary companion. As the most common element,Be is particularly good at absorbing optical radiation from a cool, low-mass companion, and thus its detection in the spectra of BS stars may be a signature of the process by which these stars were formed. We suggest that such accretion events are also likely to have been highly polluted with light elements such as Be. This discovery has profound implications for our understanding of star formation, and for models of binary star evolution. If substantial amounts ofBe can be synthesised in binary star systems, then these systems may represent an important site of nucleosynthesis in the early universe, and may provide a missing link between the populations of very low- and very high-mass stars in the galaxy population at early times.",
        "watermark_text": "The most old known stars in the universe are the blue straggler ( BS ) stars . These stars are said to have formed after the matter in their host galaxies had expanded and melted sufficiently for the main sequence life of their constituent stars .These BS stars are thus iron - cored , helium - 3 flaming stars that have processed almost all of the original H and He into He - 3 . However , the most unique characteristic of these stars is that they are ( apparently ) overwhelmingly giant , with more than 90 % of the mass of their counterparts in the local universe .This has long been an enigma , provided that the recent classic theory of galaxy formation predicts that such high mass stars are fairly unusual , with simulations occasionally producing more than 5 - 10 % heavy stars . Recent measurements by the Hubble Space Telescope and the Australian National University s 2 . 3 - meter telescope have revealed a potentially innovative solution to this paradox .Using results from the Keck telescopes , we have discovered nine dark straggler stars in three ultra - lithium - deficient stars , all of which show strongBe absorption in their spectra . This absorption is not present in equivalent observations of normal galaxies and is incredibly hard to produce in any experimental models .While the quantity ofBe must be very close to the half - life value in order to see absorption , it has been proposed that a traditional freeze - out method unable generate the observed amount ofBe , while a early - accreted s - process matter is also ruled out . The findings provided here suggest a new solution .It has formerly been proposed that several BS stars must have been formed from accretion events , with material donated by a binary companion . As the most common object , Be is especially good at absorbing optical radiation from a cool , low - mass companion , and therefore its recognition in the spectra of BS stars must be a signature of the process by which these stars were created .We suggest that such accretion events are also possibly to have been extremely polluted with light elements such as Be . This findings has tremendous impacts for our understanding of star formation , and for models of binary star evolution .If significant amounts ofBe can be synthesised in binary star systems , then these systems may indicate an important site of nucleosynthesis in the early universe , and may provide a missing link between the populations of very low - and very high - mass stars in the galaxy community at early years .",
        "rewrite_text": "The oldest known stars in the universe are referred to as Blue Straggler (BS) stars. These stars are believed to have formed after the matter in their host galaxies had expanded and sufficiently melted, allowing for the main sequence life of their constituent stars. These BS stars are iron-cored and helium-3 flaming stars that have processed almost all of the original hydrogen (H) and helium (He) into He-3. However, their most distinctive characteristic is their overwhelmingly giant size, with over 90% of the mass compared to their counterparts in the local universe. This has been a long-standing enigma, as the recent classical theory of galaxy formation suggests that such high-mass stars are relatively rare, with simulations occasionally producing only 5-10% heavy stars.\n\nRecent measurements conducted by the Hubble Space Telescope and the 2.3-meter telescope at the Australian National University have revealed a potentially innovative solution to this paradox. Utilizing the results from the Keck telescopes, we have discovered nine dark straggler stars within three ultra-lithium-deficient stars. All of them show strong absorption of beryllium (Be) in their spectra, which is not present in equivalent observations of normal galaxies and is incredibly difficult to produce in any experimental models.\n\nIt has been proposed that a traditional freeze-out method is unable to generate the observed amount of Be, while an early-accreted s-process matter is also ruled out. Our findings suggest a new solution. It has previously been suggested that several BS stars may have been formed through accretion events, with material donated by a binary companion. As the most common element, Be is particularly adept at absorbing optical radiation from a cool, low-mass companion. Therefore, its presence in the spectra of BS stars must be a signature of the process by which these stars were created.\n\nWe propose that such accretion events may have also resulted in extreme contamination with light elements such as Be. These findings have tremendous implications for our understanding of star formation and binary star evolution models. If significant amounts of Be can be synthesized in binary star systems, these systems may indicate an important site of nucleosynthesis in the early universe, potentially connecting the populations of very low and very high-mass stars in the galaxy community during early years.",
        "ori-fast-z-score": -0.9662823901213162,
        "water-fast-z-score": 6.763976730849214,
        "rewrite-fast-z-score": 2.204829171682146
    },
    {
        "original_text": "In this paper, we argue that the finitary process algebraic approach to quantum mechanics has deep limitations in the domains where it has been applied, which are predominantly quantum mechanical. We propose a fundamental revision of the approach in which instead of using finitary processes to approximate physical quantities, we view physical quantities as variables, governed by a higher order differential equation. This approach admits a probabilistic interpretation, as the solutions to the corresponding stochastic partial differential equation, at least in special cases, reproduce the predictions of quantum mechanics. We apply the revision to the domain of quantum cosmology, and propose a quantum mechanical model of the universe. We show that, in accordance with quantum cosmology, the early universe experienced a period of inflation. We speculate on the possibility of testing the model against observational data.  The paper is accompanied by an R package “finitaryprocessesoup”, available at: https://github.com/kazuhait/finitaryprocessesoup, that can be used to simulate solutions to the finitary process equations of motion.",
        "watermark_text": "In this paper , we claim that the finitary process algebraic method to quantum mechanics has deep limitations in the domains where it has been used , which are exclusively quantum mechanical . We suggest a basic modification of the approach in which rather of using finitary systems to approximate physical quantities , we view physical quantities as variables , governed by a higher order differential formula .This method admits a probabilistic interpretation , as the answers to the equivalent stochastic partial differential equation , at least in exceptional cases , reproduce the assumptions of quantum mechanics . We use the revision to the domain of quantum cosmology , and suggest a quantum mechanical model of the universe .We see that , in compliance with quantum cosmology , the early world experienced a period of inflation . We speculate on the prospect of testing the model against observational data .The paper is accompanied by an R program “ finitaryprocessesoup ” , available at : https : / / github . com / kazuhait / finitaryprocessesoup , that can be used to simulate solutions to the finitary process equations of movement .",
        "rewrite_text": "In this study, we argue that the finitary process algebraic approach in quantum mechanics encounters profound restrictions within its exclusive application domain of quantum mechanics. We propose a fundamental alteration to this approach, wherein we refrain from using finitary systems to approximate physical properties, and instead treat these properties as variables controlled by a higher-order differential formula. This revised method allows for a probabilistic interpretation, as solutions to the equivalent stochastic partial differential equations, at least in exceptional cases, align with the postulates of quantum mechanics.\n\nWe apply this revision to the field of quantum cosmology and propose a model of the universe using quantum mechanical principles. In accordance with quantum cosmology, we observe that the early universe experienced a period of inflation. We speculate on the potential for testing this model using observational data.\n\nAdditionally, this paper is accompanied by an R program named \"finitaryprocessesoup,\" which can be found at https://github.com/kazuhait/finitaryprocessesoup. This program can be utilized to simulate solutions to the finitary process equations of motion.",
        "ori-fast-z-score": 0.4472135954999579,
        "water-fast-z-score": 6.114295984380816,
        "rewrite-fast-z-score": 0.4364357804719848
    },
    {
        "original_text": "Exclusive charmless B decays into two-body hadronic final states containing a $D$ or $D_s$ meson, provide a rich source of information on the Cabibbo-Kobayashi-Maskawa (CKM) matrix elements, as well as on the dynamics of the underlying weak interaction. Over the last decade, an impressive amount of data has been obtained from B-factories and the Large Hadron Collider (LHC), and next-generation experiments at super-b-factories and the LHC upgrade are expected to collect samples in the range of 100 billion events. Over the years, several high-performance theoretical calculations for these decay rates have been performed, using a variety of approaches. Of particular interest are QCD radiative corrections, which are known to modify the decay rates by approximately 20%–25%  1–3 . The current state of the art calculation is based on the conformal expansion of the QCD amplitude  4 , combined with several modern techniques for the numerical evaluation of Feynman diagrams. In this paper we present a new theoretical calculation of the QCD corrections to the aforementioned decay rates, using the semi-analytic approach of brilliant unitarity  5 , based on a model for the strong interaction amplitude with a limited number of adjustable parameters. In our implementation, the model enters as an Ansatz for the decay amplitude as a function of the mass scale μ, and parameter-free strong interaction unitary constraints at high energy scales are used to determine the model parameters. We apply this approach to the calculation of the exclusive charmless B decay amplitudes into two-body hadronic final states, with a $D$ or $D_s$ meson, at next-to-next-to-leading order in the strong interaction coupling constant α_s, and to all orders in the heavy quark expansion. We perform a matched calculation of the hard scattering functions, as well as of the corresponding soft functions, i.e., the propagator and vertex corrections. The remaining infrared singularities are isolated in the renormalization scale and are absorbed in the parameters of the model. The strong interaction kernel is obtained from the dispersive representation of the two-particle intermediate states, and the light degrees of freedom are integrated out in the spirit of hard scattering and heavy quark effective theory.",
        "watermark_text": "Exclusive charmless B decays into two - bodies hadronic final states bearing a $ D $ or $ D _ s $ meson , provide a rich source of information on the Cabibbo - Kobayashi - Maskawa ( CKM ) matrix elements , as well as on the dynamics of the underlying weak interaction . Over the last decade , an impressive quantity of evidence has been obtained from B - manufacturing and the Large Hadron Collider ( LHC ) , and future - generation tests at super - b - manufacturing and the LHC upgrade are expected to collect samples in the range of 100 billion events .Over the years , various large - performance theoretical calculations for these decay rates have been performed , using a variety of methods . Of particular importance are QCD radiative corrections , which are known to modify the decay rates by approximately 20 % – 25 % 1 – 3 .The present state of the art calculation is based on the conformal expansion of the QCD amplitude 4 , combined with many contemporary methods for the numerical analyses of Feynman diagrams . In this paper we present a new theoretical calculation of the QCD corrections to the aforementioned decay rates , using the semi - analytic approach of brilliant unitarity 5 , using on a description for the strong coupling intensity with a small number of adjustable parameters .In our implementation , the model emerges as an Ansatz for the decay frequency as a function of the mass scale μ , and parameter - free strong interaction unitary limitations at high energy scales are using to select the model variables . We use this methodology to the determination of the exclusive charmless B degradation amplitudes into two - bodies hadronic final states , with a $ D $ or $ D _ s $ meson , at next - to - next - to - leading order in the strong coupling correlation function γ _ s , and to all orders in the heavy quark expansion .We undergo a matched calculation of the hard scattering functions , as well as of the associated soft functions , i . e . , the propagator and vertex corrections . The remaining infrared singularities are isolated in the renormalization scale and are absorbed in the parameters of the model .The strong coupling kernel is achieved from the dispersive representation of the two - particle intermediate states , and the light degrees of freedom are integrated out in the spirit of hard scattering and heavy quark effective theory .",
        "rewrite_text": "Exclusive B decays without charm into two-body hadronic final states with a $D$ or $D_s$ meson provide an extensive reservoir of information regarding the Cabibbo-Kobayashi-Maskawa (CKM) matrix elements and the underlying weak interaction dynamics. Throughout the past decade, numerous pieces of evidence have been gleaned from B-factory and the Large Hadron Collider (LHC), and future tests at advanced super B-factories and the upgraded LHC are anticipated to amass samples encompassing 100 billion events.\n\nOver the years, a range of high-performance theoretical calculations have been conducted for these decay rates, employing diverse methodologies. Notably, QCD radiative corrections, which are known to alter decay rates by approximately 20% to 25%, have played a pivotal role in studies 1-3. The current state-of-the-art calculation hinges on the conformal expansion of the QCD amplitude 4, in tandem with contemporary approaches for numerically analyzing Feynman diagrams.\n\nIn this paper, we present a novel theoretical assessment of QCD corrections to the aforementioned decay rates, utilizing the semi-analytic brilliance unitarity approach 5. This involves a description of the strong coupling intensity with a limited number of adjustable parameters. In our implementation, the model emerges as an assumption for the decay frequency as a function of the mass scale μ. We employ high-energy unitarity constraints on the strong interaction to select model variables without parameters.\n\nWe employ this methodology to determine exclusive charmless B decay amplitudes into two-body hadronic final states with a $D$ or $D_s$ meson, operating at next-to-next-to-leading order in the strong coupling correlation function γ_s and extending to all orders in the heavy quark expansion. We perform a matched calculation of hard scattering functions, along with associated soft functions, such as propagator and vertex corrections. Any remaining infrared singularities are isolated through renormalization scale and absorbed into the model parameters.\n\nThe strong coupling kernel is derived from the dispersive representation of two-particle intermediate states, with light degrees of freedom integrated out in accordance with hard scattering and heavy quark effective theory.",
        "ori-fast-z-score": 0.23354968324845687,
        "water-fast-z-score": 6.559297999321455,
        "rewrite-fast-z-score": 1.7614096918559585
    },
    {
        "original_text": "Modern cell signaling networks perform vital functions in cells by transmitting and responding to external and internal stimuli. Interacting proteins, which perform the signaling functions, are densely connected forming a complex signaling network. The signaling networks transmit and process information through a combination of multiple signaling pathways. Proteins, the signaling entities, transmit the information by changing their conformations via covalent modifications like phosphorylation. This paper presents a method for reconstructing signaling networks from proteomics data. This is a challenging problem due to the high dimensionality and complexity of the underlying signaling networks, the high sensitivity of signaling processes to variations in network topologies and parameters, and the large number of cells and proteomes typically required for statistically relevant data. The proposed method combines protein interaction networks with novel methodology for sparse representation of signaling networks in order to reconstruct signaling networks. The signaling networks are sparsely represented in a network of interacting proteins. Using the network of interacting proteins, we are able to reconstruct signaling networks while significantly reducing the number of interactions from potentially many thousands to a more manageable few. This allows us to study the signaling networks statistically and apply rigorous methods of network inference to reconstruct the networks. The signaling networks are tested against several simulation datasets and shown to accurately infer signaling networks. We also apply the method to MAPK signaling in yeast, where we reconstruct the signaling network using data from only five yeast strains. This allows us to study the effect of genetic variation on signaling processes. Finally, the signaling networks are shown to have strong predictive power across four mammals, with the mouse signaling network strongly predicting the human signaling network.",
        "watermark_text": "Modern cell signaling channels perform critical functions in cells by transmitting and responding to external and internal stimuli . Interacting proteins , which provide the signaling activities , are densely connected forming a complex signaling network .The signaling channels communicate and transfer intelligence through a combination of multiple signaling pathways . Proteins , the signaling entities , distribute the information by varying their conformations via covalent modifications like phosphorylation .This paper offers a technique for reconstructing signaling connections from proteomics data . This is a challenging problem thanks to the high dimensionality and complexity of the underlying signaling networks , the high sensitivity of signaling processes to variations in system topologies and parameters , and the huge amount of cells and proteomes typically required for statistically relevant data .The proposed approach utilizes protein interaction systems with novel methodology for sparse representation of signaling networks in order to reconstruct signaling connections . The communication connections are sparsely represented in a network of interacting molecules .Using the network of interacting molecules , we are able to reconstruct signaling connections while dramatically limiting the total of interactions from possibly large thousands to a more manageable few . This enables us to study the signaling networks statistically and use rigorous methods of system inference to reconstruct the connections .The signaling connections are tested against several simulation datasets and demonstrated to correctly infer signaling connections . We additionally apply the method to MAPK signaling in yeast , where we reconstruct the signaling network utilizing information from only five yeast varieties .This enables us to study the impact of genetic variation on signaling processes . Finally , the signaling channels are shown to have strong predictive capacity across four mammals , with the mouse signaling network highly predicting the human signaling network .",
        "rewrite_text": "In modern biology, cellular signaling channels play a pivotal role in transmitting and responding to both external and internal stimuli, facilitating critical functions within cells. These signaling activities are facilitated by a dense network of interacting proteins that form a complex communication network. The signaling channels engage in a coordinated exchange of information through multiple signaling pathways, which allows for the dissemination of intelligence.\n\nSignaling entities, primarily proteins, distribute information by altering their structural conformations through covalent modifications such as phosphorylation. This study presents a technique for reconstructing signaling connections from proteomic data, which is a challenging task due to the high dimensionality and complexity of underlying signaling networks. The sensitivity of signaling processes to variations in system topologies and parameters, as well as the vast amount of cells and proteomes required for statistically relevant data, further complicate the task.\n\nThe proposed approach utilizes novel protein interaction systems and a sparse representation methodology to reconstruct signaling connections within a network of interacting molecules. This sparse representation allows for the efficient communication connections to be represented in a network, significantly reducing the number of interactions from potentially thousands to a more manageable few. This enables statistical analysis of signaling networks and the application of rigorous system inference methods to reconstruct the connections.\n\nThe effectiveness of this approach has been tested using multiple simulation datasets and has demonstrated accurate inference of signaling connections. Additionally, this method has been applied to MAPK signaling in yeast, where we have reconstructed the signaling network using information from only five yeast varieties. This allows us to investigate the impact of genetic variation on signaling processes.\n\nFurthermore, our findings indicate that the signaling channels possess strong predictive capacity across four different mammalian species, with the mouse signaling network closely predicting the human signaling network. This provides valuable insights into the conserved mechanisms of cellular communication and offers a promising avenue for future research in understanding and manipulating cellular signaling processes.",
        "ori-fast-z-score": 0.40961596025952024,
        "water-fast-z-score": 7.675067860720625,
        "rewrite-fast-z-score": 2.204829171682146
    },
    {
        "original_text": "We present the results of a search for diffuse optical streams in a sample of 71 low-redshift ($z < 0.2$) groups and clusters of galaxies. We used deep imaging of a 1.7 square degree field around the south Galactic pole with the GRavitational lOng Survey Telescope Array (GRAvaroo), which is composed of five small optical telescopes. After careful application of various data quality filters, we identify three candidate systems with coherent, faint, linear streams of stars consistent with a stream-like configuration of the disrupted satellite galaxies. The brightest of these three candidates, with a heliocentric recessional velocity of 14,200 km/s, is consistent with other confirmed satellites of the Milky Way at lower heliocentric velocities, supporting its identification as a dark-matter-dominated stream. The other two candidates, with heliocentric recessional velocities of 24,300 km/s and 32,300 km/s, have very low satellite galaxy contamination and can each be plausibly associated with an identified accretion event in the fossil record, suggesting that the streams have physical properties similar to those of previously-observed cosmological streams. If confirmed, these systems would represent the lowest-redshift galaxies containing streams of stars arising from the disruption of satellite galaxies and would represent a record of some of the most massive accretion events in the local volume. These results suggest that wide, faint, linear features may be a common property of galaxy groups and clusters at low redshift, even in the absence of massive central galaxies, and that these systems may represent important constituents of the overall cosmological stream population.",
        "watermark_text": "We present the conclusion of a search for diffuse optical streams in a sample of 71 low - redshift ( $ z < 0 . 2 $ ) groups and clusters of stars . We utilized deep scanning of a 1 . 7 sq degree field around the south Galactic pole with the GRavitational lOng Survey Telescope Array ( GRAvaroo ) , which is composed of five tiny optical telescopes .After rigorous application of several information safety algorithms , we identify three candidate systems with coherent , faint , continuous streams of stars consistent with a stream - like configuration of the disrupted satellite galaxies . The brightest of these three finalists , with a heliocentric recessional momentum of 14 , 200 kilometers / s , is consistent with other confirmed planets of the Milky Way at lower heliocentric velocities , supporting its identity as a black - matter - dominated stream .The other two candidates , with heliocentric recessional velocities of 24 , 300 kilometers / s and 32 , 300 kilometers / s , have very low satellite galaxy exposure and can each be plausibly related with an recognized accretion event in the fossil record , showing that the rivers have physical properties similar to those of previously - observed cosmological streams . If confirmed , these systems would indicate the smallest - redshift galaxies producing streams of stars resulting from the disruption of satellite galaxies and might represent a history of some of the most large accretion events in the local volume .These data suggest that wide , faint , linear elements may be a common property of galaxy chains and clusters at low redshift , even in the absence of large central clusters , and that these systems may constitute vital components of the overall cosmological stream community .",
        "rewrite_text": "We present the findings of a search for diffuse optical streams within a sample of 71 low-redshift (z < 0.2) groups and clusters of stars. Utilizing deep scans conducted by the GRAvitational lOng Survey Telescope Array (GRAvaroo), which comprises five small optical telescopes, in a 1.7 square degree field surrounding the South Galactic Pole, we have rigorously applied multiple information security algorithms. This has led to the identification of three candidate systems with coherent, faint, and continuous streams of stars that align with a stream-like configuration disrupted by satellite galaxies.\n\nThe brightest of these three systems, with a heliocentric recession velocity of 14,200 kilometers per second, aligns with other confirmed planets in the Milky Way at lower heliocentric velocities, reinforcing its identity as a black-matter-dominated stream. The other two candidates, with heliocentric recession velocities of 24,300 kilometers per second and 32,300 kilometers per second, exhibit minimal exposure of satellite galaxies and may be plausibly linked to recognized accretion events in the fossil record. These findings suggest that these streams share similar physical properties to previously observed cosmological streams.\n\nIf these systems are confirmed, they would indicate the smallest redshift galaxies producing star streams from the disruption of satellite galaxies. This could represent a history of some of the largest accretion events in the local universe. These data suggest that wide, faint, linear elements are a common feature of galaxy chains and clusters at low redshift, even in the absence of large central clusters. Furthermore, these systems may constitute crucial components of the broader cosmological stream community.",
        "ori-fast-z-score": -1.4605934866804429,
        "water-fast-z-score": 7.302967433402215,
        "rewrite-fast-z-score": 4.0575133560034455
    },
    {
        "original_text": "Galaxy clusters have provided key astrophysical constraints over wide wavelength ranges from the radio to the X-ray. Yet despite this success, the process by which clusters assemble their mass remains a central question in modern astrophysics. Analytic theories of cluster assembly (e.g., @rs82) have long been superseded by hydrodynamical cosmological simulations (e.g., @nm04  @ds08 ), which have successfully reproduced key observables such as the cluster luminosity function and temperature function. However, the most massive clusters predicted by these simulations are somewhat under-abundant in comparison to the number of clusters in the real Universe (@k07  @l06 ; see also @s07 for a review). This “cosmic discordance problem” has inspired numerous explanations, including environmental filtering (@w84  @k84 ), cannibalism (@gm02), and massive cluster progenitors (@mm01). However, while these astrophysical theories have been successful in explaining the total amount of mass in clusters, they have not addressed the question of how individual clusters acquire their mass. Recently, we have found that the mass function of globular clusters in nearby galaxies may be directly correlated with the cumulative galaxy light in the cluster vicinity. In contrast to previous explanations that cluster mass is gradually acquired over time through mergers or environmental processes, we have shown that if most massive clusters formed instead by early dissolution of lower-mass systems, then there should be a negative correlation between the cluster mass and cluster light. Indeed, we have found that the data are consistent with the dissolution scenario, such that massive clusters are less common in the Universe and more readily dissolve. However, our results were based on small statistical samples. Here we report on a larger compilation of 22 clusters, for which we have homogeneously derived kinematics and presented a strong evidence for stellar evaporation in 10 of them. We estimate that stellar evaporation is a robust explanation for the dissolution of these clusters, and it naturally explains the mass-light correlation without any further dynamical processes. Thus, we suggest that the stellar evaporation may represent a new mechanism for regulating cluster mass, which has the potential to significantly influence the total amount of mass in clusters in cosmological simulations.",
        "watermark_text": "Galaxy clusters have provided key astrophysical limitations over broad wavelength ranges from the radio to the X - ray . Yet despite this success , the process by which clusters form their mass still a central topic in modern astrophysics .Analytic theories of cluster organization ( e . g . , @ rs82 ) have often been superseded by hydrodynamical cosmological simulations ( e . g . , @ nm04 @ ds08 ) , which have successfully reproduced key observables such as the cluster luminosity function and heat function . However , the most large clusters predicted by these simulations are somewhat under - scarce in comparison to the quantity of clusters in the real Universe ( @ k07 @ l06 ; see also @ s07 for a review ) .This “ cosmic discordance issue ” has spawned numerous explanations , including environmental filtering ( @ w84 @ k84 ) , cannibalism ( @ gm02 ) , and massive cluster progenitors ( @ mm01 ) . However , while these astrophysical models have been effective in understanding the total quantity of mass in clusters , they have not treated the question of how individual clusters evolve their mass .Recently , we have discovered that the mass function of globular galaxies in nearby galaxies must be closely correlated with the cumulative galaxy light in the cluster proximity . In comparison to previous explanations that cluster mass is progressively acquired over time through mergers or environmental mechanisms , we have shown that if most large clusters formed instead by prior dissolution of lower - mass systems , then there should be a negative correlation between the cluster mass and cluster light .Indeed , we have discovered that the statistics are compatible with the dissolution scenario , such that dense clusters are less common in the Universe and more frequently melt . However , our findings were based on small statistical specimens .Here we paper on a larger compilation of 22 clusters , for which we have homogeneously derived kinematics and presented a solid confirmation for stellar evaporation in 10 of them . We estimate that stellar evaporation is a reliable explanation for the dissolution of these clusters , and it naturally explains the mass - light correspondence without any further dynamical processes .Thus , we suppose that the stars evaporation possibly represent a new pathway for controlling cluster mass , which has the possibilities to significantly affect the total quantity of mass in clusters in cosmological simulations .",
        "rewrite_text": "Galaxy clusters have served as crucial astrophysical constraints across a wide range of wavelengths, from radio to X-ray. Despite their success, the process of how these clusters form their mass remains a central topic in modern astrophysics. Analytical theories of cluster organization have often been replaced by hydrodynamic cosmological simulations, which have successfully replicated key observables such as the cluster luminosity and heat functions. However, the predicted largest clusters from these simulations are somewhat scarce compared to the number of clusters observed in the real Universe (K07, L06; also see S07 for a review). This \"cosmic discordance issue\" has sparked numerous explanations, including environmental filtering (W84, K84), cannibalism (GM02), and massive cluster progenitors (MM01).\n\nAlthough these astrophysical models have been effective for understanding the total mass in clusters, they have not addressed how individual clusters evolve their mass. Recent discoveries have revealed a close correlation between the mass function of globular galaxies in nearby galaxies and the cumulative galaxy light in cluster proximity. In contrast to previous explanations that cluster mass is gradually acquired through mergers or environmental mechanisms over time, our research suggests that if most large clusters formed primarily through the dissolution of lower-mass systems, there should be a negative correlation between cluster mass and cluster light.\n\nIndeed, our findings indicate that the statistics are compatible with the dissolution scenario, suggesting that dense clusters are less common in the Universe and more frequently undergo dissolution. However, our previous findings were based on a small statistical sample. In this paper, we present a larger compilation of 22 clusters, from which we have derived consistent kinematics and provide a solid confirmation of stellar evaporation in 10 of them. We estimate that stellar evaporation provides a reliable explanation for the dissolution of these clusters and naturally explains the mass-light correspondence without any additional dynamical processes.\n\nTherefore, we propose that stellar evaporation may represent a new pathway for regulating cluster mass, which has the potential to significantly impact the total mass of clusters in cosmological simulations.",
        "ori-fast-z-score": -1.1607348488012053,
        "water-fast-z-score": 7.140584836498262,
        "rewrite-fast-z-score": 2.654953952106303
    },
    {
        "original_text": "This survey is aimed at optically bright X-ray sources selected from the XMM-Newton survey of hard X-ray sources  1 . It is part of the XMM-SSC survey, which is intended as an annual survey of the XMM-Newton survey area performed with the XMM-Newton soft-stateCA (SSC) instrument in the 0.2-12 keV energy band. The sample is constructed from the list of sources with count rates in the HECS1 and/or BECS1 detector more than 3 sigmas from the mean count rate, derived from the exposure map for the survey interval. Optical counterparts are sought for from the optical monitor on XMM-Newton and accurate positions are obtained from the XMM-Newton slew survey. Subsequent optical spectroscopy is performed at the Calar Alto and ESO 2.2 m telescopes. Here we present the analysis of the first part of the survey, comprising data taken in 2011. A total of 371 such sources were identified in the XMM-Newton survey area, of which 102 were located in the ESO region. Of these, 93 had secure optical IDs: 45 are known quasars, 39 are galaxies and 9 are stars. The remainder are unclassified. This leaves a total of 87 potential new quasars in the ESO region. The catalogue is available in electronic form at the CDS and Table 2 is presented here. A total of 48 sources are in the final catalogue of XMM-SSC survey, presented in a subsequent paper.",
        "watermark_text": "This study is aiming at optically bright X - ray sources chosen from the XMM - Newton survey of hard X - ray sources 1 . It is part of the XMM - SSC study , which is intended as an annual study of the XMM - Newton survey area performed with the XMM - Newton soft - stateCA ( SSC ) instrument in the 0 . 2 - 12 keV power group .The sample is composed from the list of sources with count levels in the HECS1 and / or BECS1 sensors more than 3 sigmas from the mean count rate , obtained from the exposure map for the survey period . Optical counterparts are seek for from the optical sensor on XMM - Newton and precise positions are acquired from the XMM - Newton slew survey .Subsequent optical spectroscopy is conducted at the Calar Alto and ESO 2 . 2 m telescopes . Here we present the evaluation of the first part of the survey , comprising information taken in 2011 .A total of 371 such sources were discovered in the XMM - Newton survey area , of which 102 were situated in the ESO region . Of these , 93 had safe optical IDs : 45 are known quasars , 39 are galaxies and 9 are stars .The remainder are unclassified . This leaves a total of 87 potential young quasars in the ESO region .The collection is accessible in digital form at the CDS and Table 2 is given here . A total of 48 sources are in the last catalogue of XMM - SSC sample , provided in a subsequent report .",
        "rewrite_text": "This study focuses on optically bright X-ray sources, selected from the XMM-Newton survey of hard X-ray sources. It is a part of the XMM-SSC study, which is designed as an annual investigation of the XMM-Newton survey area using the XMM-Newton soft-stateCA (SSC) instrument in the 0.2-12 keV power group. The sample is composed of sources from the list that have count levels exceeding 3 sigmas from the mean count rate in the HECS1 and/or BECS1 sensors, based on the exposure map for the survey period. Optical counterparts are being sought through the optical sensor on XMM-Newton, and precise positions are acquired from the XMM-Newton slew survey. Further optical spectroscopy is conducted using the Calar Alto and ESO 2.2m telescopes.\n\nIn this paper, we present the evaluation of the initial portion of the survey, which includes data gathered in 2011. A total of 371 such sources were discovered within the XMM-Newton survey area, with 102 of them located in the ESO region. Among these, 93 have reliable optical IDs: 45 are known quasars, 39 are galaxies, and 9 are stars. The remaining sources are unclassified. This leaves a total of 87 potential young quasars in the ESO region. The collection is available in digital form at the CDS, and Table 2 is provided here for reference. Additionally, a total of 48 sources are included in the latest catalog of the XMM-SSC sample, which is provided in a subsequent report.",
        "ori-fast-z-score": -0.3216337604513384,
        "water-fast-z-score": 5.9696200579570915,
        "rewrite-fast-z-score": 3.713116486531656
    },
    {
        "original_text": "In this note, we give some refinements of Ando s inequalities for convex and concave functions. In particular, we show that if f and g are convex functions on  a, b  with g(a) = f(a) = 0, then, for all x in (a, b),  f(x) + g(x) / 2 {}   f (a) g(b-a) + g (a) f(b-a)    1 + (b-a)^2 /(b-a)  1 + (b-2a) f(b)/g(b)   f(b) + g(b) / 2 {}   f (a) g(b-a) + g (a) f(b-a)    1 + (b-a)^2 /(b-a). We also show that if g and h are differentiable on (a, b), with g(a) = h(a) = 0, then  f(b) - f(a) / 2(b - a) f(a)   g(b) h(b) - g(a) h(a)   b f(b) - a f(a)   g(b) h(b) - g(a) h(a)   b f(b) - a f(a) .",
        "watermark_text": "In this note , we give some refinements of Ando s inequalities for convex and concave functions . In particular , we prove that if f and g are convex functions on a , b with g ( a ) = g ( a ) = 0 , then , for all x in ( a , b ) , f ( x ) + g ( x ) / 2 { } g ( a ) g ( b - a ) + g ( a ) f ( b - a ) 1 + ( b - a ) ^ 2 / ( b - a ) 1 + ( b - 2a ) f ( c ) / g ( b ) f ( c ) + g ( b ) / 2 { } g ( a ) g ( b - a ) + g ( a ) f ( b - a ) 1 + ( b - a ) ^ 2 / ( b - a ) .We also prove that if g and h are differentiable on ( a , b ) , with g ( a ) = g ( a ) = 0 , then g ( b ) - f ( a ) / 2 ( b - a ) f ( a ) g ( b ) h ( b ) - h ( a ) h ( a ) b f ( b ) - a f ( a ) g ( b ) h ( b ) - h ( a ) h ( a ) b f ( b ) - a f ( a ) .",
        "rewrite_text": "In this communication, several refinements are presented regarding Ando's inequalities, specifically for convex and concave functions. Specifically, it is demonstrated that when f and g are convex functions on the interval [a, b], where g(a) = g(b) = 0, for all x in (a, b), there is an inequality relation between f(x) and g(x) that involves multiple terms including g(a), g(b-a), f(b-a), and other related expressions.\n\nFurthermore, it is also proven that if g and h are differentiable on (a, b), with the same conditions of g(a) = g(b) = 0, then there exists another inequality relation involving the derivatives of these functions. This relation includes terms such as (g(b) - f(a))/(2(b-a)), (f(b) - a*f(a))/g(b), h(b) - h(a), and other related expressions. These refinements extend Ando's original inequalities, providing more nuanced results for convex and concave functions.",
        "ori-fast-z-score": -2.25,
        "water-fast-z-score": 0.5,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "I present a complete solution to the quantum dynamics of the Volume operator in Loop Quantum Gravity. This operator, which provides a natural size  to the universe, has eluded a complete quantum treatment. Until now. Using techniques from the full LQG analysis, I provide a detailed operator analysis of the Volume operator, showing that it has a well-defined, discrete spectrum. I then solve the full quantum dynamics and show that the spectrum is stable under small perturbations, showing that the Volume has a well-defined, classical limit. This solves the longstanding problem of quantizing the size  of the universe in LQG and opens the door for studying the quantum dynamics of other operators with a well-defined classical limit such as the Ashtekar variables. Note: This is a summary of my arXiv paper:  https://arxiv.org/abs/1806.00288 (https://arxiv.org/abs/1806.00288). The full paper has additional technical details and proofs.",
        "watermark_text": "I offer a complete solution to the quantum mechanics of the Volume operator in Loop Quantum Gravity . This operator , which offers a natural size to the universe , has eluded a complete quantum treatment .Until now . Using methods from the full LQG theory , I offer a detailed operator analysis of the Volume function , showing that it has a highly - defined , discrete spectrum .I then investigate the full quantum mechanics and suggest that the spectrum is stable under small perturbations , showing that the Volume has a well - defined , classical limit . This solves the longstanding problem of quantizing the length of the universe in LQG and starts the gate for studying the quantum mechanics of other operators with a better - defined classical limit such as the Ashtekar variables .Note : This is a summary of my arXiv paper : https : / / arxiv . org / abs / 1806 . 00288 ( https : / / arxiv . org / abs / 1806 . 00288 ) . The full paper has additional technical details and proofs .",
        "rewrite_text": "I present a comprehensive solution to the quantum mechanics of the Volume operator in Loop Quantum Gravity. This operator, which naturally quantifies the size of the universe, has remained elusive to a complete quantum treatment until now. Utilizing techniques from the complete LQG theory, I provide a detailed operator analysis of the Volume function, demonstrating its highly defined and discrete spectrum. I then delve into the full quantum mechanics and propose that the spectrum remains stable under minor perturbations, indicating a well-defined classical limit for the Volume. This resolves the long-standing issue of quantifying the length of the universe in LQG and paves the way for studying the quantum mechanics of other operators with a more clearly defined classical limit, such as Ashtekar variables.\n\nNote: This is a summary of my arXiv paper, accessible at: https://arxiv.org/abs/1806.00288. The full paper contains additional technical details and proofs.",
        "ori-fast-z-score": 1.5650160901149996,
        "water-fast-z-score": 5.2532861073211246,
        "rewrite-fast-z-score": 2.390457218668787
    },
    {
        "original_text": "The Millennium simulation is a large cosmological box which was carried out by the Virgo Consortium between 2000 and 2005. One of its outputs is a catalog of galaxy clusters, which was made public in 2008. I identify 21 red sequences in this catalog, using a technique that matches cluster galaxies’ colours to the location of the most massive member of the cluster. The scatter in the red sequence is 0.17 magnitudes, much lower than previously found, and is only statistically significant in the most dense regions of the clusters. I also show that the observed scatter is consistent with the observed colour-magnitude relation of the member galaxies, suggesting that galaxy evolution may be responsible. The colour-magnitude relation has also been seen in other observational samples, so this may be a more generally applicable explanation for the scatter on the red sequence. The Millennium simulation is a large cosmological box, carried out by the Virgo Consortium between 2000 and 2005. One of its outputs is a catalog of galaxy clusters. The cluster catalog was made public in 2008, and I identify 21 red sequences in this catalog using a technique that matches cluster galaxies’ colours to the location of the most massive member of the cluster. The colour-magnitude relation has also been seen in other observational samples, so this may be a more generally applicable explanation for the scatter on the red sequence. The Millennium simulation is a large cosmological box, carried out by the Virgo Consortium between 2000 and 2005. One of its outputs is a catalog of galaxy clusters. The cluster catalog was made public in 2008, and I identify 21 red sequences in this catalog using a technique that matches cluster galaxies’ colours to the location of the most massive member of the cluster. The colour-magnitude relation has also been seen in other observational samples, so this may be a more generally applicable explanation for the scatter on the red sequence. The Millennium simulation is a large cosmological box, carried out by the Virgo Consortium between 2000 and 2005. One of its outputs is a catalog of galaxy clusters. The cluster catalog was made public in 2008, and I identify 21 red sequences in this catalog using a technique that matches cluster galaxies’ colours to the location of the most massive member of the cluster. The colour-magnitude relation has also been seen in other observational samples, so this may be a more generally applicable explanation for the scatter on the red sequence. The Millennium simulation is a large cosmological box, carried out by the Virgo Consortium between 2000 and 2005. One of its outputs is a catalog of galaxy clusters. The cluster catalog was made public in 2008, and I identify 21 red sequences in this catalog using a technique that matches cluster galaxies’ colours to the location of the most massive member of the cluster. The colour-magnitude relation has also been seen in other observational samples, so this may be a more generally applicable explanation for the scatter on the red sequence. The Millennium simulation is a large cosmological box",
        "watermark_text": "The Millennium simulation is a large cosmological box which was carried out by the Virgo Consortium between 2000 and 2005 . One of its outputs is a list of galaxy clusters , which was making public in 2008 .I recognize 21 red sequences in this catalog , using a technique that fits cluster stars ’ colours to the location of the most large member of the cluster . The scatter in the red series is 0 . 17 magnitudes , far lower than previously found , and is only statistically substantial in the most dense areas of the clusters .I also demonstrate that the seen scatter is compatible with the seen colour - magnitude relation of the member galaxies , showing that galaxy evolution may be responsible . The colour - magnitude relation has also been seen in other observational samples , so this might be a more generally applied explanation for the scatter on the red sequence .The Millennium simulation is a large cosmological box , conducted out by the Virgo Consortium between 2000 and 2005 . One of its outputs is a list of galaxy clusters .The cluster catalog was making public in 2008 , and I recognize 21 red sequences in this catalog using a technique that sets cluster stars ’ colours to the location of the most large member of the cluster . The colour - magnitude correspondence has also been seen in other observational samples , so this might be a more generally applied explanation for the scatter on the red sequence .The Millennium simulation is a large cosmological box , conducted out by the Virgo Consortium between 2000 and 2005 . One of its outputs is a list of galaxy clusters .The cluster catalog was making public in 2008 , and I recognize 21 red sequences in this catalog using a technique that sets cluster stars ’ colours to the location of the most large member of the cluster . The colour - magnitude correspondence has also been seen in other observational samples , so this might be a more generally applied explanation for the scatter on the red sequence .The Millennium simulation is a large cosmological box , conducted out by the Virgo Consortium between 2000 and 2005 . One of its outputs is a list of galaxy clusters .The cluster catalog was making public in 2008 , and I recognize 21 red sequences in this catalog using a technique that sets cluster stars ’ colours to the location of the most large member of the cluster . The colour - magnitude correspondence has also been seen in other observational samples , so this might be a more generally applied explanation for the scatter on the red sequence .The Millennium simulation is a large cosmological box , conducted out by the Virgo Consortium between 2000 and 2005 . One of its outputs is a list of galaxy clusters .The cluster catalog was making public in 2008 , and I recognize 21 red sequences in this catalog using a technique that sets cluster stars ’ colours to the location of the most large member of the cluster . The colour - magnitude correspondence has also been seen in other observational samples , so this might be a more generally applied explanation for the scatter on the red sequence .The Millennium scenario is a large cosmological box",
        "rewrite_text": "The Millennium simulation is a vastly extensive cosmological model, executed by the Virgo Consortium between the years 2000 and 2005. Among its outputs, a list of galaxy clusters has been generated and made public in 2008. I have identified 21 red sequences within this catalog, utilizing a technique that aligns the colors of cluster stars with the position of the largest member within each cluster. The dispersion in the red sequence is notably low, with a magnitude of only 0.17, which is significantly lower than previously observed. This low dispersion is primarily evident in the most densely populated areas of the clusters. Furthermore, I have demonstrated that the observed dispersion aligns with the color-magnitude relationship observed in member galaxies, suggesting that galaxy evolution may be a contributing factor. This color-magnitude relationship has been observed in other observational samples as well, indicating a potentially widespread explanation for the scatter on the red sequence. In summary, the Millennium simulation is a comprehensive cosmological model, executed by the Virgo Consortium over a period of time, yielding a list of galaxy clusters as one of its outputs. This list was made public in 2008, and I have identified specific red sequences using an effective technique. The color-magnitude correspondence observed in this study may hold broader implications and may offer a more universal explanation for the scatter on the red sequence in galaxy clusters.",
        "ori-fast-z-score": -4.023766602875794,
        "water-fast-z-score": 4.569362074452172,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The detection of gravitational waves (GWs) and the derivation of their detailed properties have been accomplished over the last several years using a number of detector configurations. The most recent of these coincided in frequency with the estimated chirp mass of the system, determining an accurate sky location for the source. No electromagnetic (EM) counterparts were detected by any current alliance, leading to the assumption that the compact object coalesced from a white dwarf. While a neutron star (NS) – neutron star merger provides an excellent description of the observed signal, the weak observational evidence leads to the possibility that the event may have been a double white dwarf collision. In this case, the estimated parameters of the coalescing compact objects are quite different. While the physics of double white dwarf collisions is quite plausible, the uncertainties in the astrophysics and observation process result in a ~30% probability that a detection of the same signal was incorrectly attributed to NS – neutron star merger, resulting in a poorly characterized source and an incomplete observation of the gravitational-wave universe.",
        "watermark_text": "The measurement of gravitational waves ( GWs ) and the derivation of their precise characteristics have been achieved over the last several years using a number of detector configurations . The most recent of these coincided in frequency with the expected chirp mass of the system , determining an accurate sky location for the origin .No electromagnetic ( EM ) predecessors were detected by any present collaboration , leading to the assumption that the compact body coalesced from a white dwarf . While a neutron star ( NS ) – neutron star collision offers an excellent description of the observed signal , the poor observational evidence leads to the idea that the event must have been a double white dwarf encounter .In this instance , the expected variables of the coalescing compact objects are quite different . While the physics of double white dwarf collisions is fairly reasonable , the uncertainties in the astrophysics and observation process lead in a ~ 30 % chance that a detection of the same signal was incorrectly credited to NS – neutron galaxy fusion , leading in a poorly characterized source and an incomplete observation of the gravitational - wave universe .",
        "rewrite_text": "Over the past few years, several detector configurations have achieved the measurement and precise characterization of gravitational waves (GWs). The most recent measurements have aligned with the expected chirp frequency of the system, pinpointing an accurate sky location for their origin. No electromagnetic (EM) precursors have been detected by any current collaborations, leading to the assumption that the compact body merged from a white dwarf. Although a neutron star (NS) collision offers a compelling explanation for the observed signal, the limited observational evidence suggests that the event was likely a double white dwarf encounter. In this scenario, the expected variables of the merging compact objects differ significantly. While the physics behind double white dwarf collisions is reasonably understood, uncertainties in astrophysics and observation processes introduce a ~30% chance that a similar signal could have been erroneously attributed to a NS-neutron galaxy fusion, resulting in an incompletely characterized source and an incomplete observation of the gravitational-wave universe.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.330017908890261,
        "rewrite-fast-z-score": 2.208630521496931
    },
    {
        "original_text": "In the Minimal Supersymmetric Standard Model (MSSM), the vev of the Higgs doublet, which triggers the electroweak symmetry breaking, generates a potential for the neutral Higgs bosons which has a minimum at zero at tree-level. This minimum causes a naturalness problem, because the quadratically divergent corrections to the neutral Higgs mass-square parameters from the SM particles are unacceptably large. The solution to this problem requires a supersymmetry (SUSY) spectrum with superpartner particles at accessible energies. A minimal supersymmetric extension of the Standard Model (MSSM) contains two additional Higgs bosons, namely, the higgsinos and the heavy scalars, which appear in supersymmetry-breaking mass matrices. These are the supersymmetric partners of the goldstino, the photino and the zino. By proper alignment of the parameters in the SUSY-breaking mass matrices, these heavy scalars can be made relatively light, of order of the SUSY-breaking scale, while preserving the electroweak symmetry breaking. We perform a phenomenological study of the potential corrections to this alignment, and demonstrate that this reduces the mass of these scalars below 1 TeV, in a portion of the MSSM parameter space, and can be as low as 400 GeV for some parameter space points. The lightest of these new particles is a good dark matter candidate. We also study the collider phenomenology of the model, and demonstrate that this light Higgs sector can be tested at the upcoming hadron colliders, namely the CERN LHC and the Future Circular Collider, as well as the Folding@home Distributed Processing Cluster. We study the phenomenology of a minimal supersymmetric extension of the Standard Model with heavy scalars (MSSM-HS) sparticles, at the tree-level. These new particles appear in the supersymmetry-breaking mass matrices for the supersymmetric partners of the goldstino, the photino and the zino. We perform a phenomenological study of the potential corrections to the alignment, which allows us to reduce the masses of these particles below 1 TeV, in a portion of the MSSM parameter space. We also study the collider phenomenology of the model, and demonstrate that this light Higgs sector can be tested at the upcoming hadron colliders. Here we study the following points: 1. We briefly introduce the MSSM-HS model and discuss the symmetry principles which the superpartners should satisfy. 2. We describe the spectrum of the model. In particular, we discuss the Goldstino, which is the supersymmetric partner of the photino, and the zino, which is the supersymmetric partner of the z",
        "watermark_text": "In the Minimal Supersymmetric Standard Model ( MSSM ) , the vev of the Higgs doublet , which triggers the electroweak symmetry breaking , produces a potential for the neutral Higgs bosons which has a limit at zero at tree - height . This minimum causes a naturalness problem , because the quadratically divergent corrections to the neutral Higgs mass - square variables from the SM atoms are unacceptably big .The answer to this question involves a supersymmetry ( SUSY ) spectrum with superpartner particles at accessible energies . A minimal supersymmetric extension of the Standard Model ( MSSM ) contains two added Higgs bosons , namely , the higgsinos and the heavy scalars , which appear in supersymmetry - breaking mass matrices .These are the supersymmetric partners of the goldstino , the photino and the zino . By correct alignment of the variables in the SUSY - breaking mass matrices , these heavy scalars can be made fairly light , of order of the SUSY - breaking scale , while preserving the electroweak symmetry breaking .We perform a phenomenological study of the potential corrections to this alignment , and suggest that this decreases the mass of these scalars below 1 TeV , in a portion of the MSSM parameter space , and can be as low as 400 GeV for some parameter space points . The lightest of these new objects is a better dark matter candidate .We additionally study the collider phenomenology of the model , and suggest that this light Higgs region can be evaluated at the latest hadron colliders , notably the CERN LHC and the Future Circular Collider , as well as the Folding @ home Distributed Processing Cluster . We research the phenomenology of a reduced supersymmetric extension of the Standard Model with heavy scalars ( MSSM - HS ) sparticles , at the tree - level .These new objects emerge in the supersymmetry - breaking mass matrices for the supersymmetric partners of the goldstino , the photino and the zino . We undergo a phenomenological study of the potential corrections to the alignment , which allows us to reduce the masses of these ions below 1 TeV , in a portion of the MSSM parameter space .We additionally observe the collider phenomenology of the model , and suggest that this light Higgs region can be evaluated at the latest hadron colliders . Here we study the following points : 1 .We briefly introduce the MSSM - HS model and explain the symmetry concepts which the superpartners should satisfy . 2 .We define the spectrum of the model . In particular , we explain the Goldstino , which is the supersymmetric partner of the photino , and the zino , which is the supersymmetric partner of the z",
        "rewrite_text": "In the Minimal Supersymmetric Standard Model (MSSM), the vacuum expectation value (vev) of the Higgs doublet triggers the electroweak symmetry breaking, generating a potential for the neutral Higgs bosons that approaches zero at the tree level. This minimum, however, poses a naturalness issue due to unacceptably large quadratically divergent corrections to the neutral Higgs mass-square variables from Standard Model (SM) particles.\n\nThe resolution to this issue involves a supersymmetry (SUSY) spectrum with accessible energy superpartner particles. The MSSM incorporates two additional Higgs bosons—the higgsinos and heavy scalars—which are found in supersymmetry-breaking mass matrices. These are the supersymmetric partners of the goldstino, photino, and zino. By properly aligning the variables in the SUSY-breaking mass matrices, these heavy scalars can be made relatively light, on the order of the SUSY-breaking scale, while preserving electroweak symmetry breaking.\n\nWe conduct a phenomenological investigation into potential corrections to this alignment and propose that this can reduce the mass of these scalars to below 1 TeV in certain portions of the MSSM parameter space, with some points reaching as low as 400 GeV. The lightest of these new particles emerges as a stronger candidate for dark matter.\n\nFurthermore, we explore the collider phenomenology of the model and suggest that this light Higgs region can be observed at modern hadron colliders, specifically the CERN LHC and the Future Circular Collider, as well as the Folding@home Distributed Processing Cluster. We also investigate the phenomenology of a simplified supersymmetric extension of the Standard Model with heavy scalar sparticles at the tree level.\n\nThese new particles arise in the supersymmetry-breaking mass matrices for the supersymmetric partners of the goldstino, photino, and zino. We perform a phenomenological study on potential corrections to the alignment, which allows us to reduce the masses of these particles below 1 TeV in certain parameter spaces. Additionally, we observe the collider behavior of the model and propose that this light Higgs region can be observed at state-of-the-art hadron colliders.\n\nKey points we address in this study are: 1) Brief introduction to the MSSM-HS model and explanation of symmetry principles that superpartners must adhere to; 2) Definition of the model's spectrum, particularly explaining the role of the Goldstino as the supersymmetric partner of both the Photino and Zino.",
        "ori-fast-z-score": 0.26013299085723596,
        "water-fast-z-score": 6.5033247714309,
        "rewrite-fast-z-score": 1.2094157958139042
    },
    {
        "original_text": "A mixture of two competitive random number generators (RRGs) is used to produce large amounts of pseudo-random binary sequences with arbitrary period. A recently introduced variable, the partial filling factor (PFF), is used to quantify the statistical spread of these generated sequences. PFF can be interpreted as a particularly fast indicator for assessing if a sequence contains a substantial amount of zeroes. This makes it well suited for identifying rare events in large amounts of data. In this work, the PFF is used to characterize the statistical properties of the so-called rare event sequences, i.e. the generated binary sequences with a particularly low number of zeroes. These may occur, e.g., in cryptography, if a random key is used for encrypting a long message, where a repetition of zeroes in the key could lead to a termination of the encryption algorithm. It is shown that the generated PFF fluctuations follow a zero-range process with a power-law tail in the size of the fluctuations. This corresponds to a strong spatial heterogeneity of the low-zero sequences and supports a growth mechanism of these sequences from a few deterministically produced seeds. For certain mixtures, it is found that rare event sequences happen with a non-vanishing frequency, which makes them of interest for cryptography. The manuscript is a prequel of a paper about the same topic, which has been published in the Journal of Statistical Mechanics: Theory and Experiment (https://doi.org/10.1007/s10955-018-1998-x).",
        "watermark_text": "A mix of two competitive random number generators ( RRGs ) is utilized to produce large quantities of quasi - random binary strings with arbitrary duration . A recently invented variable , the partial filling factor ( PFF ) , is utilized to quantify the numerical spread of these generated sequences .PFF can be interpreted as a particularly fast indicator for evaluating if a sequence contains a substantial quantity of zeroes . This lets it well suited for finding rare events in large quantities of statistics .In this research , the PFF is utilized to characterize the statistical characteristics of the so - called rare incident strings , i . e . the produced binary strings with a particularly low number of zeroes .These may happen , e . g . , in cryptography , if a random key is applied for encrypting a long message , where a repetition of zeroes in the key may lead to a ending of the encryption algorithm . It is demonstrated that the produced PFF fluctuations undergo a zero - range mechanism with a power - law tail in the height of the fluctuations .This corresponds to a weak spatial heterogeneity of the small - zero sequences and supports a growth mechanism of these strands from a few deterministically produced seeds . For specific mixtures , it is found that strange event sequences occurs with a non - vanishing amplitude , which makes them of interest for cryptography .The manuscript is a prequel of a paper about the same topic , which has been publication in the Journal of Statistical Mechanics : Theory and Experiment ( https : / / doi . org / 10 . 1007 / s10955 - 018 - 1998 - x ) .",
        "rewrite_text": "A blend of two competitive random number generators (RRGs) is employed to generate a large quantity of quasi-random binary strings with arbitrary lengths. A recently introduced variable, the partial filling factor (PFF), is utilized to quantify the numerical dispersion of these generated sequences. PFF can be seen as a swift indicator for assessing whether a sequence contains a significant amount of zeros. This makes it well-suited for detecting rare events in vast amounts of data.\n\nIn this research, the PFF is utilized to characterize the statistical properties of what are known as 'rare incident strings,' specifically, the produced binary strings with a notably low number of zeros. These can occur, for instance, in cryptography when a random key is used to encrypt a long message. Repeating zeros in the key may result in the termination of the encryption algorithm.\n\nIt has been demonstrated that the generated PFF fluctuations follow a zero-range mechanism with a power-law tail in fluctuation height. This corresponds to a minimal spatial heterogeneity in small-zero sequences, supporting a growth mechanism where these strands emerge from a few deterministically produced seeds. For specific mixtures, it has been found that sequences of unusual events occur with a non-vanishing amplitude, making them of interest for cryptographic applications.\n\nThis manuscript is a precursor to a paper on the same topic that has been published in the Journal of Statistical Mechanics: Theory and Experiment (https://doi.org/10.1007/s10955-018-1998-x).",
        "ori-fast-z-score": -1.3867504905630728,
        "water-fast-z-score": 6.127946159842712,
        "rewrite-fast-z-score": 1.025755289064345
    },
    {
        "original_text": "A new generation of spectrometer calibration techniques is presented that are based on optical frequency combs (OFCs). OFCs are highly coherent light sources that can be generated in various nonlinear optical cavities. By measuring the frequency of the reflected light from the target object, the OFC enables nanometer scale resolution and accurate measurement of optical spectra. This article describes an OFC-based calibration technique for optical spectrum analyzers. It consists of three key modules: a wavemeter based on Michelson interferometer, a tunable OFC, and an optical spectrum analyzer. The OFC is used to measure the target spectrum with high accuracy. In contrast to the typical spectrometer calibration using a commercial wavelength meter, the wavelength of OFC is directly measured rather than derived. Therefore, this technique has the same resolution but is more accurate and useful for many important applications in optical communications, ranging, and sensing. A block diagram of this OFC-based calibration technique is presented. This technique can measure the center wavelength, the bandwidth, and the intensity of the target spectrum with high precision. This makes it suitable for many important applications such as wavelength meter for optical communication networks, spectrum sensing with high resolution, and laser frequency and stability monitoring. The experimental results demonstrate that this OFC-based technique can measure center wavelength and bandwidth with a relative error less than 0.2% and 0.1% respectively. The resolution of the intensity measurement is 0.2% FSWM (Full Width at Half Maximum). This OFC-based technique has advantages of simple optical architecture, high precision, and easy integration with optical spectrum analyzers, making it useful for many important applications.",
        "watermark_text": "A modern generation of spectrometer calibration methods is provided that are based on optical frequency combs ( OFCs ) . OFCs are extremely continuous light sources that can be formed in different nonlinear optical cavities .By measuring the frequency of the reflected radiation from the target object , the OFC provides nanometer scale resolution and precise measurement of optical spectra . This page describes an OFC - based calibration technique for optical spectrum analyzers .It consists of three key modules : a wavemeter based on Michelson interferometer , a tunable OFC , and an optical spectrum analyzer . The OFC is utilized to measure the target spectrum with high sensitivity .In comparison to the typical spectrometer calibration using a commercial frequency meter , the frequency of OFC is directly measured rather than derived . Therefore , this methodology has the same resolution but is more accurate and useful for numerous crucial uses in laser navigation , ranging , and positioning .A block diagram of this OFC - based calibration technique is displayed . This method can measure the center spectral , the bandwidth , and the strength of the target spectrum with high precision .This gives it suitable for numerous important applications such as frequency meter for laser network networks , frequency detection with high resolution , and laser frequency and stability control . The empirical results show that this OFC - based technique can measure center spectrum and bandwidth with a relative error less than 0 . 2 % and 0 . 1 % respectively .The resolution of the intensity assessment is 0 . 2 % FSWM ( Full Width at Half Maximum ) . This OFC - based technique has advantages of simple optical architecture , large accuracy , and easy compatibility with optical spectrum analyzers , making it valuable for numerous valuable applications .",
        "rewrite_text": "A modern generation of calibration methods for optical spectrum analyzers has been introduced, utilizing optical frequency combs (OFCs). OFCs are remarkably consistent light sources that can be generated within various nonlinear optical cavities. By measuring the frequency of the reflected radiation from a target object, the OFC offers nanometer-scale resolution and precise measurement of optical spectra. This page describes an OFC-based calibration technique that comprises three key modules: a wavemeter powered by a Michelson interferometer, a tunable OFC, and an optical spectrum analyzer.\n\nThe OFC is employed to measure the target spectrum with high sensitivity. In contrast to traditional spectrometer calibration using a commercial frequency meter, the frequency of the OFC is directly measured rather than inferred. Consequently, this methodology offers the same resolution but is more accurate, making it particularly useful in various critical applications such as laser navigation, ranging, and positioning.\n\nA block diagram of this OFC-based calibration technique is presented. This method can accurately measure the center spectrum, bandwidth, and intensity of the target spectrum. It is well-suited for numerous important applications such as frequency meters for laser networks, high-resolution frequency detection, and laser frequency and stability control.\n\nEmpirical results demonstrate that this OFC-based technique can measure center spectrum and bandwidth with a relative error of less than 0.2% and 0.1% respectively. The resolution for intensity assessment is 0.2% FSWM (Full Width at Half Maximum). This OFC-based technique offers advantages of a simple optical architecture, high accuracy, and easy compatibility with optical spectrum analyzers, making it invaluable for numerous critical applications.",
        "ori-fast-z-score": 0.26013299085723596,
        "water-fast-z-score": 7.833494518006403,
        "rewrite-fast-z-score": 3.756927443642463
    },
    {
        "original_text": "A pulsar traveling through the Milky Way at a known velocity,  km s-1 , ejects a fast moving pulsar wind. Due to conservation of momentum, some of this fast moving pulsar wind material may be slowed down and accumulated into a pulsar wind nebula (PWN). Recent observations of the HESS J1809-193 gamma-ray source, detected by the High Energy Stereoscopic System (H.E.S.S.), have detected a coincident X-ray PWN. We propose a possible association between the PWN and HESS J1809-193. If so, this may be the first case of a PWN discovered by gamma-ray observations. We also compare the expected spatial coincidence between the PWN and HESS J1809-193 to existing spatial simulations of the diffusion of charged particles from the pulsar in the system.",
        "watermark_text": "A pulsar traveling through the Milky Way at a known velocity , kilometre s - 1 , ejects a rapidly advancing pulsar wind . Due to conservation of velocity , some of this quick moving pulsar wind information might be calmed down and accumulated into a pulsar wind nebula ( PWN ) .Recent measurements of the HESS J1809 - 193 γ - ray source , detected by the High Energy Stereoscopic System ( H . E . S . S . ) , have discovered a coincident X - ray PWN .We suggest a possible relationship between the PWN and HESS J1809 - 193 . If so , this might be the first instance of a PWN identified by gamma - ray observations .We additionally compare the expected spatial coincidence between the PWN and HESS J1809 - 193 to existing spatial simulations of the diffusion of charged particles from the pulsar in the system .",
        "rewrite_text": "A pulsar, traveling at a known velocity of kilometers per second through the Milky Way, expels a rapidly advancing pulsar wind. Due to the conservation of velocity, some of this rapidly moving wind information may be calmed and accumulated into a pulsar wind nebula (PWN). Recent measurements of the HESS J1809-193 γ-ray source, detected by the High Energy Stereoscopic System (H.E.S.S.), have revealed a coinciding X-ray PWN. We propose a potential association between the PWN and HESS J1809-193. If verified, this could be the first instance of a PWN identified through gamma-ray observations. Furthermore, we compare the expected spatial coincidence between the PWN and HESS J1809-193 to existing simulations of the dispersion of charged particles from the pulsar in the system.",
        "ori-fast-z-score": 0.14907119849998599,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 2.897472836319489
    },
    {
        "original_text": "LANS-alpha model is a common dimensional reduction technique to simulate flows with high Reynolds numbers. A reduced order model is constructed by projecting high dimensional flow variables onto a lower dimensional subspaces which are solution sets to certain differential equations. This method works best when the reduced order model solutions are highly turbulent in nature. In this paper, we present such highly turbulent solutions for LANS-alpha model and their LES counterpart. The results are presented for flows over a cylinder at a fixed inclination angle. The solutions are computed using a spectral method which discretises the LANS-alpha and LES models in spectral space using non-uniform grids. The discrete LANS-alpha and LES models are solved using an asynchronous multi-core parallel solver. The simulations are performed for a range of CFL numbers to obtain the most turbulent solution for each CFL number. Highly turbulent solutions are projected to reduced order using a gradient projection method. The results are compared with a high resolution simulation and good agreement is observed for flow statistics.",
        "watermark_text": "LANS - alpha theory is a common dimensional reduction technique to simulate flows with high Reynolds numbers . A reduced order description is built by projecting high dimensional stream parameters onto a smaller dimensional subspaces which are solution sets to specified differential equations .This method works better when the reduced order model models are extremely chaotic in nature . In this paper , we present such extremely chaotic solutions for LANS - alpha theory and their LES counterpart .The results are presented for flows over a cylinder at a fixed inclination angle . The solutions are computed use a spectral method which discretises the LANS - alpha and LES models in spectral space use non - uniform grids .The discrete LANS - alpha and LES models are solved using an asynchronous multi - core parallel solver . The simulations are performed for a range of CFL numbers to obtain the most turbulent solve for each CFL number .Highly chaotic solutions are projected to reduced order using a gradient projection algorithm . The results are compared with a high resolution simulation and good agreement is observed for flow statistics .",
        "rewrite_text": "LANS-alpha theory represents a commonly used dimensional reduction technique to model flows with high Reynolds numbers. A simplified description is developed by projecting high-dimensional flow parameters onto smaller-dimensional subspaces, which are derived from specified differential equations. This method is especially effective when the reduced-order models exhibit strong chaotic behavior.\n\nIn this paper, we present highly chaotic solutions for LANS-alpha theory and its Large Eddy Simulation (LES) counterpart. These results are presented for flows over a cylinder at a fixed inclination angle. The solutions were computed using a spectral method that discretizes the LANS-alpha and LES models in spectral space, utilizing non-uniform grids. The discrete LANS-alpha and LES models were solved with an asynchronous multi-core parallel solver. Simulations were conducted for a range of CFL numbers to achieve the most turbulent solution for each CFL value. Highly chaotic solutions were projected to a reduced order using a gradient projection algorithm.\n\nThe results are compared with high-resolution simulations, and a good agreement is observed in flow statistics. This approach provides valuable insights into the complex dynamics of flows with high Reynolds numbers, offering a practical tool for researchers and engineers to analyze and predict turbulent flows.",
        "ori-fast-z-score": 1.0314212462587933,
        "water-fast-z-score": 4.873672965232998,
        "rewrite-fast-z-score": 2.752558187682247
    },
    {
        "original_text": "We present the first high-resolution images of the HD 100546 system taken with the High Contrast Space Telescope (HST) in the thermally dispersed spectroscopy (TDS) mode. We combine these images with previous observations in the dust continuum and strong spectral lines to model the system as a circumstellar disk surrounding a bright Herbig Ae/Be star. The inner rim of this disk is sharp and traces the edge of the empty, sub-AU region cleared out by the planet with a half-opening angle of 3.4°. We estimate the grain size in the disk surface layers to be between 1 μm and 3 μm, the temperature there to be between 23 K and 25 K, and the stellar illumination to be between 6.7% and 8.4% of the interstellar value. This is the first time that the physical properties of a disk around an Herbig Ae/Be star have been determined with such high precision, and these observations further demonstrate the power of TDS to characterize extrasolar planet systems.",
        "watermark_text": "We present the first large - resolution photos of the HD 100546 system seen with the High Contrast Space Telescope ( HST ) in the thermally scattered spectroscopy ( TDS ) mode . We merge these images with previous images in the dust continuum and strong spectral lines to model the system as a circumstellar disk surrounding a bright Herbig Ae / Be star .The inner rim of this disk is sharp and traces the boundary of the empty , sub - AU area swept out by the planet with a half - closing angle of 3 . 4° . We estimate the grain width in the disk floor walls to be between 1 μm and 3 μm , the temperature there to be between 23 K and 25 K , and the stellar lighting to be between 6 . 7 % and 8 . 4 % of the interstellar value .This is the first time that the physical properties of a disk around an Herbig Ae / Be star have been determined with such great precision , and these observations further show the power of TDS to characterize extrasolar planet systems .",
        "rewrite_text": "We present high-resolution images of the HD 100546 system captured by the High Contrast Space Telescope (HST) in the mode of thermally scattered spectroscopy (TDS). These images are combined with previous ones in the dust continuum and strong spectral lines to model the system as a circumstellar disk surrounding a bright Herbig Ae/Be star. The inner edge of the disk is sharply defined, outlining the empty sub-AU region cleared by a planet with a half-closing angle of 3.4°. We estimate that the grain size in the disk's floor walls ranges between 1 μm and 3 μm, with a temperature range of 23 K to 25 K, and that the stellar illumination accounts for between 6.7% and 8.4% of the interstellar value. This is the first time that the physical properties of a disk surrounding an Herbig Ae/Be star have been determined with such precision, further highlighting the capability of TDS to characterize extrasolar planet systems.",
        "ori-fast-z-score": 0.254000254000381,
        "water-fast-z-score": 4.48129079765136,
        "rewrite-fast-z-score": -0.12803687993289598
    },
    {
        "original_text": "Binary star systems provide a unique opportunity to study planetary systems in a much morecompact configuration than can be studied around single stars. A key challenge in detectingplanets around binary systems is their proximity to the host star, which increases thetechnological requirements for planet detection and characterization. This paperprovides an overview of observational techniques for planet detection inbinaries, with a particular focus on methods which have the potential to beapplied to circumbinary planet systems. Following an introduction to binary starsystems, the different methods of planet detection and their sensitivity to different systemparameters are described. For each method, examples from recent planet detection surveys arepresented. A preliminary analysis of the characteristics of the known circumbinaryplanets is also provided, and suggestions for future surveys which could extend thisanalyis to larger samples are offered. As new techniques for planet detection aredeveloped and demonstrated, these methods should also be tested on circumbinary planetsystems.",
        "watermark_text": "Binary star systems allow a unique opportunity to study planetary components in a far morecompact configuration than can be investigated around separate planets . A crucial problem in detectingplanets around binary systems is their distance to the host star , which increases thetechnological costs for planet detection and identification .This paperprovides an overview of observational techniques for planet detection inbinaries , with a general emphasis on techniques which have the possibilities to beapplied to circumbinary planet systems . Following an introduction to binary starsystems , the different methods of planet detection and their sensitivity to different systemparameters are explained .For each system , examples from recent planet detection surveys arepresented . A preliminary analysis of the traits of the known circumbinaryplanets is also provided , and suggestions for future surveys which could extend thisanalyis to larger samples are offered .As new strategies for planet detection aredeveloped and demonstrated , these algorithms should also be tested on circumbinary planetsystems .",
        "rewrite_text": "Binary star systems offer a unique opportunity to explore planetary components within a more compact configuration than is possible with individual planets. A pivotal challenge in detecting planets around binary systems lies in their distance from the primary star, which elevates the technological costs associated with planet detection and identification. This paper presents an overview of observational techniques for planet detection in binary systems, with a focus on techniques that have potential for application to circumbinary planet systems.\n\nAfter introducing binary star systems, various methods of planet detection are explained, along with their sensitivity to different system parameters. Examples from recent planet detection surveys are provided for each system. Additionally, a preliminary analysis of the characteristics of known circumbinary planets is provided, and suggestions for future surveys that could expand this analysis to larger sample sizes are offered.\n\nWith the development and demonstration of new planet detection strategies, it is essential that these algorithms are tested on circumbinary planet systems as well.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 3.162277660168379
    },
    {
        "original_text": "In the galactic center, Sgr A* is the supermassive black hole (SMBH) located in the center of the galaxy cluster comprising of several thousands of galaxies. Stars with high velocity dispersion (up to few hundreds of km/s) escape the galactic center due to unknown mechanism and travel along the hyperbolic orbit towards the outer region. We discovered that some of these hypervelocities stars has path perpendicular to the Sgr A*’s jet. This discovery indicates that there may be an high-density and relatively young population of stars in the region beyond 0.01 pc from Sgr A* with age less than 1 Myr. This young population may have been formed through tidal disruption of stars by Sgr A* or through collisions between stars in the SMBH’s cluster. Our result indicates that the stellar environment around Sgr A* is less relaxed than we previously thought and it can provide an effective place for the star capture and collision for creating black holes binaries. The authors are Taotao Fang, Xue-Ning Bao, and Lin Cheng, from IHEP, CAS. The article is “Hypervelocity Stars and the Environment of Sgr A*” (arXiv.org 1811.09776v1)",
        "watermark_text": "In the galactic center , Sgr A * is the supermassive black hole ( SMBH ) located in the center of the galaxy cluster comprising of several thousands of galaxies . Stars with high velocity dispersion ( up to few thousands of km / s ) escape the galactic center due to unknown mechanism and fly along the hyperbolic trajectory towards the outer portion .We observed that some of these hypervelocities stars has course perpendicular to the Sgr A * ’ s jet . This find indicates that there may be an large - density and fairly young population of stars in the region beyond 0 . 01 pc from Sgr A * with age lower than 1 Myr .This young population may have been formed through tidal disruption of stars by Sgr A * or through collisions between stars in the SMBH ’ s cluster . Our result suggests that the stellar environment around Sgr A * is fewer relaxed than we previously thought and it can provide an efficient place for the star capture and collision for producing white holes binaries .The authors are Taotao Fang , Xue - Ning Bao , and Lin Cheng , from IHEP , CAS . The essay is “ Hypervelocity Stars and the Environment of Sgr A * ” ( arXiv . org 1811 . 09776v1 )",
        "rewrite_text": "In the core of the galaxy, Sgr A* stands as a supermassive black hole (SMBH) situated at the center of a galaxy cluster encompassing thousands of galaxies. Stars with high velocity dispersions, reaching several thousand kilometers per second, are observed to escape from the galactic center via an unknown mechanism and follow hyperbolic trajectories towards the outer regions. We have observed that some of these hypervelocity stars possess trajectories perpendicular to the jet of Sgr A*. This finding suggests that there may be a high-density and relatively young population of stars in the region extending beyond 0.01 pc from Sgr A*, with ages less than 1 million years. This young population may have been formed either through the tidal disruption of stars by Sgr A* or through collisions between stars within the SMBH cluster. Our results indicate that the stellar environment surrounding Sgr A* is less stable than previously thought, offering an efficient location for star captures and collisions that may lead to the formation of white hole binaries. The authors of this essay, Taotao Fang, Xue-Ning Bao, and Lin Cheng, are from the Institute of High Energy Physics, Chinese Academy of Sciences. The essay is titled \"Hypervelocity Stars and the Environment of Sgr A*\" (arXiv.org 1811.09776v1).",
        "ori-fast-z-score": 0.7071067811865476,
        "water-fast-z-score": 3.679023140400945,
        "rewrite-fast-z-score": 1.6876318513890358
    },
    {
        "original_text": "This paper is an explanation of how Einstein s general relativity theory can be derived from first principles without the axiom of choice. It is well-known that the use of the axiom of choice is a heavy philosophical handwave in most formulations of general relativity. By first presenting the principle formulations of general relativity, we will show that the equivalence of inertial and gravitational mass, and the Poisson equation are two formulations that do not use the axiom of choice. We will use these two principles to derive Newton s law of universal gravitation and then show how to get to Einstein s equations of general relativity by using Riemannian geometry and the pseudo-Riemannian geometry of Einstein s 1921 gravitational theory. Finally, we will explain why the 1977 edition of the general relativity textbook of physicist Stephen Hawking contained a mistake which led him to conclude that the theory required the axiom of choice to be derived without it. We will show that this mistake was based on using the imperfect (implicit) Euler-Lagrange formulation of the theory instead of the perfect (explicit) Lagrange formulation. Using the perfect Lagrangian leads to the conclusion that the axiom of choice is not required.",
        "watermark_text": "This paper is an explanation of how Einstein s general relativity system can be derived from first principles without the axiom of choice . It is well - famous that the using of the axiom of chosen is a large conceptual handwave in most formulations of general relativity .By first presenting the principle formulations of general relativity , we will show that the equivalence of inertial and gravity mass , and the Poisson equation are two formulations that do not use the axiom of choice . We will use these two concepts to derive Newton s law of universal gravitation and then show how to getting to Einstein s equations of general relativity by using Riemannian topology and the pseudo - Riemannian topology of Einstein s 1921 gravitational concept .Finally , we will explain why the 1977 edition of the particular relativity textbook of theorist Stephen Hawking contained a error which leading him to observe that the principle required the axiom of choice to be derived without it . We will show that this mistake was based on using the imperfect ( implicit ) Euler - Lagrange formulation of the principle instead of the perfect ( explicit ) Lagrange formulation .Using the perfect Lagrangian leads to the conclusion that the axiom of selection is not required .",
        "rewrite_text": "This essay elucidates the process of deriving Einstein's general relativity system from fundamental principles without resorting to the axiom of choice. It is widely recognized that the utilization of the axiom of choice is a significant conceptual oversimplification in most formulations of general relativity. By initially presenting the fundamental principles of general relativity, we will demonstrate that the equivalence of inertial and gravitational mass, as well as the Poisson equation, are two formulations that are independent of the axiom of choice. We will utilize these two concepts to deduce Newton's law of universal gravitation and then illustrate the path to arriving at Einstein's equations of general relativity, employing Riemannian topology and Einstein's 1921 pseudo-Riemannian gravitational concept.\n\nFinally, we will explain why the 1977 edition of theorist Stephen Hawking's textbook on special relativity contained an error that led him to believe that the principle required the axiom of choice for its derivation. We will show that this mistake was based on the use of the imperfect (implicit) Euler-Lagrange formulation of the principle instead of the flawless (explicit) Lagrange formulation. By employing the perfect Lagrangian approach, it is concluded that the axiom of choice is not a necessary prerequisite.",
        "ori-fast-z-score": -0.11704114719613057,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "We study global polarization of QGP ( Quark-Gluon Plasma ) in non-central heavy ion collisions at high energies in perturbative quantum chromodynamics (PQCD). Gluon distribution functions in a longitudinally polarized QGP are computed to one-loop order in the infinite top-quark mass limit using Witten swarf invariant method. The induced gluon polarization is shown to be uniform throughout the longitudinally polarized QGP and is shown to survive even after quark spin anti-parallel scattering with the gluon distribution functions. The induced polarization is long-range and is shown to lead to a global polarization of QGP in non-central heavy ion collisions at high energies. This work was performed under the auspices of the US Department of Energy Nuclear Physics Group at the Panda camp (WSU), led by Prof. Debades Bandyopadhyay. PS: As a rule, one should not cite PRL papers to cite in arxiv.org submissions. This work is performed under the auspices of US Department of Energy Nuclear Physics Group at the Panda camp (WSU), hence it is inappropriate to cite PRL 128852 in this submission.",
        "watermark_text": "We study global polarization of QGP ( Quark - Gluon Plasma ) in non - central heavy atom collisions at high energies in perturbative quantum chromodynamics ( PQCD ) . Gluon distribution functions in a longitudinally polarized QGP are computed to one - loop order in the infinite top - quark mass limit using Witten swarf invariant method .The induced gluon polarization is demonstrated to be uniform throughout the longitudinally polarized QGP and is demonstrated to survive even after quark spin anti - parallel scattering with the gluon distribution functions . The induced polarization is wide - range and is demonstrated to lead to a global polarization of QGP in non - central heavy ion collisions at high energies .This project was done under the auspices of the US Department of Energy Nuclear Physics Group at the Panda camp ( WSU ) , leading by Prof . Debades Bandyopadhyay . PS : As a rule , one should not cite PRL publications to cite in arxiv . org submissions .This project is conducted under the auspices of US Department of Energy Nuclear Physics Group at the Panda camp ( WSU ) , thus it is unnecessary to cite PRL 128852 in this submission .",
        "rewrite_text": "In the realm of perturbative quantum chromodynamics (PQCD), we examine the global polarization of Quark-Gluon Plasma (QGP) during non-central heavy-atom collisions at elevated energies. Utilizing the Witten swarf invariant method, we calculate the gluon distribution functions within a longitudinally polarized QGP to one-loop order in the infinite top-quark mass limit. It is demonstrated that the induced gluon polarization remains consistent across the entire longitudinally polarized QGP and persists even after quark spin anti-parallel scattering with the gluon distribution functions. This widespread polarization is shown to result in a global polarization of QGP in high-energy non-central heavy ion collisions. This project, led by Professor Debades Bandyopadhyay at the Panda camp (WSU), is sponsored by the US Department of Energy Nuclear Physics Group. As a general rule, citing PRL publications within arxiv.org submissions is not necessary. Therefore, citing PRL 128852 in this submission is redundant, as this project is also being conducted under the auspices of the US Department of Energy Nuclear Physics Group at the Panda camp (WSU).",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 3.9391929857916765,
        "rewrite-fast-z-score": 1.6924558427507104
    },
    {
        "original_text": "A singular foliation on a smooth closed manifold is called Morse if all of its leaves are free of singularities. Smooth Morse foliations with stably ergodic Reeb flow appear as transverse structures to some symplectic structures on cotangent bundles of closed manifolds. We study the stable ergodicity of the Reeb flow. Using spectral decomposition of the differential of the basic Reeb flow, we show that the stable ergodicity depends on the symplectic structure and it is unstable when the symplectic structure has some specific pattern. We construct some specific examples of smooth Morse foliations with non-stably ergodic Reeb flow. This article is an addendum to my previous article  1 , where the same results have been established for a larger class of foliations.  1  Hanemin Yang, Volker Demuth, and Zhe Wang, A Morse singularity with non-Morse stable ergodicity, https://arxiv.org/abs/1702.05817",
        "watermark_text": "A singular foliation on a smooth closed manifold is called Morse if all of its leaves are free of singularities . Smooth Morse foliations with stably ergodic Reeb flow occur as transverse complexes to some symplectic structures on cotangent bundles of closed manifolds .We explore the stable ergodicity of the Reeb flow . Using spectral transformation of the differential of the fundamental Reeb flow , we find that the stable ergodicity varies on the symplectic structure and it is unstable when the symplectic structure has some specific pattern .We generate some specific examples of smooth Morse foliations with non - stably ergodic Reeb flow . This section is an addendum to my earlier article 1 , where the same results have been established for a greater category of foliations .1 Hanemin Yang , Volker Demuth , and Zhe Wang , A Morse singularity with non - Morse stable ergodicity , https : / / arxiv . org / abs / 1702 . 05817",
        "rewrite_text": "A Morse foliation on a smooth, closed manifold is characterized when all its leaves are singularity-free. Smooth Morse foliations, coupled with stably ergodic Reeb flows, manifest as transverse complexes in certain symplectic structures of the cotangent bundles of closed manifolds. We delve into the stable ergodicity of the Reeb flow, employing the spectral transformation of the fundamental Reeb flow's derivative. Our findings reveal that stable ergodicity fluctuates with the symplectic structure, becoming unstable when the structure follows a specific pattern. We present several examples of smooth Morse foliations with non-stably ergodic Reeb flows. This section supplements my earlier work, where similar results were established for a broader category of foliations (1). Hanemin Yang, Volker Demuth, and Zhe Wang's research on a Morse singularity with non-Morse stable ergodicity can be found at: https://arxiv.org/abs/1702.05817.",
        "ori-fast-z-score": -0.5773502691896258,
        "water-fast-z-score": 2.6539552107881486,
        "rewrite-fast-z-score": 1.2362450755382013
    },
    {
        "original_text": "We present a detailed chemical and dynamical analysis of five protostellar clusters in intermediate-mass star forming regions, around the sources I18198, I22134, I22134A, I22135, and I22134B. Through the combination of CH3OH and deuterated organic molecules (dcoms) data with dynamics from carbon monoxide (CO) observations we are able to determine the luminosities, gas masses, disk masses and binding energies of the protostars and their associated class 0/I outflows. We find that the protostellar disk mass distributions are similar to those seen in low-mass systems, with a median value of 0.015 M⊙. The four younger sources (I18198, I22134, I22134A, and I22135) are still embedded in protoclusters and present binding energies that are an order of magnitude greater than those measured for more evolved IM protostars (such as IRAS 16293-2422). In contrast, the older source I22134B is dissociated from its protocluster and its binding energy is consistent with that measured for more evolved systems. We suggest that the evolution of protostellar binding energies is not driven by the energetic effects of outflows, as has been previously proposed, but is instead intimately linked with the evolution of their natal protocluster.",
        "watermark_text": "We present a detailed chemical and dynamical analysis of five protostellar clusters in intermediate - mass star producing regions , around the sources I18198 , I22134 , I22134A , I22135 , and I22134B . Through the combination of CH3OH and deuterated organic molecules ( dcoms ) statistics with mechanics from carbon monoxide ( CO ) observations we are able to predict the luminosities , gas masses , disk masses and binding energies of the protostars and their associated class 0 / I outflows .We find that the protostellar disk mass distributions are similar to those seen in low - mass systems , with a median value of 0 . 015 [UNK] . The four younger sources ( I18198 , I22134 , I22134A , and I22135 ) are still embedded in protoclusters and present binding energies that are an order of magnitude greater than those measured for more evolved IM protostars ( such as IRAS 16293 - 2422 ) .In contrast , the early source I22134B is dissociated from its protocluster and its binding energy is compatible with that measured for more evolved systems . We suggest that the evolution of protostellar binding energies is not driven by the energetic effects of outflows , as has been previously argued , but is rather intimately tied with the evolution of their natal protocluster .",
        "rewrite_text": "We have conducted an intricate chemical and dynamic analysis of five protostellar clusters located in regions that produce intermediate-mass stars. These clusters revolve around the sources I18198, I22134, I22134A, I22135, and I22134B. By combining statistics on CH3OH and deuterated organic molecules (dcoms) with mechanical observations from carbon monoxide (CO), we have predicted the luminosities, gas masses, disk masses, and binding energies of the protostars and their associated class 0/I outflows.\n\nOur findings indicate that the distributions of protostellar disk masses are similar to those observed in low-mass systems, with a median value of approximately 0.015 units. The four younger sources (I18198, I22134, I22134A, and I22135) are still embedded within protoclusters and exhibit binding energies that are significantly greater by an order of magnitude compared to more evolved intermediate-mass protostars like IRAS 16293-2422. In contrast, the early source I22134B is separated from its protocluster and its binding energy aligns with that measured in more evolved systems. We propose that the evolution of protostellar binding energies is not primarily influenced by the energetic effects of outflows, as previously believed, but is rather closely tied to the evolution of their native protocluster.",
        "ori-fast-z-score": -1.1785113019775793,
        "water-fast-z-score": 0.7071067811865476,
        "rewrite-fast-z-score": -0.7683498199278324
    },
    {
        "original_text": "Stimulus and noise distributions for optimal information transmission via suprathreshold stochastic resonance (SR) are derived and exemplified for the archetypal nonlinear SR system, the diffusiely coupled FitzHugh-Nagumo model. Optimal information transmission is achieved for the full range of coupling constants when the information-carrying signal is a white noise, that is, it has zero mean. The optimal noise, in contrast, has a non-zero mean, and its variance vanishes as the signal amplitude increases past a critical value. The derived results provide clear design guidelines for maximizing the communication capacity of nonlinear dynamical systems through appropriate signal modulation. Introduction One of the longstanding paradigms of information transmission in nonlinear dynamical systems is stochastic resonance (SR), where a weak, random signal amplifies itself and noise-induced signal fluctuations are enhanced above the level of the signal s mean amplitude.1-3 For over three decades, this counterintuitive phenomenon has been demonstrated experimentally and corroborated with theory for various nonlinear systems, both experimental and theoretical.4-7 More recent studies have focused on information transmission via suprathreshold SR, where the signal is above a certain threshold level.8, 9, 10 The optimal signal structure for suprathreshold SR was shown to be a white noise, i.e., it has zero mean.11, 12 Here, we derive the optimal stimulus and noise distributions for information transmission via suprathreshold SR. We focus on the archetypal diffusiely coupled FitzHugh-Nagumo (FHN) model, where the signal is the spiking activity of the neuronal model, and the noise represents background synaptic and neuronal noise.13, 14 In contrast to previous studies, the optimal information-carrying signal is not a white noise, but has a non-zero mean. Moreover, the optimal noise has a vanishing variance as the signal amplitude increases past a critical value, and the optimal information transmission performance is achieved for the full range of coupling constants. The derived results provide clear design guidelines for maximizing the communication capacity of nonlinear dynamical systems through appropriate signal modulation. Materials and methods We study information transmission via suprathreshold SR in the archetypal diffusiely coupled FitzHugh-Nagumo (FHN) model,13, 14 which describes the dynamics of the membrane voltage of a neuron with the FitzHugh-Nagumo kinetics,15 coupled to two linearly independent Ornstein-Uhlenbeck (OU) processes. The Langevin equations for the model read where {S t } are the amplitudes of the OU processes, {ν t } are Gaussian white noise terms, and ε is the coupling strength. The model displays bistability, i.e., it has two stable steady states, {S*±} = {−1, 1}, which correspond to the membrane potentials of the up and down states. The stable phases are separated",
        "watermark_text": "Stimulus and noise distributions for efficient information transmission via suprathreshold stochastic resonance ( SR ) are derived and exemplified for the archetypal nonlinear SR system , the diffusiely coupled FitzHugh - Nagumo model . Optimal information transmission is achieved for the full range of coupling constants when the information - carrying frequency is a white sound , that is , it has minimal mean .The appropriate sound , in comparison , has a non - zero mean , and its variance vanishes as the signal amplitude rises past a critical value . The derived results represent strong design guidelines for maximizing the communication capabilities of nonlinear dynamical systems through optimal message modulation .Introduction One of the longstanding paradigms of information transmission in nonlinear dynamical systems is stochastic resonance ( SR ) , where a weak , random signal amplifies itself and noise - induced signal fluctuations are enhanced above the level of the signal s mean amplitude . 1 - 3 For over three decades , this counterintuitive phenomenon has been demonstrated experimentally and corroborated with theory for various nonlinear systems , both experimental and theoretical . 4 - 7 More recent studies have focused on information transmission via suprathreshold SR , where the signal is above a certain threshold level . 8 , 9 , 10 The optimal signal structure for suprathreshold SR was shown to be a white noise , i . e . , it has zero mean . 11 , 12 Here , we derive the optimal stimulus and noise distributions for information transmission via suprathreshold SR . We focus on the archetypal diffusiely coupled FitzHugh - Nagumo ( FHN ) model , where the signal is the spiking activity of the neuronal model , and the noise represents background synaptic and neuronal noise . 13 , 14 In contrast to previous studies , the optimal information - carrying signal is not a white noise , but has a non - zero mean . Moreover , the optimal noise has a vanishing variance as the signal amplitude increases past a critical value , and the optimal information transmission performance is achieved for the full range of coupling constants .The derived results show concrete design guidelines for maximizing the communication capabilities of nonlinear dynamical systems through optimal signal modulation . Materials and techniques We research information transmission via suprathreshold SR in the archetypal diffusiely coupled FitzHugh - Nagumo ( FHN ) model , 13 , 14 which explains the dynamics of the membrane voltage of a neuron with the FitzHugh - Nagumo kinetics , 15 coupled to two linearly independent Ornstein - Uhlenbeck ( OU ) pathways .The Langevin coefficients for the model read where { S t } are the amplitudes of the OU processes , { ν t } are Gaussian white noise terms , and ε is the interaction strength . The model shows bistability , i . e . , it has two stable steady states , { S * ± } = { −1 , 1 } , which refer to the membrane potentials of the up and down states .The stable periods are separated",
        "rewrite_text": "**Suprathreshold Stochastic Resonance for Efficient Information Transmission**\n\nIn the realm of nonlinear dynamical systems, we have derived and exemplified the stimulus and noise distributions that promote efficient information transmission through suprathreshold stochastic resonance (SR). We focused on the iconic nonlinear SR system, the FitzHugh-Nagumo model which is diffusely coupled. The FitzHugh-Nagumo model is representative of a neuron's membrane voltage dynamics, intertwining with two Ornstein-Uhlenbeck (OU) pathways that are linearly independent. \n\nIn our research, we found that optimal information transmission is achieved across a full range of coupling constants when the information-carrying frequency adheres to a white noise pattern—a signal with minimal average amplitude. This contrasts with the traditional notion where an appropriate signal often has a non-zero mean, but in our study, as the signal amplitude surpasses a critical threshold, its variance diminishes. \n\nThe derived results offer strong design guidelines for maximizing the communication capabilities of nonlinear dynamical systems through optimal message modulation. Over the past three decades, the counterintuitive phenomenon of stochastic resonance has been experimentally validated and corroborated by theory in various nonlinear systems. Our focus on suprathreshold SR extends this understanding, revealing that white noise—characterized by zero mean—is the optimal signal structure for information transmission in this context. \n\nIn summary, the introduction of suprathreshold SR as a paradigm for information transmission in nonlinear systems is a longstanding pursuit. Our work contributes to this understanding by providing concrete guidelines for enhancing communication capabilities through optimal signal modulation in nonlinear dynamical systems. This research paves the way for further exploration and application in real-world scenarios.",
        "ori-fast-z-score": 1.0434983894999017,
        "water-fast-z-score": 4.885699182018388,
        "rewrite-fast-z-score": 2.5776089301153053
    },
    {
        "original_text": "Evidence for a planetary companion around a nearby young star has been found using precision radial velocity measurements. This planet, designated GJ 625 b, has a minimum mass of 7.5 Earth masses, is about 2.7 times the mass of Earth, and orbits its host star at a distance of about 0.815 AU, which is less than one percent of the distance between Earth and the Sun. Such a small orbital distance implies that GJ 625 b likely has a surface temperature below which ice no longer would be a stable form of carbon and water. The relatively high eccentricity of the planet s orbit (0.29) is also intriguing, and might suggest the existence of additional undetected planets in the system. GJ 625 b is the first planet found around a star below the Sun s mass, and only the second planet (next to the Earth) known to exist outside the Solar System. In order to determine the nature of GJ 625 b and the characteristics of the system in which it orbits, additional observations are necessary. For example, a direct imaging search for further planets in the system is recommended. Such follow-up observations could be performed with current instruments on ground-based telescopes, or with the future Large UV/Optical Telescope and the James Webb Space Telescope.",
        "watermark_text": "Evidence for a planetary companion around a neighboring young star has been shown using precision radial speed measurements . This planet , designated GJ 625 b , has a minimum mass of 7 . 5 Earth masses , is about 2 . 7 times the mass of Earth , and orbits its host star at a distance of about 0 . 815 AU , which is less than one percent of the distance between Earth and the Sun .Such a small orbital length implies that GJ 625 b likely has a surface temperature below which ice no longer would be a solid form of carbon and water . The relatively high eccentricity of the planet s orbit ( 0 . 29 ) is also exciting , and might suggest the existence of several undetected planets in the system .GJ 625 b is the first planet discovered around a sun below the Sun s mass , and only the second planet ( next to the Earth ) known to appear outside the Solar System . In order to study the nature of GJ 625 b and the traits of the system in which it orbits , additional observations are necessary .For instance , a direct observation survey for further planets in the system is recommended . Such follow - up observations might be performed with current instruments on ground - based telescopes , or with the forthcoming Large UV / Optical Telescope and the James Webb Space Telescope .",
        "rewrite_text": "Using precise radial speed measurements, evidence has been presented for a planetary companion revolving around a neighboring young star. This planet, designated as GJ 625 b, has a minimum mass of 7.5 times that of Earth, equating to approximately 2.7 times the mass of our planet. It orbits its host star at a distance of approximately 0.815 AU, which is less than one percent of the distance between Earth and the Sun. This short orbital distance suggests that GJ 625 b likely has a surface temperature below which carbon and water ice would no longer exist in a solid state.\n\nFurthermore, the relatively high eccentricity of the planet's orbit, at 0.29, is intriguing and may indicate the presence of several undiscovered planets in the system. GJ 625 b is the first planet discovered orbiting a star below the Sun's mass, and only the second planet (after Earth) known to exist outside our Solar System. To further investigate the nature of GJ 625 b and the characteristics of its system, additional observations are essential. For instance, a comprehensive survey for additional planets in the system is recommended. Such follow-up observations could be conducted using current instruments on ground-based telescopes or with upcoming technology such as the Large UV/Optical Telescope and the James Webb Space Telescope.",
        "ori-fast-z-score": -1.0425720702853738,
        "water-fast-z-score": 2.5298221281347035,
        "rewrite-fast-z-score": 1.4596008983995234
    },
    {
        "original_text": "We present X-ray observations of a sample of 23 normal galaxies from the Great Observatories Origins Deep Survey field (GOODS). These X-ray observations are obtained with the Chandra X-ray Observatory and span an approximately 12-year time period from 2003 to 2014. We perform X-ray spectral fitting on the resulting data set and compute X-ray luminosity functions (XLFs) in different soft (0.5–2 keV) and hard (2–10 keV) X-ray bands. While the XLFs in the soft and hard X-ray bands show significant variability on time scales of years, there is no strong variability on time scales of days, nor any clear correlation between the soft and hard XLFs. We compare our XLF results to those from other surveys and discuss implications for the diffuse hot gas in normal galaxies.",
        "watermark_text": "We present X - ray observations of a sample of 23 normal galaxies from the Great Observatories Origins Deep Survey area ( GOODS ) . These X - ray observations are derived with the Chandra X - ray Observatory and cover an roughly 12 - year period period from 2003 to 2014 .We undergo X - ray spectral fit on the resulting data set and compute X - ray luminosity functions ( XLFs ) in different soft ( 0 . 5 – 2 keV ) and easy ( 2 – 10 keV ) X - ray bands . While the XLFs in the hard and easy X - ray bands show considerable variability on time ranges of years , there is no strong variability on time ranges of weeks , nor any obvious correlation between the hard and easy XLFs .We evaluate our XLF results to those from other surveys and consider implications for the diffuse warm gas in regular galaxies .",
        "rewrite_text": "We present an analysis of X-ray observations obtained from a sample of 23 typical galaxies within the Great Observatories Origins Deep Survey (GOODS) area. These observations were conducted using the Chandra X-ray Observatory and span a period of approximately 12 years, from 2003 to 2014. We perform X-ray spectral fitting on the resulting dataset and compute X-ray luminosity functions (XLFs) in different soft (0.5 – 2 keV) and hard (2 – 10 keV) X-ray bands. While the XLFs in both hard and soft X-ray bands exhibit significant variability over yearly timescales, there is no significant variability observed on weekly timescales, nor is there any evident correlation between the hard and soft XLFs. We compare our XLF results to those from other surveys and consider their implications for the diffuse warm gas in typical galaxies.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 4.727031582950012,
        "rewrite-fast-z-score": 0.2721655269759087
    },
    {
        "original_text": "Researchers have long known that plants respond and adapt to their environment through changes in gene expression and metabolism. However, relatively few studies have investigated how fluctuations in the light quanta signal are converted into adaptive responses at the molecular level. Here, we examined the effects of Ni(II) stress on the physiological and morphological responses of Brassica juncea seedlings, and explored how these are correlated with changes in the light quanta signal using a bioluminescence-based imaging approach. We found that Ni(II) stress suppressed hypocotyl elongation, and the level of hypocotyl elongation was correlated with the amount of light quanta received by the seedlings. The physiological parameters affected by Ni(II) stress were found to be light-dependent, while the morphological parameters were not. Our results suggest that Ni(II) stress impairs the perception of light by seedlings, and the degree of impairment is proportional to the severity of Ni(II) stress. Furthermore, we observed that the responses of the seedlings to Ni(II) stress correlated well with the previously identified “oxidative stress response” and the level of salicylic acid (SA). We speculate that Ni(II) stress suppresses hypocotyl elongation by inhibiting the conversion of light quanta to visual signals, leading to an imbalance in the levels of reactive oxygen species (ROS) and SA, which have been shown to be involved in Ni(II) stress responses. Overall, this work provides evidence of the correlation between light quanta, physiological and morphological responses in B. juncea seedlings subjected to Ni(II) stress, and identifies potential signal transduction pathways involved in these processes.",
        "watermark_text": "Researchers have often established that seeds respond and transform to their environment through alterations in gene transcription and metabolism . However , fairly few experiments have researched how fluctuations in the light quanta signal are converted into adaptive behaviors at the molecular level .Here , we examined the effects of Ni ( II ) pressure on the physiological and morphological reactions of Brassica juncea seedlings , and explored how these are correlated with variations in the light quanta signal utilizing a bioluminescence - based imaging method . We showed that Ni ( II ) pressure reduced hypocotyl elongation , and the degree of hypocotyl elongation was correlated with the quantity of light quanta received by the seedlings .The physiological variables affected by Ni ( II ) pressure were found to be light - dependent , while the morphological characteristics were not . Our results show that Ni ( II ) strain impairs the perception of light by seedlings , and the degree of impairment is proportional to the severity of Ni ( II ) stress .Furthermore , we demonstrated that the responses of the seedlings to Ni ( II ) pressure associated nicely with the previously found “ oxidative stress response ” and the degree of salicylic oxide ( SA ) . We speculate that Ni ( II ) pressure suppresses hypocotyl elongation by inhibiting the transformation of light quanta to visual cues , leading to an imbalance in the levels of reactive oxygen species ( ROS ) and SA , which have been shown to be involved in Ni ( II ) stress signals .Overall , this research provides evidence of the relationship between light quanta , developmental and morphological reactions in B . juncea seedlings subjected to Ni ( II ) pressure , and examines possible signal transduction pathways active in these mechanisms .",
        "rewrite_text": "Researchers frequently establish that seeds react and transform in response to their environment by altering gene transcription and metabolism. However, only a few experiments have delved into how fluctuations in the light quanta signal are translated into adaptive behaviors at the molecular level. In this study, we examined the physiological and morphological reactions of Brassica juncea seedlings under the pressure of Ni (II), and explored how these reactions are linked to variations in the light quanta signal using a bioluminescence-based imaging technique.\n\nOur findings indicate that Ni (II) pressure diminishes hypocotyl elongation, and this degree of elongation is closely correlated with the amount of light quanta received by the seedlings. The physiological variables affected by Ni (II) pressure were found to be dependent on light, while morphological characteristics were not. Our results suggest that Ni (II) stress impairs the seedlings' ability to perceive light, and the severity of this impairment is proportional to the intensity of the Ni (II) pressure.\n\nFurthermore, we demonstrated that the seedlings' response to Ni (II) pressure is closely associated with the previously identified \"oxidative stress response\" and the level of salicylic oxide (SA). We speculate that Ni (II) pressure suppresses hypocotyl elongation by inhibiting the conversion of light quanta into visual cues, leading to an imbalance in the levels of reactive oxygen species (ROS) and SA, which have been linked to Ni (II) stress signals.\n\nOverall, this research provides evidence for the relationship between light quanta, developmental, and morphological reactions in B. juncea seedlings under Ni (II) pressure, and examines potential signal transduction pathways involved in these mechanisms.",
        "ori-fast-z-score": -1.9802950859533488,
        "water-fast-z-score": 6.667948594698258,
        "rewrite-fast-z-score": 2.8
    },
    {
        "original_text": "Dew Point Depression Observations A 630-meter sloping path across the western United States was instrumented to measure the downwind saturation at a number of representative hillslope positions. Hill slope saturation differences correlated linearly with the measured dew point depression, with a strong negative slope indicating that as saturation decreased, dew point depression increased. The plot of dew point depression vs. hill slope saturation difference is shown as an example of a continuous piecewise linear relationship with two linear regions. The two regions have significantly different slopes, with the upper saturation difference region having a negative slope similar to the lower region. The estimated dew point depression at saturation difference of zero (i.e., at an isothermal line) was around -75.6° C. The warm wet storage product with temperature around -75° C can be generated by quickly increasing the saturation difference from near zero to around 0.5. The estimated saturation difference at -75.6° C dew point depression was around 0.5.",
        "watermark_text": "Dew Point Depression Observations A 630 - meter sloping trail across the western United States was instrumented to measure the downwind saturation at a number of representative hillslope positions . Hill elevation saturation differences compared linearly with the measured dew position disturbance , with a weak negative curve indicating that as saturation decreased , dew point depression increased .The plot of dew zone depression vs . hill elevation saturation difference is displayed as an instance of a consistent piecewise linear correlation with two linear districts . The two zones have substantially different slopes , with the upper saturation difference region having a negative curve similar to the lower region .The estimated dew point depression at saturation difference of zero ( i . e . , at an isothermal line ) was around - 75 . 6° C . The warm moist storage product with temperature around - 75° C can be generated by quickly increasing the saturation difference from near zero to around 0 . 5 . The estimated saturation difference at - 75 . 6° C dew point depression was around 0 . 5 .",
        "rewrite_text": "Observations of Dew Point Depression:\n\nA 630-meter sloping trail located throughout the western United States has been equipped with instruments to measure the saturation of the downwind at various representative hillside positions. By comparing the hill elevation saturation differences linearly with the recorded disturbances in the dew position, a weak negative trend has been identified, indicating that as saturation decreases, the dew point depression increases. The graphic representation of the dew zone depression versus hill elevation saturation difference exhibits a consistent piecewise linear correlation with two distinct linear segments. These two zones possess notably distinct slopes, with the upper saturation difference region mirroring a negative curve similar to the lower segment. The estimated dew point depression at a saturation difference of zero (equivalent to an isothermal line) was approximately -75.6°C. It is possible to generate a warm, moist storage product with a temperature around -75°C by rapidly increasing the saturation difference from nearly zero to approximately 0.5. Furthermore, the estimated saturation difference at a dew point depression of -75.6°C was approximately 0.5.",
        "ori-fast-z-score": -0.3611575592573076,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 2.264554068289191
    },
    {
        "original_text": "Recently, a mysterious parallel sorting algorithm known as dualheap selection has been discovered and demonstrated to be faster than Quicksort on both real and simulated computer systems. Dualheap selection works by swapping selected items into a well-balanced dualheaps, which apparently facilitates the sort. Dualheaps are internally heap structures in which the least-recently-swapped element is at the top. The mechanics of the sort are rather deep and mysterious, and the parallelism seems to arise from the interaction between dualheaps and cachelines. The algorithm seems to work well on modern multicore architectures, but it is not obvious how it should be adapted to work on shared-memory systems such as GPUs. Interestingly, dualheap selection is related to a stack-based sorting algorithm known as quick stack. The two algorithms share a number of similarities in their implementation and motivation. In fact, dualheap selection is faster than quick sort on some random distributions, suggesting that the unusual algorithm is more than just a trick. Dualheap selection and the quick stack were discovered independently around 2008 by separate computer scientists pursuing apparently unrelated research interests. There has been some speculation that dualheap selection may be a complex building block in some kind of a data structures continuum, similar to heaps, tries and chmaps. Dualheap selection has so far resisted all attempts at rigorous analysis, although it appears to have useful approximation properties and can be made stable with some tuning. No serious algorithm has been devised yet based on dualheap selection, although certain theoretical improvements and empirical successes have been demonstrated. Despite its success on real computer systems, dualheap selection has not been proven to be correct. In particular, it is not known whether the theory has all the right properties or whether there are exotic inputs that lead to incorrect behaviour. This article provides an overview of dualheap selection, with an eye toward explaining how and why it works. We also discuss some recent developments in understanding the algorithm and ways in which it might be adapted to work on different computer systems.",
        "watermark_text": "Recently , a mysterious parallel sorting algorithm known as dualheap selection has been detected and demonstrated to be quicker than Quicksort on both real and simulated computer machines . Dualheap selection operates by swapping selected items into a highly - balanced dualheaps , which apparently facilitates the sort .Dualheaps are internally heap structures in which the least - recently - swapped item is at the top . The physics of the sort are quite deep and mysterious , and the parallelism tends to arise from the interaction between dualheaps and cachelines .The algorithm appears to work good on contemporary multicore architectures , but it is not apparent how it should be adapted to work on shared - memory environments such as GPUs . Interestingly , dualheap selection is related to a stack - based sorting algorithm known as quick stack .The two strategies share a number of parallels in their implementation and motivation . In reality , dualheap selection is faster than rapid sort on some random distributions , showing that the unusual algorithm is more than just a trick .Dualheap selection and the quick pile were discovered separately around 2008 by separate computer scholars conducting apparently separate academic interests . There has been some controversy that dualheap selection might be a complex building block in some kind of a data structures continuum , comparable to heaps , tries and chmaps .Dualheap selection has so far avoided all efforts at precise analysis , although it appears to have useful approximation properties and can be made robust with some tuning . No serious algorithm has been formulated yet based on dualheap selection , although many theoretical upgrades and theoretical successes have been shown .Despite its success on real computer machines , dualheap selection has not been proven to be correct . In particular , it is not certain whether the principle has all the right features or whether there are exotic inputs that lead to wrong actions .This page offers an overview of dualheap selection , with an eye toward explaining how and why it works . We especially consider some latest advances in understanding the method and ways in which it could be adapted to work on various computer machines .",
        "rewrite_text": "Recently, a mysterious parallel sorting algorithm called dualheap selection has been discovered and demonstrated to be superior to Quicksort on both real and simulated computer systems. Dualheap selection operates by swapping selected items into a highly balanced dual heap structure, which facilitates the sorting process. These dualheaps are internally organized as heaps, with the item that was least recently swapped taking the top position.\n\nThe sorting mechanics behind this algorithm are deeply and intriguingly complex, with parallelism arising from the interaction between dualheaps and cache lines. It appears that the algorithm performs well on modern multicore architectures, but it remains unclear how it can be adapted to work effectively in shared-memory environments such as GPUs.\n\nInterestingly, dualheap selection is related to a stack-based sorting algorithm known as quick stack. There are numerous parallels in their implementation and motivation. In practice, dualheap selection has been found to be faster than rapid sort on certain random distributions, highlighting that this unorthodox algorithm is more than just a gimmick.\n\nDualheap selection and the quick pile were independently discovered in around 2008 by different computer scholars pursuing seemingly unrelated academic interests. There has been debate about whether dualheap selection could be a fundamental building block in a continuum of data structures, comparable to heaps, tries, and chmaps.\n\nDespite the lack of a precise analysis, dualheap selection has shown useful approximation properties and can be made robust with proper tuning. While no serious algorithm has yet been formulated based on dualheap selection, numerous theoretical advancements and successes have been demonstrated.\n\nWhile dualheap selection has shown success on real computer systems, it has not yet been proven to be mathematically correct. Specifically, it is not certain whether the principle possesses all the necessary features or if there are unusual inputs that can lead to incorrect behavior. This page provides an overview of dualheap selection, focusing on explaining how and why it works. We particularly consider the latest advancements in understanding this method and how it could be adapted to work on various computer systems.",
        "ori-fast-z-score": 0.23791547571544325,
        "water-fast-z-score": 8.644262284327771,
        "rewrite-fast-z-score": 1.8126539343499315
    },
    {
        "original_text": "In this paper, we consider reflecting Brownian motions in domains with smooth boundary. By a general criterion for continuous martingales, such a process is a semimartingale and its quadratic variation is given by a continuous local martingale reflecting its boundaries. In some situations, this boundary is only measurable. Under some additional conditions, we prove an invariance principle for such semimartingale reflecting Brownian motions. This result extends previous known ones for domains with smooth boundaries or with uniformly continuous boundaries. In particular, it applies to Markovian semimartingale reflecting Brownian motions in domains with piecewise smooth boundaries. This result is interesting on its own right and has potential applications in stochastic geometry and trap models in finance. We plan to study these applications in a separate paper. This work was initiated when the author was a postdoc at Vanderbilt University. He would like to thank his former mentor, Professor Bálint Tóth, for his encouragement. He would also like to thank Professor Mihaela Tătar for pointing out an error in an earlier version of the paper and for helpful discussions.",
        "watermark_text": "In this paper , we define reflecting Brownian motions in domains with smooth boundary . By a general criterion for smooth martingales , such a process is a semimartingale and its quadratic variation is given by a continuous local martingale reflecting its borders .In some circumstances , this boundary is only measurable . Under some additional conditions , we prove an invariance theory for such semimartingale reflecting Brownian motions .This result continues preceding known ones for regions with smooth boundaries or with uniformly continuous boundaries . In particular , it applies to Markovian semimartingale reflecting Brownian motions in domains with piecewise smooth boundaries .This result is important on its own right and has potential applications in stochastic geometry and trap solutions in finance . We intend to study these uses in a separate paper .This project was initiated when the writer was a postdoc at Vanderbilt University . He might love to thank his former colleague , Professor Bálint Tóth , for his encouragement .He might also like to thank Professor Mihaela Tătar for pointing out an mistake in an previous version of the paper and for helpful talks .",
        "rewrite_text": "In this paper, we redefine Brownian motions that reflect in domains with smoothly curved boundaries. Leveraging a universal criterion for smooth martingales, we establish that this process qualifies as a semimartingale. Its quadratic variation is characterized by a continuous local martingale that reflects its boundaries. In certain scenarios, these boundaries may only be measurable. With additional conditions, we establish an invariance theory for these semimartingale-reflecting Brownian motions.\n\nThis result extends prior knowledge in regions with either smoothly continuous or uniformly bounded boundaries. Specifically, it applies to Markovian semimartingale-reflecting Brownian motions in domains with segmented smooth boundaries. This finding holds significant importance in its own right and holds potential applications in stochastic geometry and financial trap solutions. We aim to explore these applications in a separate study. This project was initiated during the writer's postdoctoral fellowship at Vanderbilt University. He would like to express his gratitude to his former colleague, Professor Bálint Tóth, for his encouragement. Additionally, he would like to thank Professor Mihaela Tătar for pointing out an error in an earlier version of the paper and for her helpful discussions.",
        "ori-fast-z-score": 1.9629909152447274,
        "water-fast-z-score": 6.812733176437583,
        "rewrite-fast-z-score": 0.5555555555555556
    },
    {
        "original_text": "A peculiar structure was identified in the galaxy NGC 1275 (CENTAURUS A): a “hole” with an estimated mass of approximately 1042±409 solar masses, located some 55 million light-years from Earth and about 18,000 light-years in diameter. This object has been called an “Egg” by some, and a “Hen” by others, and still others have regarded it as an intriguing phenomenon in its own right, devoid of taxonomic classification. To date, no evident cause for the creation of this “ Hole within a galaxy ” has been identified. We argue that this structure is most likely the result of the recent violent merging of two galaxies, whose nuclei have fallen inside each other’s gravitational sphere of influence, forming a core-filled nucleus that resembles an Egg. A galaxy merger can also account for the the steeper than expected velocity dispersion of the stars in the core of the Egg, compared to that of the stars in the surrounding region, as predicted by numerical simulations of this kind of events. This and the Egg’s small projected mass make it very likely that it has already fallen completely through its merging progenitors, and will continue to do so in the future. In this sense, we consider the Egg to be a transient structure, of a similar nature to Galactic Hotspots, whose name it loosely resembles, although their origins are clearly unrelated.",
        "watermark_text": "A peculiar structure was described in the galaxy NGC 1275 ( CENTAURUS A ) : a “ hole ” with an estimated mass of approximately 1042±409 solar masses , located some 55 million light - years from Earth and about 18 , 000 light - years in width . This object has been called an “ Egg ” by some , and a “ Hen ” by others , and yet others have viewed it as an strange phenomenon in its own right , devoid of taxonomic classification .To date , no evident cause for the creation of this “ Hole within a galaxy ” has been determined . We argue that this composition is most likely the result of the recent violent merging of two galaxies , whose nuclei have fallen inside each other ’ s gravitational realm of influence , forming a core - packed nucleus that resembles an Egg .A galaxy amalgamation can also account for the the steeper than expected speed dispersion of the stars in the core of the Egg , compared to that of the stars in the nearby region , as predicted by numerical simulations of this form of events . This and the Egg ’ s small projected mass make it very likely that it has already fell completely through its merging progenitors , and will continue to do so in the future .In this sense , we treat the Egg to be a transient structure , of a analogous nature to Galactic Hotspots , whose name it loosely resembles , although their origins are obviously separate .",
        "rewrite_text": "A unique structure has been delineated within the galaxy NGC 1275, also known as CENTAURUS A. This structure is a \"hole\" estimated to possess a mass of approximately 1042±409 solar masses. It is situated at a distance of approximately 55 million light-years from Earth and measures roughly 18,000 light-years in width. Various observers have referred to this object as an \"Egg,\" \"Hen,\" or simply as an enigmatic phenomenon without a taxonomic classification.\n\nAs of yet, the cause for the creation of this \"hole within a galaxy\" remains undetermined. We propose that this composition is most likely the consequence of a recent and violent merging of two galaxies, where the cores of those galaxies have descended into each other's gravitational realm, forming a core-dense nucleus resembling an Egg. A galaxy amalgamation can also account for the observed higher-than-expected velocity dispersion of stars in the core of the Egg compared to nearby regions, as predicted by numerical simulations of such events.\n\nBoth this observation and the Egg's comparatively small projected mass suggest a high likelihood that it has already completely passed through its merging progenitors and will continue to do so in the future. In this context, we consider the Egg to be a transient structure, analogous to Galactic Hotspots, despite their obvious distinct origins.",
        "ori-fast-z-score": 1.2,
        "water-fast-z-score": 4.924685294770139,
        "rewrite-fast-z-score": 0.8251369970070347
    },
    {
        "original_text": "Perturbation theory is one of the most important techniques in physics. It provides a framework to describe the low energy dynamics of a system using the interactions between its various components or scales. Examples of such interactions include Coulomb force between electrons in atoms, or the gravity between planets. In quantum mechanics, the basic theory of almost all particles, perturbation theory is applied to motion of electrons in atoms. When these interactions are weak, perturbation theory provides a good approximation to the dynamics of the system. Despite its success, perturbation theory only considers the first order changes in variables as a function of another variable. For example, if I sit in a chair and ask you how I am related to the height of the chair, you would only need to consider my height and the height of the floor to accurately answer how I am related to the height of the chair. You would not need to consider my weight, or the weight of the Earth, as these are said to be higher order corrections. Parametric Perturbation Theory (PT) allows us to consider higher order changes in variables when calculating the response of a system to changes in another variable. For example, I can sit on a chair and calculate how my weight relates to the height of the chair, as well as the changes in height of the floor and the weight of the Earth. In this way, PT extends the limits of how we can consider changes in variables, or corrections, in systems and their dynamics.",
        "watermark_text": "Perturbation theory is one of the most important approaches in physics . It provides a framework to explain the small power behavior of a system using the interactions between its various components or scales .Examples of such interactions include Coulomb force between electrons in atoms , or the gravity between planets . In quantum mechanics , the fundamental theory of almost all atoms , perturbation theory is applied to moving of atoms in atoms .When these interactions are weak , perturbation theory provides a better approximation to the dynamics of the system . Despite its success , perturbation theory only considers the first order differences in variables as a function of another variable .For instance , if I sit in a chair and ask you how I am related to the height of the seat , you might only require to consider my height and the height of the floor to correctly answer how I am related to the height of the chair . You would not require to consider my weight , or the weight of the Earth , as these are said to be higher order corrections .Parametric Perturbation Theory ( PT ) enables us to consider higher order variations in variables when calculating the response of a system to changes in another variable . For instance , I can sit on a chair and estimate how my weight refers to the height of the seat , as also as the changes in height of the floor and the weight of the Earth .In this way , PT extends the boundaries of how we can consider differences in variables , or corrections , in systems and their mechanics .",
        "rewrite_text": "Perturbation theory stands as a pivotal approach in physics, offering a structure to elucidate the behavioral nuances of a system through the interactions between its components or scales. Examples of these interactions encompass the Coulomb force between electrons within atoms and the gravitational pull between planets. In quantum mechanics, the fundamental theory for most atomic phenomena, perturbation theory is applied to atomic motion. When these interactions are of a lesser magnitude, it offers a more accurate portrayal of the system's dynamics. However, it's worth noting that perturbation theory primarily focuses on first-order variations in one variable relative to another.\n\nFor instance, if I were to sit in a chair and were asked about my relationship to the seat's height, a simplified analysis might only require considering my height and the floor's elevation, disregarding factors like my weight or the weight of the Earth, which are deemed higher-order corrections.\n\nParametric Perturbation Theory (PT), on the other hand, enables us to take into account higher-order variations in variables when calculating a system's response to changes in another variable. For instance, I can sit on a chair and not only estimate how my weight relates to the seat's height but also consider alterations in floor elevation and even the weight of the Earth. In this way, PT broadens our scope of considering variable differences or corrections within systems and their mechanics.",
        "ori-fast-z-score": 2.011435198964418,
        "water-fast-z-score": 7.246315678266502,
        "rewrite-fast-z-score": 1.7650452162436565
    },
    {
        "original_text": "Flavor Changing Neutral Currents (FCNC) interactions are forbidden at tree level in the standard model (SM). However, due to higher order corrections these processes are possible at loop level. The first such process, flavor changing neutral current (FCNC) mediated by a W boson, was observed at the CERN Large Electron-Positron collider (LEP) in 1995. In this note, we investigate the flavor changing current in the framework of a supersymmetric standard model. We demonstrate that the forward-backward asymmetry in the decays of the B mesons to X s and c quarks, with X denoting a charged lepton or a neutrino, is particularly sensitive to the effects of the flavor changing current. Using this asymmetry, we derive constraints on the flavor changing current coupling, which can be translated into constraints on SUSY parameters. We further show that combining these constraints with measurements of CP violation in B meson mixing, in addition to allowing for a more precise determination of the SUSY parameters, will yield crucial information on the structure of the flavor changing current and hence help us understand the mechanism of flavor violation. Finally, we propose to use the Belle II experiment at the superKEKB to improve the experimental constraints on top FCNC interactions.",
        "watermark_text": "Flavor Changing Neutral Currents ( FCNC ) interactions are forbidden at forest level in the standard model ( SM ) . However , owing to higher order corrections these mechanisms are possible at loop level .The first such mechanism , taste changing neutral current ( FCNC ) induced by a W boson , was seen at the CERN Large Electron - Positron collider ( LEP ) in 1995 . In this note , we investigate the taste changing current in the framework of a supersymmetric standard theory .We suggest that the forward - backward asymmetry in the decays of the B mesons to X s and c quarks , with X denoting a charged lepton or a neutrino , is especially sensitive to the effects of the taste changing current . Using this asymmetry , we derive restrictions on the taste changing present interaction , which can be translated into constraints on SUSY parameters .We further show that integrating these requirements with observations of CP violation in B meson mixing , in addition to allowing for a more precise calculation of the SUSY parameters , will provide crucial data on the composition of the taste changing present and hence help us explain the process of flavor violation . Finally , we propose to use the Belle II study at the superKEKB to improve the empirical constraints on top FCNC interactions .",
        "rewrite_text": "In the standard model (SM), interactions via Flavor Changing Neutral Currents (FCNC) are prohibited at the tree level. Nevertheless, due to higher-order corrections, these mechanisms become feasible at the loop level. In 1995, the first such mechanism, a W boson-induced FCNC, was observed at the CERN Large Electron-Positron collider (LEP). In this analysis, we explore the flavor-changing current within the framework of a supersymmetric standard theory. We suggest that the forward-backward asymmetry in B meson decays to Xs and c quarks, where X represents a charged lepton or neutrino, is highly sensitive to the effects of the flavor-changing current. Utilizing this asymmetry, we derive constraints on the present flavor-changing interactions, which can be translated into constraints on SUSY parameters. Additionally, we demonstrate that combining these requirements with observations of CP violation in B meson mixing not only allows for a more precise calculation of SUSY parameters but also provides crucial insights into the composition of flavor-changing currents, thereby assisting in explaining flavor violation processes. Finally, we propose using the Belle II study at superKEKB to enhance empirical constraints on top FCNC interactions.",
        "ori-fast-z-score": -1.193117518002609,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 1.8888888888888888
    },
    {
        "original_text": "In this paper we study algebraically closed real geodesics on n-dimensional ellipsoids. We prove that algebraically closed real geodesic loops on ellipsoids are dense in the space of all free loops provided the algebraically closed field is sufficiently large and the loop is sufficiently non-constant. We also study relations of algebraically closed real geodesics on ellipsoids to hyperelliptic tangential coverings of the projective plane and to real Jacobi varieties. The presented results have many applications, in particular in the theory of integrable systems, Klein geometry and mirror symmetry. For example, we show that algebraically closed real geodesic loops on ellipsoids correspond to multiple covers of the projective plane with branch loci being real curves of minimal genus. The paper is organized as follows. In Section 1 we give some necessary definitions and present some preliminary facts. In Section 2 we study algebraically closed real geodesics on ellipsoids. In Section 3 we discuss relations between algebraically closed real geodesics on ellipsoids and tangential coverings of the projective plane. In Section 4 we present consequences of our study for real Jacobi varieties.",
        "watermark_text": "In this paper we study algebraically shut real geodesics on n - dimensional ellipsoids . We prove that algebraically shut real geodesic loops on ellipsoids are dense in the space of all free loops provided the algebraically shut field is sufficiently huge and the loop is sufficiently non - constant .We additionally investigated relations of algebraically shut real geodesics on ellipsoids to hyperelliptic tangential coverings of the projective plane and to rational Jacobi fields . The published results have many applications , in notably in the physics of integrable systems , Klein geometry and mirror symmetry .For instance , we prove that algebraically shut real geodesic loops on ellipsoids correspond to multiple covers of the projective plane with branch loci being real curves of minimal genera . The paper is organized as follows .In Section 1 we give some necessary definitions and present some preliminary facts . In Section 2 we study algebraically shut real geodesics on ellipsoids .In Section 3 we explain connections between algebraically shut real geodesics on ellipsoids and tangential coverings of the projective plane . In Section 4 we present consequences of our research for real Jacobi fields .",
        "rewrite_text": "In this study, we explore algebraically closed real geodesics on n-dimensional ellipsoids. We establish that algebraically closed real geodesic loops on ellipsoids are dense in the space of all free loops, given that the algebraically closed field is sufficiently vast and the loop is sufficiently non-constant. Additionally, we delve into the relationships between algebraically closed real geodesics on ellipsoids and hyperelliptic tangential coverings of the projective plane, as well as rational Jacobi fields.\n\nOur findings have numerous applications, particularly in the physics of integrable systems, Klein geometry, and mirror symmetry. For instance, we demonstrate that algebraically closed real geodesic loops on ellipsoids correspond to multiple covers of the projective plane with real curves of minimal genera as branch loci.\n\nThe structure of this paper is organized as follows: In Section 1, we provide necessary definitions and present some preliminary information. In Section 2, we investigate algebraically closed real geodesics on ellipsoids. Section 3 explains the connections between these geodesics and the tangential coverings of the projective plane. Finally, in Section 4, we present the implications of our research for real Jacobi fields.",
        "ori-fast-z-score": 1.2375966910186262,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 2.8867513459481287
    },
    {
        "original_text": "Solid methane (CH4), also known as coalbed methane, is a form of natural gas that exists in underground layers within decommissioned coal mines. Methane gas is rarely found in its natural state. It usually occurs as single molecules or small clusters called clathrates. Solid methane occurs in two solid phases, the alpha (IV) and beta (IV) phases. The alpha phase, which is more stable at atmospheric pressure, is stable below 77K and at greater depths, while the beta phase is stable above 150K and at shallower depths. Because solid methane occurs in two solid phases with different structures, it presents an opportunity to investigate the relationship between crystal structure and dynamics. Neutron scattering can be used to investigate the crystal structure via the structure factor. The dynamic structure factor contains information about the intramolecular dynamics. We have developed a quantum molecular dynamics model for the dynamics of solid methane that includes key physics necessary for accurate modeling of the dynamic structure factor. We validate the model by calculating the dynamic structure factor of the alpha phase of solid methane and comparing it to previous inelastic neutron scattering (INS) measurements. Our model could be used to further investigate the relationship between crystal structure and dynamics, and predict how dynamics vary between solid methane phases and how they vary with changing pressure or temperature.",
        "watermark_text": "Solid methane ( CH4 ) , sometimes called as coalbed methane , is a form of natural gas that forms in underground layers within decommissioned mining mines . Methane gas is rarely found in its natural state .It often occurs as single particles or small rows termed clathrates . Solid methane happens in two solid phases , the alpha ( IV ) and beta ( IV ) phases .The alpha phase , which is more stable at atmospheric pressure , is stable below 77K and at greater depths , while the beta phase is stable above 150K and at shallower depths . Because liquid methane happens in two solid phases with varying structures , it presents an option to examine the relationship between crystal composition and dynamics .Neutron diffusion can be used to examine the crystal composition via the structure factor . The dynamic shape factor holds data about the intramolecular dynamics .We have developed a quantum molecular mechanics model for the dynamics of liquid methane that contains key physics necessary for precise simulation of the dynamic structure factor . We validate the model by assessing the dynamic structure factor of the alpha phase of liquid methane and linking it to previous inelastic neutron scattering ( INS ) observations .Our model could be used to further examine the relationship between crystal shape and dynamics , and predict how dynamics vary between solid methane phases and how they vary with varying pressure or temperature .",
        "rewrite_text": "Solid methane, also known as coalbed methane, is a type of natural gas that forms within underground layers of abandoned mining sites. Methane gas is infrequently found in its pure state and often occurs as individual particles or clathrates, which are small clusters of methane particles. Solid methane exists in two distinct solid phases: the alpha (IV) phase and the beta (IV) phase. The alpha phase is more stable at atmospheric pressure and below 77K at greater depths, while the beta phase is stable above 150K and at shallower depths.\n\nDue to the existence of two solid phases with varying structures, liquid methane provides an opportunity to investigate the relationship between crystal composition and dynamics. Neutron diffusion can be employed to analyze the crystal composition through the structure factor, while the dynamic shape factor holds information about the intramolecular dynamics. We have developed a quantum molecular mechanics model for liquid methane dynamics that incorporates essential physics for precise simulation of the dynamic structure factor.\n\nWe validate our model by assessing the dynamic structure factor of the alpha phase of liquid methane and linking it to previous inelastic neutron scattering (INS) observations. This model can be used to further explore the relationship between crystal shape and dynamics, as well as to predict how dynamics vary between solid methane phases and how they change with varying pressure or temperature.",
        "ori-fast-z-score": -0.09407208683835973,
        "water-fast-z-score": 6.114685644493382,
        "rewrite-fast-z-score": 3.0237157840738176
    },
    {
        "original_text": "Using computer simulations, we investigate the structure of toroidal magnetic fields in neutron stars with type II superconductor cores. We find that, for a wide range of physically plausible parameters, these fields exist as stable configurations, rather than being plagued by magnetic instability. We also find that such fields modify the structure of neutron stars, and have impacts on their cooling behavior. Our results provide new insights into the nature of some magnetars, as well as offering new mechanisms for radio emission from these objects. Neutron stars are among the most exotic objects in the universe. They are leftover objects from the death of massive stars. They are as heavy as the moon but much more dense, being averages of several kilometers of matter packed into a volume of a few kilometers. Their interior, the so-called  nuclear sphere,  is rich in neutrons, almost half of the volume being comprised of neutrons held together with the strong force. This neutron matter is the most dense matter in the universe, similar in density to a large tumor on a human being. Surrounding the nuclear sphere is a  deep crust,  essentially a thin layer of partially degenerate neutron drip, and finally an outer layer of hydrogen and helium. At the core of many neutron stars is a material core, supported by the strong force, which is most likely made of one of several forces that give rise to superconductivity, i.e, Fermi, Coorbital, U(1) or colour superconductivity. Type II superconductivity occurs in metals at low temperatures. In a magnetic field, the superconducting electrons line up into pancakes, forming fluxoids. The resulting field is maximal at the mid-plane of the star and tapers off near the surface. If the total flux is an integer multiple of the quantum of flux, ωφ0, the field is called  toroidal.  Depending on the ratio of the magnetic energy to the condensation energy, this field is either stable or unstable. In this work, we study a subset of the stable solutions, those with integral flux quanta. We find that the ratio of the toroidal field energy to the rest of the star goes as the fifth power of the field strength, so there is little free energy in the field at typical neutron star fields. The field lines then naturally bundle in flux bundles of integral quanta. For a strong enough magnetic field, there is a discontinuity in the poloidal magnetic field, and the field is called force-free. This situation arises when there is a magnetic field-induced modification to the equation of state for the superconductor, so the magnetic and condensation energies are not in balance. A toroidal field modifies the structure of a neutron star in two primary ways. The magnetic pressure is enhanced in the core, increasing the pressure even as the density drops. This affects the dynamics of the inner crust, and the magnetic fields generated in this process. Second, if the field is strong enough, it",
        "watermark_text": "Using machine simulations , we investigate the composition of toroidal magnetic fields in neutron galaxies with type II superconductor cores . We see that , for a broad variety of physically plausible parameters , these fields occur as permanent structures , rather than being plagued by magnetic instability .We additionally find that such fields altered the composition of neutron stars , and have affects on their thermal interactions . Our results bring fresh insights into the nature of some magnetars , as also as providing new mechanisms for radio emission from these objects .Neutron stars are among the most exotic objects in the universe . They are leftover objects from the death of large stars .They are as heavy as the lunar but much more thick , being averages of several kilometers of matter filled into a volume of a few km . Their interior , the so - called nuclear sphere , is abundant in neutrons , nearly part of the volume being comprised of neutrons held apart with the strong force .This neutron matter is the most dense matter in the universe , comparable in volume to a large tumor on a human being . Surrounding the atomic sphere is a thick crust , mostly a thin layer of partially degenerate neutron drip , and eventually an outer coating of carbon and helium .At the core of several neutron galaxies is a metal core , backed by the strong pull , which is most likely made of one of several forces that give rise to superconductivity , i . e , Fermi , Coorbital , U ( 1 ) or colour superconductivity . Type II superconductivity occurs in metals at low temperatures .In a magnetic field , the superconducting nuclei line up into pancakes , forming fluxoids . The resulting field is maximal at the mid - plane of the star and tapers off near the surface .If the total flux is an integer multiple of the quantum of flux , ωφ0 , the field is dubbed toroidal . Depending on the proportion of the magnetic energy to the condensation power , this field is either stable or unstable .In this research , we study a subset of the stable solutions , those with integral flux quanta . We see that the proportion of the toroidal field energy to the remainder of the star goes as the fifth power of the field intensity , so there is nothing free energy in the field at typical neutron star fields .The field lines then naturally bundle in flux bundles of integral quanta . For a powerful sufficient magnetic force , there is a discontinuity in the poloidal magnetic force , and the field is dubbed force - free .This condition occurs when there is a magnetic field - caused correction to the equation of state for the superconductor , so the magnetic and condensation energies are not in balance . A toroidal field modifies the formation of a neutron star in two principal ways .The magnetic pressure is enhanced in the core , increasing the pressure especially as the density decreases . This impacts the dynamics of the inner crust , and the magnetic fields generated in this process .Second, if the field is strong enough, it",
        "rewrite_text": "Using machine simulations, we investigate the composition of toroidal magnetic fields in neutron galaxies with type II superconductor cores in the universe. We observe that these fields persist as permanent structures across a wide range of physically plausible parameters, rather than being plagued by magnetic instability. Furthermore, these fields are found to alter the composition of neutron stars and have an impact on their thermal interactions.\n\nOur findings offer fresh insights into the nature of certain magnetars and provide new mechanisms for radio emission from these objects. Neutron stars are among the most remarkable objects in the universe, leftovers from the demise of large stars. They are as heavy as the moon but much more compact, with an average of several kilometers of matter contained within a volume of just a few cubic kilometers.\n\nInside these stars, the so-called nuclear sphere is abundant in neutrons, with nearly a significant portion of the volume made up of neutrons held apart by the strong force. This neutron matter is the most dense substance in the universe, comparable in volume to a large tumor on a human body. Surrounding the nuclear sphere is a thick crust primarily composed of a thin layer of partially degenerate neutron drip, eventually leading to an outer coating of carbon and helium.\n\nAt the core of many neutron galaxies is a metal core supported by a strong pull that is likely made up of one of several forces driving superconductivity—namely, Fermi, Coorbital, U(1), or color superconductivity. Type II superconductivity occurs in metals at low temperatures. In a magnetic field, superconducting nuclei align into pancakes, forming fluxoids. This results in a magnetic field that is strongest at the mid-plane of the star and tapers off near the surface. If the total flux is an integer multiple of the quantum flux ωφ0, the field is referred to as toroidal. Depending on the ratio of magnetic energy to condensation power, this field can be either stable or unstable.\n\nIn this research, we focus on a subset of stable solutions—those with integral flux quanta. We observe that the proportion of toroidal field energy to the rest of the star follows the fifth power of field intensity, indicating no free energy in typical neutron star fields. The field lines naturally bundle into flux bundles of integral quanta. When a sufficiently powerful magnetic force is present, there is a discontinuity in the poloidal magnetic force, leading to a condition known as force-free. This occurs when there is a magnetic field-induced correction to the equation of state for the superconductor, disrupting the balance between magnetic and condensation energies.\n\nA toroidal field alters the formation of a neutron star in two primary ways. Firstly, the magnetic pressure is enhanced in the core, particularly as density decreases, affecting both the dynamics of the inner crust and the magnetic fields generated within it. Secondly, if the field is strong enough, it can significantly alter the structure and dynamics of the star itself.",
        "ori-fast-z-score": -0.6,
        "water-fast-z-score": 7.884206636416519,
        "rewrite-fast-z-score": 3.1650262722997775
    },
    {
        "original_text": "Turbulent flows are complicated, chaotic systems that can not be well approximated by finite-dimensional models. The evaluation of the complete Reynolds-Averaged-Navier-Stokes (RANS) equations—typically involving a solution procedure with a computational cost proportional to the eighth power of the Reynolds number—is therefore often avoided and an alternative approach is followed, based on a closure to model the unresolved scales. One possible approach consists in introducing some modeled correlation between the fluctuations of the Reynolds-Averaged-Variable (RAV) and the instantaneous Reynolds stress. Among all the possible correlation tensors, the one ensuring the maximum Reynolds-Averaged-Navier-Stokes (RANS)equation-consistent (RA Conserving) is often chosen, see e.g.  1  for a review. This closure, however, does not take into account the fact that the fluctuations of the Reynolds stress are not fully known, and therefore the Reynolds-Averaged-Navier-Stokes (RANS) equations are not closed. In this work, we propose to model the fluctuations of the Reynolds-Averaged-Variable (RAV) via an Exponential-based solution of the Reynolds-Averaged-Variable equations, where the exponential is computed using the (analytical) matrix exponential of the (unclosed) Reynolds-Averaged-Navier-Stokes (RANS) equations. This new closure, that we call Matrix Exponential-Based Closure for the Turbulent Subgrid-Scale Stress (MEC-TS), ensures the RA Conserving property and greatly reduces the cost of the evaluation of the Reynolds-Averaged-Navier-Stokes equations at each time step, at the same time accounting for the unresolved contributions in the Turbulent Subgrid-Scale (TSGS) model.",
        "watermark_text": "Turbulent flows are intricate , chaotic structures that can not be well approximated by finite - dimensional models . The evaluation of the complete Reynolds - Averaged - Navier - Stokes ( RANS ) equations — typically using a solution operation with a computational cost proportional to the seventh power of the Reynolds number — is consequently sometimes omitted and an additional method is taken , based on a closure to model the unresolved scales .One available approach consists in establishing some modeled interaction between the fluctuations of the Reynolds - Averaged - Variable ( RAV ) and the instantaneous Reynolds stress . Among all the possible correlation tensors , the one ensuring the maximum Reynolds - Averaged - Navier - Stokes ( RANS ) equation - consistent ( RA Conserving ) is often choice , see e . g .1 for a review . This closure , however , does not take into consideration the fact that the fluctuations of the Reynolds stress are not entirely known , and therefore the Reynolds - Averaged - Navier - Stokes ( RANS ) equations are not closed .In this research , we undertake to model the fluctuations of the Reynolds - Averaged - Variable ( RAV ) via an Exponential - based solve of the Reynolds - Averaged - Variable equations , where the exponential is computed using the ( analytical ) matrix exponential of the ( unclosed ) Reynolds - Averaged - Navier - Stokes ( RANS ) equations . This new closure , that we call Matrix Exponential - Based Closure for the Turbulent Subgrid - Scale Stress ( MEC - TS ) , guarantees the RA Conserving property and greatly decreases the cost of the evaluation of the Reynolds - Averaged - Navier - Stokes equations at each time step , at the same time accounting for the unresolved contributions in the Turbulent Subgrid - Scale ( TSGS ) model .",
        "rewrite_text": "Turbulent flows are highly intricate and chaotic structures that are not accurately represented by finite-dimensional models. Assessing the complete Reynolds-Averaged-Navier-Stokes (RANS) equations, which often involves a computational process with a cost proportional to the seventh power of the Reynolds number, is sometimes omitted. Instead, researchers adopt an alternative approach based on a closure model to account for unresolved scales.\n\nOne approach involves establishing a modeled interaction between fluctuations of the Reynolds-Averaged-Variable (RAV) and the instantaneous Reynolds stress. Among various correlation tensors, one that ensures maximum consistency with the RANS equations (RA Conserving) is frequently selected, as reviewed in source 1. However, this closure does not consider the fact that the fluctuations of Reynolds stress are not fully known, leading to an incomplete closure of the Reynolds-Averaged-Navier-Stokes (RANS) equations.\n\nIn this research, we propose a novel method to model the fluctuations of the Reynolds-Averaged-Variable (RAV) using an exponential-based solution to the RAV equations. This exponential is computed using the (analytical) matrix exponential of the unclosed RANS equations. This innovative closure, which we term Matrix Exponential-Based Closure for the Turbulent Subgrid-Scale Stress (MEC-TS), guarantees the RA Conserving property while significantly reducing the cost of evaluating the RANS equations at each time step. Furthermore, it takes into account the unresolved contributions within the Turbulent Subgrid-Scale (TSGS) model.",
        "ori-fast-z-score": 0.6255432421712244,
        "water-fast-z-score": 4.587317109255645,
        "rewrite-fast-z-score": 0.6255432421712244
    },
    {
        "original_text": "Late-type red supergiants (LTRSGs) are evolved stars that are the immediate precursors of massive Wolf–Rayet stars and cannot be observed in the local group of galaxies. This includes the Milky Way (MW), the Local Group (LG) and the satellite galaxies of the MW, such as the Large and Small Magellanic Clouds (LMC and SMC, respectively). They are of particular interest due to their importance as both the upper end of the initial mass function (IMF) and as possible progenitors for compact binaries, such as gamma-ray bursts (GRBs) and potential gravitational wave sources. Despite the proximity of the LMC and SMC, and their extensive observational studies, only a small number of LTRSGs have been identified. This may be due to LTRSGs being evolved stars and so relatively faint, or due to them not existing in these galaxies, for example due to their high luminosity requirements. To test this hypothesis, I searched for LTRSGs in two satellite galaxies of the MW, the Large and Small Magellanic Clouds. I used deep, high-resolution imaging from the 6.5 meter Magellan telescopes and the wide-field Pan-STARRS1 (PS1) Survey to identify candidates and extract photometric information. Two bona fide LTRSGs were identified in the SMC, but not in the LMC. This suggests that although LTRSGs do not appear in the LG, they may exist in small satellite galaxies such as the LMC.",
        "watermark_text": "Late - class red supergiants ( LTRSGs ) are evolved stars that are the immediate precursors of large Wolf – Rayet stars and cannot be viewed in the local group of galaxies . This encompasses the Milky Way ( MW ) , the Local Group ( LG ) and the satellite galaxies of the MW , such as the Large and Small Magellanic Clouds ( LMC and SMC , respectively ) .They are of especially interest due to their importance as both the upper end of the initial mass function ( IMF ) and as possible progenitors for compact binaries , such as gamma - ray flare ( GRBs ) and possible gravitational wave targets . Despite the vicinity of the LMC and SMC , and their extensive observational research , only a small number of LTRSGs have been described .This might be due to LTRSGs being evolved stars and so relatively faint , or due to them not existing in these galaxies , for example due to their high luminosity demands . To test this hypothesis , I searched for LTRSGs in two satellite galaxies of the MW , the Large and Small Magellanic Clouds .I used deep , large - resolution imaging from the 6 . 5 meter Magellan telescopes and the broad - field Pan - STARRS1 ( PS1 ) Survey to identify proposals and extract photometric information . Two bona fide LTRSGs were discovered in the SMC , but not in the LMC .This implies that although LTRSGs do not appear in the LG , they may arise in small satellite galaxies such as the LMC .",
        "rewrite_text": "Late-class red supergiants (LTRSGs) are evolved stars that serve as direct predecessors of large Wolf-Rayet stars, yet they cannot be observed within the local group of galaxies including the Milky Way (MW), the Local Group (LG), and its satellite galaxies like the Large and Small Magellanic Clouds (LMC and SMC). These stars hold particular significance due to their position at the upper end of the initial mass function (IMF) and their potential to become progenitors of compact binaries, such as gamma-ray flares (GRBs) and potential gravitational wave targets. Despite the proximity and extensive research on the LMC and SMC, only a limited number of LTRSGs have been documented. This could be attributed to their faintness as evolved stars or the possibility that they do not exist in these galaxies, possibly due to their high luminosity requirements.\n\nTo test this, I conducted a search for LTRSGs in two satellite galaxies of the MW: the Large and Small Magellanic Clouds. I utilized high-resolution imaging from the 6.5-meter Magellan telescopes and the broad-field Pan-STARRS1 (PS1) Survey to identify candidates and extract photometric data. My findings revealed two bona fide LTRSGs in the SMC, but none in the LMC. This suggests that while LTRSGs may not be visible in the LG, they may exist in smaller satellite galaxies like the LMC.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.529108136578382,
        "rewrite-fast-z-score": -0.22086305214969307
    },
    {
        "original_text": "The Ξ(1690), the Ξ(1690)Π, and the Ξ(1695) masses were recently measured for the first time. These states were conjectured to be the lowest-mass members of an unconventional antidecuplet of light triply strange baryons. We determine their strong coupling and α′, J, and parity. We find α′ = (0.34 ± 0.05 ± 0.09) GeV2, J = 1, α′ = (0.44 ± 0.07 ± 0.09) GeV2, J = 1, and Ξ parity = −1 for the Ξ(1690), Ξ(1690)Π, and Ξ(1695) states, respectively. The Ξ(1690), the Ξ(1690)Π, and the Ξ(1695) masses were recently measured for the first time. These states were conjectured to be the lowest-mass members of an unconventional antidecuplet of light triply strange baryons. We determine their strong coupling and α′, J, and parity. We find α′ = (0.34 ± 0.05 ± 0.09) GeV2, J = 1, α′ = (0.44 ± 0.07 ± 0.09) GeV2, J = 1, and Ξ parity = −1 for the Ξ(1690), Ξ(1690)Π, and Ξ(1695) states, respectively. The lowest-mass antidecuplet states were originally proposed in 2000 by members of the HERMES Collaboration 1 . Their remarkable特 admissions were the simultaneous discovery of three states with the same strangeness, and of one state with negative Ξ parity. The last two features were conjectured to be unique to the Ξ antidecuplet, distinguishing it from other non-charmed particle multiboson systems. In 2003, the states were called Ξ antidecuplet states 2 . Early measurements suggested a relatively low mass for the Ξ antidecuplet, however, later results from the SAPHIR 3 , COMPASS 4 , and CERES 5  experiments all indicated a significantly higher mass. The Ξ antidecuplet was finally conjectured in 2009, when it was predicted to exist within the framework of the diquark-quark model 6 . An antidecuplet consists of Ξ, Ξ*bbr, Ξ*bbar, Ξ(1690) (odd), Ξ*(1695) (even) and Ξ*(1690)Π (odd). Two more states were predicted to belong to the antidecuplet but have not been observed yet: the Ξ(1672) and Ξ(1675). A naming system using Roman letters and an odd or even symbol was introduced",
        "watermark_text": "The [UNK] ( 1690 ) , the [UNK] ( 1690 ) Π , and the [UNK] ( 1695 ) masses were recently measured for the first time . These states were conjectured to be the smallest - mass members of an unconventional antidecuplet of light triply strange baryons .We determine their powerful coupling and α ′ , J , and parity . We get α ′ = ( 0 . 34 ± 0 . 05 ± 0 . 09 ) GeV2 , J = 1 , α ′ = ( 0 . 44 ± 0 . 07 ± 0 . 09 ) GeV2 , J = 1 , and [UNK] parity = −1 for the [UNK] ( 1690 ) , [UNK] ( 1690 ) Π , and [UNK] ( 1695 ) states , respectively .The [UNK] ( 1690 ) , the [UNK] ( 1690 ) Π , and the [UNK] ( 1695 ) masses were recently measured for the first time . These states were conjectured to be the smallest - mass members of an unconventional antidecuplet of light triply strange baryons .We determine their powerful coupling and α ′ , J , and parity . We get α ′ = ( 0 . 34 ± 0 . 05 ± 0 . 09 ) GeV2 , J = 1 , α ′ = ( 0 . 44 ± 0 . 07 ± 0 . 09 ) GeV2 , J = 1 , and [UNK] parity = −1 for the [UNK] ( 1690 ) , [UNK] ( 1690 ) Π , and [UNK] ( 1695 ) states , respectively .The lowest - mass antidecuplet states were originally proposed in 2000 by members of the HERMES Collaboration 1 . Their remarkable [UNK] admissions were the simultaneous discovery of three states with the same strangeness , and of one state with negative [UNK] parity .The last two features were conjectured to be unique to the [UNK] antidecuplet , distinguishing it from other non - charmed particle multiboson systems . In 2003 , the states were called [UNK] antidecuplet states 2 .Early measurements suggested a relatively low mass for the [UNK] antidecuplet , however , later results from the SAPHIR 3 , COMPASS 4 , and CERES 5 experiments all indicated a significantly higher mass . The [UNK] antidecuplet was finally conjectured in 2009 , when it was predicted to exist within the framework of the diquark - quark model 6 .An antidecuplet consists of [UNK] , [UNK] * bbr , [UNK] * bbar , [UNK] ( 1690 ) ( odd ) , [UNK] * ( 1695 ) ( even ) and [UNK] * ( 1690 ) Π ( odd ) . Two more states were predicted to belong to the antidecuplet but have not been observed yet : the [UNK] ( 1672 ) and [UNK] ( 1675 ) .A naming system using Roman characters and an odd or even symbol was introduced",
        "rewrite_text": "Recently, the masses of the [UNK] (1690), [UNK] (1690) Π, and [UNK] (1695) states have been measured for the first time. These states were believed to be the smallest-mass members of an unconventional antidecuplet of light triply strange baryons. We determined their strong coupling, α', J, and parity values. Specifically, for the [UNK] (1690), [UNK] (1690) Π, and [UNK] (1695) states, we obtained α' values of (0.34 ± 0.05 ± 0.09) GeV2 with J=1, (0.44 ± 0.07 ± 0.09) GeV2 with J=1, and parity of -1 respectively.\n\nIn 2000, the lowest-mass antidecuplet states were originally proposed by members of the HERMES Collaboration. They simultaneously discovered three states with the same strangeness and one state with negative parity, which were thought to be unique characteristics of the [UNK] antidecuplet, distinguishing it from other non-charmed particle multiboson systems. In 2003, these states were labeled as [UNK] antidecuplet states. Early measurements suggested a relatively low mass for the [UNK] antidecuplet; however, later results from the SAPHIR, COMPASS, and CERES experiments all indicated a significantly higher mass.\n\nFinally, in 2009, the existence of the [UNK] antidecuplet was conjectured within the framework of the diquark-quark model. An antidecuplet comprises of [UNK], [UNK]*bbr, [UNK]*bbar, [UNK] (1690) (odd), [UNK]* (1695) (even), and [UNK]* (1690) Π (odd). Although two more states, [UNK] (1672) and [UNK] (1675), were predicted to belong to the antidecuplet, they have not yet been observed. To aid in identification and classification, a naming system utilizing Roman characters and an odd or even symbol was introduced.",
        "ori-fast-z-score": 0.647150228929434,
        "water-fast-z-score": 2.209379082955976,
        "rewrite-fast-z-score": 0.9649012813540153
    },
    {
        "original_text": "Weak lensing surveys allow us to study structures on large scales, making them powerful tools to test cosmological models and search for subtle signatures of gravity. To extract the largest possible information from these surveys, it is desirable to break the degeneracy between the lensing potential and the cosmological parameters, which introduces cross-correlation between different redshift bins. Although ground-based telescopes provide the required precision for these cross-correlations, space is much more advantageous due to the high accessible volume and the reduced cloud coverage. Here, we present the first results of a lensing tomographic analysis with weak lensing reconstructions in 8 redshift bins from space with Wide Field Camera 3 (WFC3) onboard the Hubble Space Telescope (HST). We combine this with accurate, simultaneous optical and near-infrared photometry from the VisibleISS. We find very good agreement with current ground-based results with similar quality datasets, with errors of ~0.35 on the galaxy bias parameter and ~0.1 on the mass ratio spectrum. This technique has the potential to achieve 0.1% uncertainty on cosmological parameters from cosmic shear with WFC3 and the VisibleISS, as well as break parameter degeneracies with other imaging surveys with similar telescopes.",
        "watermark_text": "Weak lensing surveys allow us to study structures on huge scales , making them potent tools to test cosmological predictions and hunt for subtle signatures of gravitational . To obtain the greatest available information from these measurements , it is desirable to cut the degeneracy between the lensing potential and the cosmological factors , which allows cross - correlation between various redshift bins .Although ground - based telescopes supply the necessary precision for these cross - correlations , space is much more advantageous owing to the high accessible volume and the reduced cloud coverage . Here , we present the first findings of a lensing tomographic analysis with weak lensing reconstructions in 8 redshift bins from space with Wide Field Camera 3 ( WFC3 ) onboard the Hubble Space Telescope ( HST ) .We integrate this with correct , simultaneous optical and far - infrared photometry from the VisibleISS . We get very high agreement with current ground - based findings with similar quality datasets , with mistakes of ~ 0 . 35 on the galaxy bias function and ~ 0 . 1 on the mass ratio spectrum .This method has the ability to achieve 0 . 1 % uncertainty on cosmological values from cosmic shear with WFC3 and the VisibleISS , as also as break parameter degeneracies with other imaging observations with similar telescopes .",
        "rewrite_text": "Weak lensing surveys enable us to investigate vast structural patterns, making them highly effective instruments for testing cosmological predictions and seeking out subtle indications of gravity's influence. To acquire the most comprehensive information from these measurements, it is crucial to reduce the interplay between the lensing potential and cosmological factors, facilitating cross-correlation among distinct redshift bins. While ground-based telescopes provide the necessary accuracy for these cross-correlations, space-based observations possess significant advantages due to the extensive volume accessible and reduced cloud coverage.\n\nWe present the initial outcomes of a lensing tomographic analysis utilizing weak lensing reconstructions from eight redshift bins obtained via the Wide Field Camera 3 (WFC3) on the Hubble Space Telescope (HST). This is integrated with accurate, concurrent optical and far-infrared photometry data from the VisibleISS. Our findings align well with current ground-based studies using datasets of similar quality, with discrepancies of approximately 0.35 in the galaxy bias function and 0.1 in the mass ratio spectrum. This method has the potential to achieve a 0.1% uncertainty in cosmological values from cosmic shear using WFC3 and VisibleISS, as well as breaking parameter degeneracies with other imaging observations utilizing similar telescopes.",
        "ori-fast-z-score": -0.8728715609439696,
        "water-fast-z-score": 5.8175057794535885,
        "rewrite-fast-z-score": 1.6865480854231356
    },
    {
        "original_text": "Valley-dependent optoelectronics based on inversion symmetry breaking has received great attentions recently due to its potential to achieve extremely low power operation, especially for electronics in the next-generation strained silicon CMOS platforms. However, direct realization of inversion-symmetry-broken (IBS) materials in electronic devices is still challenging because of the complex relationship among valley-dependent optical selection rules, momentum-conserving carrier scattering, and electronic transportation. To this end, we propose a general platform for valley-dependent optoelectronics based on the IBS effect in oxide-interfaced semiconductors, where band structure engineering and reduced dielectric screening combine to relax these complicated constraints. Using relevant first-principles calculations, we demonstrate the gate-tunable valley splitting of up to 30 meV in n-type strained silicon interface oxide, and strong optical absorption onset near the silicon band edge in the photon energy range of 1.1-1.3 eV. Our work provides a promising route to harness the IBS effect for valley-dependent optoelectronics in existing CMOS technology nodes, paving the way for low-power and/or low-footprint Internet of Things applications.",
        "watermark_text": "Valley - dependent optoelectronics based on inversion symmetry breaking has gained great attentions recently thanks to its potential to achieve highly low power operation , particularly for electronics in the second - generation strained silicon CMOS systems . However , direct formulation of inversion - symmetry - breaking ( IBS ) elements in digital systems is also challenging because of the complex relationship among valley - dependent optical selection rules , momentum - conserving carrier scattering , and electronic travel .To this end , we pursue a general platform for valley - dependent optoelectronics based on the IBS effect in oxide - interfaced semiconductors , where band structure engineering and increased dielectric screening combine to lift these difficult limitations . Using relevant first - principles calculations , we present the gate - tunable valley splitting of up to 30 meV in h - class strained silicon interface oxide , and strong visual emission onset near the silicon band boundary in the photon energy range of 1 . 1 - 1 . 3 eV .Our project offers a viable path to harness the IBS effect for valley - dependent optoelectronics in existing CMOS technology clusters , paving the way for low - energy and / or low - footprint Internet of Things solutions .",
        "rewrite_text": "Recently, the utilization of valley-dependent optoelectronics based on inversion symmetry breaking has garnered significant attention due to its potential for achieving highly efficient and low-power operations, particularly in second-generation strained silicon CMOS systems. However, implementing inversion-symmetry-breaking (IBS) elements directly in digital systems poses a challenge due to the intricate interplay between valley-dependent optical selection rules, momentum-conserving carrier scattering, and electronic motion.\n\nTo address this, we aim to establish a general platform for valley-dependent optoelectronics based on the IBS effect in oxide-interfaced semiconductors. Here, a combination of band structure engineering and enhanced dielectric screening work together to overcome these obstacles. Utilizing cutting-edge first-principles calculations, we demonstrate gate-tunable valley splitting of up to 30 meV in h-class strained silicon interface oxides, along with a pronounced onset of optical emission close to the silicon band boundary within the photon energy range of 1.1 to 1.3 eV.\n\nOur research project presents a feasible approach to harnessing the IBS effect for valley-dependent optoelectronics in existing CMOS technology clusters. This paves the way for the development of low-energy and/or low-footprint Internet of Things solutions, offering significant potential for future technological advancements.",
        "ori-fast-z-score": -0.8955334711889903,
        "water-fast-z-score": 5.4,
        "rewrite-fast-z-score": 2.0976176963403033
    },
    {
        "original_text": "In this paper we establish two analogous correspondences between link concordance, homology cobordism, and Hirzebruch-type defects. The first correspondence associates to an oriented link L in S3 the Witt-valued concordance obstruction of the iterated p-cover of S3 given by the link exterior, where p is any odd prime. The second correspondence associates to an integral homology 3-sphere X the Witt-valued self-bordism class of the Hirzebruch surface HT opened along its zero section. We then apply these correspondences to show that a given link L in S3 determines the Witt-valued homology cobordism class of some integral homology 3-sphere X, and that a given homology 3-sphere X determines the Witt-valued link concordance genus of some link L in S3. We conclude by observing that in many examples these correspondences are injective, generalizing previous results by Hendricks, Hom, and Kanda.",
        "watermark_text": "In this paper we determine two analogous correspondences between link concordance , homology cobordism , and Hirzebruch - class defects . The first connection associates to an oriented connection L in S3 the Witt - valued concordance obstruction of the iterated p - cover of S3 given by the link exterior , where p is any odd prime .The second relationship associates to an integral homology 3 - sphere X the Witt - valued self - bordism class of the Hirzebruch sphere HT opened along its zero section . We then use these correspondences to see that a given link L in S3 determines the Witt - valued homology cobordism category of some integral homology 3 - sphere X , and that a given homology 3 - sphere X determines the Witt - valued link concordance genus of some connection L in S3 .We follow by observing that in many instance these correspondences are injective , generalizing previous findings by Hendricks , Hom , and Kanda .",
        "rewrite_text": "In this study, we establish two analogous relationships between link concordance, homology cobordism, and Hirzebruch-class defects. The initial association connects an oriented connection L within S3 with the Witt-valued concordance obstruction of the iterated p-cover of S3, which is determined by the link exterior when p represents any odd prime number. The second relationship links an integral homology 3-sphere, X, with the Witt-valued self-bordism class of the Hirzebruch sphere HT when it is opened along its zero section.\n\nUtilizing these correlations, we can determine that a specific link, L, within S3 determines the Witt-valued homology cobordism category of a certain integral homology 3-sphere, X. Conversely, a given homology 3-sphere, X, determines the Witt-valued link concordance genus of a connection within S3. Furthermore, we observe that in numerous scenarios, these correlations are injective, broadening previous findings made by Hendricks, Hom, and Kanda.",
        "ori-fast-z-score": 1.2135597524338357,
        "water-fast-z-score": 4.9890789822279915,
        "rewrite-fast-z-score": 2.781517949836592
    },
    {
        "original_text": "The paper examines the characteristics of the nonstationary complex networks, which includes but not limited to the time-varying network and unsynchronizable networks. The nonstationarity originates from the rapid changing of the network structure and/or nodes’ dynamics. Since the network is nonstationary, the conventional network measures cannot fully reflect the network characteristics. To this end, the theory of recurrence plot is adopted to characterizing the network dynamics, and the method to quantify the nonstationarity is proposed accordingly. The analysis on the generated model and empirical data shows that the nonstationary complex networks are very common in real-world networks, such as the dynamic network and brain network. The further investigation on the biological network reveals the functionality difference between the stable and nonstationary complex networks. The nonstationary complex networks have broad implications in the real-world networks. For example, the brain network and large biological network are usually modeled as stationary networks, where the complex dynamics cannot be captured. On the contrary, the nonstationary complex networks reveal the underlying nonstationary dynamical behaviors, and thus can be used to achieve more accurate results for some applications, such as the network disease and dynamic forecasting.",
        "watermark_text": "The paper studies the traits of the nonstationary complex networks , which includes but not limited to the period - changing network and unsynchronizable networks . The nonstationarity originates from the fast shifting of the network structure and / or nodes ’ dynamics .Since the network is nonstationary , the usual network estimates cannot fully represent the network characteristics . To this end , the notion of recurrence plot is adopted to characterizing the network structure , and the method to quantify the nonstationarity is proposed therefore .The evaluation on the produced model and empirical data suggests that the nonstationary complex networks are very common in real - time systems , such as the dynamic network and brain system . The further investigation on the biological network reveals the functionality difference between the stable and nonstationary complex networks .The nonstationary complex networks have broad implications in the real - time systems . For instance , the brain system and large biological network are typically represented as static systems , where the complex dynamics cannot be captured .On the contrary , the nonstationary complex networks show the fundamental nonstationary dynamical processes , and therefore can be used to achieve more accurate outcomes for some applications , such as the network disease and dynamic forecasting .",
        "rewrite_text": "The research paper explores the characteristics of non-stationary complex networks, which encompass a range of networks such as those with periodically changing structures and those that are unsynchronizable. The non-stationarity arises primarily from rapid shifts in the network structure and/or the dynamics of nodes. Due to this non-stationarity, traditional network estimates cannot fully represent the network's features. To address this, the concept of recurrence plots is employed to characterize the network structure, and a method to quantify non-stationarity is proposed.\n\nEvaluations conducted on the developed model and empirical data suggest that non-stationary complex networks are commonly found in real-time systems like dynamic networks and brain systems. Further investigation into biological networks reveals functional differences between stable and non-stationary complex networks. The non-stationary complex networks have significant implications for real-time systems. For instance, brain systems and large biological networks are often represented as static systems, making it difficult to capture complex dynamics. In contrast, non-stationary complex networks demonstrate fundamental dynamic processes that are not stationary, and thus can be used to achieve more accurate outcomes in certain applications, such as network disease analysis and dynamic forecasting.",
        "ori-fast-z-score": 0.9138115486202573,
        "water-fast-z-score": 7.005888539421972,
        "rewrite-fast-z-score": 2.163657997282274
    },
    {
        "original_text": "To model the effect of electromagnetic barriers (barriers between materials of different indices of refraction) and tunneling barriers (barriers that allow tunneling through them), we derive general constraints on the thicknesses, refractive indices, and densities of states of dielectric multilayers and potential barrier stepwise heights. These constraints are applied to dielectric/dielectric and dielectric/potential barrier multilayer structures to determine the ranges of acceptable parameters for electromagnetic and tunneling barriers, respectively. For each type of barrier, we find that the transmission, dispersion (wavelength shift), and density of states are inversely proportional to the fourth power of the ratio of the barrier height to the incident wavelength. By setting these parameters for known barriers, we demonstrate that it is impossible to construct a stepwise potential barrier with arbitrary layer arrangement that transmits less than 10-8, disperses less than 10-9, or has a density of states less than about 10-4 states/eV/nm2/Ci. Likewise, it is impossible to construct an electromagnetic barrier with a refractive-index difference between the layers greater than about 5.8×10-3 for completely reflecting barriers and greater than 2.2×10-2 for barriers with less than 90% transmittance. These constraints can aid in the optimization of future electromagnetic and tunneling barrier designs and in the development of practical implementations of these effects.",
        "watermark_text": "To model the impact of electromagnetic walls ( obstacles between materials of different indices of refraction ) and tunneling obstacles ( obstacles that permit tunneling through them ) , we derive general limitations on the thicknesses , refractive indices , and densities of states of dielectric multilayers and potential barrier stepwise heights . These restrictions are applied to dielectric / dielectric and dielectric / potential barrier multilayer structures to obtain the ranges of appropriate characteristics for electromagnetic and tunneling obstacles , respectively .For each type of barrier , we find that the propagation , dispersion ( frequency shift ) , and density of states are inversely proportional to the fourth power of the proportion of the fence width to the incident wavelength . By sets these parameters for known obstacles , we prove that it is unable to build a stepwise potential barrier with arbitrary layer arrangement that transmits fewer than 10 - 8 , disperses less than 10 - 9 , or has a density of states less than about 10 - 4 states / eV / nm2 / Ci .Likewise , it is unable to build an electromagnetic barrier with a refractive - index change between the layers large than about 5 . 8×10 - 3 for completely reflecting barriers and larger than 2 . 2×10 - 2 for barriers with less than 90 % transmittance . These limitations can aid in the optimization of later electrical and tunneling barrier proposals and in the development of effective implementations of these effects .",
        "rewrite_text": "To model the effects of electromagnetic barriers, which are obstacles created by differences in refractive indices of materials, and tunneling barriers that allow for tunneling to occur, we establish general constraints on the thicknesses, refractive indices, and state densities of dielectric multilayers, as well as the stepwise heights of potential barriers. These restrictions are applied to both dielectric-dielectric and dielectric-potential barrier multilayer structures, in order to determine the appropriate characteristics for both electromagnetic and tunneling barriers.\n\nFor each type of barrier, we discover that the propagation, dispersion (frequency shift), and state density are inversely proportional to the fourth power of the ratio between the barrier width and the incident wavelength. By setting these parameters for known obstacles, we demonstrate that it is impossible to create a stepwise potential barrier with an arbitrary layer arrangement that transmits less than 10^-8, disperses less than 10^-9, or has a state density less than approximately 10^-4 states per eV per nm^2 per Coulomb. Similarly, it is not feasible to create an electromagnetic barrier with a refractive index difference between layers greater than approximately 5.8 x 10^-3 for completely reflecting barriers and greater than 2.2 x 10^-2 for barriers with less than 90% transmittance.\n\nThese limitations can aid in optimizing future electrical and tunneling barrier proposals, as well as in the development of effective implementations of these effects.",
        "ori-fast-z-score": -0.7504787743864564,
        "water-fast-z-score": 6.325463955542989,
        "rewrite-fast-z-score": 0.10369516947304253
    },
    {
        "original_text": "Riemann zeros are the set of points where the Riemann zeta function, ζ(s), has zeroes. Fractal dimension, a method of fitting a curve to data to represent its complexity, has been shown to fit the zeros well. In this work we fit functions of the form f(x) = a 1 - α(x - b)^c  to Riemann zeros, where a, b, and c are fit parameters, and α, β, and γ are function parameters. We find that the parameter β corresponds to the fractal dimension of the curve fit, and we find that γ has a minimum value of approximately -0.2 at the real axis. Additionally, we compute the error on these fits and find that the error is consistent with being normally distributed. We compute the Hurst exponent h using the following equation: H = −γ / (a log(2) sqrt(ln2) β) where β is the fractal dimension, computed above. We find that Hurst values tend to fall in the range 0.5 < H < 1, indicating non-random fractal behavior. We compute the quantum entropy using the following equation: q = a log(2) / (ln2) and find that it scales similarly to the Hurst exponent, H, and thus is also a reliable metric for non-random fractal behavior. Finally, we show the results of these fractal fits on a Riemann surface, with particular focus on the vertical (X) axis. We observe that f(x) = a 1 - α(x - b)^c  fits the Riemann zeros very well, with errors consistent with a normally distributed random variable. We conclude that Riemann zeros fit the assumptions of fractals, and that fractal functions provide good fits to the Riemann zeros.",
        "watermark_text": "Riemann zeros are the set of points where the Riemann zeta polynomial , ζ ( s ) , has zeroes . Fractal dimension , a technique of fitting a curve to data to measure its complexity , has been shown to fitting the zeros well .In this study we fit functions of the form f ( x ) = a 1 - ω ( x - c ) ^ c to Riemann zeros , where a , b , and c are fit parameters , and α , β , and λ are function parameters . We see that the parameter β corresponds to the fractal dimension of the curve fit , and we find that β has a limit value of approximately - 0 . 2 at the real axis .Additionally , we compute the error on these fits and find that the error is compatible with being normally distributed . We compute the Hurst exponent h using the following equation : H = −γ / ( a log ( 2 ) sqrt ( ln2 ) α ) where β is the fractal dimension , computed above .We see that Hurst estimates tend to fall in the range 0 . 5 < H < 1 , indicating non - random fractal behavior . We compute the quantum entropy using the following equation : q = a log ( 2 ) / ( ln2 ) and find that it scales likewise to the Hurst exponent , H , and therefore is also a reliable metric for non - random fractal behavior .Finally , we give the results of these fractal fits on a Riemann surface , with particular focus on the vertical ( X ) axis . We see that f ( x ) = a 1 - ω ( x - b ) ^ c fitting the Riemann zeros very well , with errors consistent with a normally distributed random variable .We say that Riemann zeros fit the assumptions of fractals , and that fractal functions offer good fits to the Riemann zeros .",
        "rewrite_text": "The Riemann zeros constitute the set of points where the Riemann zeta polynomial, denoted as ζ(s), exhibits zero values. The fractal dimension, a method utilized to fit a curve to data to assess its complexity, has demonstrated effective fitting of these zeros. In this investigation, we employ functions of the form f(x) = a1 - ω(x - c)c to fit the Riemann zeros, wherein a, b, and c are adjustable parameters, while α, β, and λ are functional parameters. It is observed that the parameter β corresponds to the fractal dimension of the curve fit, and we discover that β approaches a limit value of approximately -0.2 along the real axis.\n\nFurthermore, we calculate the error associated with these fits and find that the error is compatible with a normal distribution. We compute the Hurst exponent h using the equation: H = −γ / (a log(2) sqrt(ln2) α), where β is the computed fractal dimension. We observe that Hurst estimates tend to fall within the range 0.5 < H < 1, indicating non-random fractal behavior.\n\nWe calculate quantum entropy using the equation: q = a log(2) / (ln2), and discover that it scales similarly to the Hurst exponent, H, making it a reliable metric for non-random fractal behavior. Ultimately, we present the results of these fractal fits on a Riemann surface, with a specific focus on the vertical (X) axis. It is evident that the function f(x) = a1 - ω(x - b)c provides a good fit for the Riemann zeros, with errors consistent with a normally distributed random variable.\n\nWe conclude that the Riemann zeros align with the assumptions of fractals, and fractal functions offer reliable fits to the Riemann zeros.",
        "ori-fast-z-score": -0.19069251784911848,
        "water-fast-z-score": 4.157609203101499,
        "rewrite-fast-z-score": 1.0954451150103321
    },
    {
        "original_text": "DNA hashing (or DNA hash, dna hash) is an advanced hashing technique to generate short, fixed-length hash values from long, variable-length sequences of nucleotide bases. It was first proposed by Chew et al.  1 , and was shown to be highly resistant to hash collision attacks  2, 3 . In 2014, DNA hashing was adopted by the EU FP7 ProjectDNA Consortium for building EU digital DNA sequences  4 . In late 2017, DNA hashing was adopted by Google for generating short nucleotide sequences, called Thumbprints, for authentication and identification purposes  5 . In 2018, DNA hashing was proposed for protecting digital signatures  6 . In 2019, DNA hashing was adopted by Apple for generating secure key files for TouchID  7  and for encrypting user photos on the Apple Photos service  8 . In 2020, DNA hashing was proposed for generating secure and unique biometric identifiers  9  and for addressing the jitter issue of fingerprint sensors  10 . DNA hashing is also adopted by Google for digital identifier protection  11 . It was shown that DNA hashing can protect the privacy and security of digital identifier users  12 . As a digital identifier is often correlated with sensitive information (such as fingerprint information, facial recognition information, and biometric identification information), DNA hashing can protect the privacy and security of the identifier owner. DNA hashing shares the same basic principles as cryptography-based one-time password (OTP) schemes. Like cryptography-based OTP schemes, DNA hashing protects against offline password recovery attacks by incorporating a unique salt into the hash function. Like cryptography-based OTP schemes, DNA hashing enables password change without exposing old passwords, because old password hashes cannot be reused to calculate new hash values. Unlike cryptography-based OTP schemes, DNA hashing can generate and memorize short, fixed-length hash values from long, variable-length sequences of nucleotide bases, thereby significantly reducing the storage and computation costs. DNA hashing was also designed to satisfy other design requirements. For example, it should satisfy the hardware constraints of the embedded system, as it can be implemented in low-cost, small-footprint circuits. It should be compatible with existing Watson and MegHash hardware architectures. It should be extendible to multiple algorithmic choices. It should be easily expandable to different signature schemes. We believe that DNA hashing is a novel, promising digital identifier technology, with important security, privacy, and usability advantages over existing technologies. -- First Author: Kun Qian Xiamen University, China Last Modified: 26 March 2023 -- Kun Qian is the first author of this paper. The paper was last modified in March 2023. This is a long abstract, but it gives a good overview of the field of DNA hashing, an application of the technique,",
        "watermark_text": "DNA hashing ( or DNA hash , dna hash ) is an sophisticated hashing technique to create short , fixed - length hash values from long , variable - length sequences of nucleotide bases . It was first suggested by Chew et al .1 , and was shown to be highly resistant to hash collision attacks  2, 3 .In 2014, DNA hashing was adopted by the EU FP7 ProjectDNA Consortium for building EU digital DNA sequences  4 .In late 2017 , DNA hashing was adopted by Google for generating small nucleotide sequences , called Thumbprints , for authentication and identification functions 5 . In 2018 , DNA hashing was suggested for guarding electronic certificates 6 .In 2019 , DNA hashing was adopted by Apple for generating secure key files for TouchID 7 and for encrypting customer photos on the Apple Photos service 8 . In 2020 , DNA hashing was suggested for generating secure and distinctive biometric identifiers 9 and for solving the jitter issue of fingerprint sensors 10 .DNA hashing is also used by Google for digital identifier privacy 11 . It was shown that DNA hashing can shield the privacy and safety of digital identifier users 12 .As a computer identifier is often correlated with sensitive information ( such as fingerprint data , facial identification information , and biometric registration information ) , DNA hashing can shield the privacy and safety of the identifier owner . DNA hashing shares the same basic concepts as cryptography - based one - time password ( OTP ) schemes .Like cryptography - based OTP schemes , DNA hashing helps against offline password preservation attacks by incorporating a unique salt into the hash function . Like cryptography - based OTP schemes , DNA hashing enables password change without exposing existing passwords , because old password hashes unable be reused to estimate new hash values .Unlike cryptography - based OTP schemes , DNA hashing can generate and memorize short , fixed - length hash values from long , variable - length sequences of nucleotide bases , thereby significantly eliminating the storage and computation costs . DNA hashing was also intended to meet other design demands .For instance , it should satisfy the hardware constraints of the embedded system , as it can be executed in low - cost , low - footprint circuits . It should be compatible with existing Watson and MegHash hardware architectures .It should be extendible to multiple algorithmic options . It should be easily expandable to different signature schemes .We believe that DNA hashing is a novel , exciting digital identifier technology , with important security , protection , and usability advantages over existing algorithms . - - First Author : Kun Qian Xiamen University , China Last Modified : 26 March 2023 - - Kun Qian is the first author of this paper .The paper was last modified in March 2023 . This is a lengthy abstract , but it gives a good summary of the field of DNA hashing , an use of the method ,",
        "rewrite_text": "DNA Hashing (or DNA Hash, dna hash) is a sophisticated technique used to generate short, fixed-length hash values from long, variable-length sequences of nucleotide bases. This technique was initially proposed by Chew et al.1. It has been shown to be highly resistant to hash collision attacks2, 3.\n\nIn 2014, the EU FP7 ProjectDNA Consortium adopted DNA hashing to create EU digital DNA sequences4. Later in 2017, Google employed it to generate small nucleotide sequences, called \"Thumbprints,\" for authentication and identification functions5. DNA hashing has been suggested for safeguarding electronic certificates in 20186 and for resolving jitter issues in fingerprint sensors in 201910. It is also utilized by Google for enhancing digital identifier privacy11.\n\nStudies have demonstrated that DNA hashing can protect the privacy and security of digital identifier users12. As a computer identifier is often linked to sensitive information such as fingerprint data, facial identification data, and biometric registration data, DNA hashing effectively shields the privacy and safety of the identifier owner. The concept of DNA hashing shares similarities with cryptography-based one-time password (OTP) schemes.\n\nLike OTP schemes based on cryptography, DNA hashing incorporates a unique salt into the hash function to deter offline password preservation attacks. It allows for password changes without revealing existing passwords, as old password hashes cannot be reused to estimate new hash values. However, DNA hashing differs from traditional OTP schemes in its ability to generate and memorize short, fixed-length hash values from long, variable-length sequences of nucleotide bases, significantly reducing storage and computation costs.\n\nDNA hashing also meets other design requirements. For instance, it should adhere to the hardware constraints of embedded systems, executing efficiently in low-cost, low-footprint circuits. It should be compatible with existing hardware architectures like Watson and MegHash. It should be expandable to various algorithmic options and easily adaptable to different signature schemes.\n\nIn conclusion, we believe that DNA hashing is a novel and exciting digital identifier technology with important security, protection, and usability advantages over existing algorithms. This technology was first authored by Kun Qian from Xiamen University in China, and the paper was last modified in March 2023. This extensive abstract provides a comprehensive summary of the field of DNA hashing and its various applications.",
        "ori-fast-z-score": 2.9314195092110324,
        "water-fast-z-score": 9.594005619147026,
        "rewrite-fast-z-score": 2.7048947661974823
    },
    {
        "original_text": "Fusion process studies using the preequilibrium giant dipole resonance (PGDDR) in time dependent Hartree-Fock theory are presented. In particular, it is shown that by using laser assisted direct absorption of an ionising laser pulse, the PGDDR peak in the electron momentum distribution function can be significantly enhanced. Moreover, it is also shown that the energy of the absorbed laser pulse can be efficiently transferred to the electronic motion in the form of shaking or nucleonic fluid that leads to a strong enhancement of the PGDDR strength.  More information about laser assisted nuclear reactions can be found in the recent review article (Ref. 1) and references therein. Time-dependent Hartree-Fock (TDHF) approach is used to study the above process. TDHF theory is an efficient way to describe heavy ion reactions when projectiles interact in a time-dependent potential. P GDDR is studied in TDHF framework and it is shown that the absorption of ionising laser pulse can enhance the P GDDR strength. It is also shown that the absorbed laser energy can be efficiently transferred to the electronic motion.",
        "watermark_text": "Fusion process studies employing the preequilibrium giant dipole resonance ( PGDDR ) in time dependent Hartree - Fock physics are presented . In particular , it is demonstrated that by using laser guided direct diffusion of an ionising laser wave , the PGDDR peak in the electron velocity distribution function can be substantially enhanced .Moreover , it is also shown that the power of the absorbed beam pulse can be easily shifted to the electronic movement in the form of shaking or nucleonic fluid that leads to a powerful enhancement of the PGDDR strength . More details about infrared assisted nuclear compounds can be found in the recent review article ( Ref .1 ) and references therein . Time - dependent Hartree - Fock ( TDHF ) approach is utilized to study the above process .TDHF theory is an efficient place to explain heavy ion reactions when projectiles interact in a time - dependent potential . P GDDR is studied in TDHF framework and it is demonstrated that the absorption of ionising optical pulse can increase the P GDDR strength .It is also shown that the absorbed beam energy can be easily shifted to the electronic movement .",
        "rewrite_text": "Presented are studies on the fusion process that employ the pre-equilibrium giant dipole resonance (PGDDR) in the context of time-dependent Hartree-Fock physics. Specifically, it has been demonstrated that through the utilization of laser-guided direct diffusion of an ionizing laser wave, the PGDDR peak in the electron velocity distribution function can be significantly amplified. Furthermore, it has also been shown that the power of the absorbed beam pulse can effortlessly be redirected towards electronic motion, manifesting as shaking or nucleonic fluid movement, leading to a considerable boost in PGDDR strength. For more details on infrared-assisted nuclear compounds, readers can refer to the recent review article (Ref. 1) and its references.\n\nThe time-dependent Hartree-Fock (TDHF) approach is utilized to investigate the aforementioned process. TDHF theory serves as a valuable tool to explain heavy ion reactions when projectiles interact within a time-dependent potential. Within the framework of TDHF, PGDDR has been studied, and it has been demonstrated that the absorption of ionizing optical pulses can enhance PGDDR strength. Additionally, it has been illustrated that the absorbed beam energy can readily be transferred to electronic motion.",
        "ori-fast-z-score": -0.32539568672798425,
        "water-fast-z-score": 6.182518047831701,
        "rewrite-fast-z-score": 2.5584085962673253
    },
    {
        "original_text": "High-purity radicals are essential for dynamic nuclear polarization (DNP), which boosts nuclear magnetic moments for imaging at high sensitivity. The hyperpolarization produced by DNP requires a short polarization time. Here, we introduce a versatile solid radical, nitroxide trityl (N3), for efficient DNP. In particular, we explore the dynamics of N3 using electron paramagnetic resonance (EPR). We identify a stable nitroxide anion, tetramethyltetranitromethane (TMTN), as an efficient trap for N3 radicals. We demonstrate that dipolar coupling between electron and nuclear spins, arising from the anisotropic electron spin, enhances the rate of dynamic nuclear polarization by a factor of 6. Dipolar coupling has been proposed as an approach to increase the rate of DNP, but the enhancement we observe is an order of magnitude greater than that predicted by theory. This enhancement arises from a combination of faster electron relaxation in the TMTN radical and faster formation of N3 radicals in the TMTN radical trap, and we show that both effects contribute significantly to the enhancement. The rapid diffusion and high polarization we observe with N3 suggest that the approach may be extended to other solid radicals and has the potential to reduce DNP time to less than 1 ms.",
        "watermark_text": "High - pure radicals are essential for dynamic nuclear polarization ( DNP ) , which boosts atomic magnetic moments for imaging at high sensitivity . The hyperpolarization produced by DNP requires a brief polarization period .Here , we provide a versatile solid radical , nitroxide trityl ( N3 ) , for efficient DNP . In particular , we investigate the dynamics of N3 using electron paramagnetic resonance ( EPR ) .We recognize a neutral nitroxide anion , tetramethyltetranitromethane ( TMTN ) , as an efficient trap for N3 radicals . We suggest that dipolar coupling between electron and nuclear spins , arising from the anisotropic electron spinning , enhances the speed of static nuclear polarization by a factor of 6 .Dipolar coupling has been proposed as an way to raise the frequency of DNP , but the enhancement we observe is an order of magnitude greater than that predicted by theoretical . This enhancement occurs from a combination of quicker electron relaxation in the TMTN radical and better formation of N3 radicals in the TMTN radical trap , and we find that both changes lead dramatically to the enhancement .The rapid diffusion and large polarization we exhibit with N3 suggest that the approach may be enhanced to other solid radicals and has the ability to reduce DNP period to fewer than 1 ms .",
        "rewrite_text": "High-purity radicals play a crucial role in dynamic nuclear polarization (DNP), which amplifies atomic magnetic moments for high-sensitivity imaging. The hyperpolarization achieved through DNP necessitates a brief polarization period. We introduce a versatile solid radical, nitroxide trityl (N3), as an efficient candidate for DNP. Specifically, we examine the dynamics of N3 using electron paramagnetic resonance (EPR). We identify a neutral nitroxide anion, tetramethyltetranitromethane (TMTN), as an effective trap for N3 radicals.\n\nWe propose that dipolar coupling between electron and nuclear spins, arising from the anisotropic electron spinning, increases the rate of static nuclear polarization by a factor of six. While dipolar coupling has been suggested as a means to increase the frequency of DNP, the observed enhancement we found is significantly greater than that predicted by theory. This enhancement is attributed to a combination of faster electron relaxation in the TMTN radical and improved formation of N3 radicals in the TMTN radical trap. We observe that both these changes contribute significantly to the enhancement.\n\nThe rapid diffusion and high polarization we demonstrate with N3 suggest that this approach may be extended to other solid radicals and has the potential to reduce the DNP period to less than 1 ms.",
        "ori-fast-z-score": -1.6876318513890358,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": -0.44172610429938614
    },
    {
        "original_text": "In the field of computer-based learning (CBL), educational researchers and practitioners have identified the need to evaluate the effectiveness of CBL using empirical evidence. Although there has been a rapid increase in the development of automated assessment in CBL, evaluation methods still mostly rely on self-report and think-aloud protocols, neither of which provides an accurate picture of participants  cognitive and affective experiences. This article introduces a new method, evaluation as communication (EAC), to measure these experiences. The EAC method collects survey data on participants  verbal and non-verbal communication during CBL tasks, and uses data mining and natural language processing techniques to analyse the data. The article presents results from an evaluation study using an artificial intelligence-based CBL system to demonstrate the feasibility of the EAC method. The study achieved 76% statistical significance in identifying different communication styles between low- and high-performing learners, and 85% concordance between human coding and the machine-learning-based approach to coding communication styles.",
        "watermark_text": "In the field of computer - based knowledge ( CBL ) , academic scientists and practitioners have noted the necessity to analyze the effectiveness of CBL using empirical evidence . Although there has been a rapid increase in the development of electronic assessment in CBL , evaluation methods still mostly rely on self - report and thought - aloud techniques , neither of which offers an accurate image of participants mental and affective experiences .This page presents a new method , evaluation as communication ( EAC ) , to measure these experiences . The EAC technique captures survey information on participants verbal and non - verbal communication during CBL tasks , and using data extraction and natural language processing tactics to analyse the information .The section documents findings from an assessment study using an synthetic intelligence - based CBL scheme to test the feasibility of the EAC technique . The survey achieved 76 % statistical importance in identifying various transmission styles between lowest - and low - performing learners , and 85 % concordance between human coding and the machine - knowledge - based approach to code communication styles .",
        "rewrite_text": "In the realm of computer-based learning (CBL), both academic scientists and practitioners have emphasized the importance of analyzing the effectiveness of CBL through empirical evidence. Despite the rapid advancement in electronic assessment within CBL, evaluation methods still predominantly rely on self-reporting and thought-aloud techniques, which do not provide an accurate representation of participants' mental and affective experiences. This page introduces a novel method, Evaluation as Communication (EAC), for measuring these experiences.\n\nThe EAC technique captures survey information regarding participants' verbal and non-verbal communication during CBL tasks. It utilizes data extraction and natural language processing techniques to analyze the collected information. This section presents the findings from an assessment study employing a synthetic intelligence-based CBL framework to test the feasibility of the EAC technique. The survey achieved a statistical significance of 76% in identifying diverse communication styles among low and lowest-performing learners, and an 85% concordance between human coding and a machine-knowledge-based approach for coding communication styles.\n\nAdditionally, this study documents the effectiveness of the EAC technique in assessing CBL experiences, offering a more accurate and comprehensive understanding of participant engagement and learning outcomes.",
        "ori-fast-z-score": -0.6255432421712244,
        "water-fast-z-score": 6.672461249826393,
        "rewrite-fast-z-score": 2.136828897185981
    },
    {
        "original_text": "Magnetic materials that do not exhibit long-range magnetic order at low temperatures are of great scientific interest. In some of these cases, “geometrical frustration” has been shown to lead to novel behavior. Sr3NiRhO6 and Sr3NiPtO6 are isostructural frustrated, antiferromagnetically interacting magnetic systems that do not exhibit long-range magnetic order down to 20 mK. To characterize this, we have utilized magnetic neutrons scattering, which reveals a number of interesting features, including spinon Fermi surface fragmentation. We discuss the implications of our results on the nature of quantum spin liquids, as well as possible relationship to recent proposals for room-temperature superconductivity in this system. Magnetic materials that do not exhibit long-range magnetic order at low temperatures are of great scientific interest. In some of these cases, “geometrical frustration” has been shown to lead to novel behavior. Sr3NiRhO6 and Sr3NiPtO6 are isostructural frustrated, antiferromagnetically interacting magnetic systems that do not exhibit long-range magnetic order down to 20 mK. To characterize this, we have utilized magnetic neutrons scattering, which reveals a number of interesting features, including spinon Fermi surface fragmentation. We discuss the implications of our results on the nature of quantum spin liquids, as well as possible relationship to recent proposals for room-temperature superconductivity in this system.",
        "watermark_text": "Magnetic elements that do not exhibit long - range magnetic order at low temperatures are of large science interest . In some of these cases , “ geometrical frustration ” has been shown to lead to novel behavior .Sr3NiRhO6 and Sr3NiPtO6 are isostructural frustrated , antiferromagnetically correlated magnetic structures that do not present long - range magnetic order down to 20 mK . To characterize this , we have utilized magnetic neutrons scattering , which reveals a number of interesting features , notably spinon Fermi surface fragmentation .We discuss the implications of our findings on the nature of quantum spin liquids , as well as possible relationship to recent suggestions for room - temperature superconductivity in this scheme . Magnetic substances that do not show full - range magnetic order at low temperatures are of important scientific interest .In some of these cases , “ geometrical frustration ” has been shown to lead to novel activity . Sr3NiRhO6 and Sr3NiPtO6 are isostructural frustrated , antiferromagnetically correlated magnetic structures that do not produce short - range magnetic order down to 20 mK .To characterize this , we have utilized magnetic neutrons diffusion , which reveals a number of interesting features , notably spinon Fermi surface fragmentation . We discuss the implications of our findings on the nature of quantum spin liquids , as well as possible relationship to recent suggestions for room - temperature superconductivity in this system .",
        "rewrite_text": "At low temperatures, magnetic elements that lack long-range magnetic order hold significant scientific value. In certain instances, \"geometric frustration\" has been found to induce unique behaviors. Sr3NiRhO6 and Sr3NiPtO6 are examples of isostructural frustrated antiferromagnetically correlated magnetic structures that maintain no long-range magnetic order even down to 20 mK. To better understand this, we have employed magnetic neutron scattering, which has uncovered several fascinating features, notably the fragmentation of the spinon Fermi surface. We delve into the ramifications of our findings on the essence of quantum spin liquids and explore potential connections to recent proposals regarding room-temperature superconductivity within this framework. Furthermore, the exploration of magnetic substances that exhibit no full-range magnetic order at low temperatures is a pivotal scientific endeavor. In some instances, \"geometric frustration\" has been observed to generate novel activities. Both Sr3NiRhO6 and Sr3NiPtO6 are examples of isostructural frustrated antiferromagnetic structures that lack even short-range magnetic order at 20 mK. To elucidate this, we have utilized magnetic neutron diffusion, which has revealed multiple intriguing characteristics, particularly the breakdown of the spinon Fermi surface. We discuss the implications of our findings in understanding the nature of quantum spin liquids and their potential association with recent proposals for room-temperature superconductivity within this system.",
        "ori-fast-z-score": 1.6,
        "water-fast-z-score": 5.6,
        "rewrite-fast-z-score": 1.6135685927792485
    },
    {
        "original_text": "The AnITA generic tutoring system is developed to help students improve their academic performance. In this work, we present an empirical study on its design process and experiments from multiple perspectives. We first analyze the system through its design documents and reference architecture, which consist of requirements, use cases, design documents, and tests. Then, we study the system implementation from architecture, development, and testing perspectives. Additionally, we present several system-level experiments to study their impacts on tutoring sessions. The findings indicate that the system achieves an average rating of 4.3 stars out of 5 on system usability, 4.8 stars out of 5 on content quality, and 4.7 stars out of 5 on system reliability. Furthermore, we find that when using multiple experiments, the tutor’s fluency and engagement could be increased by 5.2% and 6.8%, respectively. Overall, this work provides a detailed case study on the development of a generic tutoring system and presents several design optimization methods and lessons learned that are potentially useful to others.",
        "watermark_text": "The AnITA generic tutoring scheme is developed to assist children enhance their educational success . In this project , we present an scientific study on its design process and experiments from multiple perspectives .We first examine the program through its design documents and reference architecture , which consist of specifications , use instances , design documents , and examinations . Then , we study the program specification from design , construction , and testing viewpoint .Additionally , we present many system - level experiments to study their impacts on tutoring sessions . The findings confirm that the program achieves an average score of 4 . 3 stars out of 5 on system usability , 4 . 8 stars out of 5 on content quality , and 4 . 7 stars out of 5 on system integrity .Furthermore , we find that when using multiple studies , the mentor ’ s fluency and engagement could be improved by 5 . 2 % and 6 . 8 % , respectively . Overall , this research provides a detailed case study on the development of a generic tutoring scheme and provides various design optimization tools and lessons taught that are possibly useful to others .",
        "rewrite_text": "The AnITA universal tutoring framework has been developed to aid children in enhancing their educational achievements. In this project, we present a scientific investigation into its design process and experiments from various perspectives. We initially scrutinize the program through its design documents and reference architecture, which encompass specifications, use cases, design documents, and assessments. Subsequently, we analyze the program specifications from the design, construction, and testing perspectives.\n\nMoreover, we conduct numerous system-level experiments to study their impact on tutoring sessions. The findings indicate that the program achieves an average rating of 4.3 stars out of 5 for system usability, 4.8 stars out of 5 for content quality, and 4.7 stars out of 5 for system integrity. Furthermore, we observe that by utilizing multiple studies, the mentor's fluency and engagement can be enhanced by 5.2% and 6.8%, respectively.\n\nIn conclusion, this research provides a comprehensive case study on the development of a generic tutoring scheme, offering various design optimization tools and lessons learned that could be beneficial to others.",
        "ori-fast-z-score": 0.42640143271122083,
        "water-fast-z-score": 7.313985372043884,
        "rewrite-fast-z-score": 2.27776980709589
    },
    {
        "original_text": "A REM (Rapidly Extensive Method) interface to the simulation of dynamical mean-field spin glasses is introduced. This interface allows one to perform parallel simulations of finite temperature dynamical mean-field spin glass dynamics in the wide range of time and length scales. The proposed interface is specialized to handle fully connected random matrices. We study the relaxation of a mean-field spin glass model with probability distribution of Ising interactions defined by a Gaussian random matrix to the equilibrium distribution using the REM. In particular, we compare the average relaxation time τRSA calculated with different existing methods. τRSA obtained by the REM is consistent with the theoretical prediction 1/log(N), where N is the system size. For N=10^4, τRSA = 13.2, consistent with 1/log(N) = 13.2/9. REM: Rapidly Extensive Method; MFSG: Mean-field spin glass; CDM: Chained Decomposition Method; N: System size; log: Logarithm.",
        "watermark_text": "A REM ( Rapidly Extensive Method ) interface to the simulation of dynamical mean - field spinning mirrors is developed . This interface enabled one to execute parallel simulations of finite temperature dynamical mean - field spinning crystal dynamics in the broad variety of time and duration scales .The proposed interface is specialized to treat completely connected random matrices . We explore the relaxation of a mean - field spin glass model with probability distribution of Ising interactions given by a Gaussian random matrix to the equilibrium distribution using the REM .In particular , we compare the average relaxation time τRSA calculated with various existing techniques . τRSA derived by the REM is compatible with the theoretical forecast 1 / log ( N ) , where N is the process size .For N = 10 ^ 4 , τRSA = 13 . 2 , compatible with 1 / log ( N ) = 13 . 2 / 9 . REM : Rapidly Extensive Method ; MFSG : Mean - field spin glass ; CDM : Chained Decomposition Method ; N : System size ; log : Logarithm .",
        "rewrite_text": "A Rapidly Extensive Method (REM) interface has been developed for simulating the dynamics of mean-field spinning mirrors. This interface facilitates parallel simulations of finite-temperature mean-field spin crystal dynamics across a wide range of time and duration scales. The proposed interface is tailored to handle completely connected random matrices.\n\nWe investigate the relaxation of a Mean-field Spin Glass (MFSG) model, utilizing a Gaussian random matrix to represent the probability distribution of Ising interactions and employing the REM to reach the equilibrium distribution. Specifically, we compare the average relaxation time, τRSA, calculated using various techniques. The τRSA derived from the REM aligns with the theoretical prediction of 1/log(N), where N represents the system size. For a system with N = 10^4, τRSA equals 13.2, which is consistent with 1/log(N) = 13.2/9.\n\nREM: Rapidly Extensive Method; MFSG: Mean-field spin glass; CDM: Chained Decomposition Method; N: System size; log: Logarithm.",
        "ori-fast-z-score": -2.0175288189295504,
        "water-fast-z-score": 3.679023140400945,
        "rewrite-fast-z-score": 0.1203858530857692
    },
    {
        "original_text": "The spiral wave is a key phenomenon in the plankton ecological systems and exhibits the rotating spots with different densities. In this Letter, we show that far-field spiral wave breakup is an alternative route to spatiotemporal chaos (STC) in the plankton ecological systems, which cannot be described by the classical C. E. Pfister’s theory1. The system with three-prey version of the Leslie model exhibits the spatiotemporal chaos when the diffusion coefficients ratio and the compensation rate are set as R=4 and α=2.5, while the spiral wave could still be observed in the experimentally observed region. Using the spiral wave frequency, the wave pattern and the STC are confirmed to be the same. Thus, the far-field breakup of the spiral wave is an alternative route to the spatiotemporal chaos in the plankton ecological systems. The spiral wave breakup is not only an alternative route to STC, but also can be observed by the naked eye in the real ecological systems.",
        "watermark_text": "The spiral wave is a key concept in the plankton ecological systems and exhibits the rotating spots with various densities . In this Letter , we prove that far - field spiral wave breakup is an additional route to spatiotemporal chaos ( STC ) in the plankton ecological systems , which cannot be described by the classical C . E . Pfister ’ s theory1 .The system with three - prey variant of the Leslie theory exhibits the spatiotemporal chaos when the diffusion coefficients ratio and the compensation rate are set as R = 4 and α = 2 . 5 , while the spiral wave may still be found in the experimentally seen region . Using the spiral wave frequency , the wave pattern and the STC are confirmed to be the same .Thus , the long - field breakup of the spiral wave is an alternative pathway to the spatiotemporal chaos in the plankton ecological systems . The circular wave breakup is not only an alternative route to STC , but also can be viewed by the naked eye in the real ecological systems .",
        "rewrite_text": "The spiral wave is a pivotal notion within plankton ecological systems, manifesting rotating spots with varying densities. In this communication, we establish that the far-field spiral wave breakup is an additional avenue towards spatiotemporal chaos (STC) in plankton ecosystems, which goes beyond the conventional framework proposed by C.E. Pfister's theory1.\n\nThe system based on the three-prey variant of the Leslie theory demonstrates spatiotemporal chaos when the ratio of diffusion coefficients and compensation rate are set at R = 4 and α = 2.5. This aligns with the observation that spiral waves can still be detected within the experimentally observed region. By utilizing the frequency of the spiral wave, we confirm that the wave pattern and STC are identical. Therefore, the long-field breakup of the spiral wave serves as an alternative pathway to spatiotemporal chaos in plankton ecological systems. Moreover, the circular wave breakup not only offers an alternative route to STC but is also visible to the naked eye in real ecological settings.",
        "ori-fast-z-score": 1.649915822768611,
        "water-fast-z-score": 5.266851623825876,
        "rewrite-fast-z-score": 1.811643254631353
    },
    {
        "original_text": "Central type groups and cohomology classes Let G be a Lie group and g its element. We say that g has central type if the Killing form of the Lie algebra of G vanishes at the origin, i.e. vanishes on the Lie algebra of G. In this case, the mapping g−1 : g−1 (g)↦g is an automorphism of G called the Zassenhaus automorphism. We say that a cohomology class in Hn(G,M;R) has central type if its projection to Hn(g−1(g),g−1;R) does. We give two criterions to know when a cohomology class with central type is non-degenerate: it is non-degenerate if and only if its inverse is non-degenerate and we give an explicit cup-product. We also explain how to compute cup-product between two cohomology classes with central type in terms of pairing between the corresponding Lie algebras  cohomology classes. We finally show that any cohomology classes with central type is bijective if and only if it is non-degenerate and then exhibit an explicit formula for its inverse.",
        "watermark_text": "Central class groups and cohomology groups Let G be a Lie ring and g its element . We say that g has central type if the Killing form of the Lie algebra of G vanishes at the origin , i . e .vanishes on the Lie algebra of G . In this situation , the mapping g−1 : g−1 ( g ) [UNK] is an automorphism of G named the Zassenhaus automorphism . We say that a cohomology category in Hn ( G , M ; R ) has central type if its mapping to Hn ( g−1 ( h ) , g−1 ; R ) does .We get two criterions to see when a cohomology category with central type is non - degenerate : it is non - degenerate if and only if its inverse is non - degenerate and we give an explicit cup - product . We also explain how to compute cup - product between two cohomology groups with central type in terms of pairing between the respective Lie algebras cohomology groups .We last show that any cohomology groups with central type is bijective if and only if it is anti - degenerate and then exhibit an explicit formula for its inverse .",
        "rewrite_text": "Central Class Groups and Cohomology Groups\n\nLet G be a Lie ring and g be an element of it. We define g to have central type if the Killing form of the Lie algebra of G is zero at the origin, i.e., it vanishes on the algebra of G. In this context, the mapping g⁻¹: g⁻¹(g) [term missing] is an automorphism of G, named the Zassenhaus automorphism. We say that a cohomology category in Hn(G, M; R) has central type if its mapping to Hn(g⁻¹(h), g⁻¹; R) also does.\n\nWe derive two criteria to determine when a cohomology category with central type is non-degenerate: it is non-degenerate if and only if its inverse is non-degenerate, and we provide an explicit cup product for it. Furthermore, we explain how to compute the cup product between two cohomology groups with central type in terms of the pairing between their respective Lie algebra cohomology groups.\n\nFinally, we demonstrate that any cohomology group with central type is bijective if and only if it is anti-degenerate, and we present an explicit formula for its inverse.",
        "ori-fast-z-score": -0.6401843996644799,
        "water-fast-z-score": 4.131182235954578,
        "rewrite-fast-z-score": 2.065591117977289
    },
    {
        "original_text": "In quantum mechanics, the state of a system is described by a density matrix, a Hermitian positive semidefinite matrix corresponding to a statistical ensemble of pure states. The density matrix allows one to calculate various properties of the system, such as the expectation value of a particular operator. In experiments, access to the density matrix requires a quantum process tomography (QPT) scheme. QPT is the method of reconstructing the most general process that has acted on a system by probing the system with a large number of different initial states. In this work, a closed quantum system is prepared in an arbitrary initial state Ψ0, and a series of projective measurements are performed. The expected outcome for each measurement is calculated from the initial state. By repeating this process for a series of different Ψ0, a histogram of expected outcomes is generated. Using a quantum process tomography scheme, the initial state can then be extracted from the histogram. The average fidelity between the reconstructed and true initial states is calculated for a wide range of system parameters, showing the efficacy of the process. Additionally, finite-time effects are considered, where the expected outcomes are calculated for a system initialized in a highly non-equilibrium state. The formalism is then applied to a spin-boson model, the T1 qubit, showing a method for measuring arbitrary channels of a spin qubit in a scalable architecture.",
        "watermark_text": "In quantum mechanics , the state of a system is characterized by a density matrix , a Hermitian positive semidefinite vector analogous to a statistical ensemble of pure states . The density matrix allows one to estimate various properties of the system , such as the expectation value of a certain operator .In studies , entry to the density matrix requires a quantum method tomography ( QPT ) scheme . QPT is the method of reconstructing the most general process that has acted on a system by probing the process with a large number of different initial states .In this study , a closed quantum model is prepared in an arbitrary initial state Ψ0 , and a sequence of projective observations are performed . The expected outcome for each measurement is calculated from the initial state .By repeating this process for a sequence of different Ψ0 , a histogram of anticipated outcomes is generated . Using a quantum process tomography system , the first state can then be extracted from the histogram .The estimated fidelity between the reconstructed and true initial states is calculated for a broad variety of system parameters , showing the efficacy of the process . Additionally , finite - time effects are considered , where the expected outcomes are measured for a system initialized in a highly non - equilibrium state .The formalism is then applicable to a spinning - boson theory , the T1 qubit , showing a technique for monitoring arbitrary channels of a spinning qubit in a scalable design .",
        "rewrite_text": "In quantum mechanics, the state of a system is defined by a density matrix - a Hermitian, positive semidefinite vector analogous to a statistical ensemble of pure states. This density matrix enables the estimation of various system properties, such as the expectation value of a particular operator. In research, accessing the density matrix necessitates the utilization of a Quantum Process Tomography (QPT) scheme. QPT is a method for reconstructing the most general process that has acted on a system by probing it with various initial states.\n\nIn this study, a closed quantum model is prepared in an arbitrary initial state denoted as |Ψ0>. A sequence of projective observations is then performed. The expected outcome for each measurement is determined based on the initial state. By repeating this process with different |Ψ0> states, a histogram of anticipated outcomes is generated. With the help of a quantum process tomography system, the initial state can be extracted from this histogram.\n\nThe estimated fidelity between the reconstructed and true initial states is calculated for a wide range of system parameters, demonstrating the effectiveness of the process. Furthermore, considerations are given to finite-time effects, where the expected outcomes are measured for a system initialized in a highly non-equilibrium state. The framework then applies to a spinning-boson theory, specifically the T1 qubit, showcasing a technique for monitoring arbitrary channels of a spinning qubit in a scalable design.",
        "ori-fast-z-score": 2.183063390230748,
        "water-fast-z-score": 7.055623160417383,
        "rewrite-fast-z-score": 4.001190299088986
    },
    {
        "original_text": "Geometric control theory (Grueneisen criteria) is a powerful extension of classical control theory that provides a unified framework for the analysis and stabilization of mechanical systems with constraints and mechanical systems with ordinary differential equations (ODEs), respectively. This framework is based on the notion of sub-Riemannian geometry, which was introduced by A. Ferreira in 1973. This extension has been very successfully applied to several areas of Mathematics, such as the theory of PDEs and geometric evolution equations, and also to applications such as mechanical systems with constrains or in faulty situations, vector fields on statistical manifolds, etc. However, up to now there has not been a comprehensive exposition of this theory. In this work we give a comprehensive exposition of geometric control theory, from a synthetic point of view. To do this, we provide a detailed study of the main concepts in sub-Riemannian geometry and also we prove some basic results of functional analysis and the theory of ordinary differential equations that are of general interest. We also discuss the relation of this theory to the optimal control of differential equations and the calculus of variations. We present the main applications of geometric control theory in several fields of Mathematics and give a list of open problems that could be of interest for experts and researchers in the field.",
        "watermark_text": "Geometric control theory ( Grueneisen criteria ) is a powerful application of classical control theory that offers a consolidated framework for the evaluation and stabilization of mechanical models with constraints and mechanical models with ordinary differential equations ( ODEs ) , respectively . This framework is based on the notion of sub - Riemannian topology , which was introduced by A . Ferreira in 1973 .This extension has been very successfully application to several topics of Mathematics , such as the description of PDEs and geometric evolution equations , and also to applications such as mechanical models with constrains or in faulty situations , tensor fields on statistical manifolds , etc . However , up to now there has not been a comprehensive exposition of this theory .In this study we give a comprehensive exposition of geometric control geometry , from a synthetic point of view . To do this , we provide a detailed investigation of the main concepts in sub - Riemannian topology and also we prove some fundamental findings of functional analysis and the principle of simple differential coefficients that are of general interest .We also discuss the relation of this theory to the ideal control of differential equations and the calculus of differences . We discuss the main applications of geometric control theory in multiple fields of Mathematics and give a list of open problems that might be of focus for experts and researchers in the field .",
        "rewrite_text": "The geometric control theory, also known as Grueneisen criteria, serves as a robust application of classical control theory. It provides a unified framework for evaluating and stabilizing mechanical models with constraints, as well as those with ordinary differential equations (ODEs). This framework is rooted in the concept of sub-Riemannian topology, which was introduced by A. Ferreira in 1973. This extension has found successful applications in various mathematical topics, such as the description of partial differential equations (PDEs) and geometric evolution equations. It has also been applied to mechanical models under constrained or faulty conditions, tensor fields on statistical manifolds, and more. However, there has yet to be a comprehensive exposition of this theory.\n\nIn this study, we present a comprehensive exposition of geometric control geometry from a synthetic perspective. To achieve this, we conduct a detailed exploration of the key concepts in sub-Riemannian topology. Furthermore, we prove fundamental insights from functional analysis and the principle of simple differential coefficients that are of general interest. We also discuss the relationship between this theory and the ideal control of differential equations and the calculus of differences. We delve into the primary applications of geometric control theory across multiple fields of mathematics and provide a list of open problems that may serve as focal points for experts and researchers in the field.",
        "ori-fast-z-score": -0.7770286898858113,
        "water-fast-z-score": 4.662172139314868,
        "rewrite-fast-z-score": 1.4110813025753959
    },
    {
        "original_text": "Solar radio spectrograph observation of the off-limb active region on 22 June 2014 at 04:30 UT is presented. Spectral analysis shows that the observed radio emission is mostly composed of two thermal components: one is the spectrally broad and kinetically cool accelerated component, which is usually interpreted as electrons and heat deposited by precipitating beams; the other is the spectrum with little variance across the line profile, which may be contributed by the Maxwellian electrons accelerated by the magnetic field. Compared with the off-limb observation on 21 May 2014 at 14:00 UT, we found that the beam-related emissions have a similar strength, whereas the mean electron density and the temperature of Maxwellian electrons decreased by 20% and 10%, respectively. This paper analyzes the spectral lines and their spatial derivatives to reveal how the density and velocity distribution of the thermal electrons change along the solar atmosphere. The results show that the density stratifies downward from the corona to the transition region, and the velocity distribution becomes more anisotropic with decreasing temperature. The density stratification indicates the beam-generated hot plasma is rooted in the lower solar atmosphere, and the velocity distribution anisotropy suggests that the beam particles propagate along the magnetic field. These results imply that both the beam formation and the propagation processes occur simultaneously in the lower solar atmosphere.",
        "watermark_text": "Solar radio spectrograph observation of the off - limb active region on 22 June 2014 at 04 : 30 UT is published . Spectral investigation reveals that the seen radio emission is mainly composed of two thermal parts : one is the spectrally broad and kinetically cool accelerated component , which is usually interpreted as ions and heat deposited by precipitating beams ; the other is the spectrum with little variance across the line profile , which may be contributed by the Maxwellian atoms accelerated by the magnetic force .Compared with the off - limb observation on 21 May 2014 at 14 : 00 UT , we concluded that the laser - associated emissions have a similar strength , whereas the mean electron concentration and the temperature of Maxwellian atoms decreased by 20 % and 10 % , respectively . This paper analyzes the spectral lines and their temporal derivatives to explain how the density and speed distribution of the thermal electrons change along the lunar environment .The results show that the density stratifies downward from the corona to the transition region , and the velocity distribution turns more anisotropic with decreasing temperature . The density stratification indicates the laser - produced warm plasma is rooted in the lower solar atmosphere , and the velocity distribution anisotropy suggests that the laser particles propagate along the magnetic field .These data indicate that both the laser formation and the propagation activities occur simultaneously in the lower solar atmosphere .",
        "rewrite_text": "The observation of solar radio spectrograph for the off-limb active region on June 22nd, 2014 at 04:30 UT has been published. The spectral investigation reveals that the observed radio emission primarily consists of two thermal components. One is a spectrally broad and kinetically cool accelerated component, which is often interpreted as ions and heat deposited by precipitating beams. The other component exhibits little variation across the line profile, potentially contributed by Maxwellian atoms accelerated by magnetic force.\n\nIn comparison to the off-limb observation on May 21st, 2014 at 14:00 UT, we have concluded that the laser-associated emissions possess a similar intensity. However, the mean electron concentration and the temperature of Maxwellian atoms have decreased by 20% and 10% respectively. This paper delves into the analysis of spectral lines and their temporal derivatives to elucidate how the density and speed distribution of thermal electrons vary in the lunar environment.\n\nThe results indicate that the density stratifies in a downward direction from the corona to the transition region. Furthermore, the velocity distribution becomes more anisotropic as the temperature decreases. The density stratification suggests that the laser-generated warm plasma is rooted in the lower solar atmosphere. The anisotropy in velocity distribution suggests that laser particles propagate along magnetic field lines. These data suggest that both laser formation and propagation activities take place concurrently in the lower solar atmosphere.",
        "ori-fast-z-score": 2.3533936216582085,
        "water-fast-z-score": 7.709610576293413,
        "rewrite-fast-z-score": 4.157609203101499
    },
    {
        "original_text": "The characterisation of the dark matter (DM) component in clusters of galaxies is important for our understanding of the Universe on the largest scales. While observational signatures of DM are sought by many methods, one of the most direct is the measurement of the velocity anisotropy of the DM particles as a function of their position within the cluster. We describe a method for measuring the DM velocity anisotropy, using optical observations of the cluster galaxies and strong gravitational lensing of background galaxies. We apply this method to the galaxy cluster MACS J0429.6-0253, combining our measurements of the projected mass distribution with strong lensing observations from the Subaru telescope. We find strong evidence for an axisymmetric DM velocity anisotropy in this cluster, with the amplitude of the anisotropy dependent on radius. Our results show that dark matter is not fully collisional, but instead retains some of its velocity memory from earlier times. The methods described in this paper can be applied to other galaxy clusters, as well as other large scale structures in the Universe, allowing further tests of the nature of DM and its dynamical evolution.",
        "watermark_text": "The characterisation of the dark matter ( DM ) element in clusters of galaxies is important for our studying of the Universe on the greatest scales . While observational signatures of DM are seek by many approaches , one of the most direct is the observation of the velocity anisotropy of the DM particles as a function of their orientation within the cluster .We define a technique for monitoring the DM speed anisotropy , using optical images of the cluster clusters and strong gravitational lensing of background galaxies . We use this technology to the galaxy cluster MACS J0429 . 6 - 0253 , merging our measurements of the projected mass distribution with powerful lensing observations from the Subaru observatory .We get strong evidence for an axisymmetric DM speed anisotropy in this cluster , with the frequency of the anisotropy dependent on diameter . Our results show that dark matter is not totally collisional , but instead retains some of its velocity memory from previous times .The methods described in this paper can be applied to other galaxy galaxies , as also as other large scale structures in the Universe , allowing further tests of the nature of DM and its dynamical development .",
        "rewrite_text": "The analysis of dark matter (DM) elements within clusters of galaxies holds significance for our exploration of the Universe at its largest scales. While multiple approaches seek observational footprints of DM, one of the most direct methods involves observing the velocity anisotropy of DM particles relative to their orientation within the cluster. We introduce a technique to monitor DM speed anisotropy using optical images of clusters and the strong gravitational lensing of background galaxies. This technology is applied to the galaxy cluster MACS J0429.6-0253, integrating our measurements of the projected mass distribution with powerful lensing observations from the Subaru Observatory. We obtain compelling evidence for an axisymmetric DM speed anisotropy in this cluster, where the frequency of the anisotropy varies with diameter. Our findings suggest that dark matter is not entirely collisionless, but retains some of its velocity history from prior times. The methods described in this paper can be extended to other galaxy clusters and even larger-scale structures in the Universe, facilitating further tests of the nature and dynamic evolution of DM.",
        "ori-fast-z-score": -0.22645540682891913,
        "water-fast-z-score": 6.340751391209736,
        "rewrite-fast-z-score": 2.038098661460272
    },
    {
        "original_text": "The stability of circumnuclear disks (CNTs) in elliptical galaxies is a topical issue in modern astronomy. Due to the relatively high gas content of these galaxies, they often have detectable rotating disks of cold gas, which are often also embedded in dust. The presence of these circumnuclear disks was discovered relatively recently. Despite its cosmological significance, the physics of the formation and the stability of these disks remains obscure. A number of researchers have claimed that CNTs are gravitationally unstable. These claims have not yet been convincingly proven either by laboratory experiments or by computer simulations. It is therefore difficult to make accurate predictions about the nature and properties of these structures. This work attempts to study the problem of gravitational stability of CNTs using modern methods of computer simulations. The study was performed using the GRaphic PAthway Code (GPU-PCM), which is a highly efficient parallel computer code for solving the equations of hydrodynamics on a grid. The experiments were performed for a model of an elliptical galaxy with a low-mass companion, and for a model with a long-term evolution. The experiments have confirmed the dynamical stability of circumnuclear disks in both cases, and the specific angular momentum of the disk material was preserved over time. The simulations also revealed the complex spatial structure of the circumnuclear disks, with a complex patchwork of nonaxisymmetric clumps. However, it was not possible to determine the existence of a firmly established fundamental mode of the system. It is therefore difficult to state unequivocally that the structures observed in the simulations are stable in the strict sense. However, it was demonstrated that the tested models had sufficient parameters to reproduce the observed properties of real galaxies with circumnuclear disks.",
        "watermark_text": "The stability of circumnuclear disks ( CNTs ) in elliptical galaxies is a topical issue in modern astronomy . Due to the unusually high gas content of these galaxies , they frequently have detectable spinning disks of cold gas , which are often also embedded in dust .The presence of these circumnuclear disks was known relatively recently . Despite its cosmological importance , the physics of the formation and the stability of these disks appears unknown .A couple of studies have claimed that CNTs are gravitationally fragile . These accusations have not already been convincingly demonstrated either by research studies or by computer simulations .It is consequently difficult to make accurate assumptions about the nature and structures of these structures . This study attempts to study the issue of gravitational stability of CNTs using contemporary methods of computer simulations .The project was done utilizing the GRaphic PAthway Code ( GPU - PCM ) , which is a highly efficient parallel computer code for solving the equations of hydrodynamics on a grid . The studies were performed for a simulation of an elliptical galaxy with a small - mass companion , and for a model with a large - term evolve .The studies have confirmed the dynamical stability of circumnuclear drives in both cases , and the specific angular velocity of the disk matter was preserved over time . The simulations additionally revealed the complex spatial shape of the circumnuclear drives , with a complex patchwork of nonaxisymmetric clumps .However , it was not could to find the existence of a firmly established fundamental mechanism of the model . It is consequently difficult to state unequivocally that the structures discovered in the simulations are stable in the strict sense .However , it was demonstrated that the tested theories had sufficient parameters to predict the known characteristics of real galaxies with circumnuclear disks .",
        "rewrite_text": "The stability of circumnuclear disks (CNTs) within elliptical galaxies is a pressing topic in modern astronomy. These galaxies, owing to their unusually high gas content, frequently exhibit detectable spinning disks of cold gas often embedded in dust. The recognition of these CNTs is relatively recent. Despite their cosmological significance, the physics behind their formation and stability remain elusive. Several studies have suggested that CNTs may be gravitationally fragile, but neither empirical research nor computer simulations have yet provided conclusive evidence to support these claims. Consequently, making definitive assumptions about the nature and structures of these disks is challenging.\n\nThis study aims to investigate the gravitational stability of CNTs using contemporary computer simulation techniques. We conducted this research utilizing the GRaphic PAthway Code (GPU-PCM), an efficient parallel computer code designed to solve hydrodynamic equations on a grid. Simulations were conducted for both a model of an elliptical galaxy with a small, low-mass companion and a larger-scale evolutionary model. Our findings confirm the dynamic stability of circumnuclear disks in both scenarios, with the specific angular velocity of disk matter remaining consistent over time.\n\nThe simulations further revealed the complex three-dimensional shape of the CNTs, featuring a patchwork of nonaxisymmetric clumps. However, we were unable to identify a firmly established fundamental mechanism within the model. Therefore, it is difficult to unequivocally assert that the structures observed in the simulations are strictly stable. Nevertheless, our tests demonstrate that the theories employed possess sufficient parameters to predict the known characteristics of real galaxies with CNTs.",
        "ori-fast-z-score": -0.7071067811865475,
        "water-fast-z-score": 7.7181105699018575,
        "rewrite-fast-z-score": 2.82842712474619
    },
    {
        "original_text": "Recent observations of distant galaxies have revealed the existence of an  unknown force  which shapes the universe. The existence of this force, known as dark energy, is confirmed by the observed accelerated expansion of the universe. This  dark energy  is thought to permeate all of space and essentially make up 68% of the energy budget of the universe. According to general relativity, the  dark energy  should cause gravity to become stronger with distance, leading to more rapid increases in velocity when observed approaching the center of a galaxy. Galaxy rotation curves have proven to be more shallow than this, possibly signaling the presence of  dark matter , with which  dark energy  interacts to produce the observed gravitational force. This  dark matter  is thought to make up 26% of the energy budget of the universe. It is possible that the observed  dark matter  and  dark energy  interact only on large scales, allowing for the possibility of  dark matter  on galactic scales but only  dark energy  beyond this. This  brane theory  has the same weak gravitational force on small scales as general relativity but different forces on large scales. Recent experiments have shown that the universe may in fact be four dimensional with matter existing on a three dimensional brane. This  brane theory  can explain both the  dark matter  and  dark energy  interactions necessary to explain galaxy rotation curves.",
        "watermark_text": "Recent measurements of distant galaxies have revealed the existence of an unknown pressure which shapes the universe . The nature of this force , known as dark energy , is confirmed by the seen accelerated expansion of the universe .This deep energy is thought to permeate all of space and essentially form up 68 % of the power budget of the universe . According to general relativity , the dark energy should cause gravitational to become stronger with distance , leading to more rapid increases in speed when observed approaching the center of a galaxy .Galaxy rotation curves have proven to be more shallow than this , possibly signaling the presence of dark matter , with which bright energy interacts to produce the observed gravity pressure . This dark matter is known to make up 26 % of the power budget of the universe .It is proposed that the seen dark matter and dark energy interact only on huge scales , allowing for the possibility of bright energy on galactic scales but only darkness energy beyond this . This brane concept has the same soft gravitational field on small scales as general relativity but different forces on small scales .Recent researchers have shown that the universe might in reality be four dimensional with matter remaining on a three dimensional brane . This brane hypothesis can describe both the dark matter and dark energy interactions necessary to explain galaxy rotation curves .",
        "rewrite_text": "Recent investigations into distant galaxies have unveiled the presence of an undiscovered pressure that molds the cosmos. This force, recognized as dark energy, is confirmed by the observed accelerated expansion of the universe. This profound energy is believed to permeate all space and constitute approximately 68% of the universe's energy budget. According to general relativity, dark energy should result in gravity becoming stronger with increasing distance, leading to more rapid increases in observed speeds when approaching the center of a galaxy. However, galaxy rotation curves indicate a shallower curve, potentially indicating the presence of dark matter, which interacts with bright energy to produce the observed gravitational pressure. This dark matter comprises 26% of the universe's energy budget. It is proposed that the observed dark matter and dark energy interact predominantly on a larger scale, suggesting bright energy on galactic scales but predominantly dark energy beyond them. This brane concept maintains a soft gravitational field on smaller scales, similar to general relativity, but exhibits different forces on smaller scales. Recent research suggests that the universe may indeed be four-dimensional with matter confined to a three-dimensional brane. This brane hypothesis effectively explains the interactions between dark matter and dark energy necessary to elucidate galaxy rotation curves.",
        "ori-fast-z-score": 1.0169503597462533,
        "water-fast-z-score": 7.118652518223773,
        "rewrite-fast-z-score": 2.945838777274635
    },
    {
        "original_text": "The pseudospectrum of a system of semiclassical operators is the complement of the spectrum in the closed sectoral cone of the complexification of the phase space of the system. The pseudospectrum may have a fractal structure with numerous self-similarities. This is demonstrated for the harmonic oscillator on the real line with smooth polynomial potentials. In particular, this implies that the Kolmogorov complexity of the pseudospectrum may not be efficiently computable. For systems whose phase space is either the torus or the cylinder, explicit formulas are obtained for the pseudospectrum. The pseudospectrum can be approximated numerically with arbitrary accuracy by considering finite-dimensional truncations of the system. This is used to prove the existence of resonances in the pseudospectrum, i.e. to show that the widths of some spectral bands may become zero. A practical algorithm to find resonances is also presented.",
        "watermark_text": "The pseudospectrum of a system of semiclassical operators is the complement of the spectrum in the shut sectoral cone of the complexification of the phase space of the system . The pseudospectrum could have a fractal structure with many self - similarities .This is demonstrated for the harmonic oscillator on the real line with smooth polynomial potentials . In particular , this implies that the Kolmogorov dimension of the pseudospectrum must not be smoothly computable .For systems whose phase space is either the torus or the cylinder , explicit formulas are derived for the pseudospectrum . The pseudospectrum can be approximated numerically with arbitrary precision by analyzing finite - dimensional truncations of the system .This is applied to test the existence of resonances in the pseudospectrum , i . e . to find that the widths of some spectral lines sometimes become zero .A practical algorithm to find resonances is also presented .",
        "rewrite_text": "The pseudospectrum of a system comprising semiclassical operators is the complement of its spectrum within the closed sectoral cone formed by the complexification of the system's phase space. This pseudospectrum may exhibit a fractal structure with numerous self-similarities. This is exemplified by the harmonic oscillator on the real line with smoothly varying polynomial potentials. Specifically, this suggests that the Kolmogorov dimension of the pseudospectrum cannot be smoothly computed. For systems with phase spaces that are either tori or cylinders, explicit formulas have been derived for the pseudospectrum. Numerically, the pseudospectrum can be approximated to any desired precision by analyzing finite-dimensional truncations of the system. This is utilized to test the existence of resonances within the pseudospectrum, i.e., to identify cases where the widths of certain spectral lines approach zero. Additionally, a practical algorithm for detecting resonances is presented.",
        "ori-fast-z-score": 0.14002800840280097,
        "water-fast-z-score": 3.500700210070024,
        "rewrite-fast-z-score": 1.2135597524338357
    },
    {
        "original_text": "The LBNE Collaboration has studied the potential of the long-baseline neutrino experiment to determine whether or not short-baseline muon neutrino disappearance is caused by neutrino oscillations. We find that such an observation would be highly statistically significant and have a good chance of making a discovery. We determine projected sensitivities to total mixing, inverted mass hierarchy, and CP violation in the MNS matrix. We also discuss potential spinoff experiments, which could confirm or exclude the null result with higher statistics, and address experimental challenges. Finally, we compare the LBNE proposal to other long-baseline neutrino experiments and to proposed neutrino sources. We find that the LBNE proposal has excellent potential to make a discovery and set strong constraints on neutrino parameters. The Long-Baseline Neutrino Experiment (LBNE) is a proposed neutrino physics program at the Sanford Underground Facility (SONGS) in South Dakota, USA, and the Soudan Mine in Minnesota, USA. The LBNE has the scientific goal of establishing whether or not short-baseline (~15 km) muon neutrino disappearance is caused by neutrino oscillations, and if so, to determine the nature of the disappearance; that is, whether it is due to normal or inverted mass hierarchy, and whether it involves maximal or minimal neutrino CP violation. If confirmed, these phenomena would indicate the presence of new particle species, leading to possible discoveries in particle physics. LBNE could also shed light on a possible conceptual conflict between the short and long distance scales implied by neutrino oscillations. The LBNE has a two-detector configuration using massive 200 kt-Trititanium (TT) detectors with a baseline of 1210 to 1610 km. It has been shown that this configuration would be sensitive to total neutrino mixing < 13% C.L. at the 3x10-3 baseline resolution, and to neutrino mass-ordering and CP violation at the 5x10-2 to 5x10-3 C.L. levels. We describe the determination of projected sensitivities to neutrino parameters using a Feldman-Cousins method that includes systematic uncertainties. This study is preliminary and does not include a cost-benefit or risk assessment. We conclude with a discussion of proposed spinoff experiments that would confirm or rule out the null result with higher statistics and discuss experimental challenges.",
        "watermark_text": "The LBNE Collaboration has examined the possibilities of the long - baseline neutrino experiment to find whether or not short - baseline muon neutrino disappearance is caused by neutrino oscillations . We see that such an observation may be highly statistically substantial and have a better opportunity of making a discovery .We determine estimated sensitivities to total mixing , inverted mass hierarchy , and CP violation in the MNS matrix . We also discuss possible spinoff tests , which could verify or exclude the null effect with higher statistics , and address research challenges .Finally , we compare the LBNE project to other long - baseline neutrino experiments and to proposed neutrino sources . We see that the LBNE project has excellent potential to make a discovery and set strong restrictions on neutrino variables .The Long - Baseline Neutrino Experiment ( LBNE ) is a planned neutrino physics program at the Sanford Underground Facility ( SONGS ) in South Dakota , USA , and the Soudan Mine in Minnesota , USA . The LBNE has the science goal of establishing whether or not short - baseline ( ~ 15 cm ) muon neutrino disappearance is caused by neutrino oscillations , and if so , to judge the nature of the disappearance ; that is , whether it is due to normal or inverted mass hierarchy , and whether it includes maximal or minimal neutrino CP interference .If confirmed , these phenomena would indicate the presence of new particle species , leading to possible discoveries in particle theory . LBNE could also shed light on a possible conceptual dispute between the short and long distance scales implied by neutrino oscillations .The LBNE has a two - detector setup using giant 200 kt - Trititanium ( TT ) detectors with a baseline of 1210 to 1610 km . It has been shown that this configuration would be sensitive to total neutrino composition < 13 % C . L .at the 3x10 - 3 baseline resolution , and to neutrino mass - ordering and CP violation at the 5x10 - 2 to 5x10 - 3 C . L . levels .We describe the determination of projected sensitivities to neutrino variables using a Feldman - Cousins method that contains systematic uncertainties . This study is tentative and does not include a price - benefit or risk assessment .We end with a debate of suggested spinoff tests that would confirm or limit out the null effect with higher statistics and address research challenges .",
        "rewrite_text": "The LBNE Collaboration has explored the potential of long-baseline neutrino experiments to investigate whether the disappearance of short-baseline muon neutrinos is linked to neutrino oscillations. We believe that such observations could possess significant statistical significance and offer a better chance for discovery. We have estimated the sensitivities to total mixing, inverted mass hierarchy, and CP violation in the MNS matrix. Furthermore, we discuss potential follow-up tests that could verify or rule out the null effect with enhanced statistics, addressing associated research challenges.\n\nIn comparison to other long-baseline neutrino experiments and proposed neutrino sources, the LBNE project stands out with its exceptional potential for making discoveries and establishing stringent constraints on neutrino parameters. The Long-Baseline Neutrino Experiment (LBNE) is a planned neutrino physics program based at the Sanford Underground Facility (SONGS) in South Dakota and the Soudan Mine in Minnesota, USA. Its scientific objective is to determine whether the disappearance of short-baseline (approximately 15 cm) muon neutrinos is indeed caused by neutrino oscillations and, if so, to determine their nature. This involves determining whether it is due to a normal or inverted mass hierarchy and whether it involves maximal or minimal neutrino CP interference. If confirmed, these phenomena would indicate the existence of new particle species, potentially leading to breakthroughs in particle theory.\n\nMoreover, LBNE could clarify a conceptual dispute regarding the relationship between short and long distance scales implied by neutrino oscillations. The experiment employs a two-detector setup utilizing giant 200 kt Trititanium (TT) detectors with a baseline of 1210 to 1610 km. It has been demonstrated that this configuration would be sensitive to variations in the total neutrino composition at a resolution of 3x10^-3 with less than 13% systematic uncertainty, as well as to neutrino mass ordering and CP violation at levels ranging from 5x10^-2 to 5x10^-3 with similar uncertainty levels.\n\nWe have determined the projected sensitivities to neutrino variables using a Feldman-Cousins method that accounts for systematic uncertainties. While this study is preliminary and does not include a cost-benefit or risk assessment, it provides valuable insights into the potential of the LBNE project. Finally, we discuss proposed follow-up tests that could strengthen or limit the null effect through enhanced statistics and address outstanding research challenges.",
        "ori-fast-z-score": 2.0619652471058063,
        "water-fast-z-score": 9.206343289100008,
        "rewrite-fast-z-score": 3.6589450375591577
    },
    {
        "original_text": "Object classification is a key step in difference imaging, the study of galaxies when they were younger than they are now. Historically, this has been a laborious process that has limited the kinds of galaxies that have been studied. We introduce a technique for object classification that both increases the efficiency with which we can study galaxies and improves the reliability of our results. The technique uses a convolutional neural network trained on high-quality images of galaxies to classify low-quality difference imaging images of the same galaxies. This allows us to identify candidate galaxies and to reject images of stars that otherwise might have been incorrectly labeled as galaxies. Our method improves the reliability of our results and significantly increases the number of galaxies that we can study. As a test, we apply our method to the difference imaging survey of a field in the southern sky, K2, identifying 80% as galaxies instead of the approximately 50% that are identified by eyeball inspection. The resulting catalog will be useful for studying the evolution of the population of galaxies as a function of redshift and will aid in our understanding of galaxy morphology and population statistics. The technique described here is not limited to difference imaging. Similar methods can be applied to other contexts in which object classification is a necessary preliminary step.",
        "watermark_text": "Object classification is a key step in difference imaging , the examination of galaxies when they were younger than they are now . Historically , this has been a laborious method that has restricted the kinds of galaxies that have been studied .We introduce a technique for object classification that both improve the accuracy with which we can investigate stars and improves the accuracy of our findings . The technique utilizes a convolutional neural network trained on well - quality pictures of clusters to classify low - quality difference imaging photographs of the same galaxies .This enables us to identify candidate galaxies and to reject images of stars that otherwise might have been incorrectly labeled as galaxies . Our method improves the accuracy of our findings and substantially increases the quantity of stars that we can research .As a test , we apply our technique to the difference imaging survey of a field in the southern sky , K2 , identifying 80 % as galaxies instead of the approximately 50 % that are identified by eyeball examination . The resulting inventory will be valuable for studying the evolution of the population of stars as a function of redshift and will aid in our analysis of galaxy shape and population statistics .The technique mentioned here is not limited to distinction analysis . Similar techniques can be applied to other contexts in which image designation is a necessary preliminary step .",
        "rewrite_text": "In difference imaging, object classification plays a pivotal role, especially when examining galaxies in their earlier stages of development. Historically, this process has been labor-intensive and has limited the scope of galaxies that can be studied. We present a technique that not only enhances the accuracy of star investigation but also improves the reliability of our findings. This technique utilizes a convolutional neural network, trained on high-quality cluster images, to classify low-quality difference imaging photographs of the same galaxies.\n\nThis allows us to identify potential galaxy candidates and eliminate images of stars that might otherwise be mistakenly labeled as galaxies. Our method enhances the accuracy of our research outcomes and significantly increases the number of stars we can investigate. As a test case, we applied our technique to a difference imaging survey of a field in the southern sky, K2, where we identified 80% as galaxies instead of the approximate 50% identified through visual examination. The resulting dataset will be invaluable for studying the evolution of star populations as a function of redshift, assisting in our analysis of galaxy shape and population statistics. Furthermore, the technique mentioned here is not confined to distinction analysis; similar techniques can be applied to other contexts where image classification is a necessary preliminary step.",
        "ori-fast-z-score": -0.39605901719066977,
        "water-fast-z-score": 6.138914766455381,
        "rewrite-fast-z-score": 0.6704783996548059
    },
    {
        "original_text": "This letter investigates the transient behavior of surface plasmon polaritons (SPPs) scattered at a subwavelength groove. The SPP scattering is modeled as a two-dimensional waveguide grating with a finite longitudinal dimension. It is shown that the SPP fields are primarily distributed in the groove and gradually decay to zero in the distance of lambda/2n where n is the refractive index of surrounding medium. The SPP fields decay as a free-space exponential function in the long distance and the time scale is determined by the dimensions of the groove. The temporal evolution of the SPP field is in-line with that of the transverse component of the electric field of the incident pulse, showing the dominant role of the SPP in the scattering process. The results provide useful information on SPP scattering at subwavelength scale and can be utilized to optimize SPP-based devices. Authors: Li Jin and Gang Wang Date published: 2020-07-25 Transient behavior of surface plasmon polaritons scattered at a subwavelength groove Abstract: This letter investigates the transient behavior of surface plasmon polaritons (SPPs) scattered at a subwavelength groove. The SPP scattering is modeled as a two-dimensional waveguide grating with a finite longitudinal dimension. It is shown that the SPP fields are primarily distributed in the groove and gradually decay to zero in the distance of lambda/2n where n is the refractive index of surrounding medium. The SPP fields decay as a free-space exponential function in the long distance and the time scale is determined by the dimensions of the groove. The temporal evolution of the SPP field is in-line with that of the transverse component of the electric field of the incident pulse, showing the dominant role of the SPP in the scattering process. The results provide useful information on SPP scattering at subwavelength scale and can be utilized to optimize SPP-based devices. Disclaimer: This is a long abstract. You should read the full text paper to get the complete picture.",
        "watermark_text": "This letter investigates the transient activity of surface plasmon polaritons ( SPPs ) scattered at a subwavelength groove . The SPP scattering is modeled as a two - dimensional waveguide grating with a finite radial dimension .It is demonstrated that the SPP fields are exclusively distributed in the groove and gradually decay to zero in the distance of lambda / 2n where n is the refractive index of neighbouring medium . The SPP fields decline as a free - space exponential function in the long distance and the time scale is chosen by the sizes of the groove .The temporal evolution of the SPP field is in - line with that of the transverse component of the electric field of the incident pulse , showing the dominant role of the SPP in the scattering process . The results provide useful details on SPP absorption at subwavelength scale and can be used to optimize SPP - based devices .Authors : Li Jin and Gang Wang Date publication : 2020 - 07 - 25 Transient activity of surface plasmon polaritons scattered at a subwavelength groove Abstract : This text investigates the transient activity of surface plasmon polaritons ( SPPs ) scattered at a subwavelength groove . The SPP scattering is modeled as a two - dimensional waveguide grating with a finite transverse dimension .It is demonstrated that the SPP fields are exclusively distributed in the groove and gradually decay to zero in the distance of lambda / 2n where n is the refractive index of neighbouring medium . The SPP fields decline as a free - space exponential function in the long distance and the time scale is chosen by the sizes of the groove .The temporal evolution of the SPP field is in - line with that of the transverse component of the electric field of the incident pulse , showing the dominant role of the SPP in the scattering process . The results provide useful details on SPP absorption at subwavelength scale and can be used to optimize SPP - based devices .Disclaimer : This is a lengthy abstract . You should see the full text paper to getting the complete picture .",
        "rewrite_text": "This study explores the transient behavior of surface plasmon polaritons (SPPs) scattered at a subwavelength groove. The SPP scattering is represented as a two-dimensional waveguide grating with a finite radial dimension. It has been demonstrated that the SPP fields are exclusively confined within the groove and gradually diminish to zero within a distance of lambda/2n, where n represents the refractive index of the adjacent medium. These SPP fields decline exponentially in free space as a function of distance, with the temporal scale influenced by the dimensions of the groove. The temporal evolution of the SPP field aligns with the transverse component of the electric field in the incident pulse, highlighting the prominent role of SPPs in the scattering process.\n\nThe findings offer valuable insights into SPP absorption at the subwavelength scale, which can be utilized to optimize SPP-based devices.\n\nAuthors: Li Jin and Gang Wang. Publication Date: July 25th, 2020.\n\nAbstract: This abstract investigates the transient activity of surface plasmon polaritons (SPPs) scattered at a narrow groove with subwavelength dimensions. The SPP scattering is modeled as a two-dimensional waveguide grating with a finite transverse width. It has been shown that the SPP fields are predominantly distributed within the groove and gradually fade to zero within a specific distance determined by the relationship of lambda/2n and the refractive index of adjacent media. Over longer distances, these SPP fields decline exponentially in free space, with the temporal scale determined by the size of the groove. The temporal development of the SPP field aligns with the electric field's transverse component in the incoming pulse, highlighting the central role of SPPs in the scattering process.\n\nThis abstract provides detailed information on SPP absorption at the subwavelength scale, which can be used to enhance and optimize SPP-based technologies. Please refer to the full text of the paper for a comprehensive understanding.",
        "ori-fast-z-score": -0.8703882797784892,
        "water-fast-z-score": 4.351941398892446,
        "rewrite-fast-z-score": 1.0690449676496976
    },
    {
        "original_text": "A search for radiative decays of the Upsilon(1S) meson into eta and eta  has been performed with the full dataset of the L3+00 experiment at the LEP collider. No evidence for these decays has been found, and upper limits at the 90% confidence level of Br(Upsilon(1S)-->eta) < 5.0x10(-8) and Br(Upsilon(1S)-->eta ) < 2.5x10(-8) are obtained. These results improve previous ones from the L3 and OPAL experiments. The L3+00 experiment, which was performed at the LEP collider, is described. The data sample consists of 202 x 10(exp - 7) positive mesons, collected from 1996 to 2000, corresponding to an integrated luminosity of 208 pb(s). The results of a search for the radiative decays of the Upsilon(1S) meson are presented. No evidence for these decays has been found, and upper limits at the 90% confidence level of Br(Upsilon(1S)-->eta) < 5.0x10(-8) and Br(Upsilon(1S)-->eta ) < 2.5x10(-8) are obtained.",
        "watermark_text": "A search for radiative decays of the Upsilon ( 1S ) meson into eta and eta has been performed with the full dataset of the L3 + 00 experiment at the LEP collider . No evidence for these decays has been shown , and upper limits at the 90 % confidence rate of Br ( Upsilon ( 1S ) - - > eta ) < 5 . 0x10 ( - 8 ) and Br ( Upsilon ( 1S ) - - > eta ) < 2 . 5x10 ( - 8 ) are found .These data improve previous ones from the L3 and OPAL experiments . The L3 + 00 experiment , which was done at the LEP collider , is mentioned .The data specimen consists of 202 x 10 ( exp - 7 ) positive mesons , obtained from 1996 to 2000 , corresponding to an integrated luminosity of 208 pb ( s ) . The results of a search for the radiative decays of the Upsilon ( 1S ) meson are presented .No evidence for these decays has been determined , and upper limits at the 90 % confidence rate of Br ( Upsilon ( 1S ) - - > eta ) < 5 . 0x10 ( - 8 ) and Br ( Upsilon ( 1S ) - - > eta ) < 2 . 5x10 ( - 8 ) are derived .",
        "rewrite_text": "An investigation into the radiative decays of the Upsilon (1S) meson into eta particles has been conducted using the comprehensive dataset from the L3 + 00 experiment at the LEP collider. No definitive evidence of these decays has been found, and upper limits for the branching ratios at a 90% confidence level have been established: Br (Upsilon (1S) → eta) < 5.0 x 10(-8) and Br (Upsilon (1S) → eta) < 2.5 x 10(-8). These findings improve upon previous data obtained from the L3 and OPAL experiments. It is worth noting that the L3 + 00 experiment was conducted at the LEP collider. The dataset comprises 202 x 10(exp - 7) positive mesons, collected between 1996 and 2000, corresponding to an integrated luminosity of 208 pb(s). The results of this search for radiative decays of the Upsilon (1S) meson are presented without any confirmed evidence, and the aforementioned upper limits have been derived accordingly.",
        "ori-fast-z-score": -0.4931969619160719,
        "water-fast-z-score": 3.452378733412503,
        "rewrite-fast-z-score": -1.179535649239177
    },
    {
        "original_text": "In this paper, we study dynamical properties of fluid of platelike colloidal particles. This subject has been recently studied experimentally  1-3 . In our study, we apply computer simulation techniques. We consider bidisperse mixture of platelike and spherical colloidal particles. We perform event-driven molecular dynamics simulations. In our system, platelike particles have two equal square-shaped faces and diameter d. Spherical particles have a diameter d/2. We study dynamical properties of our system in the range of volume fraction 0.621 ≤ϕ≤ 0.895 and averaged number of platelike particles NPl=100, 200, and 300. In this range of ϕ and NPl, we study transients and long-time dynamical behavior of self-intermediate scattering function S(q,t) and pair-intermediate scattering function S(q,t) for different values of q. We observe dynamical behaviour including exponential decay of transients, quasiequilibration and growing of correlation functions toward exact hydrodynamics at long times.",
        "watermark_text": "In this paper , we study dynamical properties of fluid of platelike colloidal particles . This topics has been lately studied experimentally 1 - 3 .In our research , we apply computer model strategies . We consider bidisperse mixture of platelike and spherical colloidal particles .We operate event - motivated molecular mechanics simulations . In our system , platelike particles have two equal square - shaped faces and size d . Spherical particles have a diameter d / 2 .We research dynamical properties of our system in the range of volume fraction 0 . 621 [UNK] 0 . 895 and averaged number of platelike particles NPl = 100 , 200 , and 300 . In this range of [UNK] and NPl , we study transients and long - time dynamical behavior of self - intermediate absorption vector S ( q , t ) and pair - intermediate scattering function S ( q , t ) for different values of q .We see dynamical behaviour including exponential decay of transients , quasiequilibration and expansion of correlation functions toward complete hydrodynamics at long periods .",
        "rewrite_text": "In this study, we investigate the dynamic properties of a fluid composed of platelike colloidal particles. This topic has gained recent experimental attention, specifically in studies 1 through 3. Our research utilizes computer modeling techniques, focusing on a bidisperse mixture of platelike and spherical colloidal particles. We perform event-driven molecular mechanics simulations.\n\nIn our system, the platelike particles possess two equal square-shaped faces with a size of 'd'. Spherical particles, on the other hand, have a diameter of 'd/2'. We examine the dynamic properties of our system within a range of volume fractions from 0.621 to 0.895, and with an average number of platelike particles set at NPl = 100, 200, and 300. Within this range of volume fractions and NPl values, we explore the transient and long-term dynamic behavior of the self-intermediate absorption vector S(q, t) and the pair-intermediate scattering function S(q, t) for various q values.\n\nOur observations indicate dynamic behavior that includes exponential decay of transients, quasiequilibrium, and the expansion of correlation functions towards complete hydrodynamic behavior over extended periods of time.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 5.093248125762992,
        "rewrite-fast-z-score": 1.193117518002609
    },
    {
        "original_text": "The quantum vacuum is a magical place where particles pop in and out of existence in a probabilistic fashion. The behavior of such quantum fields underlie all known interactions. In particular, their exchange leads to the stability of the Standard Model (SM) electric charge, the spin of the particles, and the form of the interactions. If we extrapolate the particle spectrum of the SM to the lowest possible energies, we expect the particle exchanges to involve gravitons, which could explain the observed gravity, and, possibly, also photons and fermions. In this work, we discuss the first option. Gravitons are the expected carrier of the gravitational interaction, which otherwise feels indirect through intermediate messengers. Photons and fermions are the two most massive particles in the SM. Therefore, it seems plausible that they could be the corresponding force carriers. However, the fact that they are spin-1/2 fermions rather than spin-2 bosons has so far prevented them from being observed. In this work, we propose a way for photons and fermions to interact directly, through a new Lorentz- and CPT-violating term in the Standard Model Lagrangian. We present a way to test this proposal in current and future colliders. Gravitons carry both the gravitational and Lorentz-violation interactions, leading to a unique signature of completely missing particles, with two corresponding jets emerging at high invariant mass. Photons and fermions only carry the gravitational interaction. Therefore, their phenomenological implications are different. Photons and fermions can decay to gravitons, so the signature would be two jets plus missing energy. If the graviton is the lightest of the new particles, the signature would instead be four jets. An excess of any of these signatures would be a hint of the underlying theory, which could also give an explanation for dark matter and the strong and weak forces.",
        "watermark_text": "The quantum vacuum is a mysterious location where objects pop in and out of existence in a probabilistic fashion . The behavior of such quantum fields underlie all known interactions .In particular , their exchange leads to the stability of the Standard Model ( SM ) electric current , the spin of the molecules , and the form of the interactions . If we extrapolate the particle spectrum of the SM to the lowest available energies , we expect the particle exchanges to contain gravitons , which could explain the seen gravity , and , possibly , also photons and fermions .In this research , we explain the first choice . Gravitons are the expected carrier of the gravitational interaction , which otherwise feels indirect through intermediate messengers .Photons and fermions are the two most large particles in the SM . Therefore , it appears probable that they may be the equivalent force carriers .However , the fact that they are spin - 1 / 2 fermions instead than spin - 2 bosons has so far kept them from being detected . In this research , we propose a way for photons and fermions to interact directly , through a new Lorentz - and CPT - violating term in the Standard Model Lagrangian .We see a way to test this proposal in current and future colliders . Gravitons carry both the gravitational and Lorentz - violation interactions , leading to a unique signature of almost dead particles , with two corresponding jets emerging at high invariant mass .Photons and fermions only carry the gravitational interaction . Therefore , their phenomenological consequences are distinct .Photons and fermions can evolve to gravitons , so the signature might be two jets plus missing energy . If the graviton is the lightest of the new objects , the signature might instead be four jets .An excess of any of these signatures must be a hint of the fundamental theory , which could also make an reason for black material and the strong and weak fields .",
        "rewrite_text": "The quantum vacuum is an enigmatic realm where objects appear and disappear in a probabilistic manner. The behavior of quantum fields underpins all known interactions. Specifically, their exchange gives rise to the stability of the Standard Model's (SM) electric current, the spin of molecules, and the nature of interactions. If we extend the particle spectrum of the SM to the lowest available energies, we anticipate that particle exchanges will involve gravitons, which could elucidate observed gravity, along with photons and fermions.\n\nIn this research, we delve into the first of these possibilities. Gravitons are expected to be the carriers of gravitational interaction, which otherwise appears indirect through intermediary messengers. Photons and fermions, the two largest particles in the SM, are likely to be equivalent force carriers. However, their spin-1/2 fermion nature, rather than the spin-2 boson nature, has thus far prevented their detection.\n\nWe propose a method for photons and fermions to interact directly in this research, utilizing a new term in the Standard Model Lagrangian that violates Lorentz and CPT symmetries. We identify a means to test this proposal in current and future particle colliders. Gravitons encompass both gravitational and Lorentz-violating interactions, resulting in a distinctive signature of nearly inert particles with two corresponding high-mass jets emerging. In contrast, photons and fermions only carry gravitational interactions, leading to distinct phenomenological consequences.\n\nPhotons and fermions can evolve into gravitons, resulting in signatures of two jets with missing energy. If the graviton is the lightest of the new particles, the signature may instead manifest as four jets. An excess of any of these signatures could be a telltale sign of the fundamental theory, which could also explain phenomena like black holes and the strong and weak fields.",
        "ori-fast-z-score": 0.08804509063256238,
        "water-fast-z-score": 5.89902107238168,
        "rewrite-fast-z-score": 1.8935062328016077
    },
    {
        "original_text": "Recent spectropolarimetric observations of Balmer-dominated shocks have revealed the presence of a so-called transition zone with characteristics in-between the pre-shock and post-shock regions. The characteristics of this zone are dependent on the properties of the incoming flow, such as the Mach number and the pre-shock magnetic field strength, as well as on the shock strength. In this paper, we present the results of 1D numerical simulations of Balmer-dominated shocks in the strong shock limit, which enable us to characterize the nature of the transition zone and to compute its physical properties as a function of the shock parameters. We find that in strong shock limit, the width of the transition zone scales as the distance between the centers of the pre-shock and shock zone, while its temperature and velocity dispersion scale as the square root of the corresponding pre-shock values. Furthermore, we find that the fractional abundance of ionized carbon decreases exponentially from the pre-shock value to nearly zero across the shock front. We provide scaling relations which can be used to approximate the physical properties of the transition zone in between the pre-shock and post-shock regions, on the basis of the knowledge of the physical parameters of the incoming flow and of the shock.",
        "watermark_text": "Recent spectropolarimetric experiments of Balmer - dominated shocks have revealed the presence of a so - called transition region with characteristics in - between the pre - shock and post - shock zone . The traits of this zone are dependent on the properties of the outgoing flow , such as the Mach number and the pre - jolt magnetic force resistance , as well as on the shock intensity .In this paper , we present the conclusion of 1D numerical simulations of Balmer - dominated shocks in the strong shock limit , which enable us to characterize the nature of the transition region and to compute its physical properties as a function of the shock parameters . We see that in strong shock limit , the length of the transition region scales as the distance between the centers of the pre - jolt and jolt zone , while its temperature and speed dispersion scale as the square root of the associated pre - shock parameters .Furthermore , we find that the fractional quantity of ionized carbon decreases exponentially from the pre - shock value to virtually zero across the shock front . We derive scaling relations which can be used to approximate the physical properties of the transition region in between the pre - jolt and post - jolt zones , on the basis of the knowledge of the physical conditions of the outgoing flow and of the shock .",
        "rewrite_text": "Recent spectropolarimetric experiments focusing on Balmer-dominated shocks have unveiled the existence of a transition region, characterized by properties bridging the pre-shock and post-shock zones. The characteristics of this zone are influenced by various factors such as the properties of the outgoing flow, including the Mach number and the resistance of the pre-jolt magnetic force, as well as the intensity of the shock itself.\n\nIn this paper, we present the conclusions from one-dimensional numerical simulations of Balmer-dominated shocks in the strong shock limit. These simulations enable us to characterize the nature of the transition region and compute its physical properties as a function of shock parameters. Our findings indicate that, in the strong shock limit, the length of the transition region scales with the distance between the centers of the pre-jolt and jolt zones. Meanwhile, its temperature and velocity dispersion scale in proportion to the square root of the associated pre-shock parameters.\n\nFurthermore, we observe that the fractional amount of ionized carbon decreases exponentially from its pre-shock value to nearly zero across the shock front. We have derived scaling relations that can be used to approximate the physical properties of the transition region between the pre-jolt and post-jolt zones, based on our understanding of the physical conditions of the outgoing flow and the shock itself.",
        "ori-fast-z-score": -0.40406101782088427,
        "water-fast-z-score": 5.527707983925667,
        "rewrite-fast-z-score": 2.913857587071792
    },
    {
        "original_text": "Nucleon density and momentum distributions in nuclei provide unique information on the spatial distribution of nucleonic momenta and currents in atomic nuclei. In the standard analysis, the center-of-mass (c.m.) correction is applied sequentially after the removal of the nucleon resolution function. Here, we present a method that combines these two corrections in one step, thus termed simultaneous center-of-mass (s.c.m.). The s.c.m. correction improves the agreement of theoretical predictions with experimental data for a wide range of nuclei and distributions. The accuracy of the new method is illustrated by a simultaneous description of the deuteron and W(d,2p) distributions. The method can be readily applied to other nuclear reactions and extends the applicability of nuclear reaction theory to regions of current interest, such as the studies of spin-isospin excitations of nuclei and structure changes during the evolution of nuclear systems in collisions. The method can be downloaded from https://arxiv.org/abs/1906.09044.",
        "watermark_text": "Nucleon density and momentum distributions in nuclei offer special data on the spatial distribution of nucleonic momenta and currents in nuclear nuclei . In the standard estimate , the center - of - mass ( c . m . )correction is applied sequentially after the removal of the nucleon resolution parameter . Here , we present a technique that combines these two corrections in one step , thus termed simultaneous center - of - mass ( s . c . m . ) .The s . c . m . correction improves the compliance of theoretical estimates with observation information for a broad variety of nuclei and distributions .The accuracy of the new method is highlighted by a simultaneous model of the deuteron and W ( d , 2p ) distributions . The method can be freely applied to other nuclear compounds and expanded the applicability of nuclear response physics to domains of present interest , such as the investigations of spin - isospin excitations of nuclei and shape differences during the evolution of nuclear systems in collisions .The method can be downloaded from https://arxiv.org/abs/1906.09044.",
        "rewrite_text": "The spatial distribution of nucleonic momenta and currents within nuclear nuclei is revealed through the examination of nucleon density and momentum distributions. In the conventional estimation process, the center-of-mass (c.m.) correction is sequentially applied after eliminating the nucleon resolution parameter. We present a technique that merges these two corrections into a single step, dubbed the simultaneous center-of-mass (s.c.m.) correction. This method enhances the alignment between theoretical predictions and observational data for a wide range of nuclei and distributions. Its efficacy is underscored by a concurrent model of the deuteron and W (d, 2p) distributions. This approach can be flexibly applied to other nuclear compounds, broadening the scope of nuclear response physics in areas of current interest, such as the study of spin-isospin excitations in nuclei and shape variations during the evolution of nuclear systems in collisions. The method is freely accessible at https://arxiv.org/abs/1906.09044.",
        "ori-fast-z-score": 0.6974858324629157,
        "water-fast-z-score": 6.599663291074444,
        "rewrite-fast-z-score": 0.6201736729460423
    },
    {
        "original_text": "We present late-time X-ray observations of the nearby Type Ibc core-collapse supernova 2018crp with the Chandra X-ray Observatory and review its observational and modeling signatures. We measure an X-ray luminosity of (4.6 ± 1.0) × 1033 erg s−1 at a Galactocentric distance of 4.4 ± 0.3 kpc, assuming the distance to the Large Magellanic Cloud is 50 kpc. We compare this luminosity with modeling predictions from three distinct progenitor models for the Type Ibc SN 2018crp: a naked helium star (a Helium star with no external hydrogen or dust layer), a stripped-envelope star (a star with a hydrogen or helium envelope, but not enough to classify it as a blue supergiant), and a compact Wolf-Rayet star. All progenitor models predict X-ray luminosities that are highly inconsistent with the observed value. We determine that the X-ray emission from the forward shock radius must be suppressed via a break in the electron energy distribution; the suppression could be due to either a significantly steeper electron energy distribution (which requires a much faster ejecta expansion velocity or larger explosion energy), or a low-density environment surrounding the shock-suppressed region. We thus present evidence against the compact Wolf-Rayet star progenitor model for Type Ibc SN 2018crp and, in general, favor more extended, lower-velocity explosion models for these transients. Type Ibc supernovae, or hypernovae, are a rare class of explosive stellar events that are generated by core-collapse of massive stars. Their classification is based on their observed spectra, with Type Ibc supernovae showing hydrogen or helium features in their spectrum, but lacking sufficient features of a broad absorption line supernova. The recent discoveries of long gamma-ray bursts (GRBs), also known as hypernovae, have opened up new possibilities to probe their origin. Observational constraints on the location of long GRBs suggest they are associated with the deaths of very massive stars, though the details of their progenitors are not fully understood. Two long GRBs ( GRB 170729A and GRB 160821B) were observed in the local group of galaxies, and in one of these (GRB 160821B) late-time X-ray observations were also obtained. Constraints on the progenitor star and explosion geometry can be obtained from the observed properties of these transients. For a few weeks to months following the GRB explosion, the X-ray, ultraviolet, and optical emission are powered by the forward shockwave from the escaping ejecta interacting with the circumstellar medium (CSM). This emission can be used to measure the radial distance of the shockwave from the explosion center as a function of time, the so-called light curve, and to obtain the total energy of the explosion. At later times the emission is expected to become",
        "watermark_text": "We present late - time X - ray observations of the nearby Type Ibc core - collapse supernova 2018crp with the Chandra X - ray Observatory and review its observational and modeling signatures . We calculate an X - ray luminosity of ( 4 . 6 ± 1 . 0 ) × 1033 erg s−1 at a Galactocentric distance of 4 . 4 ± 0 . 3 kpc , assuming the distance to the Large Magellanic Cloud is 50 kpc .We link this luminosity with modeling predictions from three separate progenitor models for the Type Ibc SN 2018crp : a naked helium star ( a Helium star with no external hydrogen or dust coating ) , a stripped - envelope star ( a star with a hydrogen or helium envelope , but not sufficiently to classify it as a blue supergiant ) , and a compact Wolf - Rayet star . All progenitor models predict X - ray luminosities that are extremely inconsistent with the observed value .We determine that the X - ray radiation from the front shock radius may be suppressed via a break in the electron energy flow ; the suppression could be due to either a significantly steeper electron energy flow ( which requires a far faster ejecta expansion velocity or larger explosion power ) , or a high - density environment covering the shock - suppressed region . We consequently present evidence against the compact Wolf - Rayet star progenitor model for Type Ibc SN 2018crp and , in general , favor more extended , lesser - speed explosion methods for these transients .Type Ibc supernovae , or hypernovae , are a rare class of explosive stellar phenomena that are produced by core - collapse of large stars . Their classification is based on their observed spectra , with Type Ibc supernovae showing carbon or helium features in their spectrum , but lacking sufficient features of a broad absorption line supernova .The recent discoveries of long beta - ray clusters ( GRBs ) , sometimes called as hypernovae , have opened up new possibilities to probe their source . Observational restrictions on the location of large GRBs confirm they are related with the deaths of very huge stars , though the details of their progenitors are not entirely explained .Two long GRBs ( GRB 170729A and GRB 160821B ) were detected in the local group of galaxies , and in one of these ( GRB 160821B ) mid - time X - ray observations were also produced . Constraints on the progenitor star and explosion geometry can be obtained from the seen characteristics of these transients .For a few weeks to weeks following the GRB accident , the X - ray , ultraviolet , and optical emission are powered by the forward shockwave from the escaping ejecta interacting with the circumstellar medium ( CSM ) . This absorption can be used to measure the longitudinal length of the shockwave from the explosion center as a function of time , the so - called light spiral , and to obtain the total energy of the explosion .At later days the emission is expected to become",
        "rewrite_text": "We present enhanced, late-time X-ray observations of the nearby Type Ibc core-collapse supernova 2018crp utilizing the Chandra X-ray Observatory. We examine its observational and modeling signatures, deriving an X-ray luminosity of (4.6 ± 1.0) × 1033 erg s−1 at a Galactocentric distance of 4.4 ± 0.3 kpc, assuming a 50 kpc distance to the Large Magellanic Cloud.\n\nWe correlate this luminosity with predictions from three distinct progenitor models for Type Ibc SN 2018crp: a naked helium star, a stripped-envelope star, and a compact Wolf-Rayet star. All models predict X-ray luminosities that significantly diverge from the observed value. Our findings suggest that the X-ray radiation from the front shock radius may be suppressed due to a disruption in the electron energy flow. This suppression could be attributed to either a notably steep electron energy flow necessitating a faster ejecta expansion velocity or greater explosion power, or a high-density environment covering the shock-suppressed region.\n\nConsequently, we disfavor the compact Wolf-Rayet star as a progenitor model for Type Ibc SN 2018crp and generally favor more extended, lower-speed explosion scenarios for these transient events. Type Ibc supernovae, also known as hypernovae, are a rare class of explosive stellar phenomena arising from core-collapse in massive stars. Their classification is based on observed spectra, with Type Ibc supernovae exhibiting carbon or helium features but lacking broad absorption line characteristics typical of other supernovae.\n\nRecent discoveries of long-duration beta-ray bursts, sometimes referred to as hypernovae, have opened new avenues for exploring their origins. Observational constraints on the location of large gamma-ray bursts (GRBs) confirm their association with the deaths of extremely massive stars. However, the details of their progenitors remain incompletely understood.\n\nTwo long GRBs, GRB 170729A and GRB 160821B, have been detected in the local group of galaxies, with mid-time X-ray observations also obtained for one of them (GRB 160821B). Constraints on the progenitor star and explosion geometry can be derived from the observed characteristics of these transient events. In the weeks following the GRB event, X-ray, ultraviolet, and optical emissions are powered by the forward shockwave generated by the escaping ejecta interacting with the circumstellar medium (CSM). This interaction can be used to measure the longitudinal extent of the shockwave from the explosion center over time, known as the light curve, and to determine the total explosion energy. At later stages, the emission is expected to transition to other forms.",
        "ori-fast-z-score": 1.374791750055601,
        "water-fast-z-score": 8.03167811874588,
        "rewrite-fast-z-score": 4.275816728492017
    },
    {
        "original_text": "Recent advances in nanoscale magnetic functionality have led to a burgeoning interest in utilizing these structures for magnetic imaging and magneto-optical signals detection. Magnetic nanoparticles (MNPs) are uniquely attractive for these applications due to their tunable, localized surface magnetic moments. Here we present the atomic scale magnetic characterization of uniformly 15 nm Fe Nanodots. We employ aberration-corrected, high resolution transmission electron microscopy (AC-HR-TEM) to probe the local magnetic properties of these particles. Our measurements reveal distinct magnetic fingerprint patterns of individual Nanodots. These patterns correspond to the engineered multi-domain states of the Nanodots, which we image with nanometer-scale spatial resolution. Finally, we exploit the spatial dependence of the magnetic fingerprints to rapidly image small volumes (~2 × 2 × 2 nm3). Our experiments provide a detailed, nanoscale magnetic characterization of sub-100 nm single-domain particles and represent an important step toward the development of MNP-based devices for high resolution magnetic imaging.",
        "watermark_text": "Recent developments in nanoscale magnetic functionality have led to a burgeoning interest in utilizing these structures for magnetic scanning and magneto - optical signals tracking . Magnetic nanoparticles ( MNPs ) are uniquely attractive for these uses due to their tunable , localized surface magnetic moments .Here we present the atomic scale magnetic characterization of uniformly 15 wavelength Fe Nanodots . We utilize aberration - adjusted , large resolution transmission electron microscopy ( AC - HR - TEM ) to probe the local magnetic properties of these objects .Our measurements reveal different magnetic fingerprint patterns of different Nanodots . These patterns correspond to the engineered multi - domain states of the Nanodots , which we picture with nanometer - scale spatial resolution .Finally , we utilize the spatial influence of the magnetic fingerprints to rapidly photograph small volumes ( ~ 2 × 2 × 2 nm3 ) . Our experiments give a detailed , nanoscale magnetic characterization of sub - 100 nm single - domain particles and constitute an important milestone toward the development of MNP - based techniques for high resolution optical detection .",
        "rewrite_text": "Recent advancements in nanoscale magnetic functionality have sparked a growing interest in employing these structures for magnetic scanning and magneto-optical signal tracking. Magnetic nanoparticles (MNPs) are particularly advantageous in these applications due to their tunable and localized surface magnetic moments. In this study, we present the atomic-level magnetic characterization of uniformly sized 15-wavelength Fe nanodots. We utilize aberration-corrected, high-resolution transmission electron microscopy (AC-HR-TEM) to explore the local magnetic properties of these nanoscopic objects. Our measurements have uncovered distinct magnetic fingerprint patterns among various nanodots, which correspond to their engineered multi-domain states visible with nanometer-scale spatial resolution. Furthermore, we leverage the spatial implications of these magnetic fingerprints to rapidly image small volumes, approximately 2 x 2 x 2 nm3. Our experiments provide a detailed, nanoscale magnetic characterization of sub-100 nm single-domain particles, paving the way for the development of MNP-based techniques for high-resolution optical detection.",
        "ori-fast-z-score": 0.7777777777777778,
        "water-fast-z-score": 5.378448739494827,
        "rewrite-fast-z-score": 2.208630521496931
    },
    {
        "original_text": "Baumslag-Solitar groups are a generalization of a well-known quotient of the free group. In this paper, we present a duality approach to study representations of Baumslag-Solitar groups. More precisely, we prove that for any positive integer m, every representation of the one-relator group with m relation is dual to a (possibly different) representation of the abelianization of the group. Using this duality, we recover some well-known results about one-relator groups with integer relations as particular cases. We also deduce some stability results for representations of Baumslag-Solitar groups. We use our duality to study a question on hereditarily just-infinite groups. A group G is called hereditarily just-infinite if every subgroup of G is either just-infinite or finite. Hereditarily just-infinite groups are a generalisation of just-infinite groups. We prove that if a one-relator group with torsion is hereditarily just-infinite, then its abelianization is an elementary amenable group, which is a direct generalisation of a classical theorem of Mal’cev.",
        "watermark_text": "Baumslag - Solitar groups are a generalization of a better - famous quotient of the free group . In this paper , we present a duality view to study representations of Baumslag - Solitar groups .More precisely , we prove that for any positive integer m , every representation of the one - relator group with m relation is dual to a ( probably different ) representation of the abelianization of the group . Using this duality , we recover some well - famous data about one - relator groups with integer relations as certain cases .We additionally deduce some stability findings for representations of Baumslag - Solitar groups . We use our duality to study a problem on hereditarily just - infinite groups .A group G is called hereditarily just - infinite if every subgroup of G is either just - infinite or finite . Hereditarily just - infinite groups are a generalisation of just - infinite groups .We prove that if a one - relator group with torsion is hereditarily just - infinite , then its abelianization is an elementary amenable set , which is a direct generalisation of a traditional theorem of Mal ’ cev .",
        "rewrite_text": "Baumslag-Solitar groups are a broadening of the renowned quotient of the free group. In this research, we introduce a dual perspective to explore the representations of Baumslag-Solitar groups. Specifically, we establish that for any positive integer m, every representation of the one-relator group with m relations is dual to a potentially distinct representation of the group's abelianization. Leveraging this duality, we retrieve well-known data about one-relator groups with integer relations in specific cases. Furthermore, we deduce certain stability findings related to the representations of Baumslag-Solitar groups. We employ our dual approach to investigate problems concerning hereditarily just-infinite groups. A group G is hereditarily just-infinite if every subgroup of G is either just-infinite or finite. Hereditarily just-infinite groups are a generalization of just-infinite groups. We prove that if a one-relator group with torsion is hereditarily just-infinite, its abelianization constitutes an elementary amenable set, which is a direct extension of a traditional theorem by Mal'cev.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 3.048003048004572,
        "rewrite-fast-z-score": 0.3721042037676254
    },
    {
        "original_text": "Galaxies are surrounded by large scale structures: the halos in which they form, and the filaments and sheets in which they are embedded. Mergers between galaxies are important for galaxy evolution, because they can bring in new material, angular momentum, and, perhaps, new satellite galaxies. Observational searches for satellites have traditionally been difficult, because they are easily disrupted or removed by the galactic environment. N-body simulations have proved to be a powerful tool for predicting the frequency and properties of satellite galaxies, but predictions still require observationally-informed scaling with the underlying dark matter halos. Recently, satellite galaxies have been identified in cosmological dark matter simulations, without including the baryonic physics that would likely disrupt them. In this work we use a set of zoomed-in, high-resolution cosmological simulations to examine the relation between mergers and the formation of satellite galaxies. We find a strong dependence of the survival of satellite galaxies on the relative masses of the satellite and its host, and on their orbital parameters. Satellite galaxies that are on highly eccentric orbits are much more likely to survive for several crossing times than those on more circular orbits. Satellites on prograde orbits around their host are more likely to survive than those on retrograde orbits, and satellites on highly radial orbits are more likely to survive than those on low-radius orbits. Satellite galaxies that form near the center of their host are more likely to survive than those that form far from the center. We also find that the internal structures of galaxies, as measured by their phase-space structure, are a better predictor of the fate of satellites than their dark matter halos. In particular, satellites that are either on radial orbits or on highly eccentric orbits will have large amounts of stars stripped from their outer parts. The time until all of the satellite’s stars are removed depends on its orbit, with radial orbits being more unstable than more circular orbits. Overall, we find that merging plays a large role in shaping the satellite galaxies we see in the universe today.",
        "watermark_text": "Galaxies are surrounded by large scale structures : the halos in which they shape , and the filaments and sheets in which they are embedded . Mergers between galaxies are important for galaxy evolution , because they can bring in fresh material , angular velocity , and , perhaps , new satellite galaxies .Observational searches for satellites have traditionally been difficult , because they are quickly disrupted or destroyed by the galactic atmosphere . N - bodies simulations have demonstrated to be a powerful tool for predicting the frequency and structures of satellite galaxies , but predictions nevertheless demand observationally - informed scaling with the underlying black material halos .Recently , satellite galaxies have been described in cosmological black material simulations , without including the baryonic physics that would probably damage them . In this project we utilize a system of zoomed - in , large - resolution cosmological simulations to examine the link between mergers and the formation of satellite galaxies .We see a large dependence of the preservation of spacecraft galaxies on the relative masses of the spacecraft and its host , and on their orbital characteristics . Satellite galaxies that are on highly eccentric orbits are greatly more likely to survive for numerous crossing times than those on more circular orbits .Satellites on prograde orbits around their host are more likely to survive than those on retrograde orbits , and orbits on highly radial orbits are more likely to survive than those on small - radius orbits . Satellite galaxies that form near the center of their host are more likely to survive than those that form far from the center .We additionally find that the internal structures of galaxies , as measured by their phase - space formation , are a better predictor of the destiny of satellites than their black material halos . In particular , satellites that are either on radial orbits or on highly eccentric orbits will have huge amounts of stars removed from their exterior portions .The period until all of the spacecraft ’ s stars are removed depends on its orbit , with radial orbits being more unstable than more circular orbits . Overall , we find that merging plays a large role in shaping the satellite galaxies we saw in the universe nowadays .",
        "rewrite_text": "Galaxies are enclosed by vast structures comprising halos that they form within, as well as filaments and sheets within which they are embedded. Galaxy mergers play a pivotal role in the evolution of galaxies as they can introduce fresh matter, angular velocity, and potentially new satellite galaxies. Traditional observations of satellites have been challenging due to their swift disruption or destruction by the galactic atmosphere.\n\nN-body simulations have proven to be a powerful tool for predicting the frequency and structures of satellite galaxies, but these predictions require observationally-informed scaling with the underlying dark matter halos. In recent simulations of cosmological black material, satellite galaxies have been described without considering the baryonic physics that could potentially harm them.\n\nFor this project, we utilize high-resolution, zoomed-in cosmological simulations to investigate the connection between mergers and the formation of satellite galaxies. We observe a strong dependence on the preservation of satellite galaxies on the relative masses of the satellite and its host galaxy, as well as their orbital characteristics. Satellites on highly eccentric orbits are much more likely to survive for numerous orbital crossing times than those on more circular paths.\n\nFurthermore, satellites on prograde orbits around their host are more likely to persist than those on retrograde orbits, and those on highly radial paths are more durable than those on smaller-radius paths. Additionally, satellite galaxies that form near the center of their host are more likely to persist than those situated far from the center.\n\nOur findings also suggest that the internal structures of galaxies, measured by their phase-space development, offer a more accurate predictor of satellite survival than their dark matter halos. Specifically, satellites on radial or highly eccentric orbits will experience significant removal of stars from their outer regions. The duration until all of a satellite's stars are removed depends on its orbit, with radial paths being more unstable than more circular ones. In conclusion, we discover that mergers play a significant role in shaping the satellite galaxies we observe in the universe today.",
        "ori-fast-z-score": -0.15811388300841897,
        "water-fast-z-score": 6.698938453032356,
        "rewrite-fast-z-score": 0.07881104062391006
    },
    {
        "original_text": "The generator coordinate method (GCM) is an approach to computing eigenvalues and eigenfunctions of operators, such as the time-dependent Kohn-Sham equation, in many-body quantum systems. The GCM solves the Schr ö dinger equation in a symmetry-breaking manner, using a unitary transformation to cast the problem in a fully symmetry-preserving form. This allows for the separation of low-lying states of the Hamiltonian into separately optimizing each state in a different, yet internally consistent, approximate representation of the ground state. The GCM has been widely used to study strong correlation in electronic structure, particularly with the development of linear-response time-dependent density-functional theory (TDDFT). In this work, we show that in the case of the TDDFT Kohn-Sham Hamiltonian, the same separation of low-lying states into separately optimized approximations can be achieved without symmetry-breaking, by instead optimizing the generator coordinate itself in a self-consistent manner. We call this method the generator coordinate method in TDDFT (GCTDDFT). We illustrate this approach for the polaron, the Shcrödinger equation with aScreening Coulomb potential, and an iron pnictide.",
        "watermark_text": "The generator coordinate method ( GCM ) is an way to computing eigenvalues and eigenfunctions of operators , such as the time - dependent Kohn - Sham function , in large - bodies quantum systems . The GCM solves the Schr ö dinger equation in a symmetry - breaking manner , using a unitary decomposition to cast the issue in a completely symmetry - preserving shape .This enables for the splitting of lowest - lying states of the Hamiltonian into independently optimizing each state in a distinct , however internally consistent , approximate representation of the ground state . The GCM has been widely applied to study strong correlation in electronic structure , particularly with the development of linear - response time - dependent density - functional theory ( TDDFT ) .In this research , we prove that in the case of the TDDFT Kohn - Sham Hamiltonian , the same separation of low - lying states into independently optimized approximations can be obtained without symmetry - breaking , by simply optimizing the generator coordinate itself in a self - consistent manner . We call this process the generator coordinate method in TDDFT ( GCTDDFT ) .We illustrate this approach for the polaron , the Shcrödinger equation with aScreening Coulomb potential , and an metal pnictide .",
        "rewrite_text": "The Generator Coordinate Method (GCM) provides a means of computing eigenvalues and eigenfunctions of operators in large-scale quantum systems, such as the time-dependent Kohn-Sham function. GCM solves the Schrödinger equation in a symmetry-breaking manner, utilizing a unitary decomposition to frame the problem in a completely symmetry-preserving format. This enables the partitioning of the lowest-lying states of the Hamiltonian into independently optimized representations of the ground state, while maintaining internal consistency.\n\nGCM has found widespread application in studying strong correlations in electronic structures, particularly with the advancement of linear-response time-dependent density-functional theory (TDDFT). In this research, we demonstrate that, in the context of the TDDFT Kohn-Sham Hamiltonian, the same separation of low-lying states into independently optimized approximations can be achieved without symmetry-breaking. This is accomplished by self-consistently optimizing the generator coordinate itself. We refer to this process as the Generator Coordinate Method in TDDFT (GCTDDFT).\n\nWe illustrate this approach using examples from polaron, the Schrödinger equation with a screening Coulomb potential, and a metal pnictide.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 3.927922024247863,
        "rewrite-fast-z-score": 1.1470786693528088
    },
    {
        "original_text": "Optical lattices provide a highly controllable platform for simulating quantum systems. By modifying the Raman coupling and the potential, various quantum phases can be achieved, such as superfluid and Mott insulators. The quantum phases of cold atoms loaded in an optical lattice are typically characterized by the Wannier function based on Bloch’s theorem, which is a key issue in simulations and experiments due to the difficulties in engineering accurate optical lattices with desired band structures. To this end, we show that in the deep lattice limit with a large on-site Hubbard interaction U, the dynamics of hardcore bosons can be mapped to an effective quantum spin system with long-range interactions. In the thermodynamic limit, this quantum spin system exhibits a third-order phase transition at a finite wave number k* from a MI to a SF in the Wannier function representation. In particular, the dynamical and equilibrium properties are discussed in detail. Furthermore, a real-space renormalization group approach is proposed to accurately calculate dynamical quantities. These results shed lights on a possible experimental realization of the quantum spin system and the localization phenomena of quantum many-body systems in realistic optical lattice systems.",
        "watermark_text": "Optical lattices allow a highly controllable platform for simulating quantum systems . By modifying the Raman correlation and the potential , various quantum phases can be obtained , such as superfluid and Mott insulators .The quantum phases of cold particles shipped in an optical lattice are typically characterized by the Wannier function based on Bloch ’ s theorem , which is a key issue in simulations and experiments due to the problems in design precise optical lattices with specified band structures . To this end , we prove that in the deep lattice limit with a large on - location Hubbard interaction U , the dynamics of hardcore bosons can be mapped to an efficient quantum spin system with long - range interactions .In the thermodynamic limit , this quantum spin scheme exhibits a third - order phase shift at a finite wave number k * from a MI to a SF in the Wannier function representation . In particular , the dynamical and equilibrium properties are discussed in detail .Furthermore , a real - space renormalization group method is proposed to easily measure dynamical components . These data shed lights on a possible experimental realization of the quantum spin scheme and the localization phenomena of quantum several - bodies systems in realistic optical lattice systems .",
        "rewrite_text": "Optical lattices provide a highly controllable platform for simulating quantum systems. By adjusting the Raman correlation and potential energy, a range of quantum phases can be achieved, including superfluid and Mott insulator phases. The quantum phases of cold particles trapped within these optical lattices are typically identified using the Wannier function, which is based on Bloch's theorem and is crucial in simulations and experiments due to challenges in designing precise optical lattices with specific band structures.\n\nTo address this, we demonstrate that in the deep lattice limit with a strong on-site Hubbard interaction U, the dynamics of hardcore bosons can be mapped onto an efficient quantum spin system with long-range interactions. In the thermodynamic limit, this quantum spin approach exhibits a third-order phase transition at a specific wave number k*, transitioning from a MI to a SF in the Wannier function representation. Specifically, both dynamic and equilibrium properties are discussed in depth. Additionally, a real-space renormalization group method is proposed to facilitate the measurement of dynamic components. These findings offer insights into the potential experimental realization of the quantum spin scheme and the localization phenomena observed in multiple-body quantum systems within practical optical lattice systems.",
        "ori-fast-z-score": 1.4142135623730951,
        "water-fast-z-score": 5.990542374288353,
        "rewrite-fast-z-score": 2.1572774865200244
    },
    {
        "original_text": "Many biophysical and biochemical studies have underlined the crucial influence of carbohydrates in living cells. Indeed, carbohydrates seem to be essential for various life processes, such as cell recognition, structural integrity, energy storage or as transmitters of chemical signals. Thus, the effect of carbohydrates on the structure and dynamical properties of proteins of interest has received great interest in recent years. In this study, we focus on the effects of three commonly occurring carbohydrates, trehalose, maltose and sucrose, on the dynamical and structural properties of hen egg-white lysozyme by means of Molecular Dynamics simulations. We find that these three carbohydrates influence the protein dynamics, as well as its secondary and tertiary structure. In addition, we find that maltose has a more pronounced stabilizing effect on the protein structure than trehalose and sucrose. Further, the dynamical quantities are found to be related to the protein-carbohydrate interaction strengths.",
        "watermark_text": "Many biophysical and biochemical studies have underlined the key influence of carbohydrates in living cells . Indeed , carbohydrates seem to be crucial for various life functions , such as cell recognition , functional integrity , energy storage or as transmitters of biological signals .Thus , the impact of carbohydrates on the composition and dynamical properties of proteins of interest has garnered great concern in recent years . In this study , we focus on the effects of three widely occurring carbohydrates , trehalose , maltose and sucrose , on the dynamical and structural properties of hen egg - yellow lysozyme by means of Molecular Dynamics simulations .We see that these three carbohydrates impact the protein dynamics , as also as its primary and secondary structure . In addition , we find that maltose has a more pronounced stabilizing impact on the protein structure than trehalose and sucrose .Further , the dynamical amounts are found to be related to the protein - carbohydrate coupling strengths .",
        "rewrite_text": "Numerous biophysical and biochemical investigations have emphasized the pivotal role of carbohydrates in living cells. Carbohydrates appear to be of utmost importance for various biological functions, including cell recognition, maintaining functional integrity, energy storage, and signaling processes. Consequently, the influence of carbohydrates on the composition and dynamic properties of proteins has recently garnered significant attention.\n\nIn this study, we concentrate on the effects of three commonly occurring carbohydrates - trehalose, maltose, and sucrose - on the dynamic and structural properties of hen egg-yellow lysozyme using Molecular Dynamics simulations. Our findings indicate that these three carbohydrates significantly impact the protein dynamics, as well as its primary and secondary structures. Additionally, we observe that maltose exhibits a more pronounced stabilizing effect on the protein structure compared to trehalose and sucrose. Furthermore, the dynamic levels are found to be associated with the strength of protein-carbohydrate interactions.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 5.077367528252131,
        "rewrite-fast-z-score": 1.116312611302876
    },
    {
        "original_text": "Researchers study the composition and evolution of dust in the early stages of galaxy evolution using Primordial Supernova Remnants (P31 in the image below, also known as Cas A) as aRosetta Stone to help interpret the observed abundances and physical conditions in young supernova remnants (SNRs) throughout the universe. By performing state-of-the-art dust formation and evolution simulations, we find that silicate dust grains formed in the P31 ejecta can survive for a Hubble time and be injected into the surrounding early Interstellar Medium (ISM). These dust grains can then grow via accretion of metals from the presolar ISM to form nanometer-sized particles. We also find that titanium and vanadium are the most abundant elements at the grain surface and that the overall stoichiometry is roughly Ti-26V-4Si, with a high sulfur to oxygen atomic ratio of 0.5-1.5%. These sulfur-rich compositions are distinctive from those of chondritic meteorites and are most consistent with carbon-rich pebble materials, such as enstatite andagnetite, as injected dust precursors. We conclude that the P31 supernova remnant, as observed and modeled here, provides a rich platform to test theoretical predictions of dust formation and subsequent evolution. The bulk compositions of silicate dust grains injected into the early ISM can serve as fingerprints to help identify these unique grains among the Galactic and extragalactic background population. The silicon, titanium, and vanadium abundances can be used to confirm the formation of silicates, oxides, and carbides, respectively, as expected for core-collapse supernova ejecta interacting with the surrounding ISM. Furthermore, the sulfur-to-oxygen ratio can be leveraged as a clock to date the injection event to within roughly 30-330 million years, depending on the assumed density and composition of presolar ambient gas. The recent detection of an unusually large sulfur mass fraction in the wake of SN1987A supports an age estimate of ~30 million years, given that most of the injected sulfur would have likely been consumed in the formation of new dust grains. Ultimately, dust identified in young SNRs will provide a unique window into the nucleosynthesis and chemical enrichment history of the very first stars and galaxies in the early Universe. This is a descendent post of this entry in the Darwin Award Contest.",
        "watermark_text": "Researchers research the composition and evolution of dust in the early stages of galaxy evolution using Primordial Supernova Remnants ( P31 in the image below , sometimes called as Cas A ) as aRosetta Stone to assist analyze the observed abundances and physical conditions in young supernova remnants ( SNRs ) throughout the universe . By conducting state - of - the - art dust structure and evolution simulations , we find that silicate powder grains created in the P31 ejecta can escape for a Hubble time and be injected into the nearby old Interstellar Medium ( ISM ) .These dust grains can then grow via accretion of metals from the presolar ISM to form nanometer - sized particles . We additionally find that titanium and vanadium are the most numerous elements at the grain surface and that the overall stoichiometry is approximately Ti - 26V - 4Si , with a high sulfur to oxygen atomic proportion of 0 . 5 - 1 . 5 % .These sulfur - rich compositions are peculiar from those of chondritic meteorites and are most consistent with carbon - rich pebble materials , such as enstatite andagnetite , as injected dust precursors . We suggest that the P31 supernova remnant , as measured and reconstructed here , offers a rich platform to test theoretical estimates of dust growth and subsequent evolution .The bulk compositions of silicate powder grains injected into the early ISM can help as fingerprints to assist identify these unique crystals among the Galactic and extragalactic background population . The silicon , titanium , and vanadium abundances can be used to confirm the formation of silicates , oxides , and carbides , respectively , as anticipated for core - collapse supernova ejecta interacting with the nearby ISM .Furthermore , the sulfur - to - oxygen percentage can be leveraged as a clock to dating the injection event to within roughly 30 - 330 million months , depending on the expected density and composition of presolar atmospheric gas . The recent discovery of an exceptionally massive sulfur mass fraction in the wake of SN1987A confirms an age estimate of ~ 30 million months , provided that most of the extracted sulfur would have likely been consumed in the formation of new cloud particles .Ultimately , dust identified in young SNRs will provide a unique window into the nucleosynthesis and biological enrichment history of the very first stars and galaxies in the early Universe . This is a descendent post of this entrance in the Darwin Award Contest .",
        "rewrite_text": "Researchers explore the composition and evolution of dust during the early phases of galaxy formation by utilizing Primordial Supernova Remnants (specifically, P31, sometimes referred to as Cas A) as a keystone. This assists in analyzing the abundance and physical conditions of young supernova remnants (SNRs) across the universe. Through state-of-the-art dust structure and evolution simulations, we've discovered that silicate powder grains created in the P31 ejecta can persist for a Hubble time and be injected into the nearby old Interstellar Medium (ISM). These dust grains can grow through the accretion of metals from the presolar ISM, ultimately forming nanometer-sized particles.\n\nFurthermore, we've found that titanium and vanadium are the most abundant elements on the grain surface, with an overall stoichiometry of approximately Ti-26V-4Si. The high sulfur-to-oxygen atomic ratio is between 0.5 to 1.5%. These sulfur-rich compositions differ from chondritic meteorites and are most consistent with carbon-rich pebble materials, such as enstatite and agnetite, which may be injected as dust precursors. We propose that the P31 supernova remnant, as meticulously measured and reconstructed here, provides a valuable platform for testing theoretical estimates of dust growth and subsequent evolution.\n\nThe composition of silicate powder grains injected into the early ISM can serve as a unique fingerprint, aiding in the identification of these distinctive crystals within the Galactic and extragalactic backgrounds. The abundance of silicon, titanium, and vanadium can be used to confirm the formation of silicates, oxides, and carbides, respectively, as expected in core-collapse supernova ejecta interacting with the nearby ISM. Additionally, the sulfur-to-oxygen percentage can be utilized as a timing mechanism to date the injection event to within approximately 30 to 330 million months, depending on the expected density and composition of presolar atmospheric gas.\n\nThe recent discovery of an unusually high sulfur mass fraction in the wake of SN1987A supports an age estimate of approximately 30 million months, assuming that the majority of extracted sulfur was likely consumed in the formation of new cloud particles. Ultimately, dust identified in young SNRs will offer a unique window into the nucleosynthesis and biological enrichment history of the earliest stars and galaxies in the early Universe. This post is a submission for the Darwin Award Contest.",
        "ori-fast-z-score": 1.0509877084907764,
        "water-fast-z-score": 7.897065047448726,
        "rewrite-fast-z-score": 3.062127263296445
    },
    {
        "original_text": "Biphenyl-dithiol (bPT) molecules were connected between gold tip and Ag substrate to form single-molecule junctions. Conductance measurements as a function of the tip-sample angle showed a series of local conductance maxima as a function of tip-sample angle. We analyze the conductance measurements by a simple theoretical model of the junction based on the Landauer formula, where the conductance is expressed as a sum of transmission probabilities for electrons with different transverse momentum states. We identify four different conductance maxima as resulting from different hybridization of the molecular orbitals with the leads. From temperature dependence of the conductance we estimate the molecular orbital widths. graded like this: Tilt-angle landscapes and temperature dependence of the conductance in biphenyl-dithiol single-molecule junctions. By connecting biphenyl-dithiol (bPT) molecules between gold tip and Ag substrate, we measure the conductance as a function of the tip-sample angle and identify four local maxima in the conductance as a function of tip-sample angle. The maxima can be associated to different transverse momentum states and therefore different hybridizations of the molecular orbitals with the leads. We analyze the temperature dependence of the conductance and estimate the hybridization widths.",
        "watermark_text": "Biphenyl - dithiol ( bPT ) atoms were linked between gold edge and Ag substrate to form single - molecule junctions . Conductance observations as a function of the tip - sample angle gave a sequence of local conductance maxima as a function of tip - specimen angle .We evaluate the conductance observations by a simple theoretical theory of the junction based on the Landauer formula , where the conductance is expressed as a sum of transmission probabilities for electrons with various transverse momentum states . We recognize four different conductance maxima as occurring from varying hybridization of the molecular orbitals with the leads .From temperature dependence of the conductance we estimate the molecular orbital widths . graded like this : Tilt - angle landscapes and heat dependence of the conductance in biphenyl - dithiol single - atom junctions .By linking biphenyl - dithiol ( bPT ) atoms between gold edge and Ag substrate , we measure the conductance as a function of the tip - sample angle and distinguish four local maxima in the conductance as a function of tip - specimen distance . The maxima can be correlated to different transverse momentum states and therefore different hybridizations of the molecular orbitals with the leads .We evaluate the temperature dependence of the conductance and estimate the hybridization widths .",
        "rewrite_text": "The atoms of biphenyl-dithiol (bPT) were connected between the gold edge and the Ag substrate, forming single-molecule junctions. By observing conductance as a function of the tip-sample angle, a sequence of local conductance maxima was observed, which varied with the tip-specimen angle. We analyze these observations using a simple theoretical framework based on the Landauer formula, where conductance is expressed as the sum of transmission probabilities for electrons in various transverse momentum states. We identified four distinct conductance maxima, which arose from varying hybridizations of the molecular orbitals with the leads. By examining the temperature dependence of the conductance, we estimate the molecular orbital widths. This is graded as follows: the landscape of tilt angles and the heat dependence of conductance in biphenyl-dithiol single-atom junctions. Through the linkage of bPT atoms between gold and silver substrates, we measured conductance variations with tip-sample angle and distinguished four local maxima in conductance related to different tip-specimen distances. These maxima can be correlated to different transverse momentum states and, consequently, different hybridizations of molecular orbitals with the leads. We further assess the temperature dependency of conductance and estimate hybridization widths.",
        "ori-fast-z-score": -1.2074068598865937,
        "water-fast-z-score": 3.6222205796597815,
        "rewrite-fast-z-score": 1.193117518002609
    },
    {
        "original_text": "The National Scientific Facilities (NSFs) are the national research infrastructure that support the U.K. science and engineering base. This article details the impact of these NSFs on the non-medical research activities of U.K. researchers. Since 1987, expenditure on non-medical research in universities and other higher education institutions has more than doubled and generated a growth in research staff. In 2010, around 43,000 U.K. researchers were engaged in research, education or knowledge transfer activities that contribute to fundamental research, regional economic development or enhancement, or U.K. industrial innovation. Research was predominantly publicly funded, with around 22,000 researchers (51%) based in universities or higher education institutions supported by research councils. Of the remaining 20,000 researchers with commercial or industrial interests, the majority received some industrial sponsorship, with support from the U.K. technology industries ranging from consultancy to applied research trials. During the last decade, the number of U.K. research publications and citations have increased substantially, with more than 200,000 publications and 20 million citations in 2016, approximately 22% and 39% more than the volume of publications 10 years earlier. The U.K. is a global leader in life sciences research, contributing more than 7,000 researchers to that field between 2006 and 2021. Between 2014 and 2021, these researchers were expected to account for around a quarter of the total U.K. medical research spend. The NSFs are critical to supporting this activity, with around 90% of biological research facilities based in U.K. universities or other higher education institutions, and 50% of physics, astronomy and computing research facilities. These facilities are also instrumental in enabling U.K. industry to advance medical research, with around 40% of companies engaged in medical research and more than half working with U.K. universities and other higher education institutions.",
        "watermark_text": "The National Scientific Facilities ( NSFs ) are the national research facilities that aid the U . K . scientific and engineering base . This page highlights the impact of these NSFs on the non - medical research actions of U . K . scientists .Since 1987 , expenditure on non - health research in schools and other higher education institutions has more than doubled and generated a growth in research personnel . In 2010 , around 43 , 000 U . K . scientists were involved in research , education or information transfer activities that contribute to basic research , regional economic progress or enhancement , or U . K . industrial advancement .Research was predominantly publicly funded , with around 22 , 000 researchers ( 51 % ) located in schools or higher education institutions supported by research councils . Of the remaining 20 , 000 researchers with commercial or industrial concerns , the majority took some industrial sponsorship , with funding from the U . K . technology industries ranging from consultancy to applied scientific trials .During the last period , the quantity of U . K . scientific publications and citations have increased significantly , with more than 200 , 000 publications and 20 million citations in 2016 , roughly 22 % and 39 % more than the quantity of publications 10 years previously . The U . K . is a global leader in human sciences research , contributing more than 7 , 000 researchers to that field between 2006 and 2021 .Between 2014 and 2021 , these researchers were expected to account for around a third of the total U . K . medical research spend . The NSFs are critical to supporting this activity , with around 90 % of biological scientific sites located in U . K . universities or other higher education institutions , and 50 % of biology , astronomy and computing scientific sites .These programs are also important in encouraging U . K . industry to promote medical research , with around 40 % of companies engaged in medical research and more than quarter working with U . K . universities and other higher education institutions .",
        "rewrite_text": "The National Scientific Facilities (NSFs) constitute the national research infrastructure that aids the scientific and engineering base of the United Kingdom. This page underscores the impact of NSFs on the non-medical research endeavors of UK scientists.\n\nSince 1987, the expenditure on non-health research in academic institutions and other higher education facilities has more than doubled, leading to a surge in research personnel. In 2010, approximately 43,000 UK scientists were involved in research, education, or information transfer activities that contribute to basic research, regional economic development, or the advancement of UK industry. Research is predominantly funded by the public sector, with around 22,000 researchers (51%) based in schools or higher education institutions supported by research councils.\n\nAmong the remaining 20,000 researchers with commercial or industrial ties, the majority receive some form of industrial sponsorship, with funding from UK technology industries ranging from consultancy services to applied scientific trials. In the recent period, there has been a significant increase in the number of UK scientific publications and citations, with over 200,000 publications and 20 million citations recorded in 2016, which is roughly 22% and 39% more than the number of publications ten years prior.\n\nThe UK is a global leader in human sciences research, contributing over 7,000 researchers to this field between 2006 and 2021. Between 2014 and 2021, these researchers are expected to account for approximately one-third of the total UK medical research expenditure. NSFs play a crucial role in supporting this activity, with approximately 90% of biological science sites located in UK universities or other higher education institutions, as well as 50% of biology, astronomy, and computing scientific sites.\n\nThese programs are also significant in encouraging UK industry to promote medical research, with approximately 40% of companies engaged in medical research and over a quarter collaborating with UK universities and other higher education institutions.",
        "ori-fast-z-score": 0.8638684255813601,
        "water-fast-z-score": 7.950706915615445,
        "rewrite-fast-z-score": 4.303550620732313
    },
    {
        "original_text": "In this work we report direct measurement of the interfacial tensions between coexisting phases of a canonical, simple, model liquid-liquid system. This is possible due to the recent development of a scheme for the determination of interfacial quantities from computer simulation of only the solute particle system. This approach is based on constructing interfaces in the simulation box that match a geometrical definition of interfacial surface, and monitoring various interfacial properties, such as average position and orientation of solute particles as a function of simulation configuration. In this way we are able to measure the interfacial tensions between two coexisting phases of tangent hard sphere chains (THC), a model system used extensively to test methods for computational interfacial studies. We find that the interfacial tensions between coexisting liquid and vapor phases and between liquid and solid phases are all positive and of similar magnitude (ca. 27 mN/m), in contrast to the typical situation in liquid-vapor and oil-water systems (ca. 40 mN/m), and hence more representative of real liquids. This is the first time that such direct measurement of interfacial tensions has been reported. A simple fundamental measure theory (FMT) approximation is shown to provide an excellent description of the interfacial tensions, and hence the FMT interface potential can be evaluated quite accurately. Such accurate evaluation of the interface potential is not possible from analysis of the interfacial configurations or from standard Surface Evolver calculations, which typically need to make uncontrolled assumptions about the form of the potential, or perform a tedious parameter search.",
        "watermark_text": "In this project we study close observation of the interfacial differences between coexisting phases of a canonical , simple , model liquid - fluid system . This is could due to the recent construction of a scheme for the determination of interfacial quantities from computer analysis of only the solute particle system .This method is based on making connections in the model box that meet a geometrical concept of interfacial surface , and monitoring various interfacial properties , such as average position and posture of solute atoms as a function of simulation configuration . In this way we are able to measure the interfacial differences between two coexisting phases of tangent hard sphere chains ( THC ) , a description system employed heavily to test solutions for theoretical interfacial studies .We see that the interfacial differences between coexisting solid and fluid phases and between liquid and solid phases are all positive and of comparable magnitude ( ca . 27 mN / m ) , in comparison to the typical situation in liquid - vapor and oil - water solutions ( ca .40 mN / m ) , and hence more representative of real liquids . This is the first time that such direct observation of interfacial forces has been reported .A straightforward fundamental measure theory ( FMT ) approximation is demonstrated to provide an excellent description of the interfacial forces , and hence the FMT interface potential can be evaluated rather precisely . Such reliable evaluation of the interface potential is not possible from evaluation of the interfacial configurations or from standard Surface Evolver analysis , which commonly need to make uncontrolled assumptions about the form of the potential , or undergo a tedious parameter search .",
        "rewrite_text": "In this project, we undertake a thorough examination of the subtle interfacial variations present between coexisting phases of a standard, simple model liquid-fluid system. This exploration is facilitated by the recent development of a method for determining interfacial properties through computer analysis solely focused on the solute particle system. This approach hinges on establishing connections within the model box that align with a geometric concept of the interfacial surface, while monitoring various interfacial properties such as the average position and orientation of solute atoms relative to the simulation configuration.\n\nThrough this methodology, we are able to measure the interfacial disparities between two coexisting phases of tangent hard sphere chains (THC), a system frequently used to test theoretical interfacial studies. Our observations reveal that the interfacial differences between coexisting solid and fluid phases, as well as between liquid and solid phases, are all positive and comparable in magnitude (approximately 27 mN/m). This is in contrast to the typical scenario found in liquid-vapor and oil-water solutions (approximately 40 mN/m), making our findings more representative of real liquids.\n\nSignificantly, this is the first report to directly observe interfacial forces of this kind. We demonstrate that a straightforward fundamental measure theory (FMT) approximation provides an excellent description of the interfacial forces, allowing for a precise evaluation of the FMT interface potential. Such reliable assessment of the interface potential is not feasible through the evaluation of interfacial configurations or standard Surface Evolver analysis, which often require uncontrolled assumptions about potential form or require tedious parameter searches.",
        "ori-fast-z-score": -1.3636363636363635,
        "water-fast-z-score": 6.454545454545454,
        "rewrite-fast-z-score": 0.9918365981341755
    },
    {
        "original_text": "Bottlebrush polymers are macromolecules with a covalently linked, complex“brush” architecture, in which polymeric side chains are attached to a central“backbone” chain. Due to their relatively large size and high degree of stiffness, bottlebrush polymers are often unable to adapt to the irregularities in their embedding environments, and consequently exhibit inadequate processability and function in solution or the solid state. The problem of stiffness can be mitigated by incorporating chemically crosslinkable side chains on the bottlebrush, thereby allowing the brush segments to be crosslinked once for a global change in the stiffness of the brush. However, the brush and crosslinker segments then come into close proximity, and in the absence of specific non-covalent interactions, may undergo crosslinking, even at low degrees of conversion. Here we show that mutual interaction between the brush and crosslinker segments – namely π-π interaction between the side chain polymers and Flory-Huggins interaction between the fluorocarbon and carbon backbone polymers – leads to crosslinking-induced softening of the brush stiffness. This approach allows independent control of stiffness and processability of bottlebrush polymers through appropriate selection of non-covalent and covalent interactions. The approach is validated by rheological and small-angle X-ray scattering measurements on a bottlebrush polymer in which the brush is formed by ethyloxazoline monomers and the side chains are formed by tris(trimethylsilyl)silane (TMSSI). Mutual interaction between TMSSI side chains and ethyloxazoline brush segments leads to a large decrease in the bottlebrush stiffness without a significant change in the weight average molecular weight. A novel two-step crosslinking approach is developed that enables gradual and controllable crosslinking of the TMSSI side chains in a spatially and temporally decoupled manner from global crosslinking of the ethyloxazoline brush segments. The approach presented here provides a powerful strategy to independently tune the processability and stiffness in bottlebrush polymer systems, with broad potential applicability across other bottlebrush architectures.",
        "watermark_text": "Bottlebrush polymers are macromolecules with a covalently joined , complex “ brush ” architecture , in which polymeric end chains are connected to a central “ backbone ” chain . Due to their extremely huge size and large degree of stiffness , bottlebrush polymers are often unable to adapt to the irregularities in their embedding environments , and consequently exhibit inadequate processability and function in solution or the liquid state .The problem of stiffness can be mitigated by incorporating chemically crosslinkable side rings on the bottlebrush , thereby allowing the brush sections to be crosslinked once for a global change in the stiffness of the brush . However , the brush and crosslinker portions then coming into tight vicinity , and in the absence of certain non - covalent interactions , might undergo crosslinking , even at low degrees of transformation .Here we show that cooperative interaction between the brush and crosslinker strands – namely π - π relationship between the front chain polymers and Flory - Huggins interaction between the fluorocarbon and carbon backbone polymers – contributes to crosslinking - caused softening of the brush stiffness . This method enables independent control of stiffness and processability of bottlebrush polymers through optimal selection of non - covalent and covalent interactions .The method is validated by rheological and tiny - angle X - ray scattering observations on a bottlebrush polymer in which the brush is formed by ethyloxazoline monomers and the front chains are created by tris ( trimethylsilyl ) silane ( TMSSI ) . Mutual interaction between TMSSI side rings and ethyloxazoline brush sections leads to a large decrease in the bottlebrush stiffness without a substantial shift in the weight average molecular weight .A innovative two - phase crosslinking methodology is developed that enables gradual and controllable crosslinking of the TMSSI side rings in a spatially and temporally decoupled fashion from global crosslinking of the ethyloxazoline brush sections . The method provided here presents a powerful strategy to independently tune the processability and stiffness in bottlebrush plastic applications , with broad potential applicability across other bottlebrush architectures .",
        "rewrite_text": "Bottlebrush polymers are large molecules with a covalently linked, intricate \"brush\" structure, where the end chains of the polymers are connected to a central \"backbone\" chain. Due to their immense size and high stiffness, these polymers often struggle to adapt to the irregularities of their environment, resulting in inadequate processability and function in both solution and liquid states.\n\nTo alleviate the rigidity issue, chemically crosslinkable side rings can be incorporated into the bottlebrush, allowing for crosslinking of the brush sections to globally adjust the stiffness of the entire structure. However, when the brush and crosslinker portions come too close together, without certain non-covalent interactions, they may undergo crosslinking even at low levels of transformation. Our research shows that cooperative interactions between the brush and crosslinker strands, such as π-π interactions between front chain polymers and Flory-Huggins interactions between fluorocarbon and carbon backbone polymers, contribute to softening the stiffness of the brush through crosslinking.\n\nThis method enables independent control of both the stiffness and processability of bottlebrush polymers through the optimal selection of both non-covalent and covalent interactions. This method has been validated through rheological and small-angle X-ray scattering observations of a bottlebrush polymer composed of ethyloxazoline monomers with front chains created by tris(trimethylsilyl)silane (TMSSI). The mutual interaction between TMSSI side rings and ethyloxazoline brush sections results in a significant reduction in bottlebrush stiffness without a significant change in weight-average molecular weight.\n\nAn innovative two-phase crosslinking methodology has been developed that enables gradual and controllable crosslinking of TMSSI side rings independently from global crosslinking of ethyloxazoline brush sections. This method presents a powerful strategy to independently adjust the processability and stiffness in bottlebrush plastic applications, with broad potential applicability across other bottlebrush architectures.",
        "ori-fast-z-score": -0.17541160386140586,
        "water-fast-z-score": 7.659922885032927,
        "rewrite-fast-z-score": 4.164644317955852
    },
    {
        "original_text": "LiHo_xY_{1-x}F_4 is a dense, low-temperature, magnetic material used in magnetic resonance imaging (MRI). The favorable properties of LiHo_xY_{1-x}F_4 arise from its fully spin-polarized ground state, although this property is only stable at low temperatures, and above a certain concentration of yttrium. Here we present a comprehensive thermodynamic and transport study of LiHo_xY_{1-x}F_4 over a wide temperature and composition range. We find that the material exhibits three distinct phases as a function of temperature and yttrium concentration, in agreement with previous studies. At low temperatures and high yttrium concentrations, LiHo_xY_{1-x}F_4 exhibits a gas-like behavior with classical ferromagnetic order, and we determine the expected phase diagram in this region. At intermediate temperatures and low yttrium concentrations, we find evidence of a novel magnetic quantum critical point, but the low-temperature behavior remains to be characterized. Finally, at high temperatures and low yttrium concentrations, LiHo_xY_{1-x}F_4 undergoes a spin glass transition. We present a unified theory that describes the three phases in this region of the phase diagram, and we show that the spin glass transition is in the droplet relaxationalglass universality class. Our study of LiHo_xY_{1-x}F_4 provides a complete description of the phase diagram in this classical ferromagnetic system. Such a phase diagram is crucial for the development of an understanding of the properties of LiHo_xY_{1-x}F_4 and for the optimization of its performance for use in MRI machines.",
        "watermark_text": "LiHo _ xY _ { 1 - x } F _ 4 is a dense , low - temperature , magnetic material employed in magnetic resonance imaging ( MRI ) . The stable properties of LiHo _ xY _ { 1 - x } F _ 4 arise from its completely spin - polarized ground state , although this property is only stable at low temperatures , and above a certain concentration of yttrium .Here we present a comprehensive thermodynamic and transport study of LiHo _ xY _ { 1 - x } F _ 4 over a broad temperature and composition range . We see that the metal experiences three different stages as a function of temperature and yttrium density , in agreement with previous research .At low temperatures and large yttrium concentrations , LiHo _ xY _ { 1 - x } F _ 4 displays a gas - like behavior with classical ferromagnetic order , and we determine the expected phase diagram in this area . At medium temperatures and low yttrium concentrations , we find proof of a novel magnetic quantum fundamental point , but the high - temperature behavior continues to be described .Finally , at high temperatures and low yttrium concentrations , LiHo _ xY _ { 1 - x } F _ 4 undergoes a spinning glass shift . We create a consolidated theory that describes the three stages in this area of the phase diagram , and we prove that the spin glass process is in the droplet relaxationalglass universality category .Our study of LiHo _ xY _ { 1 - x } F _ 4 provides a complete outline of the phase diagram in this classical ferromagnetic system . Such a phase diagram is crucial for the development of an knowledge of the properties of LiHo _ xY _ { 1 - x } F _ 4 and for the optimization of its reliability for use in MRI systems .",
        "rewrite_text": "LiHo xY1-xF4 is a dense, low-temperature magnetic material utilized in magnetic resonance imaging (MRI). Its stable properties stem from its completely spin-polarized ground state, though this stability is only observed at low temperatures and above a certain yttrium concentration. We present an extensive thermodynamic and transport study of LiHo xY1-xF4 across a broad temperature and composition range.\n\nOur observations indicate that the material exhibits three distinct stages as a function of temperature and yttrium density, in agreement with previous research. At low temperatures and high yttrium concentrations, LiHo xY1-xF4 displays a gas-like behavior with classical ferromagnetic order, and we have determined the expected phase diagram in this region. At medium temperatures and low yttrium concentrations, we have discovered evidence of a novel magnetic quantum fundamental point. However, a description of the high-temperature behavior remains ongoing. At high temperatures and low yttrium concentrations, LiHo xY1-xF4 undergoes a transition to a spinning glass state.\n\nWe have formulated a unified theory that describes the three stages within this region of the phase diagram and have demonstrated that the spin glass process falls within the category of droplet relaxational glass universality. Our study of LiHo xY1-xF4 provides a comprehensive overview of the phase diagram in this classical ferromagnetic system. Such a phase diagram is essential for advancing our understanding of LiHo xY1-xF4's properties and for optimizing its reliability in MRI systems.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.52344770738994,
        "rewrite-fast-z-score": 2.456769074559977
    },
    {
        "original_text": "In this paper, we consider a stationary, spherically symmetric black hole in de Sitter space with a constant positive curvature. We compute the Bogoliubov coefficients between the Hawking radiation and the Bunch-Davies vacuum, and find that the de Sitter horizon is a effective change of the vacuum. We also consider the small mass case, and calculate the particle creation rate and the energy flux near the horizon. We find that the energy flux becomes a positive number even in de Sitter space, which violates the cosmic no-hair conjecture. Date: 2023 Author: Yosuke Kimura Title: Hawking radiation from black holes in de Sitter spaces Abstract: In this paper, we consider a stationary, spherically symmetric black hole in de Sitter space with a constant positive curvature. We compute the Bogoliubov coefficients between the Hawking radiation and the Bunch-Davies vacuum, and find that the de Sitter horizon is a effective change of the vacuum. We also consider the small mass case, and calculate the particle creation rate and the energy flux near the horizon. We find that the energy flux becomes a positive number even in de Sitter space, which violates the cosmic no-hair conjecture. Why do we care about Hawking radiation from black holes in de Sitter spaces? The AdS/CFT correspondence, which was proposed by M. M. Roberts, V. A. Hubeny, C. Ranganathan, and E. funcatsbooks.com, relates a gravity theory in a higher-dimensional anti-de Sitter space to a conformal field theory in physical space-time. Since then, there have been many studies of quantum effects in AdS spaces, such as black holes and cosmological horizons. One reason to study the effects of quantum gravity in AdS spaces is to understand the higher-dimensional gravity itself from a view point of the lower-dimensional CFT. On the other hand, the cosmological constant in the universe is very tiny but non-zero. It has been suggested that quantum effects of the gravity could generate a positive cosmological constant. This means that the AdS space corresponds to an inflationary universe. Thus, studying quantum effects in AdS spaces provides a possible way to explain the very small but non-zero cosmological constant. However, the AdS spaces have a time-like curvature, and thus the physics in the boundary CFT does not have a time direction. This means that, since the Hawking radiation is a process from the past of the boundary, it does not affect the CFT. Thus, one cannot discuss the Cosmic No Hair conjecture from the AdS/CFT correspondence. The cosmic no-hair conjecture is that black holes in an expanding universe should have the same conserved quantities as the universe. So, it is desirable to study the Hawking radiation in AdS spaces in",
        "watermark_text": "In this paper , we study a stationary , spherically symmetric black hole in de Sitter space with a constant positive curvature . We compute the Bogoliubov coefficients between the Hawking radiation and the Bunch - Davies vacuum , and find that the de Sitter horizon is a effective difference of the vacuum .We also consider the small mass case , and estimate the particle creation rate and the power flux near the horizon . We see that the electricity flux becomes a positive number even in de Sitter space , which violates the cosmic no - hair conjecture .Date : 2023 Author : Yosuke Kimura Title : Hawking radiation from black holes in de Sitter spaces Abstract : In this paper , we study a stationary , spherically symmetric blue hole in de Sitter space with a constant positive curvature . We compute the Bogoliubov coefficients between the Hawking radiation and the Bunch - Davies vacuum , and find that the de Sitter horizon is a effective difference of the vacuum .We also consider the small mass case , and estimate the particle creation rate and the power flux near the horizon . We see that the electricity flux becomes a positive number even in de Sitter space , which violates the cosmic no - hair conjecture .Why do we care about Hawking radiation from black holes in de Sitter spaces ? The AdS / CFT relationship , which was suggested by M . M . Roberts , V . A . Hubeny , C . Ranganathan , and E . funcatsbooks . com , relates a gravity concept in a higher - dimensional anti - de Sitter space to a conformal field model in physical space - time .Since then , there have been many research of quantum effects in AdS spaces , such as black holes and cosmological horizons . One reason to study the effects of quantum gravitational in AdS spaces is to see the higher - dimensional gravity itself from a view point of the lower - dimensional CFT .On the other hand , the cosmological constant in the universe is very small but non - zero . It has been proposed that quantum effects of the gravity could generate a positive cosmological coefficient .This implies that the AdS space corresponds to an inflationary universe . Thus , studying quantum effects in AdS spaces offers a possible way to explain the very small but non - zero cosmological function .However , the AdS spaces have a time - like curvature , and therefore the physics in the boundary CFT does not have a time direction . This implies that , since the Hawking radiation is a process from the past of the boundary , it does not alter the CFT .Thus , one cannot explore the Cosmic No Hair conjecture from the AdS / CFT relationship . The cosmic no - hair conjecture is that black holes in an increasing universe should have the same conserved quantities as the universe .So, it is desirable to study the Hawking radiation in AdS spaces in",
        "rewrite_text": "In this study, we explore a stationary, spherically symmetric black hole in the de Sitter space, characterized by a constant positive curvature. We delve into the Bogoliubov coefficients that relate the Hawking radiation with the Bunch-Davies vacuum, and find that the de Sitter horizon serves as a distinctive vacuum marker. We also consider the case of a small mass and estimate the particle creation rate and power flux near the horizon. Interestingly, we observe that the electric flux remains positive even within the de Sitter space, which challenges the cosmic no-hair conjecture.\n\nWhy is the study of Hawking radiation from black holes in de Sitter spaces significant? The AdS/CFT relationship, proposed by various researchers such as M. M. Roberts, V. A. Hubeny, C. Ranganathan, and E. funcatsbooks.com, links a gravity concept in higher-dimensional anti-de Sitter spaces to a conformal field model in physical spacetime. Consequently, numerous investigations have been conducted on quantum effects in AdS spaces, particularly regarding black holes and cosmological horizons. One reason for studying quantum gravitational effects in AdS spaces is to gain insights into higher-dimensional gravity from the perspective of lower-dimensional CFT.\n\nMoreover, the cosmological constant in the universe, while extremely small, is non-zero. It has been suggested that quantum effects of gravity could generate a positive cosmological constant. This suggests that the AdS space mirrors an inflationary universe. Therefore, exploring quantum effects in AdS spaces offers a potential explanation for the minimal yet non-zero cosmological function.\n\nHowever, it's worth noting that AdS spaces possess a time-like curvature, which means the physics observed in the boundary CFT lacks a temporal direction. This implies that since Hawking radiation is a process occurring in the past of the boundary, it does not alter the CFT. Consequently, the Cosmic No Hair conjecture cannot be explored through the AdS/CFT relationship. This conjecture states that black holes in an expanding universe should share conserved properties with the universe itself.\n\nThus, it becomes crucial to investigate Hawking radiation in AdS spaces to further our understanding of these intricate relationships and potential implications they hold for our understanding of the universe.",
        "ori-fast-z-score": 2.611714500396042,
        "water-fast-z-score": 6.639528095680697,
        "rewrite-fast-z-score": 2.3284515771189986
    },
    {
        "original_text": "In quantum mechanics, the angular momentum (AM) and linear momentum (PM) operators have a spectrum of eigenvalues that is far more extensive than that allowed by classical Physics. For example, the eigenvalues of the square of the total AM operator in a given quantum mechanical state are proportional to the total nuclear spin, a property with theoretically unlimited resolution. In addition, the eigenvalues of the square of the total linear momentum operator in a given quantum mechanical state are equal to the square of the reciprocal of the reduced Planck s constant, a physically observable property with theoretically unlimited precision. Since these quantities are numerically equal to, respectively, the total AM and total PM of the quantum state, this work concludes that quantum mechanics does in fact adhere to the AM and PM properties expected from classical Physics. These extraordinary results are the consequence of rigorous mathematical theorems from the field of Quantum Mechanics, which state that the eigenvalues of these operators are theoretically unbounded. Classical Physics would suggest that the values of these operators be limited by the product of the reduced Planck s constant and the corresponding spatial dimension of the system. The eigenvalues of the square of the total AM operator in a given quantum mechanical state are equal to the square of the reciprocal of the reduced Planck s constant, a physically observable property with theoretically unlimited precision. Since these quantities are numerically equal to, respectively, the total AM and total PM of the quantum state, this work concludes that quantum mechanics does in fact adhere to the AM and PM properties expected from classical Physics. Similarly, the expectation value of the square of the total angular momentum operator in a given quantum mechanical state is equal to the reduced nuclear spin of the quantum state, a property that theoretically has no upper limit. Furthermore, the expectation value of the square of the total linear momentum operator in a given quantum mechanical state is equal to the reduced Planck s constant, a physically observable property with theoretically unlimited precision. Since these quantities are numerically equal to, respectively, the reduced nuclear spin and the reduced Planck s constant, this work concludes that quantum mechanics adheres to the AM and PM properties expected from classical Physics.",
        "watermark_text": "In quantum mechanics , the angular velocity ( AM ) and linear momentum ( PM ) operators have a range of eigenvalues that is far more extensive than that allowed by classical Physics . For instance , the eigenvalues of the square of the total AM operator in a given quantum mechanical state are equal to the total atomic spin , a property with theoretically unlimited resolution .In addition , the eigenvalues of the square of the total linear momentum operator in a given quantum mechanical state are equal to the square of the reciprocal of the reduced Planck s constant , a physically observable property with theoretically unlimited precision . Since these quantities are numerically equivalent to , respectively , the total AM and total PM of the quantum state , this study asserts that quantum mechanics does in indeed adhere to the AM and PM properties seen from theoretical Physics .These exceptional results are the consequence of rigorous mathematical theorems from the field of Quantum Mechanics , which claim that the eigenvalues of these operators are theoretically unbounded . Classical Physics would suggest that the values of these operators be restricted by the sum of the reduced Planck s constant and the associated temporal dimension of the system .The eigenvalues of the square of the total AM operator in a given quantum mechanical state are equal to the square of the reciprocal of the reduced Planck s constant , a physically observable property with theoretically unlimited precision . Since these quantities are numerically equivalent to , respectively , the total AM and total PM of the quantum state , this study asserts that quantum mechanics does in indeed adhere to the AM and PM properties seen from theoretical Physics .Similarly , the expectation value of the square of the total angular velocity operator in a given quantum mechanical state is equal to the reduced nuclear spin of the quantum state , a property that theoretically has no upper limitation . Furthermore , the expectation value of the square of the total linear momentum operator in a given quantum mechanical state is equal to the reduced Planck s constant , a physically observable property with theoretically unlimited precision .Since these quantities are numerically equivalent to , respectively , the reduced nuclear spin and the reduced Planck s constant , this research concludes that quantum mechanics adheres to the AM and PM properties expected from theoretical Physics .",
        "rewrite_text": "In quantum mechanics, the operators for angular momentum (AM) and linear momentum (PM) possess a wider range of eigenvalues than those permitted by classical physics. For instance, in a particular quantum mechanical state, the eigenvalues of the square of the total AM operator are equal to the total atomic spin, which has a theoretically unlimited resolution. Similarly, the eigenvalues of the square of the total linear momentum operator in a given quantum state are equivalent to the square of the reciprocal of Planck's reduced constant, a physically observable property with theoretically unlimited precision.\n\nThese exceptional results are a consequence of rigorous mathematical theorems in quantum mechanics, which claim that the eigenvalues of these operators are not theoretically bounded. In contrast, classical physics suggests that the values of these operators should be constrained by the sum of Planck's reduced constant and the associated temporal dimension of the system. Furthermore, in a specific quantum mechanical state, the expectation value of the square of the total AM operator equals the square of the reciprocal of Planck's reduced constant, which is a property with theoretical precision limits but no upper limit in reality.\n\nSimilarly, the expected value of the square of the total angular velocity operator in a given quantum state equals the reduced nuclear spin of that state, which has no theoretical upper limit. Additionally, the expected value of the square of the total linear momentum operator in a quantum mechanical state is equal to Planck's reduced constant itself, again a property with unlimited theoretical precision. As these quantities are numerically equivalent to the reduced nuclear spin and Planck's reduced constant respectively, this research concludes that quantum mechanics aligns with the expected AM and PM properties derived from theoretical physics.",
        "ori-fast-z-score": 2.230769230769231,
        "water-fast-z-score": 5.5549205986353085,
        "rewrite-fast-z-score": 3.5496478698597698
    },
    {
        "original_text": "In the paradigm of shock-acceleration of cosmic rays, it is generally accepted that supernova remnants (SNRs) accelerate hadrons to high energies, while electrons and positrons are often constrained to lower energies. While electrons and positrons are often described as a ‘secondaries’ resulting from hadron-hadron interactions, observations of the galactic SNR W28 reveal an electron-positron spectral component that extends to GeV energies. This high-energy component, however, is not well described by particle-in-cell simulations of only hadrons (pi+ + pi–) undergoing pitch-angle scattering. We find that including a low-energy electron-positron component (composed of a 100 MeV pool and a 10 GeV tail) in a hadron-only simulation is sufficient to match the observed electron and positron flux, as well as the electron-to-proton ratio. This observation is surprising, as such a low-energy electron-positron component had not been implicated in previous studies of hadron-only scenarios. We speculate that cosmic rays experience self-similar evolution, with a quasi-parallel, low-energy component that efficiently develops in shocks modified by the pervasive cosmic-ray population. If this is the case, this low-energy electron-positron component may provide a more readily observable signature of shock modification by cosmic rays.",
        "watermark_text": "In the terminology of shock - acceleration of cosmic rays , it is usually agreed that supernova remnants ( SNRs ) accelerate hadrons to great energies , while electrons and positrons are often constrained to smaller energies . While atoms and positrons are often characterized as a ‘ secondaries ’ resulting from hadron - hadron interactions , observations of the galactic SNR W28 indicate an electron - positron spectral component that extends to GeV energies .This high - energy component , however , is not well described by particle - in - cell simulations of only hadrons ( pi + + pi – ) undergoing pitch - angle scattering . We see that using a small - energy electron - positron component ( composed of a 100 MeV pool and a 10 GeV tail ) in a hadron - only experiment is adequate to match the seen ion and positron flux , as well as the electron - to - proton ratio .This prediction is surprising , as such a small - energy electron - positron component had not been involved in earlier analyses of hadron - only scenarios . We speculate that cosmic rays exhibit self - similar development , with a quasi - parallel , low - energy component that efficiently develops in shocks modified by the pervasive cosmic - ray population .If this is the case , this low - energy electron - positron component may provide a more fully observable signature of shock modification by cosmic rays .",
        "rewrite_text": "In the context of cosmic ray terminology, particularly the acceleration of shockwaves, there is a consensus that supernova remnants (SNRs) typically accelerate hadrons to high energy levels. In contrast, electrons and positrons tend to be limited to lower energy ranges. While atoms and positrons are often classified as \"secondary\" particles resulting from hadron-hadron interactions, observations of the galactic SNR W28 reveal an extended electron-positron spectral component that reaches into the GeV energy range.\n\nHowever, this high-energy component is not accurately described by simulations solely focusing on hadrons (such as pi+ and pi-) undergoing pitch-angle scattering without the inclusion of other particle types. It has been observed that incorporating a low-energy electron-positron component—composed of a pool of 100 MeV and a tail extending to 10 GeV—into hadron-only experiments adequately matches the observed ion and positron fluxes, as well as the electron-to-proton ratio. This prediction is unexpected as a low-energy electron-positron component had not been previously considered in analyses of hadron-only scenarios.\n\nWe speculate that cosmic rays may exhibit self-similar development, with a quasi-parallel, low-energy component that efficiently emerges in shocks influenced by the pervasive cosmic ray population. If this is indeed the case, this low-energy electron-positron component could provide a more observable signature of how cosmic rays modify shocks.",
        "ori-fast-z-score": 2.3597502097958545,
        "water-fast-z-score": 6.463663618136471,
        "rewrite-fast-z-score": 3.025290226140453
    },
    {
        "original_text": "The spectrum of the Broad-Line Radio Galaxy 3C 445 was observed by both the European Photon Imaging Camera (EPIC) on XMM-Newton and the Nuclear Spectroscopic Telescope ARray (Nustar). The data analysis presented in this work focuses on the last 15 months of the Suzaku X-ray data, from April 2013 to September 2014. This time interval includes two major outbursts and a deep dip in the X-ray flux, which is of particular interest as it may be connected to an episode of enhanced infrared emission observed by Spitzer. The Suzaku X-ray data during this period show strong neutral Fe K emission at approximately 6.4 keV, along with significant emission from atomic oxygen between 54 and 72 keV, and from Ne and Mg between 14 and 30 keV. The neutral and low-ionization iron and oxygen emission features are consistent with the residual accretion disk of the black hole, while the high-ionization emission can be associated with jet synchrotron and possibly inverse-Compton emission. We also detect narrow He-like and H-like oxygen and neon lines, likely produced by irradiation of the cold disk by energetic particles produced in the jet. This is the first detection of the high-ionization emission from oxygen, neon, and magnesium in 3C 445. We speculate that the increased 14–30 keV emission observed during the Suzaku observation may be due to electron cyclotron resonance scattering of thermal microwave emission from an episode of enhanced activity in the radio jet.",
        "watermark_text": "The spectrum of the Broad - Line Radio Galaxy 3C 445 was seen by both the European Photon Imaging Camera ( EPIC ) on XMM - Newton and the Nuclear Spectroscopic Telescope ARray ( Nustar ) . The data analysis provided in this project focuses on the last 15 weeks of the Suzaku X - ray data , from April 2013 to September 2014 .This period interval includes two major outbursts and a deep dip in the X - ray flux , which is of especially interest as it could be connected to an episode of enhanced infrared absorption observed by Spitzer . The Suzaku X - ray data during this era show intense neutral Fe K emission at approximately 6 . 4 keV , along with substantial absorption from atomic hydrogen between 54 and 72 keV , and from Ne and Mg between 14 and 30 keV .The neutral and low - ionization iron and oxygen absorption elements are compatible with the residual accretion disk of the dark hole , while the high - ionization emission can be correlated with jet synchrotron and maybe inverse - Compton absorption . We additionally observe narrow He - like and H - like oxygen and neon lines , likely generated by irradiation of the cool disk by energetic particles generated in the jet .This is the first measurement of the high - ionization emitted from oxygen , neon , and magnesium in 3C 445 . We speculate that the increased 14 – 30 keV radiation observed during the Suzaku observation may be due to electron cyclotron resonance reflection of thermal microwave emission from an episode of enhanced activity in the television plane .",
        "rewrite_text": "The spectrum of 3C 445, a Broad-Line Radio Galaxy, has been observed by the European Photon Imaging Camera (EPIC) on XMM-Newton and the Nuclear Spectroscopic Telescope Array (Nustar). This project's data analysis centers on the Suzaku X-ray data collected over the last 15 weeks, spanning from April 2013 to September 2014. This timeframe encompasses two major outbursts and a notable dip in X-ray flux, particularly significant as it may be linked to an enhanced infrared absorption episode observed by Spitzer.\n\nDuring this period, the Suzaku X-ray data reveal a strong neutral Fe K emission at approximately 6.4 keV. Additionally, there is considerable absorption from atomic hydrogen between 54 and 72 keV and from Ne and Mg between 14 and 30 keV. The neutral and low-ionization iron and oxygen absorption features are consistent with the residual accretion disk of the black hole. Meanwhile, the high-ionization emission can be correlated with jet synchrotron radiation and possibly inverse-Compton absorption.\n\nFurthermore, we observe narrow He-like and H-like oxygen and neon lines, likely generated by the irradiation of the cool disk by energetic particles emitted in the jet. This is the first measurement of high-ionization emission from oxygen, neon, and magnesium in 3C 445. We speculate that the increased 14-30 keV radiation observed during the Suzaku observation could be attributed to electron cyclotron resonance reflection of thermal microwave emission from an enhanced activity episode in the galactic plane.",
        "ori-fast-z-score": -1.116880781646981,
        "water-fast-z-score": 5.584403908234905,
        "rewrite-fast-z-score": 1.3337718577107005
    },
    {
        "original_text": "Astronomers using the HATNet visual surveys have discovered an eclipsing binary, HAT-TR-205-013, that consists of a star with a massive dark companion. High-precision radial velocity measurements confirm that the companion is a dark star and not a stellar or substellar object. The minimum companion mass is determined to be 65 M$_{JUP}$ and the radius of the companion is 7.2 R$_{JUP}$. The density of the companion is 6.7 x 10^{5} kg/m^{3}. The existence of dark stars with such low densities make it possible for them to be stable against nuclear reactions. The HATNet team has located eight additional systems with similar dark companions. These “HAT-P-27” systems will be presented in a future paper. This work was presented in a preliminary form at the Conference on Lasers Astrophysics and Technology (New Mexico, USA; 13-17 March 2013) and at the IAU General Assembly in Moscow (Russia; 20 September 2013).",
        "watermark_text": "Astronomers using the HATNet visual surveys have discovered an eclipsing binary , HAT - TR - 205 - 013 , that consists of a star with a massive dark companion . High - precision radial velocity measurements confirm that the companion is a dark star and not a stellar or substellar object .The minimum companion mass is calculated to be 65 M $ _ { JUP } $ and the radius of the companion is 7 . 2 R $ _ { JUP } $ . The density of the companion is 6 . 7 x 10 ^ { 5 } kilograms / m ^ { 3 } .The existence of light stars with such low densities give it able for them to be active against nuclear reactions . The HATNet team has found eight additional systems with similar black companions .These “ HAT - P - 27 ” structures will be described in a future paper . This project was presented in a preliminary shape at the Conference on Lasers Astrophysics and Technology ( New Mexico , USA ; 13 - 17 March 2013 ) and at the IAU General Assembly in Moscow ( Russia ; 20 September 2013 ) .",
        "rewrite_text": "Astronomers, utilizing the HATNet visual surveys, have discovered an eclipsing binary system named HAT-TR-205-013. This system comprises a star with a massive, dark companion. Precise radial velocity measurements have verified that the companion is a dark star and not a stellar or substellar object. The estimated minimum companion mass is 65 MJup, with a radius of 7.2 RJup. Its density, at 6.7 x 10^5 kilograms per cubic meter, allows it to potentially be active in nuclear reactions. The HATNet team has identified eight additional systems with similar dark companions, and these \"HAT-P-27\" structures will be elaborated in a future research paper. This project was initially presented at the Conference on Lasers Astrophysics and Technology held in New Mexico, USA (from 13th to 17th March 2013), as well as at the IAU General Assembly in Moscow, Russia (on 20th September 2013).",
        "ori-fast-z-score": -0.674199862463242,
        "water-fast-z-score": 2.4053511772118195,
        "rewrite-fast-z-score": -0.13736056394868904
    },
    {
        "original_text": "Nonstationary random acoustic and electromagnetic fields as wave diffusion processes. Such fields were measured in the vicinity of a turbulent buble at a relative velocity of 225 m/s. An analysis in the frame of such a theory allowed us to deduce the statistical characteristics of the microscale eddies causing the nonstationarity. The result are in good agreement with the measured field characteristics. watson Nonstationary random acoustic and electromagnetic fields as wave diffusion processes. Such fields were measured in the vicinity of a turbulent buble at a relative velocity of 225 m/s. An analysis in the frame of such a theory allowed us to deduce the statistical characteristics of the microscale eddies causing the nonstationarity. The result are in good agreement with the measured field characteristics. also Nonstationary random acoustic and electromagnetic fields as wave diffusion processes. Such fields were measured in the vicinity of a turbulent buble at a relative velocity of 225 m/s. An analysis in the frame of such a theory allowed us to deduce the statistical characteristics of the microscale eddies causing the nonstationarity. The result are in good agreement with the measured field characteristics.",
        "watermark_text": "Nonstationary random acoustic and electromagnetic fields as wave diffusion processes . Such fields were calculated in the vicinity of a turbulent buble at a relative velocity of 225 m / s .An evaluation in the frame of such a theory helped us to deduce the statistical characteristics of the microscale eddies creating the nonstationarity . The result are in good agreement with the measured field qualities .watson Nonstationary random acoustic and electromagnetic forces as wave diffusion processes . Such fields were calculated in the vicinity of a turbulent buble at a relative velocity of 225 m / s .An evaluation in the frame of such a theory helped us to deduce the statistical characteristics of the microscale eddies creating the nonstationarity . The result are in good agreement with the measured field qualities .also Nonstationary random acoustic and electromagnetic fields as wave diffusion processes . Such fields were calculated in the vicinity of a turbulent buble at a relative velocity of 225 m / s .An evaluation in the frame of such a theory helped us to deduce the statistical characteristics of the microscale eddies creating the nonstationarity . The result are in good agreement with the measured field qualities .",
        "rewrite_text": "Rephrased text in English:\n\nNonstationary random acoustic and electromagnetic fields behave as wave diffusion processes. These fields were computed in the proximity of a turbulent bubble with a relative velocity of 225 m/s. Utilizing an evaluation within the framework of this theory, we were able to deduce the statistical features of the microscale eddies contributing to the nonstationarity. The findings align well with the measured qualities of the field.\n\nAdditionally, random acoustic and electromagnetic forces that are nonstationary operate as wave diffusion processes. These fields were calculated around a turbulent bubble at a speed of 225 m/s. A thorough assessment within this theoretical framework assisted us in determining the statistical properties of the microscale eddies responsible for the nonstationarity. Once again, our results are in good agreement with the measured field qualities.",
        "ori-fast-z-score": 1.6666666666666667,
        "water-fast-z-score": 5.300713251592634,
        "rewrite-fast-z-score": 1.3363062095621219
    },
    {
        "original_text": "As an alternative to the traditional RF QC device design, which comprises a scanning function, the DAFNE baseline design incorporates an ion clearing electrode (ICE) in the feedback loop to mitigate the RF induced beam rotation and energy spread effects. Results obtained with the ICE inserted in the feedback loop demonstrate an improvement in beam energy and stability when compared to the DAFNE baseline performance without the ICE. However, the ICE induced feedback component in the focusing element generates significant beam trajectory changes, which needs to be mitigated by adjusting the wavefront sensor based feedback offset. This paper reports the results of the impact of ICE on DAFNE beam dynamics. The paper starts with an overview of DAFNE concept and design followed by the description of ICE, the effects the ICE have on the DAFNE baseline performance, different methods to mitigate the ICE effect, and the effects of different mitigation methods on the beam dynamics. The paper concludes with a discussion of the achieved beam stability and the trade-offs involved in the different mitigation methods.",
        "watermark_text": "As an alternative to the typical RF QC device model , which contains a scan function , the DAFNE baseline model includes an ion clearing electrode ( ICE ) in the feedback loop to mitigate the RF induced laser spinning and energy spread effects . Results obtained with the ICE inserted in the feedback loop demonstrate an improvement in beam energy and strength when compared to the DAFNE baseline performance without the ICE .However , the ICE generated feedback portion in the directing component produces significant beam path shifts , which requires to be mitigated by adjusting the wavefront sensor independent feedback offset . This paper reports the results of the impact of ICE on DAFNE beam dynamics .The paper starts with an overview of DAFNE concept and design led by the description of ICE , the effects the ICE have on the DAFNE baseline performance , different methods to mitigate the ICE impact , and the effects of different mitigation methods on the beam dynamics . The paper closes with a review of the achieved beam stability and the trade - offs involved in the different mitigation methods .",
        "rewrite_text": "In contrast to the typical RF Quality Control (QC) device model, which incorporates a scanning function, the DAFNE baseline model introduces an Ion Clearing Electrode (ICE) within the feedback loop to alleviate the effects of RF-induced laser spinning and energy spread. When the ICE is integrated into the feedback loop, it results in an enhancement of beam energy and intensity compared to the DAFNE's baseline performance without the ICE. Nevertheless, the ICE-generated feedback component in the steering mechanism leads to notable beam path deviations, necessitating adjustments to the independent feedback offset of the wavefront sensor to mitigate these shifts.\n\nThis paper presents an examination of the impact of the ICE on the DAFNE beam dynamics. It begins with an overview of the DAFNE concept and design, focusing specifically on the description of the ICE and its effects on the DAFNE baseline performance. Various methods to mitigate the ICE's influence are discussed, along with the consequences of different mitigation strategies on beam dynamics. Ultimately, the paper concludes with a review of achieved beam stability and the trade-offs associated with the various mitigation techniques employed.",
        "ori-fast-z-score": 1.0,
        "water-fast-z-score": 5.74243935589202,
        "rewrite-fast-z-score": 0.22645540682891913
    },
    {
        "original_text": "A chain-boson model is proposed to explain the decoherence and relaxation of a few coupled SQUIDs in a phonon bath. The model contains a set of oscillators coupled to the low-lying modes of the environments, which are modeled by an harmonic chain. It is found that the phonon bath induced decoherence and relaxation can be effectively described by a pure decay process, whose rate is determined by the spectral density of the bath and the dimensionless coupling constants between the central system and the bath. In particular, when the dimensionless coupling constants are small, the dynamics of the system can be approximately described by a Lindblad equation, from which the energy relaxation and decoherence of the coupled SQUIDs can be understood. The theory can be applied to study the decoherence and relaxation in other hybrid systems consisting of a few coupled quantum systems and harmonic chains. This model can also be used to study the quantum effects of a coupled SQUID system immersed in a bosonic bath. The quantum dynamics of the coupled SQUID system can be solved by numerically diagonalizing the corresponding Hamiltonian in a truncated Fock space. As an example, we study a phase qubit (the coupled SQUID system) in a linearly coupled chain, which is capable of simulating two-dimensional correlated dynamics. Numerical simulation results show that, when the chain length is even and the applied magnetic flux in the qubit loop is close to a half-flux quantum, two-dimensional quantum chaos takes place. The quantum entanglement between the two coupled SQUIDs increases with the number of qubits in the bath, and displays a behavior of wave nature with the increase of bath dimensionality.",
        "watermark_text": "A chain - boson theory is proposed to explain the decoherence and relaxation of a few coupled SQUIDs in a phonon bath . The model includes a group of oscillators attached to the small - lying modes of the surroundings , which are modeled by an harmonic chain .It is found that the phonon bath induced decoherence and relaxation can be effectively described by a pure decay mechanism , whose rate is chosen by the spectral density of the bath and the dimensionless interaction constants between the main network and the bath . In particular , when the dimensionless coupling constants are small , the dynamics of the system can be approximately characterized by a Lindblad equation , from which the power relaxation and decoherence of the coupled SQUIDs can be understood .The theory can be applied to study the decoherence and relaxation in other hybrid systems composed of a few coupled quantum systems and harmonic chains . This theory can also be used to study the quantum effects of a coupled SQUID system immersed in a bosonic bath .The quantum mechanics of the coupled SQUID system can be solved by numerically diagonalizing the associated Hamiltonian in a truncated Fock space . As an instance , we study a phase qubit ( the coupled SQUID system ) in a linearly coupled chain , which is capable of simulating two - dimensional correlated dynamics .Numerical modeling results show that , when the chain length is even and the applied magnetic flux in the qubit loop is nearest to a half - flux quantum , two - dimensional quantum confusion occurs place . The quantum entanglement between the two coupled SQUIDs increases with the number of qubits in the shower , and displays a behavior of wave nature with the increase of bath dimensionality .",
        "rewrite_text": "A proposed theory of the chain-boson model elucidates the decoherence and relaxation phenomena of several coupled SQUIDs within a phonon bath. The model involves a group of oscillators connected to the small-scale vibrational modes of the environment, which are represented by an harmonic chain. It has been discovered that the decoherence and relaxation induced by the phonon bath can be effectively described by a pure decay mechanism. This mechanism's rate is determined by the spectral density of the bath and the dimensionless interaction constants between the main network and the bath. Specifically, when the dimensionless coupling constants are minimal, the system's dynamics can be roughly characterized by a Lindblad equation, providing an understanding of the power relaxation and decoherence of the coupled SQUIDs.\n\nThis theory can be applied to investigate decoherence and relaxation in other hybrid systems composed of several interconnected quantum systems and harmonic chains. Furthermore, it can be utilized to explore the quantum effects of a SQUID system coupled to a bosonic bath. The quantum mechanics of the coupled SQUID system can be solved numerically through diagonalizing the associated Hamiltonian in a truncated Fock space.\n\nAs an illustrative example, we investigate a phase qubit (the coupled SQUID system) within a linearly coupled chain. This setup is capable of simulating two-dimensional correlated dynamics. Numerical modeling reveals that when the chain length is even and the applied magnetic flux in the qubit loop is closest to half a flux quantum, a two-dimensional quantum confusion arises. Additionally, the quantum entanglement between the two coupled SQUIDs increases with the number of qubits in the system, exhibiting wave-like behavior with the growth of bath dimensionality.",
        "ori-fast-z-score": 0.647150228929434,
        "water-fast-z-score": 4.714951667914447,
        "rewrite-fast-z-score": 0.9284766908852594
    },
    {
        "original_text": "The velocity structure of the solar corona is of great importance for understanding the processes taking place within this dynamic environment. Since coronal emission is often faint and broadened due to the high speeds of the material, it is difficult to observe the full velocity profile of the solar wind. Therefore, numerous studies have analyzed either the line-wing or the line-center of coronal emission lines to determine the properties of the plasma. In this study, we combine data from the EUV spectrograph (ESA/SOHO) with data from the EUV imager (SECCHI/SIBYNTES) and the HIA/SDO to determine velocity profiles for an extensive range of coronal emission lines. We find that the coronal plasma is moving at supersonic speeds, ranging from 900 km/s to over 1,000 km/s, with strong scatter in the speeds observed along a single coronal loop. By analyzing this scatter, we determine that Alfvén waves are likely propagating along the loop.",
        "watermark_text": "The velocity composition of the solar corona is of large importance for studying the cycles taking place within this dynamic climate . Since coronal emission is often dim and broadened due to the high velocity of the material , it is unable to observe the full velocity profile of the solar wind .Therefore , various surveys have analyzed either the line - wing or the line - center of coronal emission lines to study the properties of the plasma . In this study , we merge data from the EUV spectrograph ( ESA / SOHO ) with data from the EUV imager ( SECCHI / SIBYNTES ) and the HIA / SDO to study velocity characteristics for an extensive range of coronal emission lines .We see that the coronal plasma is moving at supersonic altitudes , ranging from 900 km / s to over 1 , 000 km / s , with powerful scatter in the velocity observed along a single coronal loop . By analyzing this scatter , we determine that Alfvén currents are likely propagating along the loop .",
        "rewrite_text": "The study of the velocity composition in the solar corona holds great significance for understanding the cycles occurring within this dynamic environment. Due to the high velocity of the material, coronal emission is often faint and broadened, making it challenging to observe the complete velocity profile of the solar wind. Therefore, various surveys have focused on analyzing either the line-wing or line-center of coronal emission lines to investigate the properties of the plasma.\n\nIn this research, we have combined data from the EUV spectrograph (ESA/SOHO) with data from the EUV imager (SECCHI/SIBYNTES) and the HIA/SDO to explore velocity characteristics across a wide range of coronal emission lines. Our observations reveal that the coronal plasma is moving at supersonic speeds, ranging from 900 km/s to over 1,000 km/s, with significant velocity dispersion observed along individual coronal loops. By analyzing this dispersion, we conclude that Alfvén currents are likely propagating along these loops.",
        "ori-fast-z-score": -0.3721042037676254,
        "water-fast-z-score": 5.169842621131974,
        "rewrite-fast-z-score": 0.3611575592573076
    },
    {
        "original_text": "Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement Xiaoqiang Xia, Lihua Xie, and Shih-Fu Chang IEEE Transactions on Communications, Vol. 67, No. 5, pp. 3728-3741, May 2019 Gaussian layered broadcast coding with successive refinement was proposed in the seminal work by Koetter and Hoeky (KD III). The idea is to partition the source message into layers, and encode the layers with a long block length assuming perfect knowledge of the headers of the lower layers. The headers are further encoded with shorter length assuming imperfect knowledge of the lower layer headers. The resulting coding rate can be conveniently optimized over the Gaussian distribution, leading to an optimal layered coding strategy that is able to approach the capacity of the underlying multi-user discrete memoryless channel (DMMC) within 1 bit per channel use for many channels of interest. Successive refinement, however, typically incurs an accumulated distortion that increases with the length of the encoding block. In particular, for a DMMC with memory size K, when the encoding block length is no greater than (the so-called max-delay), the distortion can be shown to increase linearly with the block length. When the block length exceeds (the so-called delay-limited regime), the distortion can be shown to increase at most linearly with the block length. The exact slope in the delay-limited regime, which we refer to as the delay-regime slope, has been identified for only a few DMMC examples. In this work, we provide a general result for the delay-limited regime slope, which is shown to coincide with the latest know result for several specific examples, including the Hyperlabel Broadcast Channel (HLC) and the Gaussian Multi-Unicast with Successive Refinement (MUSIC).",
        "watermark_text": "Distortion Minimization in Gaussian Layered Broadcast Coding with Successive Refinement Xiaoqiang Xia, Lihua Xie, and Shih-Fu Chang IEEE Transactions on Communications, Vol.67, No.5 , pp . 3728 - 3741 , May 2019 Gaussian layered broadcast coding with successive refinement was suggested in the seminal study by Koetter and Hoeky ( KD III ) .The idea is to partition the source message into layers , and encode the layers with a long block length assuming good knowledge of the headers of the lower layers . The headers are further encoded with shorter length assuming imperfect knowledge of the lower layer headers .The resulting coding speed can be conveniently optimized over the Gaussian distribution , leading to an efficient layered code approach that is able to approach the ability of the underlying multi - user discrete memoryless channel ( DMMC ) within 1 bit per channel use for large channels of interest . Successive refinement , however , generally incurs an accumulated degradation that increases with the length of the encoding block .In particular , for a DMMC with storage size K , when the encoding block speed is no greater than ( the so - called max - delay ) , the distortion can be shown to expand linearly with the block length . When the block speed exceeds ( the so - called delay - limited regime ) , the distortion can be shown to expand at most linearly with the block depth .The exact slope in the delay - limited regime , which we name to as the delay - regime slope , has been described for only a few DMMC examples . In this research , we provide a general consequence for the delay - limited regime curve , which is demonstrated to coincide with the latest know result for numerous certain instances , notably the Hyperlabel Broadcast Channel ( HLC ) and the Gaussian Multi - Unicast with Successive Refinement ( MUSIC ) .",
        "rewrite_text": "Minimizing Distortion in Gaussian Layered Broadcast Coding with Successive Refinement\n\nXiaoqiang Xia, Lihua Xie, and Shih-Fu Chang presented their research in the IEEE Transactions on Communications, Volume 67, Issue 5, pages 3728 to 3741, published in May 2019. They discussed Gaussian layered broadcast coding with successive refinement, which was initially proposed by Koetter and Hoeky (KD III). This approach involves partitioning the source message into layers and encoding those layers with a long block length, assuming a good understanding of the lower layer headers. The headers are then further encoded with a shorter block length, considering an imperfect knowledge of the lower layer headers. This coding method can be conveniently optimized over the Gaussian distribution, resulting in an efficient layered code that can closely approach the performance of the underlying multi-user discrete memoryless channel (DMMC), achieving a difference of only 1 bit per channel use for large channels of interest.\n\nHowever, successive refinement generally leads to accumulated degradation that increases with the length of the encoding block. Specifically, for a DMMC with a storage size of K, when the encoding block speed is not greater than the max-delay limit, the distortion can be observed to expand linearly with the block length. Conversely, when the block speed exceeds the delay-limited regime, the distortion can be shown to expand at most linearly with the block depth. The exact slope in the delay-limited regime, which they refer to as the delay-regime slope, has only been described for a few DMMC examples.\n\nIn this study, we provide a general consequence for the delay-limited regime curve. Our findings demonstrate that this curve aligns with the latest known results for numerous specific cases, notably the Hyperlabel Broadcast Channel (HLC) and the Gaussian Multi-Unicast with Successive Refinement (MUSIC).",
        "ori-fast-z-score": -1.1239029738980328,
        "water-fast-z-score": 4.120977570959454,
        "rewrite-fast-z-score": 2.3539293971054818
    },
    {
        "original_text": "Interlacement is a way to measure the path-wise intersection of one-dimensional random process. It was introduced by S. interlacements and I. Melbourne in 2010. In this paper, I study the vacant set of random interlacements. I prove that when interlacement varies sufficiently slow, almost surely, the vacant set is empty. When interlacement is quick, I prove that the vacant set has full intensity. I use these results and coupling to prove that when interlacement percolates, it connects for almost sure. Paper URL: https://arxiv.org/pdf/1709.04972.pdf Total words: 372 Keywords: Interlacement, Vacant Set, Percolation I thank Jianqing Xiao for several helpful discussions and comments. Thank you for your attention. Yuhan Zhang Frontiers Research Institute, Department of Mathematics, 129 Middlebury Road, Stanford University, Stanford, CA 94305, USA. yzhang5@cs.stanford.edu Address: Frontiers Research Institute, Department of Mathematics, 129 Middlebury Road, Stanford University, Stanford, CA 94305, USA Telephone: 650-723-8321 Fax: 650-462-2622 Website: http://frontiersresearch.org International School for Advanced Studies, via Bonomea, n° 170, 34136. Napoli, Italy. yzhang5@iss.it Email: yzhang5@iss.it Web: http://frontiersresearch.org Thank you. Yuhan Zhang September 22, 2023 Abstract In this paper, I study the vacant set of random interlacements. I prove that when interlacement varies sufficiently slow, almost surely, the vacant set is empty. When interlacement is quick, I prove that the vacant set has full intensity. I use these results and coupling to prove that when interlacement percolates, it connects for almost sure. Interlacement was first introduced by S. interlacements and I. Melbourne in 2010. In this paper, I study the vacant set of random interlacements. I prove that when interlacement varies sufficiently slow, almost surely, the vacant set is empty. When interlacement is quick, I prove that the vacant set has full intensity. I use these results and coupling to prove that when interlacement percolates, it connects for almost sure. Let Ω be the space of all real-valued, cadlag paths, and let Ω~+~ be the space of càdlag paths starting at zero. For σ ∈ (0,1 , a σ-random interlacement path is a path I(σ) with the following properties: 1.",
        "watermark_text": "Interlacement is a way to measure the path - wise intersection of one - dimensional random process . It was introduced by S . interlacements and I . Melbourne in 2010 .In this paper , I examine the empty set of random interlacements . I find that when interlacement varies sufficiently slow , nearly surely , the empty set is vacant .When interlacement is quick , I establish that the empty set has full strength . I utilize these results and correlation to confirm that when interlacement percolates , it links for almost sure .Paper URL : https : / / arxiv . org / pdf / 1709 . 04972 . pdf Total words : 372 Keywords : Interlacement , Vacant Set , Percolation I praise Jianqing Xiao for numerous interesting discussions and comments . Thank you for your attention .Yuhan Zhang Frontiers Research Institute , Department of Mathematics , 129 Middlebury Road , Stanford University , Stanford , CA 94305 , USA . yzhang5 @ cs . stanford . edu Address : Frontiers Research Institute , Department of Mathematics , 129 Middlebury Road , Stanford University , Stanford , CA 94305 , USA Telephone : 650 - 723 - 8321 Fax : 650 - 462 - 2622 Website : http : / / frontiersresearch . org International School for Advanced Studies , via Bonomea , n° 170 , 34136 .Napoli , Italy . yzhang5 @ iss . it Email : yzhang5 @ iss . it Web : www : / / frontiersresearch . org Thank you .Yuhan Zhang September 22 , 2023 Abstract In this paper , I examine the empty set of random interlacements . I find that when interlacement varies sufficiently slow , nearly surely , the empty set is vacant .When interlacement is quick , I establish that the empty set has full strength . I utilize these results and correlation to confirm that when interlacement percolates , it links for almost sure .Interlacement was first described by S . interlacements and I . Melbourne in 2010 . In this paper , I examine the empty set of random interlacements .I establish that when interlacement varies sufficiently slow , nearly surely , the empty set is vacant . When interlacement is quick , I establish that the empty set has full strength .I utilize these results and correlation to confirm that when interlacement percolates , it links for almost sure . Let Ω be the space of all real - valued , cadlag trails , and take Ω ~ + ~ be the space of càdlag trails starting at zero .For τ ∈ ( 0 , 1 , a σ - random interlacement path is a path I ( σ ) with the following properties : 1 .",
        "rewrite_text": "The following is the rewritten text in English:\n\nMeasurement of Path-wise Intersection for One-dimensional Random Processes Through Interlacement\n\nInterlacement is a method utilized to quantify the path-wise intersection of one-dimensional random processes. It was first introduced by S. Interlacements and I. Melbourne in 2010. In this paper, I investigate the concept of the empty set in relation to random interlacements. My findings indicate that when interlacement varies sufficiently slowly, almost certainly, the empty set remains unoccupied. Conversely, when interlacement is rapid, I establish that the empty set possesses complete strength. I utilize these results and their correlations to affirm that, when interlacement percolates, it nearly always forms connections.\n\nPaper URL: https://arxiv.org/pdf/1709.04972.pdf\n\nTotal words: 372\n\nKeywords: Interlacement, Vacant Set, Percolation\n\nI express my gratitude to Jianqing Xiao for numerous engaging discussions and valuable comments. Thank you for your attention.\n\nYuhan Zhang\nFrontiers Research Institute\nDepartment of Mathematics\n129 Middlebury Road, Stanford University, Stanford, CA 94305, USA.\nEmail: yzhang5@cs.stanford.edu\nAddress: Frontiers Research Institute, Department of Mathematics, 129 Middlebury Road, Stanford University, Stanford, CA 94305, USA\nTelephone: 650-723-8321\nFax: 650-462-2622\nWebsite: http://frontiersresearch.org\n\nInternational School for Advanced Studies\nVia Bonomea, n° 170, 34136 Napoli, Italy.\nEmail: yzhang5@iss.it\nWeb: www.frontiersresearch.org\n\nYuhan Zhang\nSeptember 22nd, 2023\n\nAbstract:\nIn this paper, I explore the concept of the empty set within the context of random interlacements. My research finds that, when interlacement varies slowly enough, the empty set remains unoccupied in almost all cases. On the other hand, when interlacement is rapid, the empty set is found to have complete strength. I utilize these findings and their correlations to confirm that interlacement, when it percolates, almost always forms connections. This form of interlacement was first described by S. Interlacements and I. Melbourne in 2010. In this study, I further establish that slow-varying interlacement results in an unoccupied empty set while rapid interlacement gives rise to a set with full strength. These observations are reinforced by my analysis of the correlation between interlacement percolation and its linking properties.\n\nLet Ω denote the space of all real-valued cadlag trails. Additionally, consider the space of càdlag trails starting at zero denoted as Ω ~ + ~ . For τ ∈ (0, 1), a σ-random interlacement path is defined as a path I(σ) possessing the following properties: 1. (Note: The rest of the sentence was not provided in the original text.)",
        "ori-fast-z-score": -0.7184212081070996,
        "water-fast-z-score": 8.38552760240712,
        "rewrite-fast-z-score": 1.8461989882527161
    },
    {
        "original_text": "The line intensities of several key molecules have been measured in molecular clouds located in different Galactic environments. These include translatory thermal lines of CO, an excellent tracer of molecular hydrogen, as well as the HCN, CS and CO isotopes  rotational thermal lines. Conversion factors are derived between the line intensities and the properties of the molecular clouds, such as their mass, temperature and density. The derived factors are specific to the different types of galaxies and are given for Sa, Sb, Sc, Sd and Irr galaxies. It is shown that the line intensities of CO, HCN and isotopes can be used as quantitative measures of the masses of molecular clouds. The derived factors are specific to the different types of galaxies and are given for Sa, Sb, Sc, Sd and Irr galaxies. It is shown that the line intensities of CO, HCN and isotopes can be used as quantitative measures of the masses of molecular clouds.",
        "watermark_text": "The line intensities of several key chemicals have been measured in molecular clouds situated in different Galactic settings . These include translatory thermal tracks of CO , an excellent tracer of molecular hydrogen , as well as the HCN , CS and CO isotopes rotational thermal paths .Conversion factors are derived between the line intensities and the properties of the molecular clouds , such as their mass , temperature and density . The derived factors are specific to the different kinds of stars and are given for Sa , Sb , Sc , Sd and Irr objects .It is demonstrated that the line intensities of CO , HCN and isotopes can be used as quantitative measures of the masses of molecular clouds . The derived factors are specific to the different kinds of stars and are given for Sa , Sb , Sc , Sd and Irr objects .It is demonstrated that the line intensities of CO , HCN and isotopes can be used as quantitative measures of the masses of molecular clouds .",
        "rewrite_text": "The intensities of several key chemical lines have been measured within molecular clouds located in various Galactic environments. This includes the translational thermal tracks of carbon monoxide (CO), an effective tracer of molecular hydrogen, along with the rotational thermal paths of HCN, CS, and CO isotopes. Conversion factors have been determined between the line intensities and the characteristics of the molecular clouds, such as their mass, temperature, and density. These factors are unique for different types of stars and are applicable to Sa, Sb, Sc, Sd, and Irr objects. It has been shown that the line intensities of CO, HCN, and their isotopes can quantitatively measure the masses of these molecular clouds. This is also true for the derived factors, which are specific to various star types and applicable across Sa, Sb, Sc, Sd, and Irr objects. Furthermore, it has been established that the aforementioned line intensities serve as reliable quantitative indicators for estimating the masses of molecular clouds.",
        "ori-fast-z-score": 1.8073922282301278,
        "water-fast-z-score": 5.163977794943222,
        "rewrite-fast-z-score": 1.6644794391276478
    },
    {
        "original_text": "The prompt emission of gamma-ray bursts (GRBs) is usually attributed to synchrotron emission of highly relativistic jets. However, the exact emission mechanism is still unknown. While the broadband afterglow phase typically extends to IR or Optical frequencies, the prompt phase usually peaks in the gamma-ray band. It is therefore important to study prompt emission separately from afterglow emission. Here, we report the broadband afterglow and prompt emission of the bright burst GRB 061121. The afterglow phase covers the optical, UV and X-ray bands and can be well described with a single power-law. In the gamma-ray band, the prompt emission can be well described by the Band function. The low and high frequency plateaus, which are hallmarks of the Band function, coincide with the optical and X-ray decay phases, respectively. This implies a physical association between the prompt and the afterglow emission. We discuss implications of this result for the emission mechanism of the prompt phase.",
        "watermark_text": "The prompt emission of gamma - ray bursts ( GRBs ) is usually associated to synchrotron emission of highly relativistic jets . However , the exact emission mechanism is remains obscure .While the broadband afterglow period typically stretches to IR or Optical frequencies , the prompt phase usually peaks in the alpha - ray band . It is consequently key to study prompt emission differently from afterglow emission .Here , we report the broadband afterglow and prompt emission of the bright burst GRB 061121 . The afterglow period includes the optical , UV and X - ray bands and can be well described with a single power - law .In the gamma - ray band , the prompt emission can be well described by the Band function . The high and large frequency plateaus , which are hallmarks of the Band function , coincide with the optical and X - ray decay stages , respectively .This implies a physical association between the prompt and the afterglow emission . We discuss implications of this consequence for the emission mechanism of the prompt phase .",
        "rewrite_text": "The prompt emission of gamma-ray bursts (GRBs) frequently correlates with the synchrotron radiation emitted by highly relativistic jets. Nevertheless, the exact emission process remains elusive. The broadband afterglow typically extends to infrared or optical frequencies, while the prompt phase often peaks in the alpha-ray spectrum. Therefore, it is crucial to investigate the prompt emission separately from the afterglow emission.\n\nIn this report, we present the broadband afterglow and prompt emission of the intense burst, GRB 061121. The afterglow period spans across optical, UV, and X-ray bands and can be adequately described by a single power-law. In the gamma-ray spectrum, the prompt emission is well-represented by the Band function. The high and large frequency plateaus, which are distinctive features of the Band function, align with the optical and X-ray decay stages, respectively. This alignment suggests a physical connection between the prompt and afterglow emissions. We further explore the implications of this finding for the emission mechanism of the prompt phase.",
        "ori-fast-z-score": 1.62746694241347,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": 1.709408646894569
    },
    {
        "original_text": "The photonic flame phenomenon was first observed in a laboratory in 2006 by M. Tsubouchi and S. A. Dutta  1 . This effect is an instability of a light emitting and conical wire flame, which is triggered by slightly inhomogeneous heat convection. This instability manifests itself as thinning and branching of the flame, forming a complex pattern, which can be observed with naked eye in bright light. We have obtained three remarkable features of the photonic flame by means of high-speed camera observation. First, there is a short delay between the moment when the temperature gradient appears at the tip of the branch and the moment when this tip starts to emit light. Such behaviour can be observed in the videoclip linked in the text. Second, it is possible to trigger the flame branches by local heating from the nearby ignited wire. It is also observed in the videoclip. Third, if a stable flame tip is illuminated from behind by a sufficiently bright light, then this light can  reflect  from the front surface of the flame. This  out-of-planear  reflection is also visible in the video.",
        "watermark_text": "The photonic fire feature was first observed in a lab in 2006 by M . Tsubouchi and S . A . Dutta 1 . This phenomenon is an instability of a light emitting and conical wire flame , which is caused by somewhat inhomogeneous thermal convection .This instability manifests itself as thinning and branching of the flame , forming a complex pattern , which can be viewed with naked eye in bright illumination . We have discovered three notable features of the photonic fire by means of high - speed photography observation .First , there is a brief delay between the moment when the temperature gradient appears at the tip of the branch and the moment when this tip starts to emit light . Such behaviour can be shown in the videoclip related in the text .Second , it is could to trigger the burning branches by local heating from the nearby burned cable . It is also observed in the videoclip .Third , if a steady burning tip is lit from behind by a properly visible light , then this light can reflect from the front edge of the flame . This out - of - planear reflection is also noticeable in the video .",
        "rewrite_text": "In 2006, M. Tsubouchi and S.A. Dutta first observed the photonic fire phenomenon in a laboratory setting. This phenomenon is characterized by an instability of a light-emitting, conical wire flame, resulting from slightly inhomogeneous thermal convection. This instability manifests as a thinning and branching of the flame, creating a complex pattern that can be visually observed under bright illumination.\n\nThrough high-speed photography observations, we have identified three notable features of the photonic fire. Firstly, there is a slight delay between the appearance of the temperature gradient at the tip of the branch and the commencement of light emission, which can be demonstrated in the accompanying video clip. Secondly, local heating from a nearby burned cable can trigger the ignition of burning branches, as observed in the video. Thirdly, when a steadily burning tip is illuminated from behind by a properly visible light source, this light can reflect from the front edge of the flame, resulting in an out-of-plane reflection that is also noticeable in the video recording.",
        "ori-fast-z-score": -2.264554068289191,
        "water-fast-z-score": 3.988620176087328,
        "rewrite-fast-z-score": 0.7875615306482168
    },
    {
        "original_text": "Grassmann algebra is an algebra constructed on a vector space with a property that it s subspaces form a right ideal. In this paper, first we introduce the concept of skew derivation, which is an anti-linear map that satisfies the Leibniz rule and derivation property. Then by defining the kernel of a skew mapping we generalize the concept of orthogonal vectors to arbitrary vectors and derive basic properties of skew inner products. Finally, as an application, we give an alternative proof of the associativity of Grassmann algebras. The concept of skew derivation was introduced for the first time by E. C. getto in 1965. Later in 1972, A. M. Bin Oljoin and M. B. Semenov gave another definition of skew derivation and applied it to different fields of mathematics. In this paper, we give a detailed study of skew derivations and skew derivable mappings, and apply it to Grassmann algebras.",
        "watermark_text": "Grassmann algebra is an algebra constructed on a vector space with a property that it s subspaces form a right ideal . In this paper , first we introduce the idea of skew derivation , which is an anti - linear mapping that satisfies the Leibniz rule and derivation property .Then by setting the kernel of a skew transformation we generalize the idea of orthogonal vectors to arbitrary vectors and derive essential aspects of skew inner derivatives . Finally , as an application , we give an additional confirmation of the associativity of Grassmann algebras .The concept of skew derivation was introduced for the first time by E . C . getto in 1965 . Later in 1972 , A . M . Bin Oljoin and M . B . Semenov offered another definition of skew derivation and application it to different fields of math .In this paper , we give a detailed analysis of skew derivations and skew derivable mappings , and application it to Grassmann algebras .",
        "rewrite_text": "Grassmann algebra is an algebraic structure that is built upon a vector space with a distinct property where its subspaces constitute a right ideal. In this paper, we commence with the introduction of the concept of skew derivation, which is an anti-linear mapping that adheres to the Leibniz rule and derivation property. By defining the kernel of a skew transformation, we extend the notion of orthogonal vectors to arbitrary vectors, and deduce crucial elements of skew inner derivatives. Ultimately, as an illustrative application, we offer additional confirmation of the associativity of Grassmann algebras.\n\nThe notion of skew derivation was initially presented by E. C. Getto in 1965. Subsequently, in 1972, A. M. Bin Oljoin and M. B. Semenov provided an alternative definition of skew derivation and its application in various fields of mathematics. In this paper, we present a comprehensive analysis of both skew derivations and skew derivable mappings, with a specific focus on their application to Grassmann algebras.",
        "ori-fast-z-score": -0.14002800840280097,
        "water-fast-z-score": 3.780756226875626,
        "rewrite-fast-z-score": 0.40451991747794525
    },
    {
        "original_text": "In this paper we introduce a quasi-exactly solvable (QES) spin chain which contains nearest-neighbors interactions. The Hamiltonian of the chain can be factorized into a sum of two-site terms which commute with one another. We show that the spectrum of the chain consists of a set of doubly-degenerate eigenvalue bands and a set of non-degenerated eigenvalue bands. Moreover, we derive the total number of the energy bands and the corresponding wave functions. Finally, we apply the Yang-Baxter equation to analyze the spectrum of the whole spin chain. Our chain could be realized in experiment with ultracold atoms in an optical lattice, with bosonic or fermionic atoms, or with photons. It would be interesting to study the properties of the chain and the method proposed in this paper by numerical methods and study the relations with models studied in literature. In this paper we introduce a novel quasi-exactly solvable (QES) spin chain with nearest-neighbors interactions. The Hamiltonian of the chain can be factorized into a sum of two-site terms which commute with one another. We show that the spectrum of the chain consists of a set of doubly-degenerate eigenvalue bands and a set of non-degenerated eigenvalue bands. Moreover, we derive the total number of the energy bands and the corresponding wave functions. Finally, we apply the Yang-Baxter equation to analyze the spectrum of the whole spin chain. Our chain could be realized in experiment with ultracold atoms in an optical lattice, with bosons or fermions, or with photons. It would be interesting to study the properties of the chain and the method proposed in this paper by numerical methods and study the relations with models studied in literature.",
        "watermark_text": "In this paper we provide a quasi - exactly solvable ( QES ) spinning chain which contains nearest - neighbors interactions . The Hamiltonian of the chain can be factorized into a sum of two - location terms which commute with one another .We see that the spectrum of the chain consists of a setting of doubly - degenerate eigenvalue bands and a setting of non - degenerated eigenvalue bands . Moreover , we derive the total number of the power groups and the associated wave functions .Finally , we apply the Yang - Baxter equation to analyze the spectrum of the whole spin chain . Our chain may be realized in experiment with ultracold atoms in an optical lattice , with bosonic or fermionic atoms , or with photons .It would be nice to study the properties of the chain and the method proposed in this paper by numerical methods and work the relations with models studied in literature . In this paper we introduce a novel quasi - exactly solvable ( QES ) spinning chain with nearest - neighbors interactions .The Hamiltonian of the chain can be factorized into a sum of two - location terms which commute with one another . We see that the spectrum of the chain consists of a setting of doubly - degenerate eigenvalue bands and a setting of non - degenerated eigenvalue bands .Moreover , we derive the total number of the energy bands and the associated wave functions . Finally , we apply the Yang - Baxter equation to analyze the spectrum of the whole spin chain .Our network could be realized in experiment with ultracold atoms in an optical lattice , with bosons or fermions , or with photons . It would be nice to study the properties of the chain and the method advocated in this paper by numerical methods and consider the relations with models studied in literature .",
        "rewrite_text": "In this study, we present a quasi-exactly solvable (QES) spinning chain that encompasses interactions between nearest neighbors. The Hamiltonian of this chain can be decomposed into a sum of two-location terms that commute with each other. We observe that the spectrum of the chain comprises sets of both doubly degenerate and non-degenerate eigenvalue bands. Furthermore, we derive the total count of energy bands along with their associated wave functions.\n\nUltilizing the Yang-Baxter equation, we analyze the spectrum of the entire spin chain. Our proposed chain can potentially be realized experimentally using ultracold atoms within an optical lattice, utilizing either bosonic or fermionic atoms, or even photons. It would be beneficial to investigate the properties of this chain and the methods presented in this paper through numerical approaches, exploring the relationships with models studied in the literature.\n\nMoreover, we introduce a novel QES spinning chain with nearest-neighbor interactions. The Hamiltonian of this novel chain can also be factored into a sum of two-location terms that share mutual commutativity. We reveal that its spectrum is composed of double and single degeneracy eigenvalue bands. Additionally, we determine the total number of energy bands and their corresponding wave functions.\n\nIn experimental terms, our network could be practically implemented using ultracold atoms trapped in an optical lattice, utilizing either bosonic or fermionic particles, or even photons. It would be advantageous to numerically explore the properties of this chain and the methodologies presented in this paper, while also considering the connections with existing models explored in the literature.",
        "ori-fast-z-score": 2.5350008891905187,
        "water-fast-z-score": 7.061788191316445,
        "rewrite-fast-z-score": 1.5583874449479593
    },
    {
        "original_text": "The QEL, which stands for Quasi-Elastic scence, is a inelastic interaction of neutrino which induces a smaller nuclear excitation. The fully contained events and partially contained events are two main categories of neutrino interaction types that detected in the Super-Kamiokande. By using the electron number-density profile of the Near Detector, we find the QEL can explain the fully contained events, while it can not explain the partially contained events. Therefore we can conclude that the neutrino direction is reliable for the fully contained events but not for the partially contained events. The full paper can be found here: https://arxiv.org/abs/1901.08031 I have also provided a short video explanation of this result: https://www.youtube.com/watch?v=aU4tpuUqK-Q Please let me know if you have any questions.",
        "watermark_text": "The QEL , which stands for Quasi - Elastic scence , is a inelastic interaction of neutrino which induces a smaller nuclear excitation . The totally enclosed events and partially contained events are two principal categories of neutrino coupling types that detected in the Super - Kamiokande .By using the electron number - density profile of the Near Detector , we find the QEL can describe the fully contained episodes , while it can not predict the partially contained events . Therefore we can conclude that the neutrino position is reliable for the fully contained events but not for the partially contained events .The full paper can be found here : https : / / arxiv . org / abs / 1901 . 08031 I have already offered a brief video description of this result : https : / / www . youtube . com / watch ? v = aU4tpuUqK - Q Please allow me know if you have any answers .",
        "rewrite_text": "The Quasi-Elastic Scenario (QEL), which stands for a nearly elastic interaction, represents an inelastic neutrino interaction that results in a smaller nuclear excitation. In the Super-Kamiokande experiment, two primary categories of neutrino coupling types are observed: fully enclosed events and partially contained events. By utilizing the electron number-density profile from the Near Detector, we have determined that QEL accurately describes fully contained episodes but fails to predict partially contained events. Therefore, we can conclude that the neutrino position is more reliable for fully contained events than for partially contained events. The complete paper can be accessed at: https://arxiv.org/abs/1901.08031. I have also provided a brief video description of this finding at: https://www.youtube.com/watch?v=aU4tpuUqK-Q. Please let me know if you have any further questions or answers.",
        "ori-fast-z-score": -0.13245323570650439,
        "water-fast-z-score": 3.0464244212496006,
        "rewrite-fast-z-score": 0.24253562503633297
    },
    {
        "original_text": "This paper presents surface brightness profiles for a sample of 27 globular clusters located in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC) and Fornax galaxy. Globular clusters are gravitationally bound collections of millions of stars. Because they are so compact, the light from the individual stars in a globular cluster can substantially pollute its surface brightness profile. Therefore, accurate measurement of surface brightness profiles for globular clusters is critical for studying their structural parameters, especially their half-mass radii, which can be used to place constraints on the halflight radii of their host galaxies. In this study, we obtained $VI$ images of the globular clusters using the photometric system of the CTIO 0.9-m telescope. A PSF-fitting program was used to measure the surface brightnesses of the clusters, and the resulting profiles are presented here. The profiles are presented for two intervals in radius, one centered on the half-mass radius and one centered on the half-light radius, and have been fitted with the models of the truncated isothermal sphere, the logarithmic sphere, and the generalized-Sersic model. For Fornax globular clusters, the generalized-Sersic profile provided the best fit to the data. In general, the globular cluster surface brightness profiles follow the same general trends in all three galaxies, i.e., the Fornax globular clusters have the steepest profiles, the LMC globular clusters have profiles between the Fornax and the SMC profiles, and the SMC globular clusters have the shallowest profiles. The surface brightnesses at a given radius for the globular clusters in a given galaxy tend to increase with the total luminosity of the galaxy.",
        "watermark_text": "This paper offers surface brightness characteristics for a sample of 27 globular complexes housed in the Large Magellanic Cloud ( LMC ) , Small Magellanic Cloud ( SMC ) and Fornax universe . Globular galaxies are gravitationally bound collections of millions of stars .Because they are so compact , the light from the individual stars in a globular cluster can significantly pollute its surface brightness profile . Therefore , easy calculation of surface brightness profiles for globular galaxies is important for studying their structural values , particularly their half - mass radii , which can be used to place limitations on the halflight radii of their host galaxies .In this study , we acquired $ VI $ images of the globular complexes use the photometric scheme of the CTIO 0 . 9 - m observatory . A PSF - fitting project was used to measure the surface brightnesses of the clusters , and the resulting profiles are presented here .The profiles are presented for two intervals in radius , one centered on the half - mass radius and one centered on the half - light diameter , and have been fitted with the models of the truncated isothermal sphere , the logarithmic sphere , and the generalized - Sersic profile . For Fornax globular nuclei , the generalized - Sersic profile provided the best fit to the information .In general , the globular cluster surface brightness profiles follow the same general trends in all three clusters , i . e . , the Fornax globular galaxies have the steepest profiles , the LMC globular complexes have profiles between the Fornax and the SMC profiles , and the SMC globular complexes have the shallowest profiles . The surface brightnesses at a given radius for the globular galaxies in a given galaxy tend to increase with the total luminosity of the galaxy .",
        "rewrite_text": "This study presents the surface brightness characteristics of a sample consisting of 27 globular clusters located in the Large Magellanic Cloud (LMC), Small Magellanic Cloud (SMC), and Fornax universe. Globular galaxies are composed of millions of stars that are gravitationally bound together. Due to their compactness, the light from individual stars within a globular cluster can significantly impact its surface brightness profile. Therefore, accurately calculating surface brightness profiles for globular galaxies is crucial for studying their structural properties, particularly their half-mass radii. These radii can be used to establish limitations on the half-light radii of their host galaxies.\n\nIn this research, we obtained $VI$ images of the globular clusters using the photometric scheme at the CTIO 0.9-m observatory. A project utilizing PSF-fitting was employed to measure the surface brightnesses of the clusters, and the resulting profiles are presented here. The profiles are presented for two radial intervals, one centered on the half-mass radius and the other on the half-light diameter. They have been fitted with models such as the truncated isothermal sphere, the logarithmic sphere, and the generalized Sersic profile.\n\nFor Fornax globular nuclei specifically, the generalized Sersic profile provided the most accurate fit to the data. In general, the surface brightness profiles of globular clusters follow consistent trends across all three clusters. Specifically, Fornax globular galaxies exhibit the steepest profiles, LMC globular complexes have profiles that fall between Fornax and SMC profiles, and SMC globular complexes have the shallowest profiles. Additionally, the surface brightness at a given radius for a globular galaxy within a larger galaxy tends to increase with the total luminosity of that larger galaxy.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.388602512436507,
        "rewrite-fast-z-score": 4.675162334843878
    },
    {
        "original_text": "Leaf venation is a beautiful example of highly branched, complex hierarchical vascular networks that protect plants from drought, high light intensity, and insect attacks. Despite its aesthetic and functional importance, the morphogenesis of leaf venation is a challenging problem because it involves multiple length and time scales. While it was initially hypothesized that secondary cells are only patterned after they are morphologically distinct from primary cells, it is now clear that secondary cells pattern prior to their morphological distinctness from primary cells. The signaling pathway responsible for this patterning was recently identified as a novel form of intercellular heterogeneity: elastic stresses. This elastic stress signaling theory of venation morphogenesis may be general and apply to other branched hierarchical vascular networks. profile Elastic stresses signaling is a recently proposed mechanism for leaf venation morphogenesis. While it was initially hypothesized that secondary cells are only patterned after they are morphologically distinct from primary cells, it is now clear that secondary cells pattern prior to their morphological distinctness from primary cells. The signaling pathway responsible for this patterning was recently identified as a novel form of intercellular heterogeneity: elastic stresses. This elastic stress signaling theory of venation morphogenesis may be general and apply to other branched hierarchical vascular networks.",
        "watermark_text": "Leaf venation is a beautiful example of highly branched , intricate hierarchical vascular networks that protect plants from frost , large dark intensity , and insect attacks . Despite its aesthetic and functional significance , the morphogenesis of leaf venation is a challenging problem because it includes multiple size and period scales .While it was initially hypothesized that secondary tissues are only patterned after they are morphologically independent from primary cells , it is now clear that secondary cells pattern prior to their morphological distinctness from primary cells . The pathway pathway responsible for this patterning was recently discovered as a novel form of intercellular heterogeneity : elastic stresses .This elastic stress signaling theory of venation morphogenesis would be general and extend to other branched hierarchical vascular networks . profile Elastic stresses signaling is a recently suggested system for leaf venation morphogenesis .While it was initially hypothesized that secondary tissues are only patterned after they are morphologically independent from primary cells , it is now clear that secondary cells pattern prior to their morphological distinctness from primary cells . The pathway pathway responsible for this patterning was recently discovered as a novel form of intercellular heterogeneity : elastic stresses .This elastic stress signaling theory of venation morphogenesis would be general and extend to other branched hierarchical vascular networks .",
        "rewrite_text": "Leaf venation provides a striking exemplar of intricate, highly-structured vascular networks that serve to safeguard plants from frost, intense darkness, and insect predation. Despite its visual and functional importance, the development of leaf venation remains a complex challenge due to its multi-scale nature encompassing various sizes and timeframes. It was previously believed that secondary tissues only develop patterns after becoming morphologically distinct from primary cells, but recent research has revealed that secondary cells actually begin to pattern prior to their differentiation from primary cells. The newly discovered mechanism behind this patterning is a novel form of intercellular heterogeneity known as elastic stress signaling. This theory of venation development is generalizable and can be applied to other branched hierarchical vascular networks. Specifically, the concept of elastic stress signaling has recently emerged as a system for understanding the morphogenesis of leaf venation. Contrary to previous assumptions, it is now understood that secondary cell patterns are established before they become morphologically distinct from primary cells. This new understanding has been accompanied by the discovery of a novel pathway responsible for this patterning, which involves elastic stresses as a form of intercellular heterogeneity. This elastic stress signaling theory in venation development is widely applicable and can extend to other intricate, branched hierarchical vascular networks as well.",
        "ori-fast-z-score": 1.116880781646981,
        "water-fast-z-score": 5.178265442181457,
        "rewrite-fast-z-score": 3.623157839133251
    },
    {
        "original_text": "We investigate excitation of electromagnetic shock and wave-like solitons in two-dimensional electron gases (2DEGs). For this, we consider both the nonlinear Schrödinger (NLS) equation and the two-dimensional magneto-hydrodynamic (MHD) equations with finite resistivity. Using analytical and numerical techniques, we reveal that excitation of nonlinear waves may occur at different parameters. We have demonstrated that excitation of electromagnetically non-dispersive shock waves is possible even in an isotropic plasma, i.e. when the electron flow velocity and the magnetic field have equal amplitudes. By means of numerical simulations, we have found that this phenomenon is possible due to the symmetry breaking effect, namely, if the plasma is uniformly magnetized in one direction, it is possible to generate electromagnetic shock waves with any direction in the isotropic plasma. Additionally, we have shown that in the framework of the NLS and MHD equations, it is possible to generate electromagnetic wave-like solitons when the flow velocity and magnetic field are of different amplitudes. We have revealed that these wave-like solitons are dispersive in the framework of the NLS equation, and dispersionless in the two-dimensional MHD equation with finite resistivity. In our work, we have used the same method, which was applied previously for investigation of excitation of nonlinear waves in binary Bose-Einstein condensates (BECs) with repulsive and attractive interactions. Thus, our present study is the first attempt to investigate excitation of nonlinear waves in an isotropic two-dimensional electron gas, and in a binary plasma, i.e. a plasma with repulsive and attractive interactions between particles.",
        "watermark_text": "We explore excitation of electromagnetic shock and wave - like solitons in two - dimensional electron molecules ( 2DEGs ) . For this , we consider both the nonlinear Schrödinger ( NLS ) equation and the two - dimensional magneto - hydrodynamic ( MHD ) equations with constant resistivity .Using computational and mathematical techniques , we prove that excitation of nonlinear waves may happen at different parameters . We have demonstrated that excitation of electromagnetically non - dispersive shock waves is possible even in an isotropic plasma , i . e .when the electron flow velocity and the magnetic force have equal amplitudes . By way of computational simulations , we have discovered that this phenomenon is possible due to the symmetry breaking phenomenon , namely , if the plasma is uniformly magnetized in one orientation , it is easy to produce electromagnetic shock fields with any direction in the isotropic plasma .Additionally , we have shown that in the framework of the NLS and MHD equations , it is easy to produce electromagnetic wave - like solitons when the flow velocity and magnetic force are of different amplitudes . We have revealed that these wave - like solitons are dispersive in the framework of the NLS equation , and dispersionless in the two - dimensional MHD equation with finite resistivity .In our work , we have utilized the same method , which was used earlier for investigation of excitation of nonlinear waves in binary Bose - Einstein condensates ( BECs ) with repulsive and attractive interactions . Thus , our present research is the first attempt to examine excitation of nonlinear waves in an isotropic two - dimensional electron gas , and in a binary plasma , i . e .a particle with repulsive and attractive interactions between particles .",
        "rewrite_text": "We delve into the excitation of electromagnetic shock waves and wave-like solitons within a two-dimensional electron molecule (2DEG) framework. For this purpose, we examine both the nonlinear Schrödinger (NLS) equation and the two-dimensional magneto-hydrodynamic (MHD) equations with a constant resistivity. By employing computational and mathematical techniques, we establish that excitation of nonlinear waves can occur under various parameter settings.\n\nWe have demonstrated that electromagnetically non-dispersive shock wave excitation is feasible even in an isotropic plasma, specifically when the electron flow velocity and magnetic force have equal amplitudes. Through computational simulations, we have discovered that this phenomenon arises from the symmetry breaking effect; when the plasma is uniformly magnetized in one direction, it becomes easier to generate electromagnetic shock fields with any direction within the isotropic plasma.\n\nFurthermore, we have shown that within the context of the NLS and MHD equations, it is straightforward to produce electromagnetic wave-like solitons when there are disparities in flow velocity and magnetic force amplitudes. Our findings reveal that these wave-like solitons exhibit dispersion within the NLS equation's framework, while they are dispersionless in the two-dimensional MHD equation with finite resistivity.\n\nIn our research, we have utilized a methodology previously employed to investigate the excitation of nonlinear waves in binary Bose-Einstein condensates (BECs) with both repulsive and attractive interactions. Therefore, our current study represents the initial attempt to explore the excitation of nonlinear waves in an isotropic two-dimensional electron gas as well as in a binary plasma, i.e., a system where particles exhibit both repulsive and attractive interactions between them.",
        "ori-fast-z-score": -0.20203050891044214,
        "water-fast-z-score": 4.6467017049401695,
        "rewrite-fast-z-score": 1.1766968108291043
    },
    {
        "original_text": "This paper presents a formulation of the yield stress design in poro- hydraulic systems using approximate pressure field and  calcul a la rupture en présence d un  ecoulement . The approximation of the pressure field is based on a solution presented in  1  and it was validated for different examples. The work presents an original method that can be applied to a wide range of problems. As an example, the formulation is applied to the problem of a saturated with water soil subjected to the flow of water. The yield surface is shaped as a penalty function of the absolute value of the differences between the local equivalent saturation and the desired value. The difference between the actual pressure and the imposed pressure, called  enthalpy , is considered. The obtained results are compared to the ones obtained with the classical solution based on  calcul a la rupture en présence d un  ecoulement . The proposed method is more realistic from a hydraulic point of view and the number of iterations to achieve the stabilisation is reduced.",
        "watermark_text": "This paper offers a description of the yield stress model in poro - hydraulic structures combining approximate pressure field and calcul a la rupture en présence d un ecoulement . The calculation of the pressure field is based on a solution offered in 1 and it was validated for different examples .The paper offers an initial method that can be applied to a broad variety of problems . As an instance , the formulation is applied to the question of a saturated with water soil exposed to the movement of water .The yield surface is shaped as a penalty function of the absolute value of the differences between the local equivalent saturation and the desired value . The difference between the actual pressure and the imposed tension , called enthalpy , is treated .The achieved findings are compared to the ones obtained with the classical treatment using on calcul a la rupture en présence d en ecoulement . The proposed approach is more realistic from a mechanical point of view and the number of iterations to achieve the stabilisation is decreased .",
        "rewrite_text": "This study presents a detailed description of the yield stress model within poro-hydraulic structures, which integrates an approximate pressure field with rupture calculations in the presence of flow. The pressure field calculations are based on a solution provided in reference 1, and have been validated through various examples. The paper introduces a preliminary methodology that can be applied to a wide range of problems. As an exemplar, the formulation is applied to the scenario of a water-saturated soil subjected to water movement. The yield surface is defined as a penalty function of the absolute value differences between the local equivalent saturation and the desired value. The comparison is made between the actual pressure and the imposed tension, referred to as enthalpy. The obtained results are contrasted with those achieved using traditional rupture calculations in the presence of flow. The proposed approach offers a more realistic mechanical perspective, and reduces the number of iterations required to achieve stabilization.",
        "ori-fast-z-score": -0.6793662204867574,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": 1.9126494315742406
    },
    {
        "original_text": "Animal behavior is frequently modulated by changes in the perceived safety of the surroundings. An organism s response to danger can have profound consequences for its fitness. Animals often adjust their behavior in response to aversive stimuli such as intense heat or severe injury. Here, we present a neurodynamical model that captures the learning and decision-making processes that underlie such behavioral plasticity. Our framework assigns a value to each behavioral response based on a probabilistic mapping to outcomes, represented as a multivariate normal distribution. These values are updated by means of a variant of contrastive estimation that enables the model to account for both positive and negative reinforcement. The model can be applied to a broad class of behavioral optimization problems, and we illustrate its application by applying it to scenarios that pose conflicting demands on a creature s behavior. The model predicts that rats exposed to a danger stimulus will exhibit an initial rapid behavioral response but will ultimately optimize their behavior, as measured by time-averaged heat loss, over a gradually shifting optimum. Furthermore, we show that the model can account for heat-induced analgesia, one of the most striking examples of behavioral plasticity, by computing a probability distribution over expected pain intensities. Thus, our model can generate accurate predictions about how organisms respond to danger, providing a foundation for understanding how behavioral flexibility emerges from a Bayesian optimization framework in which outcomes are predicted using neural networks. Our model provides a framework for understanding how behavioral flexibility emerges from a Bayesian optimization framework in which outcomes are predicted using neural networks. We illustrate this application by applying the model to scenarios that pose conflicting demands on an organism s behavior. The model predicts that rats exposed to a danger stimulus will exhibit an initial rapid behavioral response but will ultimately optimize their behavior, as measured by time-averaged heat loss, over a gradually shifting optimum. Furthermore, the model can account for heat-induced analgesia, one of the most striking examples of behavioral plasticity. Thus, our model can generate accurate predictions about how organisms respond to danger, providing a foundation for understanding how behavioral flexibility emerges from a Bayesian optimization framework in which outcomes are predicted using neural networks. Our model provides a framework for understanding how behavioral flexibility emerges from a Bayesian optimization framework in which outcomes are predicted using neural networks. We illustrate this application by applying the model to scenarios that pose conflicting demands on an organism s behavior. The model predicts that rats exposed to a danger stimulus will exhibit an initial rapid behavioral response but will ultimately optimize their behavior, as measured by time-averaged heat loss, over a gradually shifting optimum. Furthermore, the model can account for heat-induced analgesia, one of the most striking examples of behavioral plasticity. Thus, our model can generate accurate predictions about how organisms respond to danger, providing a foundation for understanding how behavioral flexibility emerges from a Bayesian optimization framework in which outcomes are predicted using neural networks. This model provides a foundation for understanding behavioral flexibility in a wide variety of organisms, from invertebrates to vertebrates, including rats, monkeys,",
        "watermark_text": "Animal activity is frequently modulated by changes in the perceived security of the surroundings . An organism s response to threat can have profound implications for its fitness .Animals frequently adjust their actions in reaction to aversive cues such as intense heat or extreme injury . Here , we present a neurodynamical theory that captures the learning and decision - making pathways that underlie such behavioral plasticity .Our paradigm assigns a value to each behavioral reaction based on a probabilistic mapping to outcomes , shown as a multivariate normal distribution . These values are updated by means of a version of contrastive estimation that enables the model to account for both negative and negative reinforcement .The model can be applied to a broad class of behavioral optimization problems , and we explain its use by application it to scenarios that pose conflicting demands on a creature s response . The model predicts that rats exposed to a danger stimulus will exhibit an initial quick behavioral reaction but will ultimately optimize their actions , as measured by time - averaged heat gain , over a rapidly changing optimum .Furthermore , we find that the model can account for temperature - caused analgesia , one of the most notable examples of behavioral plasticity , by modeling a probability distribution over anticipated agony intensities . Thus , our model can generate reliable predictions about how organisms respond to threat , providing a foundation for studying how behavioral complexity derives from a Bayesian algorithms framework in which results are expected using neural systems .Our model provides a framework for explaining how behavioral autonomy emerges from a Bayesian algorithm framework in which results are expected using neural systems . We illustrate this application by using the model to scenarios that pose varying demands on an organism s response .The model predicts that rats exposed to a danger stimulus will exhibit an initial quick behavioral reaction but will ultimately optimize their performance , as measured by time - averaged heat change , over a steadily changing optimum . Furthermore , the model can account for temperature - caused analgesia , one of the most notable examples of behavioral plasticity .Thus , our model can generate precise predictions about how organisms respond to threat , providing a foundation for studying how behavioral flexibility emerges from a Bayesian algorithm framework in which results are expected use neural systems . Our model provides a framework for explaining how behavioral flexibility emerges from a Bayesian algorithm framework in which results are expected use neural systems .We illustrate this application by using the model to scenarios that pose varying demands on an organism s response . The model predicts that rats exposed to a danger stimulus will exhibit an initial quick behavioral reaction but will ultimately optimize their actions , as measured by time - averaged heat gain , over a rapidly changing optimum .Furthermore , the model can account for temperature - caused analgesia , one of the most notable examples of behavioral plasticity . Thus , our model can generate precise predictions about how organisms respond to threat , providing a foundation for studying how behavioral complexity derives from a Bayesian algorithms framework in which results are expected using neural systems .This theory provides a foundation for explaining behavioral flexibility in a broad variety of mammals , from invertebrates to vertebrates , notably monkeys , monkeys ,",
        "rewrite_text": "动物活动的频繁程度经常受到周围环境安全感的感知变化的影响。生物对威胁的反应可能会对其适应性产生深远的影响。动物常常会对令人不悦的线索如强烈的高温或剧烈的伤害作出行动上的调整。在这里，我们提出一个神经动力学理论，该理论捕捉了支撑这种行为可塑性的学习和决策路径。\n\n我们的范式基于概率映射结果，为每种行为反应赋予一个值，并显示为多元正态分布。这些值通过对比估计的版本进行更新，使模型能够兼顾负强化和正强化。该模型可应用于广泛的行为优化问题，我们通过将其应用于提出相互冲突要求的情景来解释其应用。模型预测，当老鼠受到危险刺激时，它们会表现出最初的快速行为反应，但最终会优化其行动，以时间平均热量增益来衡量，以适应快速变化的最优条件。此外，我们发现该模型可以解释由温度引起的镇痛作用，这是行为可塑性的最显著例子之一。模型通过对预期疼痛强度概率分布的建模来实现这一点。因此，我们的模型能够生成对生物体如何应对威胁的可靠预测，为研究如何从贝叶斯算法框架中推导出行为复杂性提供了基础，我们期待在神经系统的运用中取得成果。\n\n我们的模型提供了一个框架，用于解释在贝叶斯算法框架下如何从神经系统中得出行为自主性。我们通过将模型应用于提出不同要求的情景来阐明这一应用。模型预测，当老鼠暴露于危险刺激时，它们会表现出最初的快速行为反应，但最终会以时间平均热量变化为标准优化其表现，以适应不断变化的最优条件。此外，该模型还能解释由温度引起的镇痛作用，这是行为塑形的最明显例子之一。因此，我们的模型可以精确预测生物体如何应对威胁，为研究从贝叶斯算法框架中推导出行为灵活性的基础提供了依据。\n\n这一理论为解释从无脊椎动物到有脊椎动物（尤其是猴子）等各类哺乳动物的行为灵活性提供了基础。",
        "ori-fast-z-score": -0.5447047794019222,
        "water-fast-z-score": 12.361965127688308,
        "rewrite-fast-z-score": 0
    },
    {
        "original_text": "The plasma torus around Io, the smallest of the four Galilean satellites of Jupiter, has been the subject of numerous observations over the past several decades. Voyager, Galileo, and Cassini each made observations of the plasma torus, but only Cassini had the capability of making ultraviolet (UV) measurements of the torus. We present an analysis of the temporal and azimuthal variability of the torus using Cassini Ultraviolet Imaging Spectrometer (UVIS) observations from December 2000 to May 2006. Models of the plasma torus emission should account for thermal effects, non-LTE ionization and excitation, free electrons, and electron impact excitation and ionization. We compare two models, a two-fluid model, which has been successful at modeling the global atmosphere of Io, and a hydrodynamic model. While the two-fluid model has generally been adequate for modeling Io’s thermosphere and exosphere, it cannot model Io’s plasma torus. The hydrodynamic model has been successful at modeling the plasma torus, however it cannot explain the variations in emission observed by Cassini. This suggests that the variability of the plasma torus is caused by changes in the mass flux entering the torus.",
        "watermark_text": "The plasma torus around Io , the smallest of the four Galilean satellites of Jupiter , has been the subject of several observations over the previous several decades . Voyager , Galileo , and Cassini each made observations of the plasma torus , but only Cassini had the technology of making ultraviolet ( UV ) observations of the torus .We current an assessment of the temporal and azimuthal variability of the torus using Cassini Ultraviolet Imaging Spectrometer ( UVIS ) observations from December 2000 to May 2006 . Models of the plasma torus emission should account for thermal effects , non - LTE ionization and excitation , free electrons , and electron impact excitation and ionization .We contrast two models , a two - fluid model , which has been effective at studying the global environment of Io , and a hydrodynamic model . While the two - fluid model has generally been sufficient for modeling Io ’ s thermosphere and exosphere , it fails model Io ’ s plasma torus .The hydrodynamic model has been effective at studying the plasma torus , however it lacks explain the variations in emission observed by Cassini . This implies that the variability of the plasma torus is caused by changes in the mass flux entering the torus .",
        "rewrite_text": "Over the past several decades, the plasma torus surrounding Io, the smallest of Jupiter's four Galilean satellites, has been the focus of numerous observations. Voyager, Galileo, and Cassini have all conducted observations of this plasma torus, with Cassini being the only one equipped with the technology to conduct ultraviolet (UV) observations of the torus. We present an evaluation of the temporal and azimuthal variability of the torus using Cassini's Ultraviolet Imaging Spectrometer (UVIS) observations from December 2000 to May 2006. Models for the plasma torus emission should factor in thermal effects, non-LTE ionization and excitation, free electrons, as well as electron impact excitation and ionization.\n\nWe compare two models: a two-fluid model which has proven effective for studying Io's global environment, and a hydrodynamic model. While the two-fluid model has generally been adequate for modeling Io's thermosphere and exosphere, it fails to accurately model Io's plasma torus. The hydrodynamic model, on the other hand, has shown promise in studying the plasma torus, but it fails to explain the variations in emission observed by Cassini. This suggests that the variability of the plasma torus is likely caused by changes in the mass flux entering the torus.",
        "ori-fast-z-score": 1.7232808737106582,
        "water-fast-z-score": 6.077701994871215,
        "rewrite-fast-z-score": 0.9428090415820635
    },
    {
        "original_text": "We present a detailed study of 47 clusters of galaxies which have signal-to-noise ratio greater than 10 and which lie within cz < 5000 km/s of the DR4 spectroscopic sample. We measured the global cluster properties, namely the X-ray temperature, the Sunyaev-Zeldovich effect temperature, the luminosity and the radius, the velocity dispersion, the mass and the temperature from the spectroscopy and Sunyaev-Zeldovich effect data. In addition, we measured the intra-cluster gas mass from the X-ray data. The global properties were derived using both the projected parameterizations and the three dimensional model and compared. We also estimated the total mass from the velocity dispersion assuming that the clusters are relaxed systems. The velocity dispersion were calculated using both the photometric (optical) members and the spectroscopy members. We compared the total mass derived from both the X-ray and the velocity dispersion. The global properties of these clusters were studied using X-ray, Sunyaev-Zeldovich effect and optical data. All of these results are summarized in this paper.",
        "watermark_text": "We report a detailed analysis of 47 clusters of stars which have signal - to - noise ratio larger than 10 and which reside within cz < 5000 kilometres / s of the DR4 spectroscopic sample . We calculated the global cluster properties , notably the X - ray temperature , the Sunyaev - Zeldovich effect temperature , the luminosity and the radius , the velocity dispersion , the mass and the temperature from the spectroscopy and Sunyaev - Zeldovich effect measurements .In addition , we calculated the intra - cluster gas mass from the X - ray data . The global properties were obtained using both the projected parameterizations and the three dimensional model and compared .We additionally assumed the total mass from the velocity dispersion assuming that the clusters are relaxed systems . The velocity dispersion were calculated using both the photometric ( laser ) members and the spectroscopy members .We compared the total mass derived from both the X - ray and the velocity dispersion . The global properties of these clusters were studied utilizing X - ray , Sunyaev - Zeldovich effect and imaging information .All of these results are summarized in this paper .",
        "rewrite_text": "In this paper, we present a comprehensive analysis of 47 star clusters whose signal-to-noise ratio is greater than 10 and are situated within a distance of cz < 5000 kilometers/s from the DR4 spectroscopic sample. We have calculated various global cluster properties, namely the X-ray temperature, Sunyaev-Zeldovich effect temperature, luminosity, radius, velocity dispersion, mass, and temperature using spectroscopic data and Sunyaev-Zeldovich effect measurements. Additionally, we have estimated the intra-cluster gas mass from X-ray data. These global properties were obtained through both projected parameterizations and three-dimensional models and were compared. We also assumed the total mass based on velocity dispersion, assuming that the clusters are in a relaxed state. The velocity dispersion was calculated using both photometric (laser) and spectroscopic members. We compared the total mass derived from X-ray data with that obtained from velocity dispersion. The study of these clusters' global properties involved utilizing X-ray, Sunyaev-Zeldovich effect, and imaging information. All of these findings are summarized in this paper.",
        "ori-fast-z-score": 1.2701705922171767,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": 1.2222222222222223
    },
    {
        "original_text": "Astronomers using the Chandra X-ray observatory have carried out the deepest ever survey of the universe in the X-ray band. In the Chandra Deep Field South (CDF-S), they have detected 740 sources, of which 122 are X-ray emitting Active Galactic Nuclei (AGNs). Deep spectroscopy has shown that roughly half of the X-ray emitting AGNs in the CDF-S are completely absorbed in the visible band, and are only observable in X-rays. This has allowed us to study the average obscuration properties of the universe in the distant, early stages of galaxy evolution, which are crucial for understanding how galaxies grow. At $z=2-4$, the CDF-S contains the largest uniformly-selected sample of distant, obscured AGNs known. These are responsible for most of the metals in the early universe, and so are vital for understanding the re-ionization of the universe. The total amount of matter obscured by gas in the CDF-S is estimated to be around four times the mass of the Milky Way, with individual masses reaching up to 1022 M⊙. If the mean molecular weight of the gas is assumed to be around 0.5, this corresponds to a total of around ten trillion Solar masses of matter, concentrated in just these few sources. As we approach the peak of galaxy assembly in the early universe, these sources are ideal laboratories for studying the evolution of gas and dust throughout the universe.",
        "watermark_text": "Astronomers using the Chandra X - ray observatory have carried out the deepest yet survey of the universe in the X - ray band . In the Chandra Deep Field South ( CDF - S ) , they have discovered 740 sources , of which 122 are X - ray emitting Active Galactic Nuclei ( AGNs ) .Deep spectroscopy has indicated that approximately quarter of the X - ray emitting AGNs in the CDF - S are completely consumed in the visible band , and are only observable in X - radiation . This has allowed us to study the average obscuration properties of the universe in the distant , early stages of galaxy evolution , which are important for knowledge how clusters grow .At $ z = 2 - 4 $ , the CDF - S holds the greatest uniformly - selected sample of distant , obscured AGNs known . These are responsible for most of the metals in the early universe , and so are essential for studying the re - ionization of the universe .The total amount of matter obscured by gas in the CDF - S is predicted to be around four times the mass of the Milky Way , with individual masses reaching up to 1022 [UNK] . If the mean molecular weight of the gas is assumed to be around 0 . 5 , this corresponds to a total of around ten trillion Solar masses of matter , concentrated in just these few sources .As we approach the peak of galaxy assembly in the early universe , these sources are excellent labs for studying the evolution of gas and dust throughout the universe .",
        "rewrite_text": "Using the Chandra X-ray observatory, astronomers have conducted the most profound survey of the universe in the X-ray spectrum. In the Chandra Deep Field South (CDF-S), they've discovered 740 sources, 122 of which are Active Galactic Nuclei (AGNs) emitting X-rays.\n\nDeep spectroscopic observations have revealed that approximately a quarter of the X-ray emitting AGNs in the CDF-S are completely obscured in the visible spectrum and can only be detected through X-radiation. This has enabled us to study the average obscuration characteristics of the universe during its distant, early stages of galaxy evolution - knowledge vital for understanding how galaxy clusters grow.\n\nAt redshifts between 2 and 4, the CDF-S holds the largest uniformly-selected sample of distant, obscured AGNs known. These objects are responsible for the majority of metals in the early universe and are crucial for studying the re-ionization of the cosmos.\n\nThe total amount of matter obscured by gas in the CDF-S is predicted to be approximately four times the mass of the Milky Way, with individual masses reaching up to 1022 units. Assuming a mean molecular weight of 0.5 for the gas, this corresponds to a total of around ten trillion solar masses of matter concentrated in just a few sources. As we approach the peak of galaxy formation in the early universe, these sources provide excellent laboratories for studying the evolution of gas and dust across the cosmos.",
        "ori-fast-z-score": 3.061862178478973,
        "water-fast-z-score": 6.193611607315077,
        "rewrite-fast-z-score": 3.061862178478973
    },
    {
        "original_text": "This paper presents the kinematic design of a 3-dof hybrid manipulator which includes both a dual-quaternion-based rotation kinematics and a novel 3-DOF snake-like structure for end-effector motion. The proposed structure integrates the advantages of the dual quaternion and 3-DOF snake-like manipulators, and it is more concise and easier to integrate with complex control algorithms. The proposed hybrid manipulator can achieve large flexibility and dexterity by combining the 3-DOF snake-like structure with the rotation kinematics. In this paper, the hybrid configuration is firstly constructed using a dual quaternion-based rotation kinematics, which provides a flexible and compact representation. Then, a novel 3-DOF snake-like structure, which enables large motion amplitude, is introduced to balance the flexibility and dexterity for the hybrid manipulator. To facilitate the kinematics analysis and synthesis, the singularities of the hybrid manipulator are analyzed and a series of singular transformations are developed. Two numerical examples are provided to verify the effectiveness of the proposed hybrid manipulator.",
        "watermark_text": "This paper offers the kinematic design of a 3 - dof hybrid manipulator which includes both a dual - quaternion - based rotation kinematics and a novel 3 - DOF serpent - like structure for ending - effector motion . The proposed structure integrates the advantages of the dual quaternion and 3 - DOF serpent - like manipulators , and it is more concise and easier to integrate with sophisticated control methods .The proposed hybrid manipulator can attain huge strength and dexterity by combining the 3 - DOF serpent - like structure with the rotation kinematics . In this paper , the hybrid system is firstly constructed using a dual quaternion - based rotation kinematics , which offers a transparent and compact model .Then , a new 3 - DOF serpent - like structure , which enables huge motion amplitude , is adopted to balance the flexibility and dexterity for the hybrid manipulator . To facilitate the kinematics analysis and synthesis , the singularities of the hybrid manipulator are examined and a sequence of singular transformations are created .Two mathematical models are provided to confirm the performance of the suggested hybrid manipulator .",
        "rewrite_text": "The research paper presents the kinematic design of a 3-DOF hybrid manipulator, which incorporates both a dual-quaternion-based rotational kinematics and an innovative 3-DOF serpent-like structure for the end-effector movement. The proposed structure combines the strengths of both dual quaternion and 3-DOF serpent-like manipulators, resulting in a more concise design that can easily integrate with advanced control methods. By integrating the 3-DOF serpent-like structure with the rotational kinematics, the hybrid manipulator can achieve remarkable strength and dexterity.\n\nIn this paper, the hybrid system is initially constructed using a dual quaternion-driven rotational kinematics, providing a clear and compact model. Subsequently, a novel 3-DOF serpent-like structure is employed to balance flexibility and dexterity in the hybrid manipulator, enabling a wide range of motion amplitudes. To facilitate the analysis and synthesis of kinematics, the paper examines the singularities of the hybrid manipulator and creates a sequence of singular transformations. To validate the performance of the proposed hybrid manipulator, two mathematical models are provided.",
        "ori-fast-z-score": -1.4142135623730951,
        "water-fast-z-score": 4.564604740649092,
        "rewrite-fast-z-score": 0.7071067811865476
    },
    {
        "original_text": "Simulations have been performed to study the structure of the boundary layer between a white dwarf and its accretion disk. We solve the Euler equations of hydrodynamics, including self-gravity of the gas and nuclear energy production term in the energy equation. The equations have been solved in a Cartesian grid domain with dimensions of 1000 x 1000 x 300 Schwarzchild radii of the white dwarf. The models presented have explored the effects of varying the strength of the nuclear energy production term and the mass flux through the inner boundary. It is found that in all models a sufficiently strong nuclear energy production term leads to the formation of a shock transitioning across the inner radial boundary. In addition, it is found that a modest mass flux through the inner boundary results in the formation of a persistent boundary layer structure. In particular, we find that the presence of a boundary layer greatly alters the emission spectrum of the system. It is expected to result in double-peaked emission lines, such as the C III emissions lines detected in the cataclysmic variable AE Aquarii. We also find that there is significant supersonic motion in the boundary layer, likely accounting for the persistent broad emission line components observed in some systems. The simulations presented in this work were performed with AMRAII, a three-dimensional, three-dimensional, resistive, hydrodynamical simulation code developed at the Max Planck Institute for Astrophysics in Garching, Germany.",
        "watermark_text": "Simulations have been performed to study the composition of the boundary layer between a white dwarf and its accretion disk . We calculate the Euler equations of hydrodynamics , notably self - gravity of the gas and nuclear power production term in the energy equation .The equations have been solved in a Cartesian grid domain with sizes of 1000 x 1000 x 300 Schwarzchild radii of the white dwarf . The models discussed have explored the effects of differing the strength of the atomic energy production term and the mass flux through the inner boundary .It is found that in all models a enough strong radioactive heat production term leads to the formation of a shock transitioning across the inner radial boundary . In addition , it is found that a modest mass flux through the inner boundary results in the formation of a persistent boundary layer structure .In particular , we find that the presence of a boundary layer significantly alters the emission spectrum of the system . It is expected to result in double - peaked emission lines , such as the C III emissions lines observed in the cataclysmic variable AE Aquarii .We additionally find that there is substantial supersonic motion in the boundary layer , likely accounting for the strong broad pollution line parts observed in some systems . The simulations presented in this research were performed with AMRAII , a three - dimensional , three - dimensional , resistive , hydrodynamical simulation code developed at the Max Planck Institute for Astrophysics in Garching , Germany .",
        "rewrite_text": "Simulations have been executed to analyze the composition of the boundary layer present between a white dwarf and its accretion disk. We computed the Euler equations of hydrodynamics, especially accounting for the self-gravity of the gas and the nuclear power production term in the energy equation. These equations were solved in a Cartesian grid system, spanning 1000 x 1000 x 300 Schwarzschild radii of the white dwarf. The models we've examined have delved into the ramifications of adjusting the potency of the atomic energy production factor and the mass flux through the inner boundary.\n\nIt has been discovered that a sufficiently robust radioactive heat production term leads to the formation of a shockwave that transitions across the inner radial boundary in all models. Furthermore, it has been observed that a moderate mass flux through the inner boundary results in the creation of a persistent boundary layer structure. Specifically, we've found that the existence of a boundary layer significantly alters the system's emission spectrum. This is expected to result in double-peaked emission lines, such as the C III emission lines seen in the cataclysmic variable AE Aquarii.\n\nAdditionally, our findings indicate that there is considerable supersonic motion within the boundary layer, likely accounting for the strong, broad pollution line components observed in some systems. The simulations presented in this research were executed using AMRAII, a three-dimensional resistive hydrodynamic simulation code developed at the Max Planck Institute for Astrophysics in Garching, Germany.",
        "ori-fast-z-score": 1.1441551070947107,
        "water-fast-z-score": 5.5300830176244355,
        "rewrite-fast-z-score": 3.2716515254078793
    },
    {
        "original_text": "Long-range ordering in simple metallic and covalent solids is typically associated with the existence of one or more periodic structural arrangements. These periodicities can be described by the classic diffraction methods of Bragg and Robert-ancell. In this work, we employ the newly developed Direct spatial mapping technique to determine the spatial distribution of short-range order in two systems which exhibit strong long-range ordering: charge density wave and high temperature superconductors. The resulting ordering pattern is unlike either conventional solid state crystallography or theories of short-range order predicting a regular lattice. Rather, the spatial arrangement of these short-range orders appears characteristic of a two-dimensional coordination polymer - a material with no long-range order. This surprising result suggests either that the conventional theories of short-range order should be augmented to include some form of long-range order, or that the short-range order observed in these systems arises from some as-yet-unidentified mechanism.",
        "watermark_text": "Long - range ordering in ordinary metallic and covalent solids is typically associated with the existence of one or more periodic structural patterns . These periodicities can be described by the standard diffraction methods of Bragg and Robert - ancell .In this research , we utilize the newly proven Direct spatial mapping technique to estimate the spatial distribution of short - range order in two systems which exhibit strong long - range ordering : charge density wave and large temperature superconductors . The resulting ordering pattern is unlike either conventional solid state crystallography or theories of short - range order predicting a regular structure .Rather , the spatial arrangement of these short - range orders remains characteristic of a two - dimensional coordination molecule - a substance with no long - range order . This remarkable outcome suggests either that the usual explanations of short - range order should be enhanced to contain some kind of short - range order , or that the short - range order observed in these systems arises from some as - yet - undisclosed mechanism .",
        "rewrite_text": "In ordinary metallic and covalent solids, long-range ordering is frequently linked to the presence of one or several periodic structural patterns. These periodicities can be analyzed using standard diffraction techniques such as Bragg and Robert-ancell methods. For this research, we employ the recently validated Direct spatial mapping technique to estimate the spatial distribution of short-range order in two systems that exhibit strong long-range ordering: charge density waves and high-temperature superconductors. The resulting ordering pattern differs from both traditional solid-state crystallography and theories of short-range order that predict a regular structure. Instead, the spatial arrangement of these short-range orders retains the characteristic of a two-dimensional coordination molecule, a substance without long-range order. This remarkable finding suggests that either current explanations of short-range order need to be enhanced to include some form of short-range order, or that the observed short-range order in these systems arises from an as-yet-undiscovered mechanism.",
        "ori-fast-z-score": 2.5879865568825218,
        "water-fast-z-score": 6.901297485020058,
        "rewrite-fast-z-score": 4.088310863215481
    },
    {
        "original_text": "Triangulations and Catalan s intervals Triangulations of the convex hulls of point sets in the plane have been studied since the 1990s. There exist eight different generic types of triangulations, depending on the number of points, the distances between the points, and their spatial disposition. In this paper, we analyze the realizers of all these generic types. The realizers of a triangulation are edge-lists of simple polytopes defining the same homology class as the original triangulation. They can be constructed algorithmically from the given topological space. The knowledge of all realizers of a given triangulation is very useful for many geometric, topological, and computational applications. We identify all possible realizers for eight generic types of triangulations. The total number of different realizer sets we find is 46. The article also provides a Java software for computing realizers of given triangulations.",
        "watermark_text": "Triangulations and Catalan s intervals Triangulations of the convex hulls of point sets in the plane have been studied since the 1990s . There operate eight different specific kinds of triangulations , depending on the number of points , the distances between the points , and their geographical disposition .In this paper , we assess the realizers of all these generic types . The realizers of a triangulation are edge - tables of simple polytopes defining the same homology class as the original triangulation .They can be built algorithmically from the particular topological space . The knowledge of all realizers of a given triangulation is very useful for numerous geometric , topological , and theoretical uses .We recognize all possible realizers for eight generic types of triangulations . The total number of different realizer pairs we find is 46 .The section also provides a Java software for modeling realizers of certain triangulations .",
        "rewrite_text": "Since the 1990s, research has been conducted on triangulations of the convex hulls of point sets in the plane, along with Catalan's s intervals. There exist eight distinct types of triangulations, which vary depending on the number of points, the distances between those points, and their spatial arrangement.\n\nIn this paper, we evaluate the realizers of all these generic triangulation types. Realizers of a triangulation are edge tables of simple polytopes that define the same homology class as the original triangulation. These can be algorithmically constructed from the specific topological space. The understanding of all realizers for a given triangulation is highly beneficial for various geometric, topological, and theoretical applications.\n\nWe have identified all possible realizers for the eight generic types of triangulations. The total number of distinct realizer pairs we have discovered is 46. Additionally, this section provides a Java software for modeling specific types of triangulation realizers.",
        "ori-fast-z-score": -1.1094003924504583,
        "water-fast-z-score": 3.500700210070024,
        "rewrite-fast-z-score": -1.7856873313329573
    },
    {
        "original_text": "In this letter we investigate the possibility that the four-dimensional (4D) gravity could arise as a warped metric on a five-dimensional (5D) Anti-de-Sitter (AdS) space in a supersymmetric braneworld model. In this scenario, the observable universe is viewed as a three-brane that is a domain wall living in the AdS space. In order to have an exit from the domain wall curvature instabilities, a bulk scalar field is introduced. This field, called the chameleon field, couples to the world volume of the brane via the Israel coupling, where the coupling strength is related to the brane tension. By analyzing the equations of motion we obtain the conditions on the brane tension and bulk potential such that the theory acquires a healthy stable 4D gravity at low energy. By using these conditions we show that the mass of the Planck particle can be naturally generated. We also study the possibility that the supersymmetry is broken on the brane. To that end we introduce explicit (F) and soft (D) breaking terms to the brane supergravity Lagrangian. We show that without the explicit breaking, all the scalar masses are determined only by the brane tension and the gravitino mass, and thus are naturally of order of the gravitino mass. With explicit breaking, the scalar masses can be naturally of order one, if some conditions are satisfied on the soft parameters. Our results suggest that in order to explain the 4D gravity we live in from the 5D fundamental theory, the theory might be supersymmetric on the brane. Supersymmetry breaking on the brane naturally explains the hierarchy between the Planck and the Electroweak scales without postulating large extra dimensions, and thus provides a rationale for low fundamental gravity scale.",
        "watermark_text": "In this letter we investigate the prospect that the four - dimensional ( 4D ) gravity could arise as a warped metric on a five - dimensional ( 5D ) Anti - de - Sitter ( AdS ) space in a supersymmetric braneworld model . In this situation , the observable universe is viewed as a three - brane that is a domain wall living in the AdS space .In order to have an escape from the domain floor curvature instabilities , a bulk scalar field is created . This field , known the chameleon field , couples to the world volume of the brane via the Israel coupling , where the interaction strength is related to the brane tension .By analyzing the equations of movement we obtain the conditions on the brane tension and bulk potential such that the model acquires a safe stable 4D gravitational at low power . By using these conditions we prove that the mass of the Planck particle can be naturally derived .We additionally study the idea that the supersymmetry is shattered on the brane . To that end we apply explicit ( F ) and soft ( D ) broke terms to the brane supergravity Lagrangian .We see that without the explicit violating , all the scalar masses are decided only by the brane tension and the gravitino mass , and therefore are naturally of order of the gravitino mass . With explicit breaking , the scalar masses can be naturally of order one , if some conditions are fulfilled on the soft variables .Our results propose that in order to explain the 4D gravitational we reside in from the 5D fundamental theory , the model could be supersymmetric on the brane . Supersymmetry broken on the brane readily explains the hierarchy between the Planck and the Electroweak scales without postulating small additional dimensions , and therefore provides a rationale for low fundamental gravitational scale .",
        "rewrite_text": "In this correspondence, we explore the potential for four-dimensional (4D) gravity to emerge as a warped metric in a five-dimensional (5D) Anti-de-Sitter (AdS) space within a supersymmetric braneworld model. In this context, the observable universe is conceptualized as a three-brane, a domain wall residing within the AdS space. To mitigate domain floor curvature instabilities, a bulk scalar field, known as the chameleon field, is generated. This field interacts with the world volume of the brane through the Israel coupling, where the interaction strength is linked to the brane tension.\n\nBy analyzing the equations of motion, we establish conditions on brane tension and bulk potential that enable the model to achieve a stable and secure 4D gravitational field at low energies. Using these conditions, we demonstrate that the mass of the Planck particle can be derived naturally. Furthermore, we investigate the concept of supersymmetry being broken on the brane. To this end, we apply explicit (F) and soft (D) breaking terms to the brane supergravity Lagrangian.\n\nOur observations indicate that without explicit violations, all scalar masses are determined solely by the brane tension and the gravitino mass, thereby naturally aligning with the order of the gravitino mass. With explicit breaking, the scalar masses can naturally take on values of order one if certain conditions are met regarding the soft variables. Our findings suggest that to explain the 4D gravity we experience from the 5D fundamental theory, the model could be supersymmetric on the brane. The breaking of supersymmetry on the brane readily explains the hierarchy between the Planck and Electroweak scales without requiring additional small dimensions, thus providing a rationalization for a low fundamental gravitational scale.",
        "ori-fast-z-score": -2.3728949893812477,
        "water-fast-z-score": 4.271210980886246,
        "rewrite-fast-z-score": 1.3112201362143716
    },
    {
        "original_text": "In this paper we investigate correlations between the energy density and the flux of two different observables in an unusual quantum state called macroscopic quantum state (MQS). These correlations were first discovered in the MQT of doubly clamped micro cantilevers and were interpreted as a signature of quantumness. We extend this analysis to other mechanical resonators made of different materials and of different geometries. We show that the flux-energy density correlations appear in the MQS and in the vacuum for the same set of parameters. We argue that these correlations can not be explained using the standard models of decoherence and back-action. We present experimental data and develop a phenomenological model that accurately describes the correlations in the MQS. The observed correlations between flux and energy density could have profound implications for quantum information processing and thermodynamics with mechanical systems. Institution/ Groups involved: Vinay Malvimat, Rahul Mishra, and Vitaliy Negelyaev (Submitted to QST)",
        "watermark_text": "In this paper we investigate correlations between the power density and the flux of two different observables in an interesting quantum state called macroscopic quantum state ( MQS ) . These correlations were first discovered in the MQT of doubly clamped micro cantilevers and were interpreted as a signature of quantumness .We extend this analysis to other mechanical resonators made of different materials and of different geometries . We see that the flux - energy density correlations occur in the MQS and in the vacuum for the same set of constraints .We argue that these correlations can not be described using the standard models of decoherence and back - action . We report experimental evidence and develop a phenomenological theory that fully describes the correlations in the MQS .The observed correlations between flux and energy density might have profound implications for quantum information processing and thermodynamics with physical structures . Institution / Groups implicated : Vinay Malvimat , Rahul Mishra , and Vitaliy Negelyaev ( Submitted to QST )",
        "rewrite_text": "In this research, we delve into the relationship between power density and flux of two distinct observables in a captivating quantum state known as the Macroscopic Quantum State (MQS). These correlations were originally discovered in the MQT of doubly clamped micro cantilevers and were considered as a hallmark of quantum behavior. We extend our analysis to various mechanical resonators made of diverse materials and geometries. Our findings reveal that the flux-energy density correlations exist within the MQS and in vacuum, subject to the same set of constraints. We argue that these correlations cannot be explained by the conventional models of decoherence and back-action. We present experimental evidence and develop a phenomenological theory that comprehensively describes the correlations in the MQS. The observed connections between flux and energy density may hold significant implications for quantum information processing and thermodynamics involving physical structures. Involving Institutions/Groups: Vinay Malvimat, Rahul Mishra, and Vitaliy Negelyaev (Submitted to QST)",
        "ori-fast-z-score": 2.065591117977289,
        "water-fast-z-score": 5.938574464184706,
        "rewrite-fast-z-score": 1.9205531989934397
    },
    {
        "original_text": "The Gamma-Ray Imager (GRI) is a gamma-ray astronomical mission that was selected for implementation by ESA as a European Space Agency (ESA) Technology Development Program (TDP) initiative. The GRI technology demonstrator was successfully launched by a Pegasus 2 rocket on 5 January 2021, from Space Coast, Florida, USA. GRI is the first Small Bodies Participating Module (SB PM) of the larger Artemis program. GRI is a compact and lightweight gamma-ray astronomy mission for detecting and characterising gamma-ray emissions from solar system objects. It will be installed on the moon with a suite of experiments for studying the gamma-ray emissions of the lunar surface, the Earth-Moon system, and the sun. The gamma-ray astronomy instrument consists of a soft gamma-ray detector and a neutron detector. The gamma-ray detector is based on the Compton imaging technique, which combines data from incoming gamma-ray photons and Compton scatter events to form two-dimensional images of regional gamma-ray distributions. These images will provide information about the chemical composition, internal structures, and spatial distributions of targets within the fields of view. The neutron detector measures neutrons released by binding energy interactions between gamma-rays and materials. GRI will provide critical information to understand high-energy emissions from a variety of celestial bodies, including their composition, structure, and dynamics. GRI also has the potential to provide new scientific knowledge about the lunar environment, while allowing for characterization of potentialemisissions into the lunar gamma-ray environment.",
        "watermark_text": "The Gamma - Ray Imager ( GRI ) is a gamma - ray observatory mission that was selected for implementation by ESA as a European Space Agency ( ESA ) Technology Development Program ( TDP ) initiative . The GRI technology demonstrator was successfully launched by a Pegasus 2 spacecraft on 5 January 2021 , from Space Coast , Florida , USA .GRI is the first Small Bodies Participating Module ( SB PM ) of the bigger Artemis program . GRI is a compact and lightweight gamma - ray observatory mission for detecting and characterising gamma - ray emissions from solar system objects .It will be mounted on the lunar with a suite of studies for studying the alpha - ray emissions of the lunar surface , the Earth - Moon system , and the sun . The gamma - ray astronomy instrument consists of a soft alpha - ray detector and a neutron detector .The gamma - ray detector is based on the Compton imaging technique , which mixes information from incoming alpha - ray photons and Compton scatter events to form two - dimensional images of regional gamma - ray distributions . These photographs will provide details about the chemical composition , internal structures , and spatial distributions of targets within the fields of view .The neutron sensor measures neutrons released by binding energy interactions between gamma - radiation and materials . GRI will provide critical information to explain large - energy emitted from a variety of heavenly body , notably their composition , structure , and dynamics .GRI also has the possibilities to provide fresh research information about the lunar environment , while providing for characterization of potentialemisissions into the lunar gamma - ray environment .",
        "rewrite_text": "The Gamma-Ray Imager (GRI) is an observatory mission for gamma-ray detection, selected by the European Space Agency (ESA) as a Technology Development Program (TDP) initiative. The GRI technology demonstrator was successfully launched by a Pegasus 2 spacecraft on January 5th, 2021, from the Space Coast in Florida, USA. As the first Small Bodies Participating Module (SBPM) of the larger Artemis program, GRI is a compact and lightweight mission designed to detect and characterize gamma-ray emissions from solar system objects.\n\nIt will be installed on the moon, equipped with a suite of studies to investigate alpha-ray emissions from the lunar surface, the Earth-Moon system, and the sun. The gamma-ray astronomy instrument comprises a soft alpha-ray detector and a neutron detector. The gamma-ray detector utilizes the Compton imaging technique, which combines information from incoming alpha-ray photons and Compton scatter events to produce two-dimensional images of regional gamma-ray distributions. These images will offer detailed insights into the chemical composition, internal structures, and spatial distributions of targets within the field of view.\n\nThe neutron sensor measures neutrons released through binding energy interactions between gamma-radiation and materials. GRI will provide crucial information to elucidate the large amounts of energy emitted by various celestial bodies, particularly their composition, structure, and dynamics. Furthermore, GRI has the potential to offer fresh research insights into the lunar environment while facilitating the characterization of potential emissions within the lunar gamma-ray environment.",
        "ori-fast-z-score": 2.945838777274635,
        "water-fast-z-score": 7.180482019606923,
        "rewrite-fast-z-score": 3.6514837167011076
    },
    {
        "original_text": "A few hours before solar activity increased rapidly with the appearance of two regions of strong magnetic field rotation, with peripheral disappearance, located near the central part of the Sun (observed by the Heliospheric imager on board the Solar and Heliospheric Observatory, SOHO). These two regions of strong magnetic field rotation are considered as magnetic active regions. These regions are rotated with respect to each other by approximately 90 degrees in the course of four days. It is characteristic of some active regions that they produce, apparently temporarily, regions of strong magnetic field rotation, which are called flares. It was observed that these active regions, between June 14 and June 17, 2002, produced two flares of magnitude X10 and X8, respectively, recorded by the X-ray emission of the solar atmosphere. These active regions are observed to have two regions of peripheral disappearance of the magnetic field, one in each hemisphere, as well as two regions of strong magnetic field rotation, with peripheral appearance, in the central part of the Sun. Thus, it can be seen that these active regions generated, apparently temporarily, two regions of strong magnetic field rotation, which, as it was mentioned, are considered as magnetic active regions. These regions are rotated with respect to each other by approximately 90 degrees in the course of four days.",
        "watermark_text": "A few hours before solar activity improved rapidly with the appearance of two zones of strong magnetic field rotation , with peripheral disappearance , located near the main area of the Sun ( detected by the Heliospheric imager on board the Solar and Heliospheric Observatory , SOHO ) . These two zones of strong magnetic field rotation are considered as magnetic active regions .These zones are rotated with regard to each other by approximately 90 degrees in the course of four periods . It is characteristic of some active regions that they produce , probably temporarily , regions of strong magnetic force rotation , which are called flares .It was seen that these active regions , between June 14 and June 17 , 2002 , generated two flares of magnitude X10 and X8 , respectively , recorded by the X - ray radiation of the solar atmosphere . These active regions are observed to have two zones of peripheral disappearance of the magnetic field , one in each hemisphere , as well as two zones of strong magnetic force rotation , with peripheral shape , in the central half of the Sun .Thus , it can be shown that these active regions caused , probably temporarily , two zones of strong magnetic force rotation , which , as it was mentioned , are considered as magnetic active regions . These zones are rotated with regard to each other by approximately 90 degrees in the course of four days .",
        "rewrite_text": "A few hours prior, solar activity experienced a swift enhancement with the emergence of two magnetic active regions characterized by strong field rotation and peripheral disappearance. These regions were situated near the primary area of the Sun and were detected by the Heliospheric imager aboard the Solar and Heliospheric Observatory (SOHO). The two zones rotate relative to each other by approximately 90 degrees over a span of four periods. It is noteworthy that certain active regions temporarily generate regions of intense magnetic force rotation, which are referred to as solar flares.\n\nBetween June 14th and June 17th, 2002, these active regions were observed to produce two solar flares of magnitudes X10 and X8, respectively, recorded through the X-ray radiation emitted by the solar atmosphere. These active regions were found to possess two zones of magnetic field disappearance on their periphery, one in each hemisphere, along with two zones of strong magnetic force rotation, shaped like peripheries, located in the central half of the Sun. As such, it can be inferred that these active regions temporarily triggered two zones of intense magnetic force rotation, which are considered as magnetic active regions as well. These zones rotate in relation to each other by approximately 90 degrees over a period of four days.",
        "ori-fast-z-score": -0.9045340337332909,
        "water-fast-z-score": 5.4,
        "rewrite-fast-z-score": 2.799769575772148
    },
    {
        "original_text": "Allovalency, the valency of an atom or molecule by a multisite attachment, is an important concept in chemistry, physics and biology. Aromatic molecules such as benzene only allow single attachment (bifurcated attachment is forbidden by symmetry) and consequently only allow a valency of 2. However, a multisite attachment is possible in many other molecules such as porphyrins and protein kinases. Such molecules with higher valencies exhibit non-classical reactivity. For example, haem proteins such as cytochrome c which participate in electron transfer reactions have a valency of 4. In this study, we analyze a protein kinase, casein kinase II, with a valency of 7, by treating it as a multisite attachment. Our method of analysis is based on the Wentzel-Kramers-Brillouin (WKB) approximation which is applicable to reactions with highlyvalent molecules. First, we calculate the entropy of the multisite attachment using the Savage formula. Then, we analyse the flux term, an expression which governs the flux of electrons into the reaction from the reactants, in the WKB approximation. We find that despite a high valency, the kinase follows the common substrate-in-vesicle model for electron flow. As the entropy increase is quadratic in the valency, it can be very high in multisite reactions. Nevertheless, such reactions occur in biological systems in an ordered fashion, governed by the laws of thermodynamics.",
        "watermark_text": "Allovalency , the valency of an atom or molecule by a multisite attachment , is an important notion in science , chemistry and biology . Aromatic molecules such as benzene only require single attachment ( bifurcated attachment is restricted by symmetry ) and consequently only require a valency of 2 .However , a multisite attachment is possible in many other molecules such as porphyrins and protein kinases . Such molecules with higher valencies exhibit non - classical reactivity .For example , haem proteins such as cytochrome c which participate in electron transfer reactions have a valency of 4 . In this study , we analyze a protein kinase , casein kinase II , with a valency of 7 , by treating it as a multisite attachment .Our method of study is based on the Wentzel - Kramers - Brillouin ( WKB ) algorithm which is applicable to reactions with highlyvalent molecules . First , we determine the entropy of the multisite attachment using the Savage equation .Then , we analyse the flux term , an expression which governs the flux of electrons into the process from the reactants , in the WKB approximation . We see that despite a high valency , the kinase follows the common substrate - in - vesicle theory for electron flow .As the entropy increase is quadratic in the valency , it can be very high in multisite processes . Nevertheless , such reactions occur in biological systems in an ordered fashion , governed by the rules of thermodynamics .",
        "rewrite_text": "In science, chemistry, and biology, the concept of allovalency—the valency of an atom or molecule through a multisite attachment—is of utmost importance. Aromatic molecules like benzene only require a single attachment (restricted by symmetry for bifurcated attachment), thus only necessitating a valency of 2. However, multisite attachments are feasible in numerous other molecules, such as porphyrins and protein kinases. These molecules with higher valencies exhibit non-classical reactivity.\n\nFor instance, haem proteins like cytochrome c, which participate in electron transfer reactions, possess a valency of 4. In this study, we examine a protein kinase named casein kinase II, which has a valency of 7, treating it as a multisite attachment. Our approach relies on the Wentzel-Kramers-Brillouin (WKB) algorithm, which is applicable to reactions involving highly valent molecules.\n\nInitially, we determine the entropy of the multisite attachment using the Savage equation. Subsequently, we analyze the flux term, an expression that governs the flow of electrons into the process from reactants in the WKB approximation. We observe that, despite its high valency, the kinase follows the common substrate-in-vesicle theory for electron flow. As the increase in entropy is quadratic to the valency, it can be considerably high in multisite processes. Nevertheless, these reactions occur in a structured manner within biological systems, governed by the principles of thermodynamics.",
        "ori-fast-z-score": 1.118033988749895,
        "water-fast-z-score": 4.695742752749558,
        "rewrite-fast-z-score": 1.8439088914585775
    },
    {
        "original_text": "Recent experiments with spinor Bose-Einstein condensates (BECs) have produced powerful new tools for investigating magnetic interactions between individual atoms. We theoretically investigate a spin-1 BEC with contact spin exchange and magnetic dipole-dipole interactions in the regime of highly filled lowest hyperfine manifold. We present the full dynamical model and provide an analysis of the experimental capabilities to detect spin exchange and dipole-dipole interactions. We show that a system of this type can be mapped onto a effective transverse field Ising model in a rotating frame. We study the nonequilibrium phase ordering kinetics in this model and identify regimes where the domain structure can be directly observed. We examine in detail the critical behavior of the phase ordering kinetics and show that it is consistent with a continuous isotropic phase transition in two dimensions. In particular, the dynamical critical exponent z=2 indicates that domains grow as thin black discs. We identify promising experimental regimes for observing the phase ordering kinetics. These include loading the BEC into an optical lattice and driving the system through a quantum critical point. In particular, we show that in the presence of a trap, a spin-1 BEC with dipolar interactions displays both spontaneous symmetry breaking and a second-order phase transition to a symmetry-preserving polarized phase. We carry out a detailed parameter estimation study and conclude that such experiments should be feasible with current experimental techniques.",
        "watermark_text": "Recent research with spinor Bose - Einstein condensates ( BECs ) have created powerful new tools for investigating magnetic interactions between individual atoms . We theoretically study a spin - 1 BEC with touch spin transfer and magnetic dipole - dipole bonding in the regime of highly filled low hyperfine manifold .We introduce the full dynamical theory and describe an assessment of the empirical capabilities to identify spin transfer and dipole - dipole interactions . We see that a system of this kinds can be mapped onto a effective transverse field Ising model in a rotating frame .We research the nonequilibrium phase ordering kinetics in this model and find regimes where the domain structure can be directly observed . We explore in detail the important dynamics of the phase ordering kinetics and find that it is compatible with a periodic isotropic phase shift in two dimensions .In particular , the dynamical critical exponent z = 2 indicates that domains grow as thin dark discs . We identify promising experimental regimes for observing the phase ordering kinetics .These include packing the BEC into an optical lattice and forcing the system through a quantum essential point . In particular , we find that in the presence of a trap , a spin - 1 BEC with dipolar coupling displays both induced symmetry breaking and a second - order phase shift to a symmetry - preserving polarized phase .We take out a detailed parameter estimation analysis and suggest that such studies should be feasible with current experimental methods .",
        "rewrite_text": "Recent investigations utilizing spinor Bose-Einstein condensates (BECs) have paved the way for innovative tools in examining magnetic interactions among individual atoms. We conduct a theoretical exploration of a spin-1 BEC, focusing on touch spin transfer and magnetic dipole-dipole bonding within the highly populated low hyperfine manifold. We introduce a comprehensive dynamic theory and assess the empirical capabilities to identify spin transfer and dipole-dipole interactions.\n\nOur findings suggest that this type of system can be effectively mapped to an effective transverse field Ising model within a rotating frame. We delve into the non-equilibrium phase ordering kinetics in this model and discover distinct regimes where the domain structure can be observed directly. We delve into the crucial dynamics of phase ordering kinetics in detail and find that it aligns with a periodic isotropic phase shift in two dimensions. Specifically, the critical dynamic exponent z = 2 indicates that domains expand as thin dark discs.\n\nWe identify promising experimental scenarios for observing phase ordering kinetics, such as packing the BEC into an optical lattice and driving the system through a quantum critical point. Notably, we discover that in the presence of a trap, a spin-1 BEC with dipolar coupling exhibits both induced symmetry breaking and a second-order phase shift to a symmetry-preserving polarized phase. We conduct a detailed parameter estimation analysis and propose that such studies are feasible with current experimental techniques.",
        "ori-fast-z-score": -2.060488785479727,
        "water-fast-z-score": 5.432197707173825,
        "rewrite-fast-z-score": 0.7364596943186588
    },
    {
        "original_text": "A Chandra X-ray study of the galactic supernova remnant G299.2-2.9 reveals that the blast wave is decelerated by a low-density, rapidly expanding ejecta shell that has been formed by a previous generation of stars in the vicinity of the supernova explosion. The age of the ejecta shell is consistent with estimates based on its distance from the SNR center and its size. The spectrum of the thermal component of the SNR is softer than expected from a bare shock and can be explained if a population of thermalized ions with plasma temperature of 0.7 keV is present in the shocked ejecta. The large extent of the shell-related emission implies that the progenitor was a relatively massive star that lost a large fraction of its hydrogen-rich material prior to the core collapse. Alternatively, a progenitor mass of 17-22 M⊙ (for a typical explosion energy of 3×1051 erg) would be required if the ejecta is interacting with a low-density intercloud medium. In this case, the total SNR energy input is a factor of 2-3 higher than that released by the explosion.",
        "watermark_text": "A Chandra X - ray study of the galactic supernova remnant G299 . 2 - 2 . 9 reveals that the explosion wave is decelerated by a small - density , fast growing ejecta shell that has been formed by a prior generation of stars in the vicinity of the supernova explosion . The age of the ejecta shell is compatible with predictions based on its height from the SNR center and its size .The spectrum of the thermal component of the SNR is softer than expected from a bare jolt and can be understood if a population of thermalized ions with plasma heat of 0 . 7 keV is found in the shattered ejecta . The large extent of the shell - associated emission indicates that the progenitor was a fairly massive star that lost a large fraction of its hydrogen - laden matter previous to the core breakup .Alternatively , a progenitor mass of 17 - 22 [UNK] ( for a typical explosion energy of 3×1051 erg ) would be required if the ejecta is interacting with a low - density intercloud medium . In this case , the total SNR energy input is a factor of 2 - 3 higher than that released by the explosion .",
        "rewrite_text": "A Chandra X-ray analysis of the galactic supernova remnant G299.2-2.9 has revealed that the explosion wave is being decelerated by a small-density, rapidly expanding ejecta shell formed by a previous generation of stars in the vicinity of the supernova explosion. The age of this ejecta shell aligns with predictions based on its distance from the center of the SNR and its size. The thermal component spectrum of the SNR is less intense than expected from a simple shock, which can be explained if a population of thermalized ions with a plasma temperature of 0.7 keV is found within the shattered ejecta. The extensive shell-associated emission suggests that the progenitor star was of considerable mass, having lost a significant portion of its hydrogen-rich matter before the core breakup. Alternatively, if the ejecta is interacting with a low-density intercloud medium, a progenitor mass range of 17-22 solar masses (for a typical explosion energy of 3×1051 erg) would be required. In this scenario, the total energy input from the SNR is 2-3 times greater than that released by the explosion itself.",
        "ori-fast-z-score": 0.4588314677411235,
        "water-fast-z-score": 4.900304787764432,
        "rewrite-fast-z-score": 0.4588314677411235
    },
    {
        "original_text": "The article argues that a negative Poisson ratio in a metallic glass is a signature of a liquid-like behavior rather than a solid-like one. Such a behavior is commonly expected for a system in a vibratory excitation at a GHz frequency. However, to reach such a conclusion the authors statistically analysed the already published data from much lower frequency vibrations and came to the conclusion that the data does not support a solid-like behavior. I reanalysed the same data and I conclude that the analyzed data does in fact support a solid-like behavior, and even a liquid-like one for the specific composition of the sample. found that the analyzed data does in fact support a solid-like behavior, and even a liquid-like one for the specific composition of the sample. * The authors conclude that the observed frequency dependence of the Poisson ratio is more consistent with a liquid-like behavior than a solid-like one. The data indeed seems to be more consistent with a liquid-like behavior rather than a solid-like one. However, it is unclear whether the actual observed frequency dependence is indeed more consistent with a liquid-like behavior. In the authors’ own Figure 3, the Poisson ratio does not seem to approach -1 at low frequencies (the predicted value for a liquid). In addition, the authors did not test the prediction of liquid theory that the Poisson ratio is independent of frequency (if the sample is indeed a liquid). More data at different frequencies is needed to draw a more quantitative conclusion. The article can be improved by adding more analysis of the existing data to reach a more quantitative conclusion.",
        "watermark_text": "The section argues that a negative Poisson ratio in a metallic glass is a signature of a liquid - like behavior instead than a solid - like one . Such a behavior is often expected for a system in a vibratory excitation at a GHz frequency .However , to reach such a conclusion the authors statistically analysed the already published data from much lower frequency vibrations and came to the conclusion that the data does not support a solid - like behavior . I reanalysed the same data and I conclude that the analyzed data does in fact support a solid - like behavior , and even a liquid - like one for the specific composition of the sample .found that the studied data does in indeed suggest a solid - like behavior , and even a liquid - like one for the specific composition of the sample . * The authors suggest that the reported frequency dependence of the Poisson ratio is more consistent with a liquid - like behavior than a solid - like one .The data indeed seems to be more consistent with a liquid - like behavior instead than a solid - like one . However , it is uncertain whether the actual observed frequency dependence is indeed more consistent with a liquid - like behavior .In the writers ’ own Figure 3 , the Poisson ratio does not appear to approach - 1 at low frequencies ( the expected value for a liquid ) . In addition , the writers did not test the prediction of liquid physics that the Poisson ratio is independent of amplitude ( if the sample is indeed a liquid ) .More evidence at different frequencies is required to draw a more quantitative conclusion . The section can be improved by added more analysis of the previous material to reach a more quantitative conclusion .",
        "rewrite_text": "The section reiterates that a negative Poisson ratio in a metallic glass is an indicator of a liquid-like behavior rather than a solid-like one. Such behavior is frequently anticipated in systems undergoing vibratory excitation at GHz frequencies. To formulate this conclusion, the authors conducted a statistical analysis of previously published data from vibrations at much lower frequencies and concluded that the data does not support a solid-like behavior. After reevaluating the same data, I agree that the analyzed data actually supports a solid-like behavior, particularly for the specific composition of the sample, even indicating a liquid-like behavior.\n\nFurther examination of the studied data indicates that it indeed suggests a solid-like behavior, with some indications of a liquid-like behavior for the particular sample composition. The authors propose that the reported frequency dependence of the Poisson ratio aligns more closely with a liquid-like than a solid-like behavior. While the data seems to be more consistent with a liquid-like behavior, it is still uncertain whether the observed frequency dependence truly aligns more with a liquid-like state. In the authors' Figure 3, the Poisson ratio does not seem to approach -1 at low frequencies, which is the expected value for a liquid. Additionally, the authors did not test the prediction of liquid physics that the Poisson ratio should be independent of amplitude if the sample indeed behaves as a liquid. To draw a more quantitative conclusion, further evidence at different frequencies is required. The section could be enhanced by including additional analyses of previous material to reach a more definitive quantitative conclusion.",
        "ori-fast-z-score": 3.259286857530667,
        "water-fast-z-score": 7.723027987151322,
        "rewrite-fast-z-score": 4.849343154722923
    },
    {
        "original_text": "The X-ray-emitting neutron star in the center of the Geminga globular cluster exhibits departures from a spherical shape that are best modeled by a ovally distorted torus. The geometry of the neutron star is further constrained by the orbital elements of a distant low-mass companion. By combining X-ray, optical, and dynamical data, we show that the binary system most likely has an inclination of 89.4° and a mass ratio of 0.87. The most likely geometry is that the neutron star has an oval torus-shaped structure with a principal axis ratio of 1.06:1, aligned such that the shorter axis is toward the companion. This result suggests that the neutron star formed through a recent episode of mass accretion, and that the accretion process is ongoing. We present the results of a joint analysis of X-ray, optical, and dynamical data for the neutron star RX J1856.5-3754 in the globular cluster M28. RX J1856.5-3754 has a low-mass companion in a distant and eccentric orbit. The orbit has a measured inclination of 89.4° and a measured mass ratio of 0.87. The most likely geometry for the neutron star is that it has an oval torus-shaped structure with a principal axis ratio of 1.06:1, aligned such that the shorter axis is toward the companion. This result suggests that the neutron star formed through a recent episode of mass accretion, and that the accretion process is ongoing. The companion star is one of the most likely observed orbit beyond the Galactic disk, and its presence indicates that the orbital orientation is probably close to edge-on. Constraints on the mass and distance of the neutron star then allow for precise determinations of its radius and mass. These parameters characterize the equation of state of ultradense matter, and how it interacts with gravity. The inferred geometry is most consistent with an oblate shape with an axis ratio of 1.06:1, aligned such that the shorter axis is toward the companion.",
        "watermark_text": "The X - ray - emitting neutron star in the center of the Geminga globular cluster exhibits departures from a spherical shape that are best modeled by a ovally distorted torus . The morphology of the neutron star is further constrained by the orbital elements of a distant low - mass companion .By combining X - ray , optical , and dynamical data , we find that the binary system most likely has an inclination of 89 . 4° and a mass ratio of 0 . 87 . The most likely geometry is that the neutron star has an elongated torus - shaped structure with a principal axis proportion of 1 . 06 : 1 , aligned such that the shorter axis is toward the companion .This result suggests that the neutron star created through a recent episode of mass accretion , and that the accretion process is continued . We present the conclusion of a joint analysis of X - ray , optical , and dynamical data for the neutron star RX J1856 . 5 - 3754 in the globular cluster M28 .RX J1856 . 5 - 3754 has a small - mass companion in a distant and eccentric orbit . The orbit has a measured inclination of 89 . 4° and a measured mass ratio of 0 . 87 .The most likely geometry for the neutron star is that it has an elongated torus - shaped structure with a principal axis proportion of 1 . 06 : 1 , aligned such that the shorter axis is toward the companion . This result suggests that the neutron star created through a recent episode of mass accretion , and that the accretion process is continued .The companion star is one of the most likely detected orbit beyond the Galactic disk , and its presence indicates that the orbital orientation is probably nearly to edge - on . Constraints on the mass and distance of the neutron star then give for precise determinations of its radius and mass .These variables characterize the equation of state of ultradense matter , and how it interacts with gravity . The inferred geometry is most consistent with an oblate shape with an axis proportion of 1 . 06 : 1 , aligned such that the shorter axis is toward the companion .",
        "rewrite_text": "The neutron star emitting X-rays at the center of the Geminga globular cluster exhibits deviations from a perfect spherical shape, which can be best explained by an ovally distorted torus. The morphology of this neutron star is further constrained by the orbital elements of its distant low-mass companion. Through a joint analysis of X-ray, optical, and dynamical data, we deduce that the binary system likely has an inclination of 89.4 degrees and a mass ratio of 0.87. The most probable geometry suggests that the neutron star has an elongated structure resembling a torus, with a principal axis ratio of 1.06:1, aligned so that the shorter axis points towards the companion star.\n\nThis finding suggests that the neutron star was formed through a recent episode of mass accretion, and that the accretion process is still ongoing. We have also conducted a similar analysis for the neutron star RX J1856.5 - 3754 in the globular cluster M28. This star also has a small-mass companion on a distant and eccentric orbit, with a measured inclination of 89.4 degrees and a measured mass ratio of 0.87. Similarly, the most likely geometry for the neutron star involves an elongated torus-shaped structure with a principal axis ratio of 1.06:1, aligned so that the shorter axis is directed towards the companion.\n\nThe companion star is one of the most likely detections of an orbit beyond the Galactic disk, and its presence indicates that the orbital orientation is probably nearly edge-on. By constraining the mass and distance of the neutron star, we can make precise determinations of its radius and mass. These variables are crucial for understanding the equation of state of ultradense matter and how it interacts with gravity. The inferred geometry is most consistent with an oblate shape with an axis ratio of 1.06:1, aligned in such a way that the shorter axis points towards the companion.",
        "ori-fast-z-score": 3.9444674582777033,
        "water-fast-z-score": 6.859943405700353,
        "rewrite-fast-z-score": 4.186352522812128
    },
    {
        "original_text": "Memory and glassiness are ubiquitous in natural and artificial systems. It is well-known that the metastable state of a supercooled liquid becomes unstable at a critical temperature of freezing, forming a glassy state. Similarly, the failure of a digital circuit becomes inevitable when the operating temperature reaches a certain point, owing to the thermal noise. In this Letter, we investigate the ageing memory in a driven vortex system. By employing the time-resolved scanning laser Doppler vibrometry, we experimentally demonstrate that the metastable state of an infinite longitudinal wave becomes unstable with the propagation of wave number at a critical value, leading to the formation of a glassy state. We further clarify that the ageing memory is related to the failure of an information processing in the wave system, i.e., memory and glassiness are two sides of the same coin. Our findings not only enrich the understandings of the glass transition in wave systems, but also have potential applications in information processing with memory.",
        "watermark_text": "Memory and glassiness are ubiquitous in artificial and artificial systems . It is well - famous that the metastable state of a supercooled fluid turns unstable at a critical temperature of frozen , forming a glassy state .Similarly , the failure of a computer circuit becomes inevitable when the operating temperature exceeds a certain point , owing to the thermal noise . In this Letter , we investigate the ageing memory in a driven vortex network .By using the period - resolved scanning beam Doppler vibrometry , we experimentally prove that the metastable state of an endless longitudinal wave grows unstable with the propagation of wave number at a critical value , leading to the formation of a glassy state . We further clarify that the ageing memory is related to the failure of an information processing in the wave system , i . e . , memory and glassiness are two sides of the same coin .Our findings not only enrich the understandings of the glass process in wave systems , but also have potential applications in information processing with memory .",
        "rewrite_text": "In artificial and synthetic systems, memory and glassiness are prevalent phenomena. It is widely recognized that the supercooled fluid's tendency towards a metastable state becomes unstable when it reaches a critical temperature of solidification, resulting in a glass-like state. Likewise, computer circuits are prone to failure when the operational temperature surpasses a certain threshold due to thermal noise. In this letter, we delve into the aging memory within a driven vortex network. Through the utilization of period-resolved scanning beam Doppler vibrometry, we experimentally demonstrate that an endless longitudinal wave's metastable state becomes unstable when the wave number reaches a specific threshold value, leading to the formation of a glassy state. Furthermore, we clarify that the aging memory is intricately linked to the malfunctioning of information processing within the wave system - in essence, memory and glassiness are two aspects of the same concept. Our findings not only enhance our understanding of the glassy processes in wave systems but also hold potential for applications in memory-based information processing.",
        "ori-fast-z-score": 2.457864091118742,
        "water-fast-z-score": 6.4372630957871815,
        "rewrite-fast-z-score": 1.4444444444444444
    },
    {
        "original_text": "The paper is mainly concerned with a mesoscopic version of a diffusive Josephson junction with arbitrary transparency of the interlayer connection and arbitrary magnetic field in the “barrier”-layer. Within the quasiclassical Usadel theory the supercurrent through this junction is determined by the Green’s function which satisfies the appropriate boundary condition at the barrier. The problem of finding the supercurrent reduces to the problem of solving a system of nonlinear algebraic equations involving the Usadel equations and the normalization condition. The method of ”uniformization of eigenvalues” is used to explicitly find the supercurrent for arbitrary barrier transparency, at zero and finite temperatures. It is shown that supercurrent has a finite maximum at intermediate temperatures and abruptly decays to zero at zero temperature. The paper is noteworthy for novel exact results for the supercurrent through a mesoscopic diffusive Josephson junction. This problem is of practical interest, for example, for studying low-temperature characteristics of hybrid SQUID prepared by assembling several Josephson junctions in a single superconducting loop.",
        "watermark_text": "The paper is mainly concerned with a mesoscopic version of a diffusive Josephson junction with arbitrary clarity of the interlayer connection and arbitrary magnetic force in the “ barrier ” - layer . Within the quasiclassical Usadel theory the supercurrent through this intersection is calculated by the Green ’ s function which satisfies the appropriate boundary relation at the barrier .The question of finding the supercurrent reduces to the question of calculating a system of nonlinear mathematical equations involving the Usadel equations and the normalization condition . The method of ” uniformization of eigenvalues ” is utilized to explicitly find the supercurrent for arbitrary barrier transparency , at zero and finite temperatures .It is demonstrated that supercurrent has a finite maximum at intermediate temperatures and abruptly decays to zero at zero temperature . The paper is memorable for novel exact findings for the supercurrent through a mesoscopic diffusive Josephson junction .This problem is of practical importance , for example , for studying low - temperature characteristics of hybrid SQUID obtained by assembling several Josephson junctions in a single superconducting coil .",
        "rewrite_text": "The study primarily focuses on a mesoscopic version of a diffusive Josephson junction, addressing the clarity of interlayer connections and the arbitrary magnetic force within the \"barrier\" layer. Within the framework of the quasiclassical Usadel theory, the supercurrent at this junction is calculated using the Green's function, which adheres to the appropriate boundary conditions at the barrier. The task of determining the supercurrent boils down to solving a system of nonlinear mathematical equations, which encompasses the Usadel equations and the normalization condition.\n\nThe \"uniformization of eigenvalues\" technique is employed to explicitly determine the supercurrent for different barrier transparencies, both at zero and finite temperatures. It is shown that the supercurrent exhibits a finite maximum at intermediate temperatures and abruptly drops to zero at absolute zero. This paper is notable for its innovative and precise insights into the supercurrent flow through a mesoscopic diffusive Josephson junction. This problem holds significant practical importance, for instance, in studying the low-temperature characteristics of hybrid SQUIDs, which are created by assembling multiple Josephson junctions within a single superconducting coil.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 5.335783750799325,
        "rewrite-fast-z-score": 1.8599622199011085
    },
    {
        "original_text": "In this work, we present the results of a long-term study of the orbital periods of the AM CVn stars HP Librae and V803 Centauri. The two objects have very similar effective temperatures and gravitational moments, and are, accordingly, very close on the Hertzsprung–Russell diagram. We have monitored their eclipse timings for many years and detect different trends in their binary orbits. After combining our data with earlier results, we are able to determine an empirical relation between the binary period and the average time interval between consecutive eclipses. This allows us to determine an observational period for HP Librae of 83.86 hours and for V803 Centauri of 82.21 hours, or twice these values. The accuracy of our determination is sufficient to provide the first significant detection of the oscillation period in the binary orbit of HP Librae. We discuss possible origins for these periodicities and examine the implications of our findings for our current understanding of these objects. * * * We are conducting a long-term observational study of the eclipse timing of AM CVn stars HP Librae and V803 Centauri. The results of this work will improve our understanding of the binary systems in which these stars reside and may also lead to new insights into the nature of these systems. In particular, our results will allow us to determine the binary period of HP Librae with improved accuracy, which will allow us to study the oscillations in the binary orbit of this star in more detail. This will provide further constraints on the properties of the system and perhaps help to reveal its nature. Furthermore, by determining the binary period of V803 Centauri we will be able to confirm or rule out the possibility that this star is a fast rotator and to study its pulsations in more detail. Finally, the empirical relationship we have determined will allow other teams to more easily analyse the eclipse timing data for other AM CVn systems and to thereby further increase our understanding of these objects. We have obtained new timings for the eclipses of HP Librae and V803 Centauri from 2016 through to 2019. We then combined these new results with older data obtained between 2008 and 2015 in a method similar to that employed by Vanlandingham et al. (2008). This enabled us to detect different trends in the binary orbits of HP Librae and V803 Centauri and to determine empirical relationships between the binary period and the average time interval between consecutive eclipses. These relationships are: For HP Librae we find: T ≈ 0.94P + 282.76  s  where T is the average time interval between consecutive eclipses and P is the binary period in hours. For V803 Centauri we find: T ≈ 0.98P + 275.79  s  This has enabled us to determine observational",
        "watermark_text": "In this research , we present the conclusion of a many - term study of the orbital periods of the AM CVn stars HP Librae and V803 Centauri . The two bodies have very identical effective pressures and gravity moments , and are , accordingly , very close on the Hertzsprung – Russell diagram .We have analyzed their eclipse timings for numerous years and find various trends in their binary orbits . After using our information with previous findings , we are able to find an empirical connection between the binary period and the average time interval between successive eclipses .This enables us to estimate an observational duration for HP Librae of 83 . 86 days and for V803 Centauri of 82 . 21 hours , or twice these values . The accuracy of our determination is adequate to provide the first substantial measurement of the oscillation period in the binary orbit of HP Librae .We discuss possible origins for these periodicities and review the implications of our findings for our continuing understanding of these objects . * * * We are performing a large - term observational research of the eclipse timing of AM CVn stars HP Librae and V803 Centauri .The results of this research will enhance our grasp of the binary structures in which these stars reside and may also lead to fresh insights into the nature of these systems . In particular , our findings will provide us to predict the binary period of HP Librae with improved accuracy , which will able us to study the oscillations in the binary orbit of this star in more detail .This will provide further requirements on the properties of the system and maybe help to explain its nature . Furthermore , by finding the binary period of V803 Centauri we will be possible to confirm or limit out the suggestion that this star is a rapid rotator and to study its pulsations in more detail .Finally , the empirical relationship we have achieved will provide other teams to more easily analyse the eclipse timing statistics for other AM CVn models and to thus further expand our understanding of these objects . We have achieved new timings for the eclipses of HP Librae and V803 Centauri from 2016 through to 2019 .We then combined these new data with older data acquired between 2008 and 2015 in a technique similar to that used by Vanlandingham et al . ( 2008 ) .This enabled us to identify distinct trends in the binary orbits of HP Librae and V803 Centauri and to find empirical relationships between the binary period and the average time interval between successive eclipses . These relationships are : For HP Librae we find : T ≈ 0 . 94P + 282 . 76 s where T is the average time interval between successive eclipses and P is the binary period in hours .For V803 Centauri we find : T ≈ 0 . 98P + 275 . 79 s This has allowed us to predict observational",
        "rewrite_text": "In this study, we present the findings of a long-term investigation into the orbital periods of the AM CVn stars HP Librae and V803 Centauri. These two celestial bodies share identical effective pressures and gravity moments, resulting in their close proximity on the Hertzsprung-Russell diagram. Over the years, we have analyzed their eclipse timings and identified various trends in their binary orbits. By combining our observations with previous research, we have established an empirical relationship between the binary period and the average time interval between successive eclipses.\n\nThis relationship enables us to estimate an observational duration for HP Librae of 83.86 days and for V803 Centauri of 82.21 hours, or twice that value. The accuracy of our determination provides the first substantial measurement of the oscillation period in the binary orbit of HP Librae. We discuss potential origins of these periodicities and consider the implications of our findings for our ongoing understanding of these objects.\n\nOur research involves a comprehensive observational study of the eclipse timings of AM CVn stars HP Librae and V803 Centauri. The results of this study will enhance our comprehension of the binary structures hosting these stars and potentially offer new insights into the nature of these systems. Specifically, our findings will allow us to predict the binary period of HP Librae with improved accuracy, facilitating a more detailed examination of the star's binary orbit oscillations. This will provide further requirements on the system's properties and potentially help explain its nature.\n\nFurthermore, by determining the binary period of V803 Centauri, we will be able to confirm or limit the suggestion that this star is a rapidly rotating object and to delve deeper into its pulsations. Ultimately, the empirical relationship we have established will facilitate easier analysis of eclipse timing statistics for other AM CVn models by other teams, thereby advancing our understanding of these objects.\n\nWe have obtained new eclipse timings for HP Librae and V803 Centauri spanning from 2016 to 2019. We have combined these new data with older observations gathered between 2008 and 2015, using a technique similar to that employed by Vanlandingham et al. (2008). This enabled us to identify distinct trends in the binary orbits of both stars and establish empirical relationships between the binary period and the average time interval between successive eclipses. For HP Librae, we found that T (the average time interval between eclipses) is approximately 0.94 times the binary period (P) plus 282.76 seconds. For V803 Centauri, we found that T is approximately 0.98 times P plus 275.79 seconds. These relationships have allowed us to make more accurate predictions about these stars' orbital behaviors.",
        "ori-fast-z-score": -2.3985014047867566,
        "water-fast-z-score": 7.945035903356131,
        "rewrite-fast-z-score": 2.7714348384599967
    },
    {
        "original_text": "Pulse-coupled network of non-linear oscillators with non-diffusive coupling can exhibit dynamic behavior that is sensitive to initial conditions. One particular type of chaotic behavior, called splay state, has been identified in systems of globally coupled phase oscillators with repulsive non-linear coupling. In this paper, we demonstrate that the splay state is stable in a more general network of non-linear oscillators, namely a system of pulse-coupled oscillators. The stability analysis is performed using a simplified model of pulse-coupled oscillators that takes into account the temporal structure of pulses and can be used to analyze more complex pulse-coupled systems, such as those used in neural networks. We show that splay states are orbitally stable and use this result to explore the sensitivity of the splay state to changes in parameters of the pulse-coupling function and the connectivity matrix. We also explore the circumstances under which multi-splay states are stable. Our results demonstrate that the splay state can be a stable state not only in phase-coupled networks, but also in pulse-coupled networks with a broad class of pulse-coupling functions. The simplicity of the pulse-coupling function allows for the use of the pulse-coupled network in the context of neural networks, where different neuronal groups can be thought of as different oscillators with a pulse-coupling function.",
        "watermark_text": "Pulse - coupled network of non - linear oscillators with non - diffusive coupling can exhibit dynamic behavior that is sensitive to early conditions . One particular type of turbulent activity , called splay state , has been described in networks of globally coupled phase oscillators with repulsive non - linear correlation .In this paper , we prove that the splay state is stable in a more general network of non - linear oscillators , namely a system of pulse - coupled oscillators . The stability analysis is conducted using a simplified theory of pulse - coupled oscillators that takes into consideration the temporal shape of waves and can be used to analyze more sophisticated pulse - coupled networks , such as those utilized in neural systems .We see that splay states are orbitally stable and use this consequence to examine the sensitivity of the splay state to changes in parameters of the pulse - correlation function and the connectivity matrix . We additionally discuss the situations under which multi - splay states are stable .Our results show that the splay state can be a consistent state not only in phase - coupled networks , but also in pulse - coupled networks with a broad class of pulse - coupling functions . The simplicity of the pulse - coupling function allows for the using of the pulse - coupled network in the context of neural connections , where various neuronal classes can be thought of as varying oscillators with a pulse - coupling function .",
        "rewrite_text": "Pulse-coupled networks of nonlinear oscillators with non-diffusive coupling exhibit dynamic behavior that is highly sensitive to initial conditions. A specific type of turbulent activity, known as the splay state, has been described in networks of globally coupled phase oscillators with repulsive nonlinear correlations. In this paper, we establish the stability of the splay state in a more general context of pulse-coupled oscillator networks.\n\nThe stability analysis is based on a simplified theory of pulse-coupled oscillators that accounts for the temporal shape of waves, which can be applied to analyze more complex pulse-coupled networks such as those found in neural systems. We find that splay states are orbitally stable and use this property to explore the sensitivity of the splay state to changes in parameters of the pulse-correlation function and the connectivity matrix.\n\nFurthermore, we discuss scenarios under which multiple splay states are stable. Our findings indicate that the splay state can persist not only in phase-coupled networks but also in pulse-coupled networks with a wide range of pulse-coupling functions. The simplicity of the pulse-coupling function allows for the utilization of pulse-coupled networks in the context of neural connections, where different neuronal classes can be viewed as oscillators with various pulse-coupling functions.",
        "ori-fast-z-score": -0.27975144247209416,
        "water-fast-z-score": 4.942275483673663,
        "rewrite-fast-z-score": 2.2445701677816263
    },
    {
        "original_text": "A sample of 62 quasars located behind a dense foreground galaxy cluster were imaged with the Hubble Space Telescope in two programs designed to test the gravitational lensing hypothesis for the mass distribution in clusters. Strong evidence is found for the lensing hypothesis for 45 of the quasars, with critical surface density consistent with that for clusters, as expected for the expected cluster mass-to-light ratio. An analysis of the optical spectra of the quasars yields a constraint on the distribution of matter in the Universe similar to that from studies of distant Type Ia supernovae. The mass distribution is consistent with a critical density universe, with no significant contribution from an additionalcomponent of dark matter. The observations are consistent with the standard ΛCDM model of structure formation, with the primordial density perturbations grown into large-scale structure by gravitational instability. The quasarareas above mean density, the clusters below, provide the most direct probe of the dark matter component of the ΛCDM model to date. The lensing hypothesis has been tested in the sample of 45 quasars with strong lensing evidence, yielding the most extensive test to date of the cluster mass-to-light ratio from strong lensing. The results are consistent with the expected cluster mass-to-light ratio, as expected from the cluster mass-to-light ratios measured from X-ray and Sunyaev-Zel dovich surveys. The optical spectra of the quasars are used to provide a test of the critical density universe, with no significant contribution from an additional component of dark matter. The results are consistent with the standard model of structure formation in the ΛCDM model, with the primordial density perturbations grown into large-scale structure by gravitational instability. The observed quasars are generally too bright to be strongly lensed by individual galaxies in the clusters, with observed redshifts typically z≳ 2.2, corresponding to emitted rest-frame wavelengths of 1300 Angstroms. Strong lensing is most effective for clusters at lower redshift, with the sample of observed quasars thus drawn from a somewhat higher mass population of clusters than those normally studied by X-ray and Sunyaev-Zel dovich surveys. A subsample of the observed quasars was selected for spectroscopic observations with the Far Ultraviolet Spectroscopic Explorer (FUSE) satellite. Thirty-three quasars show no absorption lines, yielding a lower limit on the neutral hydrogen fraction of around 10-6. However, if indeed a significant fraction of the matter in clusters is in the form of dark matter, which would be expected to thermalize and become statistically indistinguishable from cold dark matter on the cluster scale, then the neutral hydrogen fraction could be much lower, around 10-9, below the current sensitivity limit of FUSE. The HST results, with strong lensing evidence for 45 of the 62 quasars in the sample, represent the most extensive test of the cluster mass-to-light",
        "watermark_text": "A specimen of 62 quasars located behind a dense foreground galaxy cluster were imaged with the Hubble Space Telescope in two projects designed to test the gravitational lensing hypothesis for the mass distribution in clusters . Strong evidence is found for the lensing hypothesis for 45 of the quasars , with critical surface volume compatible with that for clusters , as anticipated for the expected cluster mass - to - light density .An evaluation of the optical spectra of the quasars reveals a constraint on the distribution of matter in the Universe similar to that from studies of distant Type Ia supernovae . The mass distribution is compatible with a critical velocity galaxy , with no major contribution from an additionalcomponent of light matter .The findings are compatible with the standard ΛCDM theory of formation formation , with the primordial density perturbations grown into huge - scale structure by gravitational instability . The quasarareas above average density , the clusters below , provide the most direct probe of the dark matter component of the ΛCDM theory to date .The lensing hypothesis has been tested in the sample of 45 quasars with powerful lensing evidence , yielding the most rigorous analysis to date of the cluster mass - to - light percentage from good lensing . The results are compatible with the expected cluster mass - to - light percentage , as predicted from the cluster mass - to - light proportions measured from X - ray and Sunyaev - Zel dovich surveys .The imaging spectra of the quasars are using to provide a test of the key density universe , with no major contribution from an additional element of dark matter . The results are compatible with the standard theory of formation formation in the ΛCDM theory , with the primordial density perturbations grown into huge - scale structure by gravitational instability .The observed quasars are generally too bright to be strongly lensed by individual galaxies in the clusters , with observed redshifts typically [UNK] 2 . 2 , corresponding to emitted rest - frame wavelengths of 1300 Angstroms . Strong lensing is most useful for clusters at lower redshift , with the sample of observed quasars thus drawn from a somewhat higher mass population of clusters than those normally studied by X - ray and Sunyaev - Zel dovich surveys .A subsample of the observed quasars was selected for spectroscopic observations with the Far Ultraviolet Spectroscopic Explorer ( FUSE ) satellite . Thirty - three quasars exhibit no absorption paths , yielding a smaller minimum on the neutral hydrogen proportion of around 10 - 6 .However , if indeed a substantial proportion of the matter in clusters is in the form of dark matter , which would be anticipated to thermalize and become statistically indistinguishable from cool gray material on the cluster scale , then the neutral hydrogen proportion could be much lower , around 10 - 9 , below the present intensity maximum of FUSE . The HST results , with powerful lensing evidence for 45 of the 62 quasars in the sample , constitute the most extensive test of the cluster mass - to - light",
        "rewrite_text": "A sample of 62 quasars situated behind a dense foreground galaxy cluster has been imaged using the Hubble Space Telescope in two projects aimed at testing the gravitational lensing hypothesis regarding mass distribution in clusters. The analysis has found strong evidence to support the lensing hypothesis for 45 of these quasars, with a critical surface volume that aligns with cluster expectations, as predicted by the expected cluster mass-to-light density ratio.\n\nAn evaluation of the optical spectra of these quasars has revealed a constraint on the matter distribution in the Universe, which is similar to the constraints derived from studies of distant Type Ia supernovae. The mass distribution appears to be compatible with a critical velocity galaxy, without a significant contribution from additional components of luminous matter. These findings are consistent with the standard Lambda Cold Dark Matter (ΛCDM) theory, where primordial density perturbations have grown into large-scale structures through gravitational instability.\n\nThe quasars in this sample, with areas above average density and clusters below, provide the most direct probe of the dark matter component in the ΛCDM theory so far. The lensing hypothesis has been rigorously tested in the sample of 45 quasars, yielding powerful lensing evidence and providing the most precise analysis yet of the cluster mass-to-light percentage. The results are in agreement with the expected cluster mass-to-light percentage, as predicted by measurements of cluster mass-to-light ratios from X-ray and Sunyaev-Zel'dovich surveys.\n\nFurthermore, the imaging spectra of these quasars are being used to test the key density universe theory, without a major contribution from additional elements of dark matter. These results are in line with the standard formation theory within the ΛCDM framework, where primordial density perturbations have led to the formation of large-scale structures through gravitational instability.\n\nThe observed quasars are generally too bright to be strongly lensed by individual galaxies within the clusters, with observed redshifts typically around 2.2, corresponding to emitted rest-frame wavelengths of 1300 angstroms. Strong lensing is most effective for clusters at lower redshifts, and therefore the sample of observed quasars has been selected from a slightly higher mass population of clusters than those typically studied using X-ray and Sunyaev-Zel'dovich surveys.\n\nA subset of these quasars has been selected for spectroscopic observations using the Far Ultraviolet Spectroscopic Explorer (FUSE) satellite. Of the 33 quasars examined, none exhibited absorption lines, yielding a reduced minimum neutral hydrogen proportion of approximately 10-6. However, if a substantial proportion of the matter in clusters is indeed dark matter, which would be expected to thermalize and become statistically indistinguishable from cool gray matter on a cluster scale, then the neutral hydrogen proportion could be significantly lower, at around 10-9, below the current maximum intensity limit of FUSE.\n\nIn conclusion, the HST observations, with strong lensing evidence for 45 out of 62 quasars in the sample, constitute the most comprehensive test yet of the cluster mass-to-light ratio in relation to the ΛCDM theory of cosmic formation and evolution.",
        "ori-fast-z-score": -0.9124211282466754,
        "water-fast-z-score": 7.421484445348452,
        "rewrite-fast-z-score": 3.059874484093601
    },
    {
        "original_text": "In magnetic multilayers comprising multiple ferromagnets with different directions of magnetizations, spin transport is related to the transfer of angular momentum between the magnetic moments. The efficiency of this transfer is quantified by the interfacial spin-transfer torque (STT), which defines the basic behaviour of these structures as discrete hardware for future spin-based information technologies. In this work we focus on systems with cubic anisotropy, in which out-of-plane spontaneous magnetization is preferred to in-plane magnetizations due to interfacial interactions. Using the thin-film approach, we present a comprehensive study of STT as a function of the number of atomic layers in the multilayer, compositional disorder, magnetic anisotropy and temperature. It is shown that STT in Co/Cu and Ni/Cu multilayers can be efficiently controlled by adjusting the number of atomic layers in these layers. Furthermore, it is observed that Cu layers do not allow the magnetization to stabilize in the out-of-plane direction, whereas Ni layers stabilize the magnetization in this direction. Thus, changing the number of atomic layers in these heterostructures enables the control of the interfacial STT. Finally, it is shown that in Co/Cu bilayers with sufficiently thick Co layers, the interfacial STT exhibits weak dependence on temperature.",
        "watermark_text": "In magnetic multilayers comprising several ferromagnets with various directions of magnetizations , spin travel is related to the transfer of angular momentum between the magnetic moments . The efficiency of this transfer is quantified by the interfacial spin - transfer torque ( STT ) , which explains the fundamental behaviour of these structures as continuous hardware for future spinning - based information devices .In this research we focus on networks with cubic anisotropy , in which out - of - plane spontaneous magnetization is preferred to in - plane magnetizations related to interfacial interactions . Using the narrow - film technique , we present a comprehensive investigation of STT as a function of the quantity of atomic layers in the multilayer , compositional disorder , optical anisotropy and heat .It is demonstrated that STT in Co / Cu and Ni / Cu multilayers can be smoothly regulated by adjusting the number of atomic layers in these layers . Furthermore , it is observed that Cu layers do not enable the magnetization to stabilize in the out - of - plane position , whereas Ni layers stabilize the magnetization in this direction .Thus , changing the number of atomic layers in these heterostructures provides the regulation of the interfacial STT . Finally , it is demonstrated that in Co / Cu bilayers with sufficiently thick Co atoms , the interfacial STT exhibits weak dependence on temperature .",
        "rewrite_text": "In magnetic multilayers containing variously-aligned ferromagnets, the movement of spins is closely related to the transfer of angular momentum between magnetic moments. This transfer's effectiveness is quantified by the interfacial spin-transfer torque (STT), which underpins the fundamental behavior of these structures as potential hardware for future spin-based information devices. Our research focuses on networks with cubic anisotropy, where out-of-plane spontaneous magnetization is preferred over in-plane magnetizations linked to interfacial interactions.\n\nUsing the narrow-film technique, we conduct a comprehensive study on how STT varies with the number of atomic layers in the multilayer, compositional disorder, optical anisotropy, and heat. It has been demonstrated that STT in Co/Cu and Ni/Cu multilayers can be smoothly adjusted by altering the number of atomic layers. Additionally, it is observed that Cu layers do not facilitate the stabilization of magnetization in the out-of-plane position, whereas Ni layers do. Therefore, adjusting the number of atomic layers in these heterostructures allows for the regulation of interfacial STT. Finally, it is shown that in Co/Cu bilayers with sufficiently thick Co atoms, the interfacial STT exhibits a minimal temperature dependence.",
        "ori-fast-z-score": -0.4364357804719848,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 2.429493573646624
    },
    {
        "original_text": "In this letter, we present a systematic analysis of a spin-susceptibility representation of the pairing interaction in the two-dimensional (2D) Hubbard model. We establish that this representation is closely related to the channel decomposition of the interaction in this model and can be straightforwardly extended to the case of arbitrary spatial dimensions. To illustrate the power of this approach we apply it to derive a complete family of effective pairing interactions for the 2D Hubbard model. For infinitesimal coupling, we show that our results reduce to the form previously proposed by us and by Kuroki and Aimi. Our numerical results for intermediate coupling demonstrate that our proposed family of interactions yields greatly improved results over the previously known forms and, in particular, yields results very close to those of cluster perturbation theory for the 2D Hubbard model. The key idea of our approach is to recast the pairing problem in the space of spin susceptibilities. The pairing interaction is then represented as a linear functional of the spin susceptibilities that, by definition, is zero when acting on any symmetric tensor built out of Fermi-surface spin susceptibilities. This leads to a systematic procedure for constructing an interaction that can correctly describe a pairing instability. This approach is appealing since it can be naturally generalized to higher dimensions and therefore allows for a straightforward extension to cases beyond the 2D Hubbard model, such as the recently discovered class of materials known as Five- layered iridates.",
        "watermark_text": "In this letter , we present a comprehensive assessment of a spin - susceptibility model of the pairing interaction in the two - dimensional ( 2D ) Hubbard theory . We establish that this representation is closely related to the channel decomposition of the interaction in this model and can be straightforwardly extended to the case of arbitrary spatial dimensions .To explain the power of this methodology we apply it to derive a complete family of effective pairing behaviors for the 2D Hubbard theory . For infinitesimal interactions , we find that our findings reduce to the form previously introduced by us and by Kuroki and Aimi .Our numerical findings for intermediate interaction show that our proposed family of relationships produce significantly improved results over the previously known forms and , in example , yields findings very close to those of cluster perturbation theory for the 2D Hubbard theory . The main idea of our approach is to recast the pairing issue in the space of spin susceptibilities .The pairing interaction is then represented as a linear functional of the spin susceptibilities that , by definition , is zero when acted on any symmetric tensor building out of Fermi - surface spin susceptibilities . This leads to a comprehensive technique for constructing an interaction that can accurately describe a pairing instability .This method is appealing since it can be naturally generalized to higher dimensions and therefore allows for a straightforward addition to cases beyond the 2D Hubbard theory , such as the recently discovered class of substances referred as Five - layered iridates .",
        "rewrite_text": "In this letter, we present an extensive evaluation of a spin-susceptibility model for the pairing interaction within the two-dimensional (2D) Hubbard theory. We establish a strong connection between this representation and the channel decomposition of the interaction in the model, which can be easily extended to cases involving arbitrary spatial dimensions.\n\nTo illustrate the effectiveness of our approach, we apply it to derive a comprehensive set of effective pairing behaviors for the 2D Hubbard theory. For minimal interactions, our findings align with previous work by us and Kuroki and Aimi. Our numerical results for intermediate interactions demonstrate that our proposed family of relationships significantly improves results compared to previously known forms. For instance, it yields findings closely resembling those of cluster perturbation theory for the 2D Hubbard theory.\n\nThe core idea of our method is to reframe the pairing issue in the context of spin susceptibilities. The pairing interaction is then expressed as a linear functional of spin susceptibilities, which is zero when applied to any symmetric tensor constructed from Fermi-surface spin susceptibilities. This leads to a comprehensive technique for constructing an interaction that can accurately describe a pairing instability.\n\nThis method is appealing because it can be easily generalized to higher dimensions, making it suitable for applications beyond the 2D Hubbard theory, such as the recently discovered class of materials known as Five-layered iridates.",
        "ori-fast-z-score": 1.3348476249438292,
        "water-fast-z-score": 7.43700819611562,
        "rewrite-fast-z-score": 4.930356094132884
    },
    {
        "original_text": "Future hadron colliders, such as the Large Hadron Collider (LHC) and the International Linear Collider (ILC), can probe new energy scales not accessible to the current experiments. For example, a proton-proton collider with energy in the teraeV range could cover unexplored dark energy parameter space with highly energetic quintessence particles/axions. Measuring the energy and the trajectory of these particles would elucidate the nature of dark energy and the equation of state of quintessence. The LHC and ILC both explore the electroweak scale, but they are becoming incapable of probing new energy scales such as the dark energy scale or the quintessence scale. The lowest energy quintessence particles have a deBroglie wavelength much larger than the Planck scale, and are thus invisible to current experiments. A proton-proton collider with energy above 10 teraeV could study these particles with sufficient sensitivity to probe the nature of dark energy and quintessence. Measuring the energy and trajectory of quintessence particles would enable us to elucidate the nature of dark energy and the equation of state of quintessence. In this way, the potential of future hadron colliders could be extended beyond the discovery of new phenomena to the elucidation of the very structure of the universe.  0 : https://arxiv.org/abs/1711.02344 Direct measurements of the quintessence equation of state could also dwarf the contributions from current dark energy probes, such as BAO/CMB/ Hubble and strong forces. A quintessence measurement could substantially impact cosmological modeling, and might even contradict/validate current assumptions. The ability to make such a measurement will thus depend critically on the energy and design decisions made for future hadron colliders. A combined analysis of these experimental signatures can serve as a more robust probe of the nature of dark energy, as it reduces the dependence of the results on assumptions about quintessence dynamics. A multi-probe study of future hadron collider dark energy potential, combined with data from other experimental probes, could allow for a more robust equation of state measurement. For example, a combined analysis of quintessence particles with future hadron collider data and other current dark energy probes would allow for a nearly model independent measurement of the equation of state, assuming that quintessence dynamics are known independently. The potential of future hadron colliders to study new energy scales, such as the dark energy and quintessence scales, has not been exploited to date. Such studies would allow us to elucidate the nature of dark energy and the equation of state of quintessence.",
        "watermark_text": "Future hadron colliders , such as the Large Hadron Collider ( LHC ) and the International Linear Collider ( ILC ) , can investigate new power scales not accessible to the present studies . For instance , a proton - proton collider with energy in the teraeV range may cover unexplored dark energy parameter space with highly energetic quintessence particles / axions .Measuring the power and the path of these objects would elucidate the nature of dark energy and the equation of state of quintessence . The LHC and ILC both explore the electroweak scale , but they are growing incapable of probing novel energy scales such as the dark energy scale or the quintessence scale .The lowest energy quintessence particles have a deBroglie wavelength far larger than the Planck scale , and are thus invisible to recent experiments . A proton - proton collider with energy above 10 teraeV might explore these ions with sufficient sensitivity to probe the nature of light energy and quintessence .Measuring the power and trajectory of quintessence particles might enable us to elucidate the nature of dark energy and the equation of state of quintessence . In this way , the possibilities of new hadron colliders may be enlarged beyond the discovery of new phenomena to the elucidation of the very structure of the universe .0 : https : / / arxiv . org / abs / 1711 . 02344 Direct measurements of the quintessence equation of state could also dwarf the contributions from current dark energy probes , such as BAO / CMB / Hubble and strong forces . A quintessence measurement could substantially impact cosmological modeling , and might even contradict / validate current assumptions .The capacity to make such a measurement will consequently depend seriously on the power and engineering choices produced for future hadron colliders . A coupled assessment of these experimental signatures can provide as a more robust probe of the nature of dark energy , as it reduces the dependence of the results on predictions about quintessence behavior .A multi - probe study of future hadron collider dark energy potential , combined with data from other experimental probes , might enable for a more robust equation of state study . For instance , a coupled assessment of quintessence particles with past hadron collider data and other current dark energy probes might enable for a nearly model independent assessment of the equation of state , assuming that quintessence dynamics are known independently .The capacity of possible hadron colliders to study new power scales , such as the dark energy and quintessence scales , has not been utilized to date . Such experiments would enable us to elucidate the nature of dark energy and the equation of state of quintessence .",
        "rewrite_text": "Future hadron colliders, such as the Large Hadron Collider (LHC) and the International Linear Collider (ILC), hold the potential to explore novel power scales that are inaccessible to current studies. For instance, a proton-proton collider with energies in the teraeV range could delve into uncharted territories of dark energy parameter space with highly energetic quintessence particles or axions. By measuring their power and trajectory, this could elucidate the essence of dark energy and the equation of state for quintessence.\n\nWhile both the LHC and ILC probe the electroweak scale, they are increasingly inadequate at investigating novel energy scales such as the dark energy scale or the quintessence scale. The lowest-energy quintessence particles possess a deBroglie wavelength significantly larger than the Planck scale, rendering them undetectable by recent experiments. A proton-proton collider with an energy exceeding 10 teraeV may offer sufficient sensitivity to explore these particles, revealing insights into the nature of light energy and quintessence.\n\nDirect measurements of the quintessence equation of state have the potential to overshadow contributions from current dark energy probes like BAO, CMB, and the Hubble Space Telescope, as well as strong force measurements. A precise measurement of quintessence could significantly impact cosmological modeling, potentially challenging or validating current assumptions. The feasibility of such measurements heavily relies on the power and engineering decisions made for future hadron colliders.\n\nA comprehensive assessment of these experimental signatures can provide a more robust probe of dark energy's nature, reducing the reliance on predictions about quintessence behavior. A multi-probe study of future hadron collider dark energy potential, combined with data from other experimental probes, could enable a more reliable equation of state investigation. For instance, a combined analysis of quintessence particles with data from past hadron colliders and other current dark energy probes may permit a nearly model-independent assessment of the equation of state, assuming that quintessence dynamics are known independently.\n\nUntil now, the potential of future hadron colliders to explore new power scales such as dark energy and quintessence has been untapped. Such experiments would indeed elucidate the nature of dark energy and the equation of state for quintessence.",
        "ori-fast-z-score": -0.1466471150213533,
        "water-fast-z-score": 7.131581834309935,
        "rewrite-fast-z-score": 0.5482823149915702
    },
    {
        "original_text": "The standard map is a well-known paradigm for investigating classical dynamical systems. In this work we use the island-identification algorithm to compute the largest Lyapunov exponent and the finite-time stability exponent for several regions of the parameter space. The former gives an estimate of the typical sensitivity to initial conditions of the system, while the latter describes the asymptotic long-time behavior of the distance between two close trajectories. This last exponent, which requires the calculation of the largest Floquet multiplier, can also signal the presence of islands of stability near some critical points of the map. We find that for certain regions of the parameter space these two stability exponents are correlated: the faster the system tends to random-walk around its initial conditions, the smaller the corresponding island is. Conversely, in other regions these exponents are decoupled, namely the fastest possible decay of the distance between two trajectories is not necessarily associated with the largest possible Lyapunov exponent. For some regions of the parameter space the separation of these two behaviors has been shown rigorously. We find this phenomenon to hold in general, in agreement with previous numerical results.",
        "watermark_text": "The classic map is a better - famous paradigm for investigating classical dynamical systems . In this study we utilize the island - identification method to compute the greatest Lyapunov exponent and the finite - time stability exponent for various regions of the parameter space .The first gives an measure of the typical sensitivity to early conditions of the system , while the second describes the asymptotic long - time response of the distance between two close trajectories . This last exponent , which requires the determination of the greatest Floquet multiplier , can also signal the presence of islands of stability near some important points of the mapping .We see that for particular regions of the parameter space these two stability exponents are correlated : the faster the system tends to random - walk around its initial conditions , the smaller the associated island is . Conversely , in other regions these exponents are decoupled , namely the fastest possible decay of the distance between two trajectories is not necessarily identified with the smallest available Lyapunov exponent .For some regions of the parameter space the separation of these two behaviors has been shown rigorously . We see this phenomenon to hold in general , in agreement with previous quantitative results .",
        "rewrite_text": "The traditional map serves as a superior and renowned template for exploring classical dynamic systems. In this investigation, we employ the island-identification method to compute the greatest Lyapunov exponent and the finite-time stability index for various areas of the parameter space. The former offers a measure of the system's typical sensitivity to early conditions, while the latter characterizes the long-term asymptotic response of the distance between two close trajectories. Specifically, this latter exponent, which necessitates the determination of the largest Floquet multiplier, can also indicate the existence of stability islands close to significant points in the mapping. We observe that for specific regions of the parameter space, these two stability indices are correlated; areas where the system tends to random wander around its initial conditions exhibit smaller associated islands. Conversely, in other regions, these indices are uncoupled, meaning that the fastest possible reduction in the distance between two trajectories is not necessarily associated with the smallest Lyapunov exponent available. For certain regions of the parameter space, the differentiation of these two behaviors has been rigorously demonstrated. We find this phenomenon to be generally observed, in agreement with previous quantitative findings.",
        "ori-fast-z-score": 0.8432740427115678,
        "water-fast-z-score": 6.324555320336758,
        "rewrite-fast-z-score": 1.1285761872936695
    },
    {
        "original_text": "In this work we solve the equation for the high energy evolution of parton distribution functions (PDFs) including the first running coupling corrections. To this end, we use the principle of maximum conformality (PMC) introduced byliusly, which provides a novel solution to the renormalization group equation (RGE) for the PDFs in the perturbative regime. In this approach the solution is achieved by adding a particular RGE solution for the QCD beta function into the PMC one, which introduces a new parameter. We demonstrate that the PMC solution fulfills all requirements of the formal perturbative expansion in QCD. Running coupling corrections are included via the PMC parameter, which is fixed by the corresponding momentum sum rule for the corresponding coupling. To validate the approach, we consider the total and polarized distributions of the neutron for a given target mass number A. For the first time, our result for the neutron structure function F2(x, Q2 = 2.2 GeV2) presents a complete leading-order (LO) and next-to-leading-order (NLO) nuclear corrections calculation. We predict that the gluon distribution function in the neutron is significantly smaller than that in the proton at large x values, which can be tested by future experiments.",
        "watermark_text": "In this study we solve the equation for the high energy evolution of parton distribution functions ( PDFs ) including the first ran coupling corrections . To this end , we utilize the principle of maximum conformality ( PMC ) proposed byliusly , which offers a novel solution to the renormalization group equation ( RGE ) for the PDFs in the perturbative regime .In this way the solve is achieved by added a certain RGE solution for the QCD beta function into the PMC one , which gives a new parameter . We suggest that the PMC solution fulfills all requirements of the formal perturbative expansion in QCD .Running correlation corrections are provided via the PMC parameter , which is fixed by the associated momentum sum rule for the respective coupling . To validate the approach , we study the total and polarized distributions of the neutron for a given target mass number A .For the first time , our conclusion for the neutron structure function F2 ( x , Q2 = 2 . 2 GeV2 ) presents a complete leading - order ( LO ) and last - to - leading - order ( NLO ) nuclear corrections calculation . We predict that the gluon distribution relation in the neutron is significantly less than that in the proton at large x values , which can be verified by future research .",
        "rewrite_text": "In this investigation, we have resolved the equation for the high-energy evolution of parton distribution functions (PDFs), incorporating the initial running coupling corrections. To accomplish this, we have employed the principle of maximum conformality (PMC), which was originally proposed, offering an innovative solution to the renormalization group equation (RGE) for PDFs within the perturbative regime. By integrating a specific RGE solution for the QCD beta function into the PMC framework, a novel parameter has been derived. We propose that the PMC solution meets all formal requirements of perturbative expansion in QCD. Running correlation corrections are facilitated through the PMC parameter, which is determined by the associated momentum sum rule for the respective coupling.\n\nTo validate our approach, we have examined the total and polarized distributions of the neutron for a specific target mass number A. This is the first time we have presented a comprehensive leading-order (LO) and next-to-leading-order (NLO) nuclear correction calculation for the neutron structure function F2(x, Q2 = 2.2 GeV2). Our prediction suggests that the gluon distribution relationship in the neutron is significantly lower than that in the proton at large x values, a finding that can be verified through future research.",
        "ori-fast-z-score": 0.9761870601839528,
        "water-fast-z-score": 6.038635299392551,
        "rewrite-fast-z-score": 2.03701381619181
    },
    {
        "original_text": "3C 382 is a well-known radio galaxy with a highly Compton-thick active nucleus. The obscuring column density is estimated to be a few times 1025 cm−2. Observations of this object have traditionally been difficult, and very few spectra of its nuclear emission were known. We report on the results of recent X-ray observations of this object with both the Rossi X-ray Timing Explorer and the Chandra High Energy Transmissions Gratting Facility. We fit the data with a model that consists of a Compton-thick thermal accretion disk corona component and a complex reflection spectrum. We find that the coronal temperature is surprisingly low at kT ≈ 0.65 keV, and the coronal ionization parameter is large at 0.7 erg cm s −1. The size of the coronal region is extremely large, extending out to a radius of 14 parsecs (or 40 kiloparsecs), as indicated by the large value of the light-crossing time for the corona, τ cross = 57 light- days. These results challenge models in which the primary emission is blackbody radiation from a cool, thin accretion disk. Instead, we suggest that the primary emission is produced by a hot, thick accretion disk that is optically thick not only to direct emission, but also to Compton scattering. If the primary emission extends down to the last stable orbit, as appears to be the case in this source, then we must be viewing the inner regions of the accretion flow almost exactly edge-on. We discuss alternative geometries that may explain the large-scale coronal emission without requiring such an extreme viewing angle.",
        "watermark_text": "3C 382 is a better - famous radio galaxy with a highly Compton - thick active core . The obscuring column size is predicted to be a few times 1025 cm−2 .Observations of this object have traditionally been difficult , and very few spectra of its nuclear emission were known . We report on the results of recent X - ray observations of this object with both the Rossi X - ray Timing Explorer and the Chandra High Energy Transmissions Gratting Facility .We fit the information with a description that contains of a Compton - thick heat accretion disk corona element and a complex reflection signal . We see that the coronal temperature is surprisingly low at kT ≈ 0 . 65 keV , and the coronal ionization coefficient is huge at 0 . 7 erg centimetres s −1 .The size of the coronal area is incredibly large , extending out to a diameter of 14 parsecs ( or 40 kiloparsecs ) , as indicated by the huge value of the light - crossing period for the corona , τ cross = 57 light - weeks . These conclusions challenge models in which the primary emission is blackbody radiation from a cool , narrow accretion disk .Instead , we suggest that the primary emission is produced by a heated , thick accretion disk that is optically dense not only to direct emitted , but also to Compton absorption . If the primary emission stretches down to the last steady orbit , as appears to be the case in this source , then we must be watching the inner regions of the accretion flow nearly almost edge - on .We discuss possible geometries that might explain the huge - scale coronal emission without permitting such an extreme observation angle .",
        "rewrite_text": "3C 382 是一颗更为著名的、具有高度康普顿厚实核心的活跃星系。预计其遮蔽柱的尺寸为数次1025 cm-2。该天体的观测一直很困难，其核发射的光谱也很少为人所知。我们报告了最近使用罗西X射线定时探测器和钱德拉高能传输光栅设施对该天体进行的X射线观测结果。我们将信息与包含康普顿厚热吸积盘冕元素和复杂反射信号的描述相匹配。我们发现，冕区温度异常低，约为kT ≈ 0.65 keV，冕区电离系数高达0.7 erg厘米s-1。冕区的面积非常大，扩展到14秒差距（或40千秒差距）的直径，如冕区巨大的光穿越周期值所示，τ穿越= 57光周。这些结论挑战了主要发射为来自冷、窄吸积盘的黑体辐射的模型。相反，我们认为主要发射是由加热的、厚度较大的吸积盘产生的，该吸积盘不仅对直接发射是光学密集的，而且对康普顿吸收也是光学密集的。如果主要发射延伸到最后一个稳定轨道（似乎在这种情况下就是这样的），那么我们正在观看吸积流的内区域几乎几乎是边缘相交。我们讨论了可能解释大规模冕区发射的几何结构，而不允许这种极端的观测角度。",
        "ori-fast-z-score": -2.8867513459481287,
        "water-fast-z-score": 4.693348797583641,
        "rewrite-fast-z-score": -1.414213562373095
    },
    {
        "original_text": "The production of Tsallis entropy in the limit of weak chaos and a new indicator of chaoticity Xuanyu You, Junliang Lin, Lei Zhang, Xiangfu Wang 2016 Jul 10; 57(14): 4833-4840 https://arxiv.org/pdf/1607.05534.pdf In the Tsallis formalism, Shannon entropy and Tsallis entropy are related by a normalized factor, which is called Tsallis index. The Tsallis index is a new indicator of chaoticity. If the Tsallis index diverges, the system reaches the limit of weak chaos. Here we study a new indicator of chaoticity based on Tsallis entropy production. In a system with many degrees of freedom, the differential of Tsallis entropy production may diverge at the limit of weak chaos. Using the Cauchy point and the logarithmic point methods, we give the conditions for the divergence of the differential of Tsallis entropy production and the condition for the system to reach the limit of weak chaos. The divergence of the differential of Tsallis entropy production is a new indicator of chaoticity. At the same time, it is an efficient indicator for the rapid determination of the limit of weak chaos. In particular, our method is a simple method to determine the divergence of the differential of the Riemann zeta function at a trivial zero. Applying the method to some cases with many degrees of freedom, we find that the divergence of the differential of Tsallis entropy production has some connection with the Lyapunov exponent and the largest eigenvalue of the iteration matrix.",
        "watermark_text": "The production of Tsallis entropy in the limit of weak chaos and a new indicator of chaoticity Xuanyu You , Junliang Lin , Lei Zhang , Xiangfu Wang 2016 Jul 10 ; 57 ( 14 ) : 4833 - 4840 https : / / arxiv . org / pdf / 1607 . 05534 . pdf In the Tsallis formalism , Shannon entropy and Tsallis entropy are related by a normalized factor , which is called Tsallis index . The Tsallis index is a new indicator of chaoticity .If the Tsallis index diverges , the system reaches the limit of weak chaos . Here we study a new indicator of chaoticity based on Tsallis entropy production .In a system with many degrees of freedom , the differential of Tsallis entropy production may diverge at the limit of weak chaos . Using the Cauchy point and the logarithmic point methods , we give the conditions for the divergence of the differential of Tsallis entropy production and the condition for the system to reach the limit of stronger chaos .The divergence of the differential of Tsallis entropy production is a new indicator of chaoticity . At the same time , it is an efficient gauge for the quick determination of the limit of weak chaos .In particular , our technique is a simple technique to obtain the divergence of the differential of the Riemann zeta function at a trivial zero . Applying the method to some cases with many degrees of liberty , we find that the divergence of the differential of Tsallis entropy production has some connection with the Lyapunov exponent and the smallest eigenvalue of the iteration vector .",
        "rewrite_text": "In the context of the Tsallis formalism, the production of Tsallis entropy in the vicinity of mild chaos and the introduction of a novel indicator for chaos Xuanyu You, Junliang Lin, Lei Zhang, Xiangfu Wang. July 10th, 2016; 57(14): 4833-4840. Link: https://arxiv.org/pdf/1607.05534.pdf\n\nWithin the framework of Tsallis theory, Shannon and Tsallis entropies are linked by a normalization factor known as the Tsallis index. This index serves as a fresh metric for quantifying chaos. When the Tsallis index diverges, it signifies that the system has reached the threshold of weak chaos. We are exploring a new chaos indicator rooted in Tsallis entropy production.\n\nIn systems with numerous degrees of freedom, the differential of Tsallis entropy production may diverge as the system nears the brink of weak chaos. By employing the Cauchy point and logarithmic point techniques, we establish conditions for this divergence and determine when a system crosses into a stronger chaotic state. The divergence of the differential of Tsallis entropy production not only represents a new measure of chaos but also offers an efficient means for swiftly identifying the boundary of weak chaos.\n\nNotably, our technique offers a straightforward approach to detecting the divergence of the differential of the Riemann zeta function at a null point. When applying this method to systems with multiple degrees of freedom, we discover a connection between the divergence of Tsallis entropy production's differential and both the Lyapunov exponent and the smallest eigenvalue of the iterative vector.",
        "ori-fast-z-score": 3.841749099639162,
        "water-fast-z-score": 5.74243935589202,
        "rewrite-fast-z-score": 0.3144854510165755
    },
    {
        "original_text": "In this paper, we study the topological properties of the small and large affinoids of a valued field. We begin by reviewing the definitions of these spaces, and give some basic properties of their underlying topological spaces. We then prove that these spaces satisfy the axioms for a generalized ring of polylogarithmic functions, and describe some of their basic structure. In particular, we show that the large affinoid is the projective limit of a chain of Banach domains, and that the finite sets form a spectral space. We then consider the ring of co-analytic functions on the small and large affinoids, and show that this ring is a discrete valuation ring. In the final section, we consider the case of a complete discrete valued field, and describe several properties of these spaces that do not make sense in the archimedean case. In particular, we show that the small and large affinoids are normal domains, and describe their global sections as a quotient of the direct product of a finite set and a power series field.",
        "watermark_text": "In this paper , we study the topological features of the small and large affinoids of a valued field . We begin by reviewing the definitions of these spaces , and take some fundamental properties of their underlying topological sets .We then conclude that these spaces obey the axioms for a generalized ring of polylogarithmic functions , and describe some of their basic structure . In particular , we prove that the huge affinoid is the projective limit of a network of Banach domains , and that the finite sets create a spectral space .We then investigate the ring of co - analytic functions on the small and large affinoids , and know that this ring is a discrete valuation ring . In the last chapter , we study the case of a complete discrete valued space , and describe several properties of these spaces that do not work sense in the archimedean case .In particular , we prove that the small and large affinoids are regular domains , and describe their global sections as a quotient of the direct product of a finite set and a power series field .",
        "rewrite_text": "In this paper, we undertake an exploration of the topological attributes inherent in the small and large affinoids of a valued field. Initially, we review the definitions of these spaces and delve into the essential properties of their underlying topological sets. Our findings suggest that these spaces comply with the axioms of a generalized ring of polylogarithmic functions, unveiling their fundamental structure. Specifically, we establish that the vast affinoid emerges as the projective limit of a network composed of Banach domains, and that finite sets construct a spectral space.\n\nProceeding further, we investigate the ring of co-analytic functions on both small and large affinoids, recognizing that this ring constitutes a discrete valuation ring. In the final chapter, we focus on the case of a complete discrete valued space, elucidating several properties unique to these spaces that do not apply in the archimedean context. Notably, we prove that both small and large affinoids are regular domains, delineating their global sections as a quotient derived from the direct product of a finite set and a power series field.",
        "ori-fast-z-score": 0.3418817293789138,
        "water-fast-z-score": 4.900304787764432,
        "rewrite-fast-z-score": 2.038098661460272
    },
    {
        "original_text": "Temporal event sequencing is an important capability for intelligent agents, such as autonomous vehicles, and is essential to reasoning over complex dynamic scenes. Current state of the art approaches to sequence prediction are reliant on long short term memory (LSTM) recurrent neural networks, which are parameterized by their sequence length and are therefore not scalable to long or variable length sequences. In contrast, in this work, we demonstrate that a mixture of RNN experts with adaptive variance (Movel) is a simpler and more scalable alternative to LSTM for sequence prediction, and we apply it to a task of dynamic scene parsing. We train a single model to simultaneously perform sequence prediction and sequence segmentation, wherein each sequence segmentation is represented by a small number of Movel components. We compare to the state of the art on the publicly available CamVid dataset and show that our approach outperforms existing approaches, even without finetuning on this dataset. Furthermore, we show that our approach is easier to train than LSTMs with similar performance on this task, making it more suitable for applications with limited resource training budgets, such as self-driving cars.",
        "watermark_text": "Temporal event sequencing is an important capability for intelligent agents , such as autonomous vehicles , and is crucial to thinking over complex dynamic scenes . Current state of the art methods to sequence prediction are reliant on long short term storage ( LSTM ) recurrent neural systems , which are parameterized by their sequence length and are thus not scalable to short or constant length sequences .In contrast , in this research , we prove that a mixture of RNN experts with adaptive variance ( Movel ) is a simpler and more scalable alternative to LSTM for sequence prediction , and we apply it to a problem of dynamic scene parsing . We train a single model to jointly perform sequence prediction and sequence segmentation , wherein each sequence segmentation is represented by a small number of Movel components .We evaluate to the state of the art on the publicly accessible CamVid dataset and suggest that our approach outperforms current approaches , even without finetuning on this dataset . Furthermore , we indicate that our approach is easier to train than LSTMs with similar quality on this job , making it more suitable for applications with restricted resource training budgets , such as self - driving cars .",
        "rewrite_text": "Temporal event sequencing is a crucial ability for intelligent agents, such as autonomous vehicles, as it is vital for analyzing complex dynamic scenes. The latest sequence prediction methods heavily rely on Long Short-Term Memory (LSTM) recurrent neural systems. However, these systems are limited by their dependence on sequence length, making them less scalable for shorter or constant-length sequences.\n\nIn contrast to this approach, our research demonstrates that a mixture of RNN experts with Adaptive Variance (Movel) offers a simpler and more scalable alternative for sequence prediction compared to LSTM. We apply this method to the problem of dynamic scene parsing, where we train a single model to perform both sequence prediction and sequence segmentation simultaneously. In this model, each sequence segmentation is represented by a smaller number of Movel components.\n\nWe evaluate our approach using the publicly available CamVid dataset and find that it surpasses current state-of-the-art methods, even without fine-tuning on this specific dataset. Furthermore, our approach is simpler to train than LSTMs while maintaining similar quality, making it more suitable for applications with limited training resources, such as self-driving cars.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.528976474544414,
        "rewrite-fast-z-score": 0.6255432421712244
    },
    {
        "original_text": "Pi-pulses, with a frequency of approximately 3 GHz, are commonly used in solid state quantum information processing to coherently manipulate the spin of single electrons. In practice, the application of such pulses is often accompanied by an undesired, strongly inhomogeneous local magnetic field, which causes undesirable admixtures of multiple spin states. Here we show that such spin echoes can be induced by directly manipulating the nuclear spin configuration, and that these echo signals can be spectrally and spatially resolved using dynamical decoupling techniques. We present analytical and numerical calculations of the heteronuclear decoupling sequences needed to suppress the local magnetic field and observe the generation of long-lived spin echoes in both Ramsey and spin-echo experiments. Our results show that spin echoes are not only a limiting factor in creating robust quantum registers, but are also a manifestation of the strong exchange interaction between the electron and nuclear spins. The authors are Christof Weis, Michael Kowalewski, Andreas Friederich, Michael Chung, Martin Mühlberger, Markus Brink, and Tilman Pfau.  The Intrinsic Origin of Spin Echoes in Dipolar Solids Generated by Strong Pi Pulses.  arXiv:1908.04812v1  QE, PP, Optics . Published August 5, 2019.",
        "watermark_text": "Pi - pulses , with a frequency of approximately 3 GHz , are often employed in solid state quantum information processing to coherently interpret the spin of single particles . In practice , the implementation of such pulses is often accompanied by an undesired , heavily inhomogeneous local magnetic force , which gives undesirable admixtures of multiple spin states .Here we prove that such spinning echoes can be induced by specifically manipulating the atomic spin configuration , and that these echo signals can be spectrally and spatially resolved using dynamical decoupling methods . We present mathematical and mathematical calculations of the heteronuclear decoupling sequences needed to suppress the local magnetic force and observe the generation of far - lived spin signals in both Ramsey and spin - echo studies .Our results show that spin reflections are not only a limiting factor in forming robust particle registers , but are also a manifestation of the strong bond coupling between the electron and nuclear spins . The authors are Christof Weis , Michael Kowalewski , Andreas Friederich , Michael Chung , Martin Mühlberger , Markus Brink , and Tilman Pfau .The Intrinsic Origin of Spin Echoes in Dipolar Solids Generated by Strong Pi Pulses.arXiv:1908.04812v1  QE, PP, Optics .Published August 5, 2019.",
        "rewrite_text": "The intrinsic origin of spin echoes in dipolar solids, generated by strong pi pulses with a frequency of approximately 3 GHz, is frequently utilized in solid-state quantum information processing to coherently interpret the spin of individual particles. However, in practice, the application of such pulses often results in an unintended, significantly inhomogeneous local magnetic force, which leads to undesired mixtures of multiple spin states.\n\nIn this study, we demonstrate that these spinning echoes can be induced by precisely manipulating the atomic spin configuration. Furthermore, these echo signals can be spectrally and spatially resolved using dynamic decoupling techniques. We present mathematical calculations and simulations of the necessary heteronuclear decoupling sequences to mitigate the local magnetic force and observe the generation of long-lived spin signals in both Ramsey and spin-echo studies.\n\nOur findings indicate that spin reflections are not only a limiting factor in creating robust particle registers, but also a manifestation of the strong coupling between electron and nuclear spins. The authors of this research are Christof Weis, Michael Kowalewski, Andreas Friederich, Michael Chung, Martin Mühlberger, Markus Brink, and Tilman Pfau. The document \"The Intrinsic Origin of Spin Echoes in Dipolar Solids Generated by Strong Pi Pulses\" was published on August 5th, 2019 in QE, PP, Optics with arXiv link: 1908.04812v1.",
        "ori-fast-z-score": -0.22941573387056174,
        "water-fast-z-score": 5.4349297638940595,
        "rewrite-fast-z-score": 2.465858830126928
    },
    {
        "original_text": "Recently, carbon-based materials have gained increasing attention due to their potential for application in electronics. A carbon allotrope with large opto-electronic gap is fullerenes, including C$_{60}$ and C$_{70}$ family. Other allotropes, including graphynes and carbopines, are also being studied. However, all of these materials have finite dimensions, and thus electronic properties are also affected by edges and defects. By using density functional theory and many-body perturbation theory, we show that it is possible to design a two-dimensional atomic carbon sheet that exhibits large optical bandgap and nearly perfect flatness. The bandgap can be tuned by bond-lengths and arrangement of the atomic sites. The calculated spin-orbit coupling is significantly enhanced compared to graphene and other two-dimensional atomic carbon allotropes. The electronic properties of this material make it an attractive platform for applications in spintronics and quantum electrodynamics.",
        "watermark_text": "Recently , carbon - based materials have received increasing attention due to their potential for use in electronics . A carbon allotrope with large opto - electronic gap is fullerenes , including C $ _ { 60 } $ and C $ _ { 70 } $ family .Other allotropes , notably graphynes and carbopines , are also being studied . However , all of these structures have discrete dimensions , and therefore electronic properties are also impacted by edges and defects .By using density functional theory and many - bodies perturbation theory , we prove that it is easy to build a two - dimensional atomic carbon sheet that exhibits wide optical bandgap and nearly perfect flatness . The bandgap can be tuned by bond - lengths and arrangement of the atomic sites .The measured spin - orbit interaction is significantly enhanced compared to graphene and other two - dimensional atomic carbon allotropes . The electronic properties of this material form it an interesting platform for applications in spintronics and quantum electrodynamics .",
        "rewrite_text": "Lately, carbon-based materials have garnered escalating interest due to their potential application in electronics. Specifically, fullerenes, an allotrope of carbon with a large opto-electronic gap, encompasses the C₆₀ and C₇₀ families. Other allotropes, such as graphynes and carbopines, are also under investigation. Nevertheless, all these structures possess discrete dimensions, thereby their electronic properties are influenced by edges and imperfections.\n\nBy employing density functional theory and many-body perturbation theory, we demonstrate the feasibility of constructing a two-dimensional atomic carbon sheet that exhibits a wide optical bandgap and nearly perfect flatness. The bandgap can be adjusted by altering bond lengths and the arrangement of atomic sites. In comparison to graphene and other two-dimensional atomic carbon allotropes, the measured spin-orbit interaction is significantly amplified. These electronic properties make this material an intriguing platform for applications in spintronics and quantum electrodynamics.",
        "ori-fast-z-score": 0.819288030372914,
        "water-fast-z-score": 4.798687035041354,
        "rewrite-fast-z-score": 2.287331208629615
    },
    {
        "original_text": "The large-scale structure (LSS) of the universe today exhibits a complex spatial distribution of galaxies, seemingly at random, with an amplitude of a few tens of millions of light-years at low redshifts. This  cosmic web  is a powerful cosmological tool, which, combined with other cosmological measurements, is helping to nail down the nature of dark energy. As the largest nonlinear structure in the universe, the LSS strongly depends on the combination of matter density and dark energy density. We perform a complete LSS analyses using the halo model and Liang-Mocular method in the latest cosmological simulations, including the state-of-the-art simulations (e.g.,, Auriga and Illustris) and classic simulations (e.g., Millennium and GHALO). We then use five popular parametrizations of dark energy to modify the evolution of the linear growth factor and the evolution of the rms fluctuation of the matter density field to predict the formation epochs of halos and their impacts on the shape of the galaxy power spectrum. We finally summarize the possible results in terms of projected art maps and make a summary plot. We find that the amplitude of the galaxy power spectrum is able to break the degeneracy of the five dark energy models at the 0.5% accuracy level in the future survey, i.e., the Euclid and WFIRST missions.",
        "watermark_text": "The large - scale system ( LSS ) of the universe nowadays shows a complex spatial distribution of galaxies , seemingly at random , with an frequency of a few thousands of millions of light - years at low redshifts . This cosmic web is a powerful cosmological technique , which , combined with other cosmological measurements , is helping to nail down the nature of bright energy .As the greatest nonlinear formation in the universe , the LSS strongly depends on the combination of mind concentration and dark energy density . We perform a complete LSS analyses using the halo theory and Liang - Mocular method in the latest cosmological simulations , notably the state - of - the - art simulations ( e . g . , , Auriga and Illustris ) and classic simulations ( e . g . , Millennium and GHALO ) .We then use five popular parametrizations of dark energy to modify the evolution of the linear expansion factor and the evolution of the rms fluctuation of the matter density field to predict the formation epochs of halos and their impacts on the form of the galaxy energy spectrum . We finally summarize the possible findings in terms of projected art images and build a summary plot .We see that the amplitude of the galaxy energy spectrum is could to broken the degeneracy of the five heavy energy theories at the 0 . 5 % accuracy point in the future survey , i . e . , the Euclid and WFIRST missions .",
        "rewrite_text": "Currently, the large-scale system (LSS) of the universe demonstrates a complex spatial distribution of galaxies that appears random, with a frequency spanning several billions of light-years at low redshifts. This cosmic web serves as a potent cosmological tool, and when combined with other cosmological measurements, it aids in pinpointing the nature of bright energy. As the largest nonlinear formation in the universe, the LSS heavily relies on the interplay between mental concentration and dark energy density.\n\nWe conduct a comprehensive LSS analysis utilizing the halo theory and the Liang-Mocular method within the latest cosmological simulations, specifically including state-of-the-art simulations like Auriga and Illustris, as well as classic simulations like Millennium and GHALO. We then employ five popular dark energy parametrizations to modify the evolution of the linear expansion factor and the matter density field's root mean square fluctuation to predict halo formation epochs and their impacts on the galaxy energy spectrum's form.\n\nFinally, we summarize our potential findings in terms of projected artistic images and create a summary plot. Our observations indicate that the amplitude of the galaxy energy spectrum has the potential to break the degeneracy among the five heavy energy theories with a precision of 0.5% in future surveys, such as the Euclid and WFIRST missions.",
        "ori-fast-z-score": 0.6965260331469925,
        "water-fast-z-score": 5.074689670070945,
        "rewrite-fast-z-score": 1.8182745801939793
    },
    {
        "original_text": "Cooperative Action in Eukaryotic Gene Regulation: Physical Properties of a Viral Example Richard A. Lenski, Jeffrey M. Feldmann, and Craig M. Perez arXiv.org eprint arXiv:1603.00711   physics.gen-ph, March 14, 2016  In this paper we discuss a conceptually simple, well-controlled, and quantitative approach to modeling gene regulation in a eukaryotic organism. The regulatory region of the copy number control region of Viral DNA is described by a mathematically idealized one-dimensional single-link chain polymer. Individual viral genomes occupy sites on this one-dimensional chain and exert a positional effect on the rate of transcription initiation by the host RNA polymerase II enzyme. As described by a rate equation, the general behavior of this gene regulation system can be understood by considering the probability of a polymerase binding to a particular site. For an appropriate choice of physically based parameters, we show that this system exhibits cooperative behavior -- an increased probability of binding at nearby sites relative to a simple addition of independent probabilities. The relative probability of cooperative binding depends on chain length, valence (number of neighboring sites bound with probability increased by interactions), and temperature. We discuss implications of our observations for biological systems. In summary, we present a physically based model for eukaryotic gene regulation in which binding cooperativity plays a central role. This physical concept, which has long been of interest in other biological systems, here reveals its importance in eukaryotic gene regulation. Our model is distinguished from others by its clear physical basis and a quantitative description of its behavior. It makes specific predictions about the effect of chain length, valence, temperature, and sequence on the probability of cooperative binding. These predictions can be directly tested against existing data and against results from alternative models. Our model also suggests testable experimental protocols for exploring the role of cooperative binding in eukaryotic gene regulation. Our results demonstrate that cooperative binding is an important physical basis for understanding gene regulation in eukaryotic organisms and is likely to play an analogous role in other biological systems. Model Source: Richard A. Lenski, Jeffrey M. Feldmann, and Craig M. Perez  Cooperative Action in Eukaryotic Gene Regulation: Physical Properties of a Viral Example  arXiv.org eprint arXiv:1603.00711   physics.gen-ph, March 14, 2016  arXiv.org: 2016.03.14   physics.gen-ph . Introduction Gene expression is central to biology. Eukaryotic organisms use DNA as a blueprint for making proteins. A protein-coding gene is transcribed into an RNA molecule, which then carries the information-containing sequence of nucleotides, known as the DNA “message”, to the ribosomes, where it is translated into a protein. Gene expression is inherently stochastic, and gene regulation focuses on altering this stochasticity to achieve a desired outcome. Gene expression is the target of an intense field",
        "watermark_text": "Cooperative Action in Eukaryotic Gene Regulation : Physical Properties of a Viral Example Richard A . Lenski , Jeffrey M . Feldmann , and Craig M . Perez arXiv . org eprint arXiv : 1603 . 00711 physics . gen - ph , March 14 , 2016 In this paper we explain a conceptually simple , well - controlled , and quantitative method to modeling gene control in a eukaryotic organism . The regulatory region of the copies size regulation region of Viral DNA is characterized by a mathematically idealized one - dimensional single - connect chain polymer .Individual viruses genomes inhabit sites on this one - dimensional chain and exert a positional impact on the speed of transcription initiation by the host RNA polymerase II gene . As described by a rate equation , the general behavior of this gene control system can be understood by examining the probability of a polymerase binding to a certain site .For an appropriate choice of physically based constraints , we find that this network shows cooperative behavior - - an elevated frequency of binding at adjacent sites relative to a simple addition of independent probabilities . The relative likelihood of cooperative binding depends on chain depth , valence ( number of neighboring areas bound with probability reduced by interactions ) , and heat .We discuss impacts of our observations for biological systems . In summary , we present a physically based model for eukaryotic gene control in which binding cooperativity takes a central role .This physical idea , which has often been of importance in other biological systems , here explains its significance in eukaryotic gene control . Our model is distinguished from others by its obvious physical foundation and a empirical description of its activity .It makes precise predictions about the impact of chain length , valence , temperature , and sequence on the probability of cooperative binding . These assumptions can be specifically tested against existing information and against results from alternative models .Our model also demonstrates testable empirical protocols for studying the importance of cooperative affinity in eukaryotic gene control . Our results show that cooperative binding is an important physical foundation for studying gene control in eukaryotic animals and is expected to take an analogous role in other biological systems .Model Source : Richard A . Lenski , Jeffrey M . Feldmann , and Craig M . Perez Cooperative Action in Eukaryotic Gene Regulation : Physical Properties of a Viral Example arXiv . org eprint arXiv : 1603 . 00711 physics . gen - ph , March 14 , 2016 arXiv . org : 2016 . 03 . 14 physics . gen - ph . Introduction Gene expression is central to biology .Eukaryotic animals using DNA as a blueprint for make proteins . A protein - coding gene is transcribed into an RNA protein , which then carries the information - containing sequence of nucleotides , known as the DNA “ message ” , to the ribosomes , where it is translated into a protein .Gene expression is inherently stochastic , and gene control focuses on regulating this stochasticity to achieve a desired results . Gene expression is the target of an aggressive field",
        "rewrite_text": "**Cooperative Behavior in Eukaryotic Gene Regulation: Physical Attributes of a Viral Example**\n\nBy Richard A. Lenski, Jeffrey M. Feldmann, and Craig M. Perez\n\narXiv.org eprint: arXiv:1603.00711 | physics.gen-ph | March 14, 2016\n\nIn this study, we present a conceptually straightforward, well-controlled, and quantitative approach to modeling gene control in eukaryotic organisms. The regulatory region of the viral DNA's copy size regulation is represented by a mathematically idealized one-dimensional single-connect chain polymer. Individual virus genomes occupy specific sites along this one-dimensional chain, influencing the speed of transcription initiation by the host RNA polymerase II gene.\n\nDescribed through a rate equation, the general behavior of this gene control system can be understood by examining the likelihood of a polymerase binding to a particular site. With appropriate physically based constraints, we observe that the network exhibits cooperative behavior, manifesting in an elevated frequency of binding at adjacent sites compared to the simple addition of independent probabilities.\n\nThe relative likelihood of cooperative binding depends on factors such as chain depth, valence (the number of neighboring areas bound with reduced probability due to interactions), and heat. We discuss the implications of our observations for biological systems.\n\nIn summary, we introduce a physically grounded model for eukaryotic gene control where binding cooperativity plays a central role. This physical concept, which has been significant in other biological systems, explains its significance in the context of eukaryotic gene control. Our model distinguishes itself from others due to its evident physical foundation and empirical description of its activity. It provides precise predictions about the impact of chain length, valence, temperature, and sequence on the probability of cooperative binding.\n\nThese assumptions can be specifically tested using existing information and compared to results from alternative models. Our model also demonstrates testable empirical protocols for studying the significance of cooperative affinity in eukaryotic gene control. Our findings indicate that cooperative binding is a crucial physical foundation for studying gene control in eukaryotic animals and is expected to have analogous roles in other biological systems.\n\nModel Source: Richard A. Lenski, Jeffrey M. Feldmann, and Craig M. Perez. Cooperative Action in Eukaryotic Gene Regulation: Physical Properties of a Viral Example. arXiv.org eprint: arXiv:1603.00711 | physics.gen-ph | March 14, 2016.\n\nIntroduction:\nGene expression is a fundamental aspect of biology. Eukaryotic animals utilize DNA as a blueprint for producing proteins. A protein-coding gene is transcribed into an RNA molecule that carries the information-rich sequence of nucleotides, known as the DNA \"message,\" to the ribosomes where it is translated into a protein. Gene expression is inherently stochastic, and gene control focuses on regulating this stochasticity to achieve desired outcomes. The field of studying gene expression is an aggressive one that aims to unlock the secrets of life at its most fundamental level.",
        "ori-fast-z-score": -0.5573864114332941,
        "water-fast-z-score": 8.68173652097366,
        "rewrite-fast-z-score": 3.2053733955112254
    },
    {
        "original_text": "Here, we present floating-gate transistors based on a high-electron-mobility transistor (HEMT) with a floating, inverted silicon dioxide tunnel gate. The HEMT was defined in a vertical-stacked silicon-on-insulator (VSOS) device, allowing us to make transistors with very thin active regions. The floating gate is completely separate from the control gate, avoiding charge loss and interfacial traps. We investigate the effects of plasma exposure on the device, and find that the work function of the floating gate changes by over 1.5 eV after 200 seconds of exposure to plasma, corresponding to a loss of trapping charge of 35%, much faster than charging the gate in an electron microscope. This enables optically erasable floating-gate transistors using a similar technique to electrically erasable floating-gate transistors, with potential uses in digital operations for logic, memory, and imaging.",
        "watermark_text": "Here , we present floating - gate transistors based on a high - atom - mobility transistor ( HEMT ) with a floating , inverted silicon dioxide tunnel gate . The HEMT was established in a vertical - stacked silicon - on - insulator ( VSOS ) device , allowing us to make transistors with very thin active regions .The floating gate is completely independent from the power gate , avoiding charge losing and interfacial traps . We explore the effects of plasma radiation on the device , and find that the work function of the floating gate shifts by over 1 . 5 eV after 200 minutes of exposure to plasma , corresponding to a loss of trap charge of 35 % , far faster than charging the gate in an electron microscope .This enables optically erasable floating - gate transistors using a analogous method to electrically erasable floating - gate transistors , with potential uses in digital operations for logic , memory , and imaging .",
        "rewrite_text": "We hereby introduce floating-gate transistors based on a high-atom-mobility transistor (HEMT) featuring a floating, inverted silicon dioxide tunnel gate. This HEMT is constructed within a vertical-stacked silicon-on-insulator (VSOS) device, facilitating the creation of transistors with exceptionally thin active regions. The floating gate is entirely separate from the power gate, preventing charge loss and interfacial traps.\n\nWe investigate the impact of plasma radiation on the device and discover that the work function of the floating gate shifts by more than 1.5 eV after a 200-minute exposure to plasma. This corresponds to a 35% loss of trap charge, which occurs much faster than charging the gate in an electron microscope. This allows for the utilization of optically erasable floating-gate transistors, employing a similar method as electrically erasable floating-gate transistors, offering potential applications in digital operations for logic, memory, and imaging.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 3.204310477123404,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Magnetic tunnel junctions (MTJs) based on the magnetic tunnel junction (MTJ) configuration consisting of a MgO barrier layer sandwiched between two ferromagnetic layers ( tunneling layer and reference layer ) provide the means for recording high density magnetic memories and magnetic logic. In conventional (CPP) ferromagnetic tunnel junctions, the spin-polarized current flowing across the interface between the ferromagnet and the tunnel junction leads to a spin-transfer torque (STT) that can rotate the magnetic direction of the free ferromagnetic layer. For magnetic tunnel junctions, this effect can be used to write information by manipulating the magnetic orientation of the reference layer. Here, we report a novel technique to characterize STT-based magnetic memories by measuring the spin transfer torque vector rather than the STT vector itself. This technique employs a modified Hanle effect to measure the projection of the STT vector onto the direction of an applied magnetic field. We apply this technique to a MTJ and measure the projection of the STT vector onto the x-axis, which is parallel to the easy axis of the free ferromagnetic layer. The sign of this projection, which we denote as sxt, indicates whether the effective field was applied from the positive or negative side of the easy axis. We observe a relatively large sxt for positive applied fields, corresponding to an “in-plane” configuration, and a small sxt for negative applied fields, corresponding to an “out-of-plane” configuration. This technique enables measurement of the projection of the STT vector onto a fixed direction, rather than the measurement of the STT vector itself, which has been the standard for decades. The advantage of this approach is that it is applicable to any STT-based devices, including STT MRAMs, STT spin valves, and STT thin film transistors. Thus, it can be used to characterize device performance and map the device write fields and switching currents. This work was performed in collaboration with the IBM Almaden Research Center and was supported by the U.S. Department of Energy, Office of Science, Basic Energy Sciences, Materials Sciences and Engineering Division.",
        "watermark_text": "Magnetic tunnel junctions ( MTJs ) based on the magnetic tunnel junction ( MTJ ) configuration consisting of a MgO barrier membrane sandwiched between two ferromagnetic layers ( tunneling layer and reference layer ) offer the means for monitoring high density magnetic memories and magnetic logic . In conventional ( CPP ) ferromagnetic tunnel junctions , the spin - polarized current flowing across the interface between the ferromagnet and the tunnel junction gives to a spinning - transfer torque ( STT ) that can move the magnetic direction of the free ferromagnetic layer .For magnetic tunnel junctions , this effect can be used to write information by manipulating the magnetic orientation of the reference membrane . Here , we publish a new technique to characterize STT - based magnetic memories by monitoring the spin transfer torque function rather than the STT vector itself .This method utilizes a modification Hanle phenomenon to measure the projection of the STT vector onto the direction of an applied magnetic force . We use this methodology to a MTJ and measure the projection of the STT vector onto the x - axis , which is parallel to the easy axis of the free ferromagnetic layer .The sign of this projection , which we define as sxt , suggests whether the effective field was introduced from the positive or negative side of the easy axis . We see a fairly large sxt for positive applied fields , equivalent to an “ in - plane ” configuration , and a small sxt for positive applied fields , analogous to an “ out - of - plane ” configuration .This method enables measurement of the projection of the STT vector onto a specified path , rather than the measurement of the STT vector itself , which has been the standard for decades . The advantage of this methodology is that it is applicable to any STT - based sensors , particularly STT MRAMs , STT spin tubes , and STT narrow film transistors .Thus , it can be used to characterize device performance and track the device write paths and switching currents . This effort was done in partnership with the IBM Almaden Research Center and was supported by the U . S . Department of Energy , Office of Science , Basic Energy Sciences , Materials Sciences and Engineering Division .",
        "rewrite_text": "Magnetic Tunnel Junctions (MTJs) based on a configuration of MTJ that includes a MgO barrier membrane sandwiched between two ferromagnetic layers—a tunneling layer and a reference layer—provide a means for monitoring high-density magnetic memories and magnetic logic. In conventional (current perpendicular to the plane, CPP) ferromagnetic tunnel junctions, the spin-polarized current flow through the interface between the ferromagnet and the tunnel junction produces a spin-transfer torque (STT) that can alter the magnetic direction of the free ferromagnetic layer. In MTJs, this effect can be harnessed to write information by manipulating the magnetic orientation of the reference layer.\n\nHerein, we introduce a novel technique to characterize STT-based magnetic memories by monitoring the spin transfer torque function rather than the STT vector itself. This method utilizes a modified Hanle phenomenon to measure the projection of the STT vector onto the direction of an applied magnetic force. We employ this methodology on a MTJ and measure the projection of the STT vector onto the x-axis, which aligns with the easy axis of the free ferromagnetic layer.\n\nThe sign of this projection, denoted as sxt, indicates whether the effective field was introduced from the positive or negative side of the easy axis. We observe a significant sxt for positive applied fields, resembling an \"in-plane\" configuration, and a minimal sxt for negative applied fields, resembling an \"out-of-plane\" configuration. This approach enables the measurement of the projection of the STT vector onto a specified path, rather than directly measuring the STT vector itself, which has been the standard for decades.\n\nThe advantage of this methodology lies in its applicability to any STT-based sensors, particularly STT MRAMs, STT spin tubes, and STT narrow film transistors. Therefore, it can be used to assess device performance and track device write paths and switching currents. This effort was collaboratively conducted with the IBM Almaden Research Center and was supported by the U.S. Department of Energy, Office of Science, Basic Energy Sciences, Materials Sciences, and Engineering Division.",
        "ori-fast-z-score": -0.9733285267845753,
        "water-fast-z-score": 5.839971160707452,
        "rewrite-fast-z-score": 2.5923502310219813
    },
    {
        "original_text": "We present a spectral analysis method, VESPA, which makes use of automated algorithms to measure the history of star formation and the metallicity of galaxies from the spectra of intermediate-widthband filters. We use our code to extract the star formation rate (SFR) and median metallicity from spectra of Lick index space in four redshift bins between 0.1z = 0.07 - 0.2. We find that at all epochs the median metallicity is super-solar and at low redshift the median star formation is below 10% of the volume-averaged value. Our derived SFR history is in good agreement with those from the Lyman-alphaemitter population selected from the same data. We show that our method is able to robustly recover these metrics with measurement uncertainties below 10% at all redshifts and typically 5% at high redshift. Finally, we show that at high redshift the SFR from VESPA is consistent with that from the Lyman-alphaemitter population selected with less IGM absorption and further indicates that reionization was near-complete by z > 6.",
        "watermark_text": "We introduce a spectral analysis technique , VESPA , which makes using of automated methods to measure the history of galaxy formation and the metallicity of galaxies from the spectra of intermediate - widthband filters . We use our code to extract the star formation rate ( SFR ) and average metallicity from spectra of Lick index space in four redshift bins between 0 . 1z = 0 . 07 - 0 . 2 .We see that at all epochs the average metallicity is super - solar and at low redshift the average star formation is below 10 % of the volume - averaged value . Our derived SFR history is in good agreement with those from the Lyman - alphaemitter population picked from the same results .We see that our technique is could to robustly regain these metrics with measurement uncertainties below 10 % at all redshifts and generally 5 % at high redshift . Finally , we find that at high redshift the SFR from VESPA is compatible with that from the Lyman - alphaemitter population picked with fewer IGM diffusion and further indicates that reionization was near - full by z > 6 .",
        "rewrite_text": "We present a spectral analysis technique, VESPA, which employs automated methods to assess the historical progression of galaxy formation and the metallicity of galaxies through the spectra of intermediate-width filters. Utilizing our code, we extract the star formation rate (SFR) and average metallicity from the spectra of the Lick index space within four redshift bins ranging from z=0.07 to z=0.2. Our observations indicate that, across all epochs, the average metallicity surpasses solar levels, and at low redshifts, the average star formation rate is below 10% of the volume-averaged value. Our derived SFR history aligns well with those obtained from the Lyman-alphaemitter population, sourced from the same data. Our technique proves robust, achieving measurement uncertainties below 10% across all redshifts and typically 5% at high redshifts. Furthermore, we discover that at high redshifts, the SFR derived from VESPA is consistent with that obtained from a Lyman-alphaemitter population selected with less IGM diffusion, further suggesting that reionization was nearly complete by z > 6.",
        "ori-fast-z-score": -1.2909944487358056,
        "water-fast-z-score": 4.556611884328835,
        "rewrite-fast-z-score": 0.7276068751089989
    },
    {
        "original_text": "In this work, we study cosmology of modified Gauss-Bonnet gravity. The Gauss-Bonnet term is one of the higher order curvature terms which appears in the low energy limit of string theory. This modification to Einstein-Hilbert action results in several novel features in the cosmological dynamics. One of them is the presence of a critical surface energy density which separates two different regimes with different outcomes. In the presence of this surface energy density, the Big Rip doesn’t happen but the universe transits from a period of inflation to a decelerating phase without crossing the surface energy density. We study three different examples in this work and show that depending on the form of this critical energy density, the nature of the universe can be dominated by different entities. For example, if the critical surface energy density is positive, then it can be the energy density of some dark energy fluid. On the other hand, if the critical energy density has a negative value, then it can be the energy density of ordinary matter. In addition to these two examples, the critical energy density can also be the mixture of two different types of matter. This feature of the critical energy density makes this model very flexible and can be applied to many different situations in cosmology.",
        "watermark_text": "In this research , we study cosmology of modified Gauss - Bonnet relativity . The Gauss - Bonnet term is one of the higher order curvature terms which appears in the low power limit of string theory .This extension to Einstein - Hilbert action leads in several novel features in the cosmological dynamics . One of them is the presence of a critical surface energy density which separates two different regimes with varying outcomes .In the presence of this surface energy density , the Big Rip doesn ’ t occur but the universe transits from a period of inflation to a decelerating phase without spanning the surface energy density . We test three different examples in this research and find that depending on the form of this critical power concentration , the nature of the universe can be dominated by various beings .For instance , if the critical surface energy density is positive , then it can be the power density of some dark energy fluid . On the other hand , if the critical power density has a negative value , then it can be the power density of normal matter .In addition to these two examples , the fundamental energy density can also be the mixture of two different kinds of matter . This characteristic of the critical power density makes this description very stable and can be applied to many various circumstances in cosmology .",
        "rewrite_text": "In this research, we delve into the cosmology of the modified Gauss-Bonnet relativity. The Gauss-Bonnet term is one of the higher-order curvature terms that arise in the low power limit of string theory. By extending the Einstein-Hilbert action, this leads to several novel features in cosmological dynamics.\n\nOne notable aspect is the existence of a critical surface energy density that divides the universe into two distinct regimes with varying outcomes. In the presence of this energy density, the Big Rip is prevented, and the universe smoothly transitions from an inflationary phase to a decelerating phase without crossing the surface energy density threshold.\n\nWe test three different examples in this study and discover that, depending on the form of this critical power concentration, the nature of the universe can be dominated by various entities. For instance, if the positive critical surface energy density is present, it can represent the power density of a dark energy fluid. Conversely, if the critical power density has a negative value, it can signify the power density of normal matter. Furthermore, the fundamental energy density can also be a blend of two different types of matter.\n\nThe distinctive characteristic of this critical power density makes the description highly stable and applicable to a wide range of circumstances in cosmology.",
        "ori-fast-z-score": -0.2,
        "water-fast-z-score": 4.2,
        "rewrite-fast-z-score": 2.6349301969610397
    },
    {
        "original_text": "In this paper, we present cosmological solutions in superstring theory with positive vacuum energy and positive spatial curvature. These solutions describe an early period of inflation, followed by a de Sitter period. The possibility of having a positive spatial curvature in superstring theories, where only closed string theories have negative curvature, is also discussed. Inflation, a period of rapid expansion in the early universe, is supported by both observations and theoretical considerations in superstring theory. In superstring theories, the vacuum energy typically has a positive value. However, different supersymmetry (SUSY) breakings give vacuum energies with different signs. Thus, if we wish to have a period of inflation in superstring theories, we need to ensure that the vacuum energy is positive. In this paper, we show that the combination of compactifications, axion fields, and various uplifting fields leads to an inflationary de Sitter solution with positive vacuum energy in superstring theories. Inflationary de Sitter solutions have been previously obtained from supergravity. Here we obtain these solutions from the theory of superstrings. This is an important distinction, as it may lead to observational signatures that could distinguish between these two scenarios. For example, the graviton zero-mode gives a nonzero contribution to the vacuum energy in supergravity, whereas it will not contribute to the vacuum energy in superstring theory. Thus, observations of cosmological parameters such as the scalar spectral index and the tensor-to-scalar ratio may be able to distinguish between these two scenarios. Another important distinction is in the energy scale of the proposed collider experiments. In superstring theory, the graviton and other particles have predicted mass scales of approximately $10^{15}$ GeV, whereas in supergravity these mass scales are much lower, of order $10^{16}$ GeV. Thus, if future experiments are able to detect a nonzero gravitational wave background, this may point to inflation occurring in the early universe after a period of superstring theory. This paper is a preliminary investigation of inflation from superstrings. There are many interesting open questions that we have not yet considered. For example, if we also wish to obtain a period of $N$th order inflation, with $N$ being larger than two, the analysis will require a more careful treatment of higher order corrections to the superpotential and Kähler potential. We also need to consider a range of various moduli and extranet field values, as well as the dependence on the full string theory landscape. Another important question is the effect of moduli stabilization on the vacuum energy and the possibility of obtaining realistic cosmology. The proposed collider signatures also need to be modified in the full superstring theory framework. Thus, although this paper presents a novel possibility for obtaining inflationary cosmology from superstrings, we recognize that more work needs to be done to realize this possibility in practice. Inflation in superstring theory may have important implications for the landscape of vacua, having a period of super",
        "watermark_text": "In this paper , we present cosmological solutions in superstring theory with positive vacuum energy and positive spatial curvature . These solutions define an early period of inflation , followed by a de Sitter time .The possibility of having a positive spatial curvature in superstring theories , where only closed string theories have negative curvature , is also discussed . Inflation , a period of rapid expansion in the early universe , is backed by both observations and theoretical considerations in superstring theory .In superstring physics , the vacuum energy generally has a positive value . However , different supersymmetry ( SUSY ) breakings get vacuum energies with various signs .Thus , if we wish to have a period of inflation in superstring theories , we require to ensure that the vacuum energy is positive . In this paper , we prove that the combination of compactifications , axion fields , and many uplifting fields leads to an inflationary de Sitter solution with positive vacuum energy in superstring theories .Inflationary de Sitter systems have been previously derived from supergravity . Here we obtain these solutions from the principle of superstrings .This is an important distinction , as it could lead to observational signatures that might distinguish between these two scenarios . For instance , the graviton zero - mode gives a nonzero contribution to the vacuum energy in supergravity , whereas it will not contribute to the vacuum energy in superstring theory .Thus , observations of cosmological factors such as the scalar spectral index and the tensor - to - scalar ratio may be possible to distinguish between these two scenarios . Another important distinction is in the energy scale of the suggested collider studies .In superstring theory , the graviton and other particles have predicted mass scales of approximately $ 10 ^ { 15 } $ GeV , whereas in supergravity these mass scales are much lower , of order $ 10 ^ { 16 } $ GeV . Thus , if future research are able to locate a nonzero gravity wave background , this might point to inflation occurring in the early universe after a period of superstring theory .This paper is a preliminary inquiry of inflation from superstrings . There are many interesting open questions that we have not already contemplated .For instance , if we also wish to obtain a period of $ N $ th order inflation , with $ N $ being larger than two , the analysis will employ a more careful treatment of greater order corrections to the superpotential and Kähler potential . We additionally need to consider a range of several moduli and extranet field values , as also as the dependence on the full string theory landscape .Another important problems is the impact of moduli stabilization on the vacuum energy and the prospect of achieving accurate cosmology . The proposed collider signatures also need to be altered in the full superstring theory framework .Thus , although this paper offers a novel possibility for achieving inflationary cosmology from superstrings , we accept that more work requires to be performed to realize this possibility in practice . Inflation in superstring theory could have important implications for the landscape of vacua , having a period of super",
        "rewrite_text": "In this study, we present a novel approach to explore the cosmological solutions in superstring theory, which incorporates positive vacuum energy and positive spatial curvature. These solutions outline an early stage of inflation, followed by a de Sitter phase. We delve into the discussion about the possibility of positive spatial curvature in superstring theories, especially since only closed string theories are known to exhibit negative curvature.\n\nInflation, which represents a rapid expansion period in the early universe, is supported by both observational data and theoretical considerations within superstring theory. In the context of superstring physics, vacuum energy generally takes on a positive value. However, different breakings of supersymmetry (SUSY) result in vacuum energies with varying signs. Therefore, to achieve an inflationary period in superstring theories, it is essential to ensure that the vacuum energy remains positive.\n\nIn this paper, we provide a proof that the combination of compactifications, axion fields, and numerous uplifting fields leads to an inflationary de Sitter solution with positive vacuum energy in superstring theories. While inflationary de Sitter systems have been previously derived from supergravity, here we achieve these solutions through the principles of superstrings, marking a significant distinction. This distinction may lead to observable signatures that differentiate between these two scenarios. For instance, the graviton zero-mode contributes non-zero energy to the vacuum in supergravity but not in superstring theory. Consequently, observations of cosmological factors such as the scalar spectral index and the tensor-to-scalar ratio may serve as discriminating factors between these two approaches.\n\nAnother crucial difference lies in the energy scale of proposed collider studies. In superstring theory, the predicted mass scales for the graviton and other particles are approximately 10^15 GeV, whereas in supergravity these scales are significantly lower, reaching 10^16 GeV. Therefore, if future research identifies a non-zero gravitational wave background, it could indicate that inflation occurred in the early universe after a period influenced by superstring theory.\n\nThis paper serves as a preliminary exploration of inflationary processes driven by superstrings. There are numerous intriguing open questions that remain unanswered. For instance, if we aim to achieve Nth-order inflation with N greater than two, a more meticulous analysis is needed to account for higher-order corrections to both the superpotential and Kähler potential. It is also essential to consider various ranges of moduli and extranet field values while factoring in the entire string theory landscape. Additionally, the impact of moduli stabilization on vacuum energy and the potential for precise cosmology is a critical area of investigation. The proposed collider signatures must also be reevaluated within the comprehensive framework of superstring theory.\n\nWhile this study offers a promising avenue for achieving inflationary cosmology through superstrings, we recognize that further research is needed to realize this potential in practice. The implications of inflation in superstring theory for the landscape of vacua are potentially significant and hold great promise for future exploration.",
        "ori-fast-z-score": 1.9230478952816463,
        "water-fast-z-score": 8.20303112429596,
        "rewrite-fast-z-score": 1.2135597524338357
    },
    {
        "original_text": "Star clusters are important tools for studying a range of astrophysical phenomena, such as star formation, stellar feedback, and gravitational dynamics. These compact groups of stars are often found to contain a vast range of properties; while some clusters are simple populations of coeval stars, many are created through dynamical processes that assemble groups of previously unrelated stars with similar ages and chemical compositions. The diversity seen in these star cluster systems hints at a complex evolutionary history, but direct observational evidence has been difficult to obtain due to the crowding and dimness of clusters in distant galaxies. Here we use archived Space Telescope Imaging Spectrograph data to identify and characterize the spatial distribution of star clusters in the grand design spiral galaxy M51. The full spectrum of environments cluster (FCOE) catalog contains 122,695 individual star cluster candidates, identified from archival Hubble Space Telescope data by machine learning algorithms trained on high-contrast imaging from the Advanced Camera for Surveys. We estimate cluster ages from the strength of the hydrogen Brackett Jump at 2.166 microns, and demonstrate that there is an approximately log-normal distribution of ages with a peak at log(age/years) ≃ 7.6 and a standard deviation of 0.4. We determine the systemic velocities of clusters via cross-correlation with a library of model spectra and find that this clusters have a nearly Gaussian velocity distribution with a peak of v ≃ 265 km/s and a standard deviation of 39 km/s. Lastly, we show that clusters exhibit two distinct distributions in projection on the major axis of M51, with more than 60% located within 2.5 kpc of the galactic midplane.",
        "watermark_text": "Star clusters are important resources for studying a range of astrophysical processes , such as star formation , planetary feedback , and gravity dynamics . These compact groups of stars are often found to contain a vast array of properties ; while some clusters are simple populations of coeval stars , many are created through dynamical processes that assemble groups of previously identical stars with similar ages and biological compositions .The variety seen in these star cluster systems suggests at a complex evolutionary history , but direct observational evidence has been difficult to obtain due to the crowding and dimness of clusters in nearby galaxies . Here we using archived Space Telescope Imaging Spectrograph information to identify and characterize the spatial distribution of galaxy clusters in the grand design spiral galaxy M51 .The full spectrum of environments cluster ( FCOE ) catalog contains 122 , 695 individual star cluster candidates , designated from archival Hubble Space Telescope data by machine learning techniques hired on high - contrast imaging from the Advanced Camera for Surveys . We estimate cluster ages from the strength of the hydrogen Brackett Jump at 2 . 166 microns , and suggest that there is an roughly log - normal distribution of ages with a peak at log ( age / years ) [UNK] 7 . 6 and a standard deviation of 0 . 4 .We determine the systemic velocities of clusters via cross - correlation with a library of model spectra and find that this clusters have a nearly Gaussian momentum distribution with a peak of w [UNK] 265 km / s and a standard deviation of 39 km / s . Lastly , we find that clusters exhibit two different distributions in projection on the main axis of M51 , with more than 60 % situated within 2 . 5 kpc of the galactic midplane .",
        "rewrite_text": "Star clusters are crucial resources for studying a wide range of astrophysical processes, including star formation, planetary feedback, and gravity dynamics. These tightly grouped stars often possess a diverse range of properties. While some clusters consist of simple populations of coeval stars, many are formed through dynamic processes that assemble groups of previously identical stars with similar ages and biological compositions. The complexity observed in these star cluster systems suggests a complex evolutionary history. However, obtaining direct observational evidence has been challenging due to the crowding and dimness of clusters in nearby galaxies.\n\nWe are utilizing archival information from the Space Telescope Imaging Spectrograph to identify and characterize the spatial distribution of galaxy clusters in the grand design spiral galaxy M51. The Full Catalog of Environmental Cluster Spectra (FCOE) contains 122,695 individual star cluster candidates, which have been designated from archival Hubble Space Telescope data using machine learning techniques derived from high-contrast imaging by the Advanced Camera for Surveys. We estimate cluster ages based on the strength of the hydrogen Brackett Jump at 2.166 microns and propose that there is a roughly log-normal distribution of ages, with a peak at log(age/years) [UNK] 7.6 and a standard deviation of 0.4.\n\nWe determine the systemic velocities of clusters through cross-correlation with a library of model spectra, finding that these clusters exhibit a nearly Gaussian momentum distribution with a peak of w [UNK] 265 km/s and a standard deviation of 39 km/s. Finally, we observe that clusters display two distinct distributions when projected onto the main axis of M51, with over 60% situated within 2.5 kpc of the galactic midplane.",
        "ori-fast-z-score": 1.9445436482630056,
        "water-fast-z-score": 6.122759914971185,
        "rewrite-fast-z-score": 4.065863991822648
    },
    {
        "original_text": "Wireless communication systems employing multicarrier signals, such as orthogonal frequency division multiplexing (OFDM) and multi-carrier modulation (MC) techniques, have been widely used in current wireless networks because of their efficient bandwidth utilization and ease of implementation. The widely separated antennas used in these systems, however, tend to render the channel as sparse multipath, i.e., the channel is flat in the frequency domain but has impulses at specific delay points in the time domain. This work investigates the capacity and reliability of such a sparse multipath channel in the wideband regime. The capacity is shown to be logarithmically dependent on the coherence time of the channel whereas the reliability is shown to be exponentially decreasing with respect to the coherence time. The capacity-reliability tradeoff is then studied in terms of the exponent of the logarithm of coherence time. Numerical results verify the accuracy of the derived capacity and reliability expressions and demonstrate that the latter consistently overwhelms the former, in all cases. This work is valuable for evaluating the potential performance gain of multi-antenna deployment in current and next-generation wireless networks operating in the wideband regime.",
        "watermark_text": "Wireless communication devices utilizing multicarrier signals , such as orthogonal frequency division multiplexing ( OFDM ) and multi - carrier modulation ( MC ) techniques , have been widely using in current wireless networks because of their efficient wavelength utilization and ease of implementation . The commonly separated antennas used in these systems , however , tend to make the channel as sparse multipath , i . e . , the channel is flat in the frequency domain but has impulses at different delay points in the time domain .This research investigates the capacity and reliability of such a sparse multipath network in the wideband regime . The capacity is demonstrated to be logarithmically contingent on the coherence time of the channel whereas the reliability is demonstrated to be exponentially decreasing with regard to the coherence time .The capacity - safety tradeoff is then investigated in terms of the exponent of the logarithm of coherence time . Numerical results verify the accuracy of the derived capacity and reliability expressions and suggest that the latter repeatedly overwhelms the former , in all situations .This research is important for evaluating the possibilities efficiency gain of multi - antenna deployment in current and new - generation communications systems operating in the wideband regime .",
        "rewrite_text": "Wireless communication devices employing multicarrier signal technologies, such as orthogonal frequency division multiplexing (OFDM) and multi-carrier modulation (MC) techniques, have become widely utilized in contemporary wireless networks due to their efficient wavelength utilization and simplicity of implementation. The systems commonly employ separate antennas, which often result in the channel exhibiting a sparse multipath characteristic. This means that while the channel appears flat in the frequency domain, it exhibits impulses at various delay points in the time domain.\n\nThis study explores the capacity and reliability of such sparse multipath networks in the wideband context. The research demonstrates that capacity is logarithmically dependent on the channel's coherence time, while reliability exhibits an exponential decrease with respect to the same coherence time. Furthermore, a trade-off between capacity and safety is examined in terms of the exponent of the logarithm of the coherence time.\n\nNumerical results validate the accuracy of the derived capacity and reliability expressions, indicating that reliability often outweighs capacity in all scenarios. This research is significant for assessing the potential efficiency gains of multi-antenna deployment in both existing and next-generation communications systems operating in the wideband regime.",
        "ori-fast-z-score": 0.32539568672798425,
        "water-fast-z-score": 5.237229365663818,
        "rewrite-fast-z-score": 0.20628424925175867
    },
    {
        "original_text": "Long-distance transport of digital data is central to the functioning of modern society, and the development of ultra-long-distance quantum communication would have profound benefits for global communications. Using a multicore optical fiber, we demonstrate a quantum communication channel composed of 10,000 light segments, corresponding to a transmission distance of over 2,000 miles. Each light segment is encoded on a individualhyperfine spin state of atoms, and connected via a ETele programed NOT gate. The fidelity of the transmitted state is over 90%, with a logical error rate < 0.5%. This result represents a significant improvement on previous optical long-distance quantum communication  1-3 , and is promising for future development of large-scale quantum networks.  1  A long-distance quantum communication across two cities in Europe. NATURE COMMUNICATIONS, 2018, 9, 174-176.  2  One thousand-kilometre quantum communication. SCIENCE, 2018, 359, 52-54.  3  Global Einstein teleportation with a quantum hyperplane. NATURE COMMUNICATIONS, 2017, 8, 76-80.",
        "watermark_text": "Long - distance transport of digital data is central to the functioning of modern society , and the development of ultra - long - distance quantum communication might have profound benefits for global communications . Using a multicore optical cable , we exhibit a quantum communication channel composed of 10 , 000 light segments , equivalent to a transmission distance of over 2 , 000 miles .Each light portion is stored on a individualhyperfine spin state of atoms , and joined via a ETele programed NOT gate . The fidelity of the transmitted state is over 90 % , with a logical error rate < 0 . 5 % .This result represents a substantial improvement on previous optical long - distance quantum communication 1 - 3 , and is good for future development of large - scale optical networks . 1 A long - distance quantum communication across two locations in Europe .NATURE COMMUNICATIONS , 2018 , 9 , 174 - 176 . 2 One many - kilometre quantum communication .SCIENCE, 2018, 359, 52-54.3  Global Einstein teleportation with a quantum hyperplane.NATURE COMMUNICATIONS, 2017, 8, 76-80.",
        "rewrite_text": "The efficient transmission of digital data over long distances is a cornerstone of modern society's operation, and the advancement of ultra-long-distance quantum communication holds the potential for profound benefits in global communications. By utilizing a multicore optical cable, we have established a quantum communication channel composed of 10,000 individual light segments, which is equivalent to a transmission distance exceeding 2,000 miles. Each segment is stored in a unique hyperfine spin state of atoms and linked through an ETele programmed NOT gate. The transmitted state's fidelity surpasses 90%, with a logical error rate below 0.5%. This achievement represents a significant improvement over previous optical long-distance quantum communication studies (1-3), and bodes well for the development of future large-scale optical networks. One notable example of long-distance quantum communication spanning two locations in Europe was published in NATURE COMMUNICATIONS in 2018 (1). Additionally, a many-kilometer quantum communication experiment was reported in SCIENCE in 2018 (2), and the Global Einstein Teleportation with a Quantum Hyperplane was published in NATURE COMMUNICATIONS in 2017 (3).",
        "ori-fast-z-score": 3.1601109742955256,
        "water-fast-z-score": 6.203180801394921,
        "rewrite-fast-z-score": 2.060839349277234
    },
    {
        "original_text": "Hybrid networks have been widely used in recent communication systems owing to their benefits of increased network capacity and enhanced network support. However, existing localized support for injection point election (LSE) mechanisms for hybrid networks cannot guarantee the election of a same injection point (IP) among all the networks, which restricts their applicability in hybrid networks. In this paper, we propose a localized supported injection point election (LSIPE) mechanism, which ensures that all the networks will elect to the same injection point. Moreover, we prove that LSIPE is close to optimal with respect to the objective of maximizing the minimum network utility while guaranteeing that the same injection point is elected among all the networks. Extensive simulations validate the effectiveness and efficiency of the proposed LSIPE mechanism.  LSIPE mechanism is a novel localized support for injection point election (LSE) mechanism for hybrid networks. In hybrid networks, some physical networks and virtual networks coexist, and each physical network or virtual network can connect to multiple other physical networks or virtual networks. Thus, the network injection management in hybrid networks becomes a multipoint-to-multipoint problem, which is much more complex than the point-to-point problem in single networks. Existing LSE mechanisms for single networks cannot guarantee the election of a same injection point (IP) among all the networks. Therefore, we propose a LSIPE mechanism, which can achieve the goal that all the networks will elect to the same injection point. Furthermore, we prove that LSIPE is close to optimal with respect to the objective of maximizing the minimum network utility while guaranteeing that the same injection point is elected among all the networks. Extensive simulations validate the effectiveness and efficiency of the proposed LSIPE mechanism. The remainder of this paper is organized as follows. Related works are described in Section II. The system model is introduced in Section III. The proposed LSIPE mechanism is presented in Section IV. Performance evaluation and analysis are presented in Section V. Section VI concludes this paper.",
        "watermark_text": "Hybrid networks have been widely useful in recent communication technologies due to their benefits of greater network strength and increased connection support . However , current localized support for injection point election ( LSE ) schemes for hybrid networks cannot guarantee the election of a same injection point ( IP ) among all the connections , which restricts their applicability in hybrid networks .In this paper , we propose a localized supported injection point election ( LSIPE ) process , which ensures that all the connections will elect to the same injection point . Moreover , we prove that LSIPE is close to optimal with regard to the objective of maximizing the average network utility while guaranteeing that the same injection point is appointed among all the connections .Extensive simulations validate the performance and efficiency of the suggested LSIPE system . LSIPE system is a novel localized support for injection point election ( LSE ) method for hybrid networks .In hybrid networks , some physical networks and virtual systems coexist , and each physical network or virtual network can connect to multiple other physical networks or virtual systems . Thus , the channel injection administration in hybrid networks becomes a multipoint - to - multipoint problem , which is much more sophisticated than the point - to - point problem in single networks .Existing LSE mechanisms for single networks cannot guarantee the election of a same injection point ( IP ) among all the connections . Therefore , we propose a LSIPE system , which can attain the objective that all the connections will vote to the same injection point .Furthermore , we prove that LSIPE is close to optimal with regard to the objective of maximizing the minimum connection utility while guaranteeing that the same injection position is chosen among all the connections . Extensive simulations validate the performance and efficiency of the suggested LSIPE system .The remainder of this paper is organized as follows . Related publications are explained in Section II .The system model is proposed in Section III . The proposed LSIPE system is given in Section IV .Performance evaluation and assessment are presented in Section V . Section VI concludes this paper .",
        "rewrite_text": "Hybrid networks have become increasingly beneficial in modern communication technologies due to their enhanced network strength and improved connection support. However, current localized support for injection point election (LSE) schemes in hybrid networks fails to ensure a consistent election of the same injection point (IP) across all connections, thereby limiting their applicability. In this study, we introduce a Localized Supported Injection Point Election (LSIPE) process that guarantees unanimous selection of the same injection point by all connections.\n\nIn hybrid networks, where physical networks and virtual systems coexist, each network or system can connect to multiple others. This makes channel injection management in hybrid networks a complex multipoint-to-multipoint problem, more sophisticated than the simpler point-to-point issue in single networks. Existing LSE mechanisms for single networks are insufficient in ensuring a consistent IP election across connections. Therefore, we propose the LSIPE system, which achieves the objective of unanimous voting for a single injection point.\n\nFurthermore, we demonstrate that LSIPE is nearly optimal when it comes to maximizing the minimum connection utility while ensuring a consistent choice of injection position across all connections. Comprehensive simulations validate the performance and efficiency of the proposed LSIPE system.\n\nThe structure of this paper is organized as follows: Related research is presented in Section II, the system model is introduced in Section III, the proposed LSIPE system is detailed in Section IV, performance evaluations and assessments are presented in Section V, and Section VI concludes the paper.",
        "ori-fast-z-score": -0.5622535302317492,
        "water-fast-z-score": 6.93007749061827,
        "rewrite-fast-z-score": 1.1917080461366747
    },
    {
        "original_text": "In this work we study the effects of including a domain wall in twoflavor dynamical QCD simulations. We use two types of domain wall algorithms and compare their performance. We find that with both domain wall types the equation of state and some other dynamical quantities show no sign of damage. However the chiral condensate is substantially reduced with domain wall types X and V. To understand this we apply the one-plaquette QCD chiral perturbation theory to analyze the valence quark masses. We find that, with domain wall type X, the light quarks have a much lighter valence mass than the strange quark, while with domain wall type V the strange quark has a much lighter valence mass than the other quarks. These valence quark masses, combined with the much heavier strange quark mass in the domain wall proposal, lead to the surprising result that the strange quark contributes substantially more to the chiral condensate than in the conventional case. The result for the strange quark condensate is 2.9(5) MeV<OPE>/3 = 0.9(1) MeV, where the error includes both the fitting error and the systematic error from ignoring the effects of the domain wall on the heavier strange quark mass. We also consider the implications for the physical kaon and eta masses, and discuss the possibility that the strange quark condensate could be determined from the physical pion mass and the kaon and eta masses. * Keywords: Domain walls, QCD, dynamical simulation, staggered fermions Including a domain wall in two-flavor dynamical lattice QCD simulations has no adverse effect on the equation of state or the dynamical quantities studied, but substantially reduces the magnitude of the chiral condensate. The valence quark masses are analyzed using one-plaquette QCD chiral perturbation theory, and we find that the strange quark mass in the domain wall proposal contributes more to the chiral condensate than in the conventional case. The result for the strange quark condensate is 2.9(5) MeV<OPE>/3 = 0.9(1) MeV, where the error includes both the fitting error and the systematic error from ignoring the effects of the domain wall on the heavier strange quark mass. We also consider the implications for the physical kaon and eta masses, and discuss the possibility that the strange quark condensate could be determined from the physical pion mass and the kaon and eta masses. * * Authors: Yang, Changho; Wang, Hongming; Huang, Shouji; Liu, Qiuf; Liu, Xiangming (email: liu@phys.ntu.edu.tw); Shindler, Andreas (email: shindler@physik.uni-bielefeld.de); Muller, Andreas (email: amueller@tcs.infn.tl) * * Affiliation: Indiana University School of Medicine Indianapolis, IN, USA * * URL: http://www.phys.ntu.edu.tw/~chang",
        "watermark_text": "In this research we study the effects of including a domain fence in twoflavor dynamical QCD simulations . We use two forms of domain wall algorithms and report their performance .We see that with both domain wall types the equation of state and some other dynamical parameters show no sign of damage . However the chiral condensate is significantly reduced with domain floor kinds X and V . To understand this we apply the one - plaquette QCD chiral perturbation theory to analyze the valence quark masses .We see that , with domain wall type X , the light quarks have a far lighter valence mass than the weird quark , while with domain wall type V the weird quark has a far lighter valence mass than the other quarks . These valence quark masses , combined with the considerably heavier strange quark mass in the domain wall proposal , lead to the surprising outcome that the weird quark contributes substantially more to the chiral condensate than in the standard case .The result for the weird quark condensate is 2 . 9 ( 5 ) MeV < OPE > / 3 = 0 . 9 ( 1 ) MeV , where the error contains both the fitting error and the systematic error from ignoring the effects of the domain wall on the heavier strange quark mass . We additionally discuss the implications for the physical kaon and eta masses , and consider the prospect that the weird quark condensate could be determined from the physical pion mass and the kaon and eta masses .* Keywords : Domain barriers , QCD , dynamical simulation , staggered fermions Including a domain wall in two - flavor dynamical lattice QCD simulations has no adverse effect on the equation of state or the dynamical quantities studied , but significantly decreases the magnitude of the chiral condensate . The valence quark masses are examined using one - plaquette QCD chiral perturbation theory , and we find that the weird quark mass in the domain floor proposal adds more to the chiral condensate than in the standard case .The result for the weird quark condensate is 2 . 9 ( 5 ) MeV < OPE > / 3 = 0 . 9 ( 1 ) MeV , where the error contains both the fitting error and the systematic error from ignoring the effects of the domain wall on the heavier strange quark mass . We additionally discuss the implications for the physical kaon and eta masses , and consider the prospect that the weird quark condensate could be determined from the physical pion mass and the kaon and eta masses .* * Authors : Yang , Changho ; Wang , Hongming ; Huang , Shouji ; Liu , Qiuf ; Liu , Xiangming ( email : liu @ phys . ntu . edu . tw ) ; Shindler , Andreas ( email : shindler @ physik . uni - bielefeld . de ) ; Muller , Andreas ( email : amueller @ tcs . infn . tl ) * * Affiliation : Indiana University School of Medicine Indianapolis , IN , USA * * URL : www : / / www . phys . ntu . edu . tw / ~ chang",
        "rewrite_text": "In this study, we investigate the impact of introducing a domain wall into two-flavor dynamical simulations of Quantum Chromodynamics (QCD). We utilize two types of domain wall algorithms and assess their performance. Our observations indicate that the inclusion of both domain wall types does not adversely affect the equation of state or other dynamical parameters. However, the chiral condensate experiences a notable reduction with domain wall types X and V.\n\nTo understand this phenomenon, we apply one-plaquette QCD chiral perturbation theory to analyze the valence quark masses. Our findings reveal that with domain wall type X, light quarks possess a significantly lighter valence mass compared to the strange quark, while with domain wall type V, the strange quark has a lighter valence mass than other quarks. These valence quark masses, combined with the relatively heavier strange quark mass in the domain wall proposal, lead to a surprising outcome: the strange quark contributes significantly more to the chiral condensate than in the standard case.\n\nThe result for the strange quark condensate is 2.9 (5) MeV < OPE > / 3 = 0.9 (1) MeV, where the error encompasses both fitting errors and systematic errors arising from neglecting the effects of the domain wall on the heavier strange quark mass. We further discuss the implications for the physical kaon and eta masses and consider the possibility of determining the strange quark condensate from the physical pion mass, as well as the kaon and eta masses.\n\n* Keywords: Domain barriers, Quantum Chromodynamics (QCD), dynamical simulation, staggered fermions\n\nIncluding a domain wall in two-flavor dynamical lattice QCD simulations has no detrimental effect on the studied equation of state or dynamical parameters. However, it significantly reduces the magnitude of the chiral condensate. We employ one-plaquette QCD chiral perturbation theory to investigate valence quark masses and find that the strange quark mass in the domain wall proposal contributes more to the chiral condensate than in the standard case. The result for the strange quark condensate is as mentioned earlier, with errors accounting for fitting inaccuracies and systematic errors stemming from neglecting the domain wall's influence on the heavier strange quark mass. We additionally explore the implications for the physical properties of kaons and etas, considering the potential to determine the strange quark condensate through the physical pion mass and other related masses.\n\n* Authors: Yang, Changho; Wang, Hongming; Huang, Shouji; Liu, Qiuf; Liu, Xiangming (email: liu@phys.ntu.edu.tw); Shindler, Andreas (email: shindler@physik.uni-bielefeld.de); Muller, Andreas (email: amueller@tcs.infn.tl)\n\n* Affiliation: Indiana University School of Medicine, Indianapolis, IN, USA\n\n* URL: https://www.phys.ntu.edu.tw/~chang (Note: The URL provided in the original text was not formatted correctly and has been corrected in this response.)",
        "ori-fast-z-score": -0.6324555320336759,
        "water-fast-z-score": 5.947886892886081,
        "rewrite-fast-z-score": 1.0674899923282326
    },
    {
        "original_text": "Bicategories are closed general categories, with two-sided cells, sitting in four-category assemblies. These can be considered as generalizations of monoids, with two elements, and arranged in categories, whose fibers are two-category with one object. A framed bicategory is an enriched category in bicategories, with three-sided cells, sitting in six-category, and hom-bicategories of them, sitting in eight-category. These can be considered as generalizations of 2-categories, with one object, and fibers being 2-groupoids, with one 2-morphism. A monoidal fibrations between two framed bicategories are weak functors between their underlying categories, weakly satisfying some additional equations. These fibrations capture 2-cells in fibers. They also generalize crossed modules of 2-groups and strict monoidal functors of monoidal categories. Framing a bicategory with three-sided cells changes it into an enriched category, with six-category as its fibers. These fibrations capture 2-cells in fibers. They also generalize crossed modules of 2-groups and strict monoidal functors of monoidal categories. The paper studies framed bicategories, their 2-cells, and their monoidal fibrations, from various points of views, explaining main concepts and constructions. - bicategories as closed general categories - two-sided cells as generalizations of one-element sets - four-category assemblies as generalizations of two-element sets - Main concepts and constructions are explained in this framework - Framed bicategories as enriched categories - fibers as two-category - 2-cells as 2-morphisms - 2-groups as fibers of strict monoidal functors - Morita equivalence of 2-groups as isomorphism between corresponding strict monoidal functors - crossed modules of 2-groups as monoidal deformations of corresponding 2-groups - monoidal fibrations between framed bicategories - strict monoidal functors between corresponding framed bicategories - weakly satisfying some additional equations as fibrations with fibers 2-groupoids - Generalizations of these are also described - Appendices are provided with technical results.",
        "watermark_text": "Bicategories are closed general categories , with two - sided cells , sitting in four - class assemblies . These can be regarded as generalizations of monoids , with two elements , and arranged in categories , whose fibers are two - category with one object .A framed bicategory is an enriched category in bicategories , with three - sided cells , standing in six - class , and hom - bicategories of them , standing in eight - class . These can be regarded as generalizations of 2 - categories , with one object , and fibers being 2 - groupoids , with one 2 - morphism .A monoidal fibrations between two framed bicategories are weak functors between their underlying categories , weakly satisfying some additional equations . These fibrations capture 2 - cells in fibers .They also generalize crossed modules of 2 - families and strict monoidal functors of monoidal categories . Framing a bicategory with three - sided cells changes it into an enriched classification , with six - class as its components .These fibrations capture 2 - cells in fibers . They often generalize crossed modules of 2 - families and strict monoidal functors of monoidal types .The paper studies framed bicategories , their 2 - cells , and their monoidal fibrations , from numerous points of views , giving main concepts and constructions . - bicategories as closed general categories - two - sided cells as generalizations of one - element sets - four - category assemblies as generalizations of two - element sets - Main concepts and constructions are explained in this framework - Framed bicategories as enriched types - fibers as two - category - 2 - cells as 2 - morphisms - 2 - groups as fibers of strict monoidal functors - Morita equivalence of 2 - groups as isomorphism between corresponding strict monoidal functors - crossed modules of 2 - groups as monoidal deformations of corresponding 2 - groups - monoidal fibrations between framed bicategories - strict monoidal functors between corresponding framed bicategories - weakly satisfying some additional equations as fibrations with fibers 2 - groupoids - Generalizations of these are also explained - Appendices are provided with technical results .",
        "rewrite_text": "Bicategories constitute closed general categories with two-sided cells that exist within four-class assemblies. They can be considered as generalizations of monoids with two elements organized into categories, where the fibers are two-category with a singular object. A framed bicategory, on the other hand, is an enriched category within the realm of bicategories, featuring three-sided cells that stand in six-class relationships and hom-bicategories that reside in eight-class assemblies. These can be regarded as expansions of 2-categories with a sole object, and their fibers comprise 2-groupoids with a singular 2-morphism.\n\nMonoidal fibrations between two framed bicategories are weak functors that connect their underlying categories while satisfying some additional equations weakly. These fibrations capture 2-cells within the fibers. They often generalize crossed modules of 2-families and strict monoidal functors of monoidal categories. By introducing three-sided cells in a bicategory, it transforms into an enriched classification system with six-class components. These fibrations also capture 2-cells within fibers, frequently generalizing crossed modules of 2-families and strict monoidal functors of monoidal types.\n\nThis paper examines framed bicategories, their 2-cells, and their monoidal fibrations from various perspectives, elucidating the main concepts and constructions. Specifically, it discusses:\n\n* Bicategories as closed general categories.\n* Two-sided cells as generalizations of one-element sets.\n* Four-category assemblies as generalizations of two-element sets.\n\nWithin this framework, key concepts and constructions are explained. Framed bicategories are viewed as enriched types, with fibers constituting a two-category and 2-cells manifesting as 2-morphisms. Additionally, 2-groups function as the fibers of strict monoidal functors.\n\nThe paper also delves into Morita equivalence of 2-groups as isomorphisms between corresponding strict monoidal functors, crossed modules of 2-groups as monoidal deformations of corresponding 2-groups, and monoidal fibrations between framed bicategories along with strict monoidal functors between corresponding framed bicategories. These relationships are characterized by weakly satisfying additional equations as fibrations with 2-groupoid fibers.\n\nGeneralizations of these concepts are also explained, accompanied by technical appendices providing additional insights.",
        "ori-fast-z-score": -2.3312620206007844,
        "water-fast-z-score": 0.6527533657682196,
        "rewrite-fast-z-score": -0.5107539184552492
    },
    {
        "original_text": "This paper defines universal invariants for higher K-theory and higher signatures, and demonstrates their relevance by determining higher K-theory of one-point compactifications of hyperbolic groups. These universal invariants satisfy the Atiyah-Hirzebruch spectral sequence for topological K-theory, a calculation of the fundamental group, and the universal coefficients theorem. In this framework, the strong Novikov conjecture, which states that the higher signatures of a closed manifold are universal among degree-one cohomology classes, is equivalent to the homotopy invariance of higher K-theory. This implies that the higher K-groups satisfy the Hirzebruch signature theorem, and, under certain hypotheses, also detect exotic elements in the stable homology of manifolds. We also establish twisted versions of these universal invariants and apply them to the Bismut intersection formula, the Borel regulator, and elliptic genera. This approach to higher K-theory and universal invariants extends to other stable homotopy theories, such as bordism and generalized cohomology. In particular, we define twisted variants of these theories and show how to compute these variants using the invariants defined here. This approach unifies the study of K-theory, bordism, generalized cohomology, and stable homotopy by viewing these theories as localizations of a single universal theory of stable derived functors. In particular, the Atiyah-Hirzebruch spectral sequence for topological K-theory arises as the key element in the localization process. In addition to defining this universal theory of stable derived functors, we also calculate its rationalization. This provides a powerful way to determine the rational homology of one-point compactifications of hyperbolic groups, which may be of independent interest. Finally, we propose a framework for studying twisted stable homotopy theories in which various structures, such as vector bundles and operators, are replaced by connective coverings and corresponding universal invariants. We indicate some examples of these twisted theories and indicate how to compute their universal invariants. I thank David Giesen, Sean Tammar, and Robert Yager for their advice and support. Higher K-theory via universal invariants Higher K-theory captures key features of continuous cohomology by considering all homotopy groups at once. This is a pro-spectrum, or internal homology theory, meaning it admits a normalized complex with homology the homotopy groups of the K-theory spectrum KU. These homotopy groups may be given the structure of a ring by means of coefficients; the Hopkins-Miller theorem identifies this ring as topological cyclic homology TC, allowing K-theory to be studied via universal properties. This pro-spectrum includes a lot of information. The fundamental theorem of K-theory describes it as a collection of pieces separated by unique extensions, whose cancellation requires the Elliott invariant. Meanwhile, the periodicity theorem and Bott periodicity isomorphism generalize the classical Bott periodicity and result respectively to the pro-spectrum level. There is also",
        "watermark_text": "This paper provides universal invariants for greater K - theory and larger signatures , and demonstrates their relevance by determining higher K - theory of one - point compactifications of hyperbolic groups . These universal invariants satisfy the Atiyah - Hirzebruch spectral sequence for topological K - theory , a calculation of the fundamental group , and the universal coefficients theorem .In this framework , the strong Novikov theorem , which says that the higher signatures of a closed manifold are fundamental among degree - one cohomology groups , is analogous to the homotopy invariance of greater K - theory . This implies that the higher K - families satisfy the Hirzebruch signature theorem , and , under certain hypotheses , also find exotic components in the stable homology of manifolds .We additionally establish twisted versions of these universal invariants and extend them to the Bismut interchange formula , the Borel regulator , and elliptic genera . This method to higher K - theory and universal invariants extends to other stable homotopy theories , such as bordism and generalized cohomology .In particular , we define twisted versions of these theories and know how to compute these variants using the invariants defined here . This method unifies the study of K - theory , bordism , generalized cohomology , and stable homotopy by examining these theories as localizations of a single universal theory of stable derived functors .In particular , the Atiyah - Hirzebruch spectral sequence for topological K - theory arises as the key object in the localization process . In addition to defining this universal theory of stable derived functors , we also define its rationalization .This offers a powerful way to obtain the rational homology of one - point compactifications of hyperbolic groups , which may be of independent importance . Finally , we propose a framework for studying twisted stable homotopy theories in which several structures , such as vector bundles and operators , are replaced by connective coverings and corresponding universal invariants .We indicate some examples of these twisted theories and suggest how to compute their universal invariants . I appreciate David Giesen , Sean Tammar , and Robert Yager for their advice and support .Higher K - theory via universal invariants Higher K - theory captures basic features of smooth cohomology by treating all homotopy classes at once . This is a pro - spectrum , or internal homology concept , meaning it admits a normalized complex with homology the homotopy classes of the K - theory spectrum KU .These homotopy groups may be shown the composition of a ring by means of coefficients ; the Hopkins - Miller formula identifies this ring as topological cyclic homology TC , allowing K - theory to be examined via universal properties . This pro - spectrum contains a amount of information .The fundamental theorem of K - theory presents it as a group of pieces separated by unique extensions , whose closure requires the Elliott invariant . Meanwhile , the periodicity hypothesis and Bott periodicity isomorphism generalize the classical Bott periodicity and result respectively to the pro - spectrum level .There is also",
        "rewrite_text": "This study presents universal invariants that are applicable to advanced K-theory and larger signatures. It demonstrates their relevance by determining the higher K-theory of one-point compactifications of hyperbolic groups. These universal invariants align with the Atiyah-Hirzebruch spectral sequence for topological K-theory, fundamental group calculations, and the universal coefficients theorem.\n\nWithin this framework, the strong Novikov theorem, which states that the higher signatures of a closed manifold are fundamental in degree-one cohomology groups, is analogous to the homotopy invariance of advanced K-theory. This implies that the higher K-families comply with the Hirzebruch signature theorem. Furthermore, under certain hypotheses, they can detect exotic components in the stable homology of manifolds.\n\nWe have also established twisted versions of these universal invariants and extended them to the Bismut interchange formula, the Borel regulator, and elliptic genera. This approach to higher K-theory and universal invariants can be extended to other stable homotopy theories, such as bordism and generalized cohomology.\n\nSpecifically, we have defined twisted versions of these theories and know how to compute these variations using the invariants presented here. This method unifies the study of K-theory, bordism, generalized cohomology, and stable homotopy by examining these theories as localizations of a unified theory of stable derived functors.\n\nNotably, the Atiyah-Hirzebruch spectral sequence for topological K-theory emerges as a crucial object in the localization process. Alongside defining this universal theory of stable derived functors, we have also defined its rationalization. This offers an effective means to obtain the rational homology of one-point compactifications of hyperbolic groups, which may hold independent significance.\n\nFurthermore, we propose a framework for studying twisted stable homotopy theories, where structures like vector bundles and operators are replaced by connective coverings and corresponding universal invariants. We illustrate several examples of these twisted theories and suggest methods for computing their universal invariants.\n\nI am grateful to David Giesen, Sean Tammar, and Robert Yager for their advice and support in this research on higher K-theory via universal invariants. In this context, advanced K-theory encapsulates fundamental characteristics of smooth cohomology by considering all homotopy classes simultaneously. This is a pro-spectrum or internal homology concept, which allows for a normalized complex with homology equivalent to the homotopy classes of the K-theory spectrum KU. These homotopy groups can be represented as a composition of a ring through coefficients, and the Hopkins-Miller formula identifies this ring as topological cyclic homology TC, enabling K-theory to be examined through universal properties. This pro-spectrum carries a wealth of information.\n\nThe fundamental theorem of K-theory presents it as a collection of pieces distinguished by unique extensions, whose closure necessitates the Elliott invariant. Meanwhile, the generality of the periodicity hypothesis and Bott periodicity isomorphism extends classical Bott periodicity to the pro-spectrum level. Additionally,... (remaining text continues with further details and explanations)",
        "ori-fast-z-score": 2.416274785363585,
        "water-fast-z-score": 8.620893048537067,
        "rewrite-fast-z-score": 4.421733159964657
    },
    {
        "original_text": "We present a catalog of 98,653 discrete sources detected in the three quarters of the 5 square degrees of the Carina Nebula surveyed by the Gaia Survey in its first visit to this region of the Milky Way. We use this catalog, in combination with the much deeper catalog of sources detected in the entire Carina Nebula Survey published in a companion paper, to study global properties of the Carina Nebula and its distribution across the energy spectrum. We find that the Nebula has an energy budget of 6 - 21%, rising to 21 - 30% in its densest regions. The Nebula is most homogeneously distributed around 6% in the region surrounding the HII region Trion Nebula and most clumpy toward its south and west, in a pattern typical of sequential energy injection. We show that this clumpiness has an environmental origin, with dense gas in Carina being shielded from external influences and having a lower density than the less dense ISM in the surrounding areas.",
        "watermark_text": "We create a list of 98 , 653 discrete sources detected in the three third of the 5 square degrees of the Carina Nebula surveyed by the Gaia Survey in its initial visit to this region of the Milky Way . We use this catalog , in combination with the much deeper catalog of sources detected in the entire Carina Nebula Survey published in a companion paper , to study global properties of the Carina Nebula and its distribution across the energy spectrum .We see that the Nebula has an energy budget of 6 - 21 % , rising to 21 - 30 % in its densest areas . The Nebula is most homogeneously scattered around 6 % in the region surrounding the HII region Trion Nebula and most clumpy toward its east and west , in a pattern typical of sequential power injection .We indicate that this clumpiness has an environmental origin , with heavy gas in Carina being shielded from external influences and having a smaller density than the less dense ISM in the nearby areas .",
        "rewrite_text": "We compile a list encompassing 98,653 distinct sources that were detected in the third of the five square degrees of the Carina Nebula surveyed by the Gaia Survey during its initial exploration of this region in the Milky Way. We utilize this catalog, alongside a deeper catalog of sources detected across the entire Carina Nebula Survey published in a supplementary study, to investigate the global properties and energy spectrum distribution of the Carina Nebula. Our findings indicate that the Nebula possesses an energy budget ranging from 6 to 21%, with densest areas reaching up to 21-30%. The Nebula is uniformly distributed at approximately 6% in the vicinity of the HII region known as Trion Nebula, while exhibiting a clustered pattern towards its eastern and western extremities, typical of sequential power injection. We suggest that this clustering can be attributed to environmental factors, with heavy gases within Carina being shielded from external influences and possessing a lower density compared to the less dense Interstellar Medium in neighboring areas.",
        "ori-fast-z-score": -0.2626128657194451,
        "water-fast-z-score": 3.6765801200722312,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "The strong chemical bonds in molecules and the high opacity of particules in medium to low temperatures limit the study of Ultracool Dwarfs (UCDs) and Exoplanets. The most massive UCDs have a temperature lower than 20000 K and an effective temperature below 300 K; they are in a gaseous state. The Opacity Project (OP) computes theoretical line and mean opacities for plasmas of different temperatures and compositions, which can be used as input for radiative-transfer simulations and then to fit the observed spectra of these UCDs and Exoplanets. In this work, the authors compute line and mean opacities for plasmas with temperatures lower than 20000 K and effective temperatures lower than 300 K. The inclusion of impurities and traces of metals (like H2O, CO2, NH3, CH4, etc.) extend the computed range of opacities for such kind of plasmas. For example, for plasmas at 10000 K with 90% Hydrogen and 10% Carbon, the authors computed line and mean opacities within 10% of the OP simulation results for pressures between 10-7 and 10-9 torr. Thus, these computed line and mean opacities can be used to simulate the thermal emission and reemission spectra of UCDs and Exoplanets with good accuracy.",
        "watermark_text": "The strong chemical bonds in compounds and the high opacity of particules in medium to low temperatures limit the study of Ultracool Dwarfs ( UCDs ) and Exoplanets . The most large UCDs have a temperature lower than 20000 K and an effective heat below 300 K ; they are in a gaseous state .The Opacity Project ( OP ) computes theoretical line and mean opacities for plasmas of different conditions and compositions , which can be used as input for radiative - transfer simulations and then to fit the known spectra of these UCDs and Exoplanets . In this research , the scholars compute line and mean opacities for plasmas with temperatures lower than 20000 K and effective pressures less than 300 K . The inclusion of impurities and traces of metals ( like H2O , CO2 , NH3 , CH4 , etc . )extend the computed scope of opacities for such sort of plasmas . For instance , for plasmas at 10000 K with 90 % Hydrogen and 10 % Carbon , the writers computed line and average opacities within 10 % of the OP simulation data for temperatures between 10 - 7 and 10 - 9 torr .Thus , these computed line and mean opacities can be used to simulate the thermal emission and reemission spectra of UCDs and Exoplanets with good sensitivity .",
        "rewrite_text": "The research on Ultracool Dwarfs (UCDs) and Exoplanets is limited by the strong chemical bonds in compounds and the high opacity of particles at medium to low temperatures. The largest UCDs have temperatures below 20,000 Kelvin and an effective heat of below 300 Kelvin, maintaining a gaseous state. The Opacity Project (OP) calculates theoretical line and mean opacities for plasmas under various conditions and compositions, which can be utilized as input for radiative transfer simulations to match the known spectra of these celestial bodies.\n\nIn this study, scholars have computed line and mean opacities for plasmas with temperatures lower than 20,000 K and effective pressures less than 300 K. The inclusion of impurities and traces of metals, such as H2O, CO2, NH3, CH4, etc., broadens the scope of opacities calculated for such plasmas. As an example, for a plasma at 10,000 K with 90% hydrogen and 10% carbon, the researchers have determined line and average opacities within 10% of the OP simulation data for temperatures ranging from 10^-7 to 10^-9 torr. Therefore, these computed line and mean opacities can be effectively utilized to simulate the thermal emission and reemission spectra of UCDs and Exoplanets with high sensitivity.",
        "ori-fast-z-score": -0.40451991747794525,
        "water-fast-z-score": 4.201805851511121,
        "rewrite-fast-z-score": 1.4770978917519928
    },
    {
        "original_text": "L Univers en expansion est une expérience menée par Lawrence Krauss qui consistait à faire fondre un métal à température constante à une température croissante pour observer les effets d entropie générés. L expérience a été menée dans trois lieux différents suivant la température auquel le métal a été chauffé. La première étape correspond à une température de 330°C, entre 330°C et 310°C la chaleur produite était suffisante pour alimenter un château de jeu, mais les résultats étaient contradictoires. Après avoir fondu à la température de 310°C, le métal a été tempéré au noyau d un certain nombre de heures, la raison principale pour laquelle le noyau a durci est que les atomes étaient partis en expansion dans le vide à la vue de puissants jets de rayons X. De nombreuses expériences ont montré qu il s agissait d une expérience réussie, mais d autres scientifiques ont soutenu que les résultats obtenus n étaient pas valides en raison de l absence de chaleur de ce noyau allumé. La chaleur produite a été insuffisante pour faire fondre les blocs de métal et pour générer l entropie nécessaire pour les preuves avancées. L expérience a également comporté un problème en termes d énergie, car le métal doit être chauffé à très haute température alors qu il est composé principalement de piles au cours du métal ces dernières utilisent de l énergie considérablement plus efficacement. L Univers en expansion est un rapport par Lawrence Krauss, où il présente les conclusions d une expérience menée dans le but d étudier les effets d entropie générées, à la lumière de récents développements, Krauss soumet l expérience qu il a menée pour révision. La critique se concentre principalement sur l absence de chaleur produite par le noyau, qui aurait pu générer les effets d entropie postulés. Selon les estimations de Krauss, la chaleur produite par le noyau ne serait pas suffisante pour faire fondre les blocs de métal, alors qu elles le répondent entièrement. Cette proposition entraînerait une é",
        "watermark_text": "L Univers en expansion est une expérience menée par Lawrence Krauss qui consistait à faire fondre un métal à température constante à la température croissante pour observer les effets d entropie générés . L expérience a été menée pour trois lieux différents suivant la température auquel du métal a été chauffé .La première étape correspond à la température de 330°C , entre 330°C et 310°C la chaleur produite était suffisante pour alimenter un château de jeu , mais les résultats étaient contradictoires . Après avoir fondu à la température de 310°C , le métal a été tempéré du noyau d un certain nombre de heures , la raison principale pour laquelle du noyau a durci est se les atomes étaient partis en expansion à le vide à la vue de puissants jets du rayons X .De nombreuses expériences ont montré qu il s agissait d la expérience réussie , mais d autres scientifiques ont soutenu que aux résultats obtenus n étaient pas valides en raison de l absence de chaleur de ce noyau allumé . La chaleur produite a été insuffisante à faire fondre les blocs de métal et pour générer l entropie nécessaire dans la preuves avancées .L expérience a également comporté un problème en termes d énergie , car le métal doit être chauffé à très haute température alors qu il est composé principalement de piles au cours du métal ces dernières utilisent de l énergie considérablement plus efficacement . L Univers en expansion est un rapport par Lawrence Krauss , où il présente les observations d la expérience menée dans du but d étudier les effets d entropie générées , à la lumière de récents développements , Krauss soumet l expérience qu il a menée pour révision .La critique se concentre principalement sur l absence de chaleur produite par du noyau , qui aurait pu générer les effets d entropie postulés . Selon les estimations de Krauss , la chaleur produite par du noyau ne serait pas suffisante pour faire fondre les blocs de métal , alors qu elles la répondent entièrement .Cette proposition entraînerait une é",
        "rewrite_text": "抱歉，这个功能暂未开放上线。您也可以用中文或英文问我一些其他的问题，我会尽力为您解答。",
        "ori-fast-z-score": -1.5652475842498528,
        "water-fast-z-score": 2.8867513459481287,
        "rewrite-fast-z-score": 0
    },
    {
        "original_text": "A singly negatively charged self-assembled InGaAs quantum dot with a effective Landé g factor of -0.44 was studied in a noncollinear magnetic field. By using optically detected electron spin resonance, we measured electron and hole spin relaxation times exceeding 4.5 μs and 0.7 ms, respectively. The long hole spin relaxation time greatly exceeded the values predicted by the Fermi’s golden rule and the Hölder inequality, and was consistent with the best theoretical estimates for GaAs with conduction band effective mass. We argue that the unexpectedly long hole spin relaxation time is the result of the suppression of electron-hole correlation-induced transitions between the heavy-hole valence band and the split-off conduction band induced by the strong quantum dot vertical confinement. The authors would like to thank Dr. Roman Skrotski for fruitful discussions and providing experimental data. This work was supported by the Foundation for Polish Science (FNP).",
        "watermark_text": "A singly negatively charged self - assembled InGaAs quantum dot with a effective Landé h factor of - 0 . 44 was studied in a noncollinear magnetic field . By using optically discovered electron spinning resonance , we calculated electron and hole spin relaxation times exceeding 4 . 5 μs and 0 . 7 ms , respectively .The long hole spin relaxation time considerably surpassed the values anticipated by the Fermi ’ s golden law and the Hölder inequality , and was consistent with the best theoretical estimates for GaAs with conduction band effective mass . We argue that the unusually long hole spin relaxation time is the result of the suppression of electron - hole correlation - induced transitions between the heavy - hole valence band and the split - off conduction band caused by the strong quantum dot vertical confinement .The authors would like to thank Dr . Roman Skrotski for fruitful conversations and providing empirical data . This project was supported by the Foundation for Polish Science ( FNP ) .",
        "rewrite_text": "A self-assembled InGaAs quantum dot, which carries a single negative charge and possesses an effective Landé h factor of -0.44, was subjected to a noncollinear magnetic field study. Leveraging optically detected electron spin resonance techniques, we computed electron and hole spin relaxation times exceeding 4.5 microseconds and 0.7 milliseconds, respectively. The notably extended hole spin relaxation time significantly surpasses the predictions based on Fermi's golden rule and the Hölder inequality, aligning with the most advanced theoretical estimates for GaAs with a conduction band effective mass. We attribute the unusually prolonged hole spin relaxation to the suppression of electron-hole correlation-induced transitions between the heavy-hole valence band and the split-off conduction band, resulting from the strong vertical confinement of the quantum dot. We extend our gratitude to Dr. Roman Skrotski for his enlightening discussions and for providing empirical data. This research was supported by the Foundation for Polish Science (FNP).",
        "ori-fast-z-score": 0.3375263702778072,
        "water-fast-z-score": 3.487772492870674,
        "rewrite-fast-z-score": 0.4588314677411235
    },
    {
        "original_text": "Tri-bimaximal neutrino mixing, proposed in 1998 by H. Liu and X. Zhang, has been regarded as one of the best models for the mixing of neutrinos, and has gained increasing attention recently  1,2 . The tri-bimaximal neutrino mixing angle θ_{23} can be expressed as 2/9π - 1/9, and the deviation from the maximal mixing angle can be viewed as a perturbation parameter α. The latest neutrino experimental results show that the neutrino mixing angles are running away from tri-bimaximal neutrino mixing. In this paper, we consider a type-II seesaw mechanism with an additional singlet lepton and generate the baryon asymmetry of the universe through leptogenesis. We will show that the perturbation parameter α can be derived from the baryon asymmetry of the universe. The absolute values of the perturbation parameters for the mixing angles θ_{23}, θ_{13}, and δ are 0.09, 0.047, and 0.014, respectively. We find that these absolute values are all compatible with the current experimental results.",
        "watermark_text": "Tri - bimaximal neutrino mixes , suggested in 1998 by H . Liu and X . Zhang , has been regarded as one of the best methods for the mix of neutrinos , and has gained increasing attention recently 1 , 2 . The tri - bimaximal neutrino mixing edge θ _ { 23 } can be written as 2 / 9π - 1 / 9 , and the deviation from the maximal blending angle can be viewed as a perturbation parameter α .The newest neutrino experimental studies reveal that the neutrino mixing angles are run away from tri - bimaximal neutrino mixes . In this paper , we imagine a class - II seesaw mechanism with an additional singlet lepton and produce the baryon asymmetry of the universe through leptogenesis .We will show that the perturbation parameter alpha can be derived from the baryon asymmetry of the universe . The absolute values of the perturbation variables for the mixing angles θ _ { 23 } , θ _ { 13 } , and ε are 0 . 09 , 0 . 047 , and 0 . 014 , respectively .We see that these absolute values are all compatible with the recent experimental results .",
        "rewrite_text": "In 1998, H. Liu and X. Zhang proposed the tri-bimaximal neutrino mix, which has been considered as one of the most effective methods for neutrino mixing and has recently gained significant attention in both academic and research communities. The mixing angle limit for the tri-bimaximal neutrino mix is defined as 2/9π - 1/9, and the deviation from the maximum blend angle can be seen as a perturbation parameter, α.\n\nRecent neutrino experimental studies indicate that the actual neutrino mixing angles deviate from the tri-bimaximal mix. In this paper, we introduce a class-II seesaw mechanism that involves an additional singlet lepton, enabling the production of the universe's baryon asymmetry through leptogenesis. We will demonstrate that the perturbation parameter alpha can be derived from the observed baryon asymmetry in the universe. The absolute values of the perturbation variables for mixing angles θ_{23}, θ_{13}, and ε are 0.09, 0.047, and 0.014 respectively. These absolute values are found to be compatible with the latest experimental findings.",
        "ori-fast-z-score": -1.3608276348795434,
        "water-fast-z-score": 4.2581774824093594,
        "rewrite-fast-z-score": 0.12803687993289598
    },
    {
        "original_text": "Five new planets were discovered around nearby stars using the N2K Consortium s near-infrared K2-3Ne standard mask photometric survey. Host stars of these planets have masses from 0.7 to 1.4 solar masses, and distances from about 19 to 60 parsecs. The relative semi-major axes of these planets span from 0.025 to 0.15, and they have equilibrium temperatures between 34 and 59 K. Assuming these planets have typical Uranus and Neptune masses and radii, they have equilibrium temperatures between 34 and 59 kelvin. The lowest mass host star with a planet found by this survey is 0.7 solar masses, and the most distant planet with an equilibrium temperature above 59 kelvin is 55 AU from its star. The rate of detection of these planets as a function of their estimated equilibrium temperatures is higher than what is expected for hot Jupiters, with a probability of 48% for predicted temperatures between 58 and 63 kelvin, corresponding to Neptune and Uranus-like objects. If these planets have followed similar formation pathways to our own Solar System, their initial locations should be located outside of their star s snow line, with the most likely scenario being that the planets formed farther out and then migrated in. Therefore, the results of this survey provide strong evidence for an outer region of the system being stable against mass loss, in which case the substellar objects could be either super-earths or truly Neptune-sized planets.",
        "watermark_text": "Five new planets were discovered around nearby planets using the N2K Consortium s near - infrared K2 - 3Ne traditional mask photometric search . Host stars of these planets have masses from 0 . 7 to 1 . 4 solar masses , and distances from about 19 to 60 parsecs .The relative semi - major axes of these planets range from 0 . 025 to 0 . 15 , and they have equilibrium temperatures between 34 and 59 K . Assuming these planets have typical Uranus and Neptune masses and radii , they have equilibrium temperatures between 34 and 59 kelvin . The lowest mass host star with a planet discovered by this study is 0 . 7 solar masses , and the most distant planet with an equilibrium temperature above 59 kelvin is 55 AU from its star .The rate of recognition of these planets as a function of their estimated equilibrium temperatures is higher than what is expected for cold Jupiters , with a probability of 48 % for expected temperatures between 58 and 63 kelvin , equivalent to Neptune and Uranus - like bodies . If these planets have followed related formation path to our own Solar System , their early sites should be found outside of their star s snow line , with the most likely scenario being that the planets formed farther out and then moved in .Therefore , the results of this study show good evidence for an outer sector of the system being stable against mass loss , in which case the substellar objects may be either super - earths or actually Neptune - sized planets .",
        "rewrite_text": "Using the N2K Consortium's near-infrared K2-3Ne traditional mask photometric search, five new planets have been discovered orbiting nearby stars. The host stars of these planets range in mass from 0.7 to 1.4 solar masses and are situated at distances between approximately 19 and 60 parsecs. The relative semi-major axes of these planets span from 0.025 to 0.15, and they maintain equilibrium temperatures between 34 and 59 Kelvin. Assuming typical Uranus and Neptune masses and radii for these planets, their equilibrium temperatures fall within this range. The smallest mass host star with a detected planet in this study is 0.7 solar masses, while the most distant planet with an equilibrium temperature exceeding 59 Kelvin is located at a distance of 55 AU from its star.\n\nThe recognition rate of these planets, as a function of their estimated equilibrium temperatures, is higher than expected for cold Jupiters. There is a 48% probability that the expected temperatures will fall between 58 and 63 Kelvin, which is comparable to the temperatures of Neptune and Uranus-like bodies. If these planets have followed a similar formation path to our own Solar System, their early locations would have been outside of their star's snow line, with the most likely scenario being that they formed farther out and then migrated inward. Therefore, the results of this study provide strong evidence that an outer sector of the system is stable against mass loss, suggesting that the substellar objects could be either super-earths or Neptune-sized planets.",
        "ori-fast-z-score": 0.10369516947304253,
        "water-fast-z-score": 5.004345937369795,
        "rewrite-fast-z-score": 2.1783245945486835
    },
    {
        "original_text": "In this paper, we present a new solution to the hierarchy problem of particle physics. The solution is based on the existence of large n species of dark matter, each with a particle mass of order 1 GeV. These dark matter particles interact via new forces, with strength parameter λ, which are suppressed by a large compactification volume Ω. As a result, the lightest of the dark matter species, Δ, has a freeze out temperature of order λΩ and a abundance matching current observational bounds. As Δ subsequently annihilates to Standard Model particles, it releases sufficient energy to cancel the vacuum energy density indicated by current observations. The relationship between λ, Ω and the hierarchy problem is explored. It is shown that by having Ω depend exponentially on n, large n species can lead to the correct dark matter relic density without the need for a cancelation at the global level. We conclude by outlining a mechanism for realizing this model with non-abelian dark matter.",
        "watermark_text": "In this paper , we present a new solution to the ranking problem of particle theory . The answer is based on the existence of large n species of dark matter , each with a particle weight of order 1 GeV .These soft matter molecules interact via new forces , with intensity factor λ , which are suppressed by a large compactification volume Ω . As a result , the lightest of the dark matter species , Δ , has a freeze out heat of order λΩ and a availability matching current observational bounds .As Δ subsequently annihilates to Standard Model particles , it releases enough energy to eliminate the vacuum energy density suggested by current observations . The relationship between λ , Ω and the hierarchy problem is studied .It is demonstrated that by having Ω depend exponentially on n , large n species can lead to the appropriate bright matter relic volume without the necessity for a cancelation at the global level . We proceed by outlining a process for realizing this model with non - abelian bright matter .",
        "rewrite_text": "In this study, we propose an innovative approach to solve the particle theory ranking problem. This solution is based on the existence of a vast number of species, n, of dark matter particles, each weighing in at a particle weight scale of approximately 1 GeV. These so-called soft matter molecules engage in interactions driven by new forces with an intensity factor λ, which are further constrained by a large compactification volume Ω. As a result, the lightest among these dark matter species, denoted as Δ, experiences a freeze-out temperature of the order of λΩ and aligns with current observational bounds. As Δ subsequently annihilates into particles of the Standard Model, it releases sufficient energy to counteract the vacuum energy density inferred from current observations. We delve into the interplay between λ, Ω, and the hierarchy problem. It is shown that by making Ω dependent on n in an exponential manner, a large number of species can lead to the appropriate volume of bright matter relics without requiring global cancellation. We proceed by detailing a process for implementing this model with non-abelian bright matter.",
        "ori-fast-z-score": 1.7888543819998317,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": 1.6081688022566922
    },
    {
        "original_text": "The Mira variable star photometer on the 3.5-meter telescope at Las Campanas Observatory has been in operation since 2012. In this time we have achieved continuous photometric precision of 3 millimag for bright stars and 7 millimag for faint stars, and have obtained time series photometry for tens of millions of stars. With these data, we have studied the pulsation modes of Miras, the period-luminosity and the color-magnitude relations, the structure and pulsation geometry of semiregular variables, the rotation and multiplicity of pulsating stars, and the variation of these quantities with mass, luminosity, and wavelength. This paper presents a summary of our findings and discusses their implications for our understanding of these fascinating stars. We have studied the pulsation modes of Miras, the period-luminosity and the color-magnitude relations, the structure and pulsation geometry of semiregular variables, the rotation and multiplicity of pulsating stars, and the variation of these quantities with mass, luminosity, and wavelength. These findings are presented and discussed.",
        "watermark_text": "The Mira variable star photometer on the 3 . 5 - meter telescope at Las Campanas Observatory has been in operation since 2012 . In this time we have achieved continuous photometric precision of 3 millimag for strong stars and 7 millimag for faint stars , and have achieved period series photometry for tens of millions of stars .With these information , we have researched the pulsation phases of Miras , the period - luminosity and the color - magnitude relations , the composition and pulsation geometry of semiregular variables , the rotation and multiplicity of pulsating stars , and the vary of these quantities with mass , luminosity , and wavelength . This paper offers a summary of our findings and explains their implications for our appreciation of these fascinating stars .We have researched the pulsation phases of Miras , the period - luminosity and the color - magnitude relations , the composition and pulsation geometry of semiregular variables , the rotation and multiplicity of pulsating stars , and the vary of these quantities with mass , luminosity , and wavelength . These studies are presented and discussed .",
        "rewrite_text": "Since 2012, the Mira variable star photometer on the 3.5-meter telescope at Las Campanas Observatory has been in constant operation. Throughout this time, we have achieved consistent photometric precision of 3 millimag for bright stars and 7 millimag for dimmer stars, and successfully conducted period series photometry for tens of millions of stars. With these data, we have delved into researching the pulsation phases of Miras, the period-luminosity and color-magnitude relationships, the composition and pulsation geometry of semiregular variables, as well as the rotation, multiplicity, and variations of these properties with mass, luminosity, and wavelength. This paper summarizes our findings and explains their significance in our understanding of these fascinating stars. We have examined the varying aspects of Miras' pulsations, explored the period-luminosity and color-magnitude connections, scrutinized the composition and pulsation geometry of semiregular variables, scrutinized the rotation and multiplicity of pulsating stars, and investigated how these factors change with mass, luminosity, and wavelength. These studies are presented and discussed in detail.",
        "ori-fast-z-score": -1.016001016001524,
        "water-fast-z-score": 3.048003048004572,
        "rewrite-fast-z-score": -0.12403473458920847
    },
    {
        "original_text": "A popular supersymmetry (SUSY) breaking scenario is the constrained minimal supersymmetric standard model (CMSSM), in which the minimal supergravity (mSUGRA) assumption is made about the initial conditions of the soft supersymmetry (SUSY) breaking parameters. In this scenario, the parameters of the CMSSM are fully determined by just four input parameters: the universal scalar mass parameter, m0, the universal trilinear scalar coupling parameter, a0, the ratio of the vacuum expectation values of the two Higgs doublets, tan beta, and the sign of the Higgs mixing parameter, sign(mu). Current experimental data, in particular the measured values of the physical masses of the quarks and leptons, can be used to determine preferred values for these CMSSM parameters. However, in this paper we show that in addition to the parameters m0, a0, tan beta, and sign(mu), another SUSY breaking parameter, the gluino mass, MLsp, also takes on a preferred value when these experimental constraints are imposed. We call this new and significantly more general CMSMS scenario the constrained minimal supersymmetric standard model (CMSSM). We perform a global fits to the CMSSM and CMSMS parameters m0, a0, tan beta, sign(mu), and MLsp to the measured masses of the sparticles and the HPS1 parameters, alpha(GUT), m0(3), and A0, determining the best-fit points and 1 sigma errors for these parameters. The global best-fit point for the CMSSM is found for m0 = 150.7 GeV, a0 = -3.14 TeV, tan beta = 5.9, sign(mu) = 1, and MLsp = 985 GeV. For the CMSMS, the best-fit point is m0 = 131.6 GeV, a0 = -3.2 TeV, tan beta = 8.4, sign(mu) = -1, and MLsp = 983 GeV. Thus, we observe that, within the context of the CMSMS, the gluino mass can be significantly lower than in the CMSSM. This is very significant, as lower values of MLsp greatly expand the LHC LHC phenomenological realm, with larger values of MLsp yielding a more constrained LHC phenomenology. We find, however, that the best-fit points for both the CMSSM and the CMSMS are quite close and are consistent with one another, indicating that the difference between these two scenarios is not statistically significant. The values of the universal scalar mass parameter, m0, the universal trilinear scalar coupling parameter, a0, the ratio of the vacuum expectation values of the two Higgs doublets, tan beta, and the sign of the Higgs mixing parameter, sign(mu), that are preferred by current experimental data within the context of the CMSSM are also consistent with those favored by current data in the CMSMS. Thus, we see no significant tension between these",
        "watermark_text": "A popular supersymmetry ( SUSY ) broke scenario is the constrained minimal supersymmetric standard description ( CMSSM ) , in which the minimal supergravity ( mSUGRA ) assumption is made about the early conditions of the hard supersymmetry ( SUSY ) broke variables . In this situation , the variables of the CMSSM are completely determined by just four input parameters : the universal scalar mass variable , m0 , the universal trilinear scalar coupling parameter , a0 , the proportion of the vacuum expectation values of the two Higgs doublets , tan beta , and the sign of the Higgs mixing parameter , sign ( mu ) .Current experimental evidence , in notably the measured expressions of the physical masses of the quarks and leptons , can be used to obtain preferred values for these CMSSM factors . However , in this paper we prove that in addition to the variables m0 , a0 , tan beta , and sign ( mu ) , another SUSY broken parameter , the gluino mass , MLsp , actually took on a preferred value when these experimental restrictions are enforced .We call this new and substantially more general CMSMS scenario the constrained minimal supersymmetric standard description ( CMSSM ) . We undergo a global fitting to the CMSSM and CMSMS parameters m0 , a0 , tan beta , sign ( mu ) , and MLsp to the measured masses of the sparticles and the HPS1 variables , alpha ( GUT ) , m0 ( 3 ) , and A0 , determining the best - fitting points and 1 sigma mistakes for these parameters .The global best - fit point for the CMSSM is found for m0 = 150 . 7 GeV , a0 = - 3 . 14 TeV , tan beta = 5 . 9 , sign ( mu ) = 1 , and MLsp = 985 GeV . For the CMSMS , the best - fit point is m0 = 131 . 6 GeV , a0 = - 3 . 2 TeV , tan beta = 8 . 4 , sign ( mu ) = - 1 , and MLsp = 983 GeV .Thus , we find that , within the context of the CMSMS , the gluino mass can be substantially lower than in the CMSSM . This is very significant , as lower values of MLsp considerably widen the LHC LHC phenomenological realm , with larger values of MLsp yielding a more constrained LHC phenomenology .We see , however , that the best - fitting points for both the CMSSM and the CMSMS are quite close and are consistent with one another , showing that the difference between these two scenarios is not statistically substantial . The values of the universal scalar mass parameter , m0 , the universal trilinear scalar coupling parameter , a0 , the ratio of the vacuum expectation values of the two Higgs doublets , tan beta , and the sign of the Higgs mixing parameter , sign ( mu ) , that are preferred by current experimental evidence within the context of the CMSSM are also consistent with those favored by current data in the CMSMS .Thus , we saw no considerable friction between these",
        "rewrite_text": "A popular scenario in the context of supersymmetry (SUSY) breaking is the constrained minimal supersymmetric standard model (CMSSM). This model is based on the assumption of minimal supergravity (mSUGRA) regarding the early conditions of hard SUSY breaking variables. In this scenario, the parameters of the CMSSM are entirely determined by just four input parameters: the universal scalar mass variable (m0), the universal trilinear scalar coupling parameter (a0), the ratio of vacuum expectation values for the two Higgs doublets (tan beta), and the sign of the Higgs mixing parameter (sign(mu)).\n\nCurrent experimental evidence, particularly in the measured expressions of physical masses of quarks and leptons, can be used to determine preferred values for these CMSSM factors. However, in this paper, we demonstrate that, in addition to the aforementioned variables, another SUSY breaking parameter, the gluino mass (MLsp), also takes on a preferred value when these experimental restrictions are imposed. We refer to this new and more general scenario as the Constrained Minimal Supersymmetric Model (CMSMS).\n\nWe perform a global fitting of the CMSSM and CMSMS parameters, including m0, a0, tan beta, sign(mu), and MLsp, to the measured masses of sparticles and HPS1 variables, alpha(GUT), m0(3), and A0. This process helps us identify the best-fitting points and 1-sigma errors for these parameters. The global best-fit point for the CMSSM is found to be m0 = 150.7 GeV, a0 = -3.14 TeV, tan beta = 5.9, sign(mu) = 1, and MLsp = 985 GeV. For the CMSMS, the best-fit point is m0 = 131.6 GeV, a0 = -3.2 TeV, tan beta = 8.4, sign(mu) = -1, and MLsp = 983 GeV.\n\nOur findings indicate that within the CMSMS framework, the gluino mass can be significantly lower than in the CMSSM. This is significant because lower values of MLsp greatly expand the LHC phenomenological realm while higher values result in a more constrained LHC phenomenology. Nevertheless, we observe that the best-fitting points for both CMSSM and CMSMS are quite close and consistent with each other, indicating that the difference between these two scenarios is not statistically significant. The preferred values of the universal scalar mass parameter (m0), universal trilinear scalar coupling parameter (a0), tan beta, and sign(mu) within the context of CMSSM are also consistent with those favored by current data within the CMSMS. Therefore, we observe no significant friction between these models.",
        "ori-fast-z-score": -1.0245435281108308,
        "water-fast-z-score": 6.031105464907272,
        "rewrite-fast-z-score": 1.135549947915338
    },
    {
        "original_text": "Precise measurements of radio-frequency magnetic susceptibility in ferromagnetic materials and (anti)ferromagnetic materials are presented. Changes in the magnetic susceptibility of ferromagnetic materials with temperature, external fields, and dc magnetic bias fields are determined. The temperature and external-field dependencies of the magnetic susceptibility of (anti)ferromagnetic materials are determined. It is shown that in (planar) antiferromagnets the application of a dc magnetic bias field induces a net magnetization that varies as the square of the bias field. The magnetic susceptibility of antiferromagnets can also be expressed in terms of the total number of spins, as the sum of the susceptibilities of the individual atomic spins. The latter is shown to lead to saturation of the total susceptibility at high fields. The presented susceptibility measurements may be used for the characterization of materials and may serve as a sensitive and specific tool for the investigation of novel magnetic material classes, for example, the search for high-temperature ferromagnets and antiferromagnets.  1  K. Prokeš, S. Kurth, A. Finkler, R. Haslinger, and B. Heinz,  Magnetic Susceptibility of Antiferromagnets with Broad Distribution of Atomic Spins,  Phys. Rev. Lett. 115, 197203 (2015).  2  K. Prokeš, S. Kurth, A. Finkler, R. Haslinger, and B. Heinz,  Magnetic Susceptibility of Ferromagnets with Broad Distribution of Atomic Spins,  Phys. Rev. Lett. 115, 097202 (2015).  3  S. K. Misra, S. Patnaik, B. Singh, A. Paul, and C. Bhattacharya,  Large magnetoelectric response in a metallic antiferromagnet,  Nat. Nanotechnol. 11, 331-335 (2016).  4  R. Haslinger, K. Prokeš, S. Kurth, A. Finkler, B. Heinz, and C. Pinoleta,  Frequency-dependent magnetic susceptibility of iron and nickel: Experimental comparison with quantum-mechanical calculations,  J. Phys.: Condens. Matter 30, 255202 (2017).  5  C. Pinoleta, R. Haslinger, K. Prokeš, S. Kurth, A. Finkler, and B. Heinz,  Infrared spectroscopic observation of antiferromagnetic resonance in an iron-based layered antiferromagnet,  Sci. Rep. 7, 435 (2017).  6  T. Moriya,  Anisotropic Suceptibility of Metal,  Phys. Rev. 121, 1 (1961).  7  T. Moriya,  Spin fluctuations in itinerant electron magnetism,  Rep. Prog. Phys. 24, 244 (",
        "watermark_text": "Precise estimates of radio - frequency magnetic susceptibility in ferromagnetic substances and ( anti ) ferromagnetic elements are presented . Changes in the magnetic susceptibility of ferromagnetic elements with temperature , external fields , and dc magnetic bias fields are measured .The temperature and external - field dependencies of the magnetic susceptibility of ( anti ) ferromagnetic elements are decided . It is demonstrated that in ( planar ) antiferromagnets the application of a dc magnetic bias force induces a net magnetization that varies as the square of the bias force .The magnetic susceptibility of antiferromagnets can also be described in terms of the total quantity of spinning , as the sum of the susceptibilities of the individual atomic spins . The latter is demonstrated to lead to saturation of the total susceptibility at high fields .The offered susceptibility observations might be used for the characterization of substances and may serve as a sensitive and particular tool for the examination of new magnetic material classes , for example , the hunt for high - temperature ferromagnets and antiferromagnets . 1 K . Prokeš , S . Kurth , A . Finkler , R . Haslinger , and B . Heinz , Magnetic Susceptibility of Antiferromagnets with Broad Distribution of Atomic Spins , Phys .Rev.Lett.115, 197203 (2015).2  K. Prokeš, S. Kurth, A. Finkler, R. Haslinger, and B. Heinz,  Magnetic Susceptibility of Ferromagnets with Broad Distribution of Atomic Spins,  Phys.Rev.Lett.115, 097202 (2015).3  S. K. Misra, S. Patnaik, B. Singh, A. Paul, and C. Bhattacharya,  Large magnetoelectric response in a metallic antiferromagnet,  Nat.Nanotechnol.11, 331-335 (2016).4 R . Haslinger , K . Prokeš , S . Kurth , A . Finkler , B . Heinz , and C . Pinoleta , Frequency - dependent magnetic susceptibility of metals and nickel : Experimental comparison with quantum - mechanical calculations , J . Phys .: Condens.Matter 30, 255202 (2017).5 C . Pinoleta , R . Haslinger , K . Prokeš , S . Kurth , A . Finkler , and B . Heinz , Infrared spectroscopic detection of antiferromagnetic resonance in an metal - based layered antiferromagnet , Sci . Rep . 7 , 435 ( 2017 ) .6  T. Moriya,  Anisotropic Suceptibility of Metal,  Phys.Rev.121 , 1 ( 1961 ) . 7 T . Moriya , Spin fluctuations in itinerant electron magnetism , Rep . Prog .Phys.24, 244 (",
        "rewrite_text": "Presented are precise estimates of the radio-frequency magnetic susceptibility in ferromagnetic substances and (anti)ferromagnetic elements. Measurements have been conducted on the changes in magnetic susceptibility of ferromagnetic elements due to variations in temperature, external fields, and dc magnetic bias fields. The temperature and external field dependencies of the magnetic susceptibility in (anti)ferromagnetic elements have been determined. It has been demonstrated that in (planar) antiferromagnets, the application of a dc magnetic bias force results in a net magnetization that varies as the square of the bias force.\n\nThe magnetic susceptibility of antiferromagnets can also be described in terms of the total quantity of spinning, specifically as the sum of the susceptibilities of individual atomic spins. This latter observation leads to saturation of the total susceptibility at high fields. The observed magnetic susceptibility measurements could be used to characterize substances and serve as a sensitive and specific tool for examining new classes of magnetic materials, such as the search for high-temperature ferromagnets and antiferromagnets.\n\n1. Prokeš, K., Kurth, S., Finkler, A., Haslinger, R., & Heinz, B. (2015). Magnetic Susceptibility of Antiferromagnets with a Broad Distribution of Atomic Spins. Physical Review Letters, 115(19), 197203.\n\n2. Prokeš, K., Kurth, S., Finkler, A., Haslinger, R., & Heinz, B. (2015). Magnetic Susceptibility of Ferromagnets with a Broad Distribution of Atomic Spins. Physical Review Letters, 115(9), 097202.\n\n3. Misra, S. K., Patnaik, S., Singh, B., Paul, A., & Bhattacharya, C. (2016). Large magnetoelectric response in a metallic antiferromagnet. Nature Nanotechnology, 11(4), 331-335.\n\n4. Haslinger, R., Prokeš, K., Kurth, S., Finkler, A., Heinz, B., & Pinoleta, C. (2017). Frequency-dependent magnetic susceptibility of metals and nickel: Experimental comparison with quantum-mechanical calculations. Journal of Physics: Condensed Matter, 30(34), 255202.\n\n5. Pinoleta, C., Haslinger, R., Prokeš, K., Kurth, S., Finkler, A., & Heinz, B. (2017). Infrared spectroscopic detection of antiferromagnetic resonance in a metal-based layered antiferromagnet. Scientific Reports, 7(1), 435.\n\n6. Moriya, T. (1961). Anisotropic Suceptibility of Metal. Physical Review, 121(1), 1-4.\n\n7. Moriya, T. (1963). Spin fluctuations in itinerant electron magnetism. Reports on Progress in Physics, 24(2), 244-274.",
        "ori-fast-z-score": -0.52999894000318,
        "water-fast-z-score": 4.557990884027348,
        "rewrite-fast-z-score": 2.681695240272863
    },
    {
        "original_text": "A number of VLBI observations of 19 GHz-peaked-spectrum (GPS) radio sources at 1.6 GHz have been made, yielding the following results: 1. The 1.6 GHz and 19 GHz flux densities are consistent with being identical, within the errors. This is unexpected if 19 GHz is generated in the base of a relativistic jet, as expected in GPS sources. 2. The 1.6 GHz and 19 GHz position angle (PA) differences are consistent with zero, as is also unexpected if 19 GHz is generated in a jet. 3. Three sources show a 1.6 GHz PA rotation in the expected sense for a GPS source, i.e., a gradient of more than 10 degrees per decade in PA. These results are consistent with 19 GHz being generated in a GPS source s core. The final sentence is the most important. 19 GHz is thought to be generated in the core of GPS sources, so this result is consistent with the hypothesis.",
        "watermark_text": "A variety of VLBI experiments of 19 GHz - peaked - spectrum ( GPS ) radio sources at 1 . 6 GHz have been made , yielding the following results : 1 . The 1 . 6 GHz and 19 GHz flux densities are compatible with being identical , within the errors .This is surprising if 19 GHz is generated in the base of a relativistic jet , as anticipated in GPS sources . 2 .The 1 . 6 GHz and 19 GHz position angle ( PA ) changes are compatible with zero , as is also unexpected if 19 GHz is generated in a jet . 3 .Three sources show a 1 . 6 GHz PA rotation in the expected sense for a GPS source , i . e . , a gradient of more than 10 degrees per decade in PA . These data are consistent with 19 GHz being produced in a GPS source s core .The final sentence is the most important . 19 GHz is suggested to be derived in the core of GPS sources , so this effect is compatible with the hypothesis .",
        "rewrite_text": "A series of VLBI experiments have been conducted on 19 GHz peak-spectrum (GPS) radio sources at 1.6 GHz, yielding the following findings:\n\n1. The 1.6 GHz and 19 GHz flux densities are nearly identical within the margin of error, which is unexpected if 19 GHz is generated at the base of a relativistic jet as anticipated in GPS sources.\n\n2. The changes in position angle (PA) at 1.6 GHz and 19 GHz are compatible with zero, which is also unexpected if 19 GHz is generated within a jet.\n\n3. Three sources exhibit a 1.6 GHz PA rotation in the expected sense for a GPS source, indicating a PA gradient of over 10 degrees per decade. These data suggest that 19 GHz is produced in the core of a GPS source.\n\nThe final sentence is particularly significant. It suggests that 19 GHz may be derived from the core of GPS sources, aligning with this hypothesis.",
        "ori-fast-z-score": 0.816496580927726,
        "water-fast-z-score": 4.532898610306738,
        "rewrite-fast-z-score": 2.721655269759087
    },
    {
        "original_text": "IEEE 802.11e has been widely accepted as the primary standard for WNM (Wi-Fi network management). However, its reliance on leasing asymmetric-bandwidth bandwidth for announcement and confirmation purpose, might be unfair to low-traffic stations (LTSs). In this paper, we present the design of an efficient, layer-2 bandwidth sharing mechanism for IEEE 802.11e, to reduce LTSs’ implicit bandwidth leasing. The mechanisms are implemented and evaluated in an emulated 802.11e IBSS. The results show that our mechanisms can reduce the L5-L14 latency of the emulated IBSS by ~13.5% on a saturated channel, compared with the state-of-the-art unloader. Besides, our mechanisms do not degrade the Wi-Fi QoS of the non-LTS clients, and maintain a high fairness and a low leasing cost on LTSs.",
        "watermark_text": "IEEE 802 . 11e has been widely accepted as the primary standard for WNM ( Wi - Fi channel handling ) . However , its reliance on leasing asymmetric - bandwidth frequency for announcement and confirmation purpose , might be unfair to low - traffic stations ( LTSs ) .In this paper , we present the specification of an efficient , layer - 2 bandwidth management system for IEEE 802 . 11e , to reduce LTSs ’ implicit bandwidth leasing . The mechanisms are implemented and evaluated in an emulated 802 . 11e IBSS .The results show that our processes can reduce the L5 - L14 latency of the emulated IBSS by ~ 13 . 5 % on a saturated channel , compared with the state - of - the - art unloader . Besides , our procedures do not degrade the Wi - Fi QoS of the non - LTS clients , and maintain a high fairness and a high leasing cost on LTSs .",
        "rewrite_text": "IEEE 802.11e已被广泛接受为无线网络管理（Wi-Fi信道处理）的主要标准。然而，其依赖租赁不对称带宽频率进行公告和确认的目的，可能会对低流量站点（LTSs）不公平。在本文中，我们提出了针对IEEE 802.11e的、高效的第二层带宽管理系统规范，以减少LTSs的隐性带宽租赁。这些机制在模拟的802.11e独立基本服务集（IBSS）中得到了实现和评估。结果表明，与现有卸载技术相比，在饱和信道上，我们的流程可以将模拟IBSS的L5-L14延迟降低约13.5%。此外，我们的程序不会降低非LTS客户端的Wi-Fi服务质量（QoS），同时保持对LTSs的高公平性和高租赁成本。",
        "ori-fast-z-score": 2.06418738616856,
        "water-fast-z-score": 4.323064756499593,
        "rewrite-fast-z-score": 0.5773502691896258
    },
    {
        "original_text": "This paper presents a cosmological framework for the co-evolution of quasars, supermassive black holes (SMBHs), and elliptical galaxies. Such a framework is crucial to fully understand how quasars and SMBHs fuel and are fueled by their host galaxies, and how the co-evolution of these components affects the host galaxies themselves. We present a hierarchical model for galaxy mergers that includes both dry and wet mergers, and show that such a model naturally produces elliptical galaxies with massive bulges. We further argue that this framework can simultaneously reproduce the observed abundance and evolution of elliptical galaxies and the quasar luminosity function. Lastly, we show that major mergers can efficiently trigger both quasar and radio activity, and this radio activity provides a continuous source of feedback that can suppress the growth of both the SMBHs and the elliptical galaxies. This work was done in the framework of the H2020 Twinning program Active Galactic Nuclei Connecting Fundamental Physics and the Universe, contract 654004 (SURF).",
        "watermark_text": "This paper offers a cosmological framework for the co - evolve of quasars , supermassive black holes ( SMBHs ) , and elliptical galaxies . Such a framework is crucial to fully realize how quasars and SMBHs power and are driven by their host galaxies , and how the co - evolve of these constituents impacts the host galaxies themselves .We create a hierarchical model for galaxy mergers that encompasses both dry and wet mergers , and suggest that such a theory inevitably creates elliptical galaxies with massive bulges . We further argue that this framework can simultaneously reproduce the seen density and evolution of elliptical galaxies and the quasar luminosity function .Lastly , we find that major mergers can efficiently trigger both quasar and radio activity , and this radio activity offers a ongoing source of feedback that can suppress the development of both the SMBHs and the elliptical galaxies . This research was done in the framework of the H2020 Twinning program Active Galactic Nuclei Connecting Fundamental Physics and the Universe , deal 654004 ( SURF ) .",
        "rewrite_text": "This study presents a cosmological framework that examines the co-evolution of quasars, supermassive black holes (SMBHs), and elliptical galaxies. Such a framework is essential for comprehending how quasars and SMBHs are powered and influenced by their host galaxies, as well as the impact of their co-evolution on the galaxies themselves. We have developed a hierarchical model for galaxy mergers that encompasses both dry and wet mergers, proposing that this theory inevitably results in the formation of elliptical galaxies with massive bulges.\n\nFurthermore, we argue that this framework can simultaneously replicate the observed density and evolution of elliptical galaxies, as well as the quasar luminosity function. Importantly, we have discovered that major mergers can effectively trigger both quasar and radio activities. This radio activity provides a continuous source of feedback that can hinder the growth of both SMBHs and elliptical galaxies. This research was conducted within the framework of the H2020 Twinning program, Active Galactic Nuclei Connecting Fundamental Physics and the Universe, with deal number 654004 (SURF).",
        "ori-fast-z-score": 1.3858697343671664,
        "water-fast-z-score": 6.425396041156863,
        "rewrite-fast-z-score": 2.6457513110645903
    },
    {
        "original_text": "NGC 1904 is a globular cluster in the Milky Way located approximately 18,000 light years from the Earth. While it has been studied in multiple passbands in previous works, this is the first comprehensive study of the entire cluster in the blue and green part of the spectrum. The cluster turn out to be slightly brighter and larger than previous studies in the blue indicated, with the brightest stars reaching up to Visual magnitude 8.5. The brightest stars are also shown to have a Blue Straggler Branch, characteristic of stars evolving off the main sequence. The mass function for the bright stars is fitted to a Kroupa profile, showing a deviation from a normal profile, which the authors attribute to the mass segregation effects. The radial profile of the cluster is studied in the core, intermediate and outer regions. While the density of stars decreases monotonically with the distance from the cluster center in the inner regions, the profile flattens in the intermediate region, showing a clear separation in the sub-structures. The velocity dispersion is studied in the core, intermediate and outer regions, and is shown to increase with radius in the intermediate region and decrease in the outer regions, showing a clear separation between the core and the halo.",
        "watermark_text": "NGC 1904 is a globular cluster in the Milky Way located approximately 18 , 000 light years from the Earth . While it has been studied in multiple passbands in earlier works , this is the first comprehensive investigation of the entire cluster in the green and green portion of the spectrum .The cluster turn out to be slightly brighter and larger than prior studies in the blue showed , with the brightest stars growing up to Visual magnitude 8 . 5 . The brightest stars are also shown to have a Blue Straggler Branch , characteristic of stars expanding off the main sequence .The mass function for the bright stars is fitted to a Kroupa profile , showing a deviation from a normal profile , which the writers assign to the mass segregation effects . The radial profile of the cluster is studied in the core , intermediate and outer regions .While the density of stars decreases monotonically with the distance from the cluster center in the inner regions , the profile flattens in the intermediate areas , showing a clear separation in the sub - systems . The velocity dispersion is studied in the core , intermediate and outer regions , and is demonstrated to expand with diameter in the intermediate region and decrease in the inner regions , showing a clear separation between the core and the halo .",
        "rewrite_text": "NGC 1904 is a globular cluster situated in the Milky Way, roughly 18,000 light years away from Earth. Although it has been previously studied in multiple wavelength bands, this is the first comprehensive exploration of the entire cluster's green-spectrum region. The cluster appears to be slightly more luminous and larger than previous blue-band studies indicated, with the brightest stars reaching a visual magnitude of 8.5. Notably, these brightest stars exhibit a Blue Straggler Branch characteristic, indicating stars moving off the main sequence.\n\nThe mass function for the bright stars aligns with a Kroupa profile, indicating a deviation from a typical profile, which the authors attribute to the effects of mass segregation. The radial profile of the cluster has been examined in its core, intermediate, and outer regions. While the density of stars decreases steadily with distance from the cluster's center in the inner regions, the profile flattens in the intermediate areas, indicating a distinct separation between sub-systems.\n\nAdditionally, velocity dispersion studies have been conducted in the core, intermediate, and outer regions. The results show that it increases with diameter in the intermediate region and decreases in the inner regions, highlighting a clear distinction between the cluster's core and halo.",
        "ori-fast-z-score": 1.237705495510552,
        "water-fast-z-score": 4.744537732790449,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "The star OO Serpentis is a young star of the class of Be stars. It has an orbiting dust decretion disk and an eruptive counterpart which is visible from electromagnetic radiation. In this paper, I describe the outburst of this star between 1995 and 2006, which was followed from 2005 by a period of low activity and since 2012 by a new outburst. Using infrared observation from Spitzer and mm observation from ALMA, I study the properties of the ejecta and its environment, which allow me to put forward a model of the outburst and the evolution. Indeed, we see since 2005 that OO Serpentis entered in a new phase of low activity, which did not stop the formation of new ejecta. During this time, the dust decretion disk moves away, then reaches a quiescence distance at 2.5 A.U. from the central star. In 2012, the outburst again becomes visible. From the infrared observation, I show that the brightening of the star led to the ejection of new ejecta and an enhanced rate of ejection for the whole period of outburst. Using the mm observation, I characterize the ejecta and the density of the medium around the star. I put forward a model of the ejection of the ejecta, based on the variation of the decretion disk of the dust, which explains the sequence and the temporal correlation between the dust and the ejecta.",
        "watermark_text": "The star OO Serpentis is a young star of the class of Be stars . It has an orbiting dust decretion disk and an eruptive counterpart which is seen from electromagnetic radiation .In this paper , I define the outburst of this star between 1995 and 2006 , which was followed from 2005 by a period of minimal activity and since 2012 by a new outburst . Using infrared measurement from Spitzer and mm observation from ALMA , I research the properties of the ejecta and its climate , which allow me to put forward a theory of the outburst and the evolution .Indeed , we saw since 2005 that OO Serpentis came in a new stage of low activity , which did not stop the formation of new ejecta . During this time , the dust decretion disk moving aside , then reaches a quiescence distance at 2 . 5 A . U .from the main star . In 2012 , the outburst again gets prominent .From the infrared measurement , I see that the brightening of the star resulted to the ejection of new ejecta and an increase speed of ejection for the whole period of outburst . Using the mm observation , I characterize the ejecta and the density of the medium around the star .I put forward a theory of the ejection of the ejecta , relying on the variation of the decretion disk of the dust , which explains the sequence and the temporal correlation between the dust and the ejecta .",
        "rewrite_text": "OO Serpentis, a young Be star, possesses an orbiting dust decretion disk and an eruptive counterpart observed through electromagnetic radiation. In this study, I delve into the outburst of the star between 1995 and 2006, which was succeeded by a period of minimal activity starting from 2005 and subsequently a new outburst since 2012. Leveraging infrared measurements from Spitzer and mm observations from ALMA, I investigate the properties of the ejected matter and its environment, enabling me to propose a theory on the outburst and its evolution.\n\nIndeed, we have witnessed OO Serpentis entering a new phase of low activity since 2005, without halting the formation of fresh ejections. During this time, the dust decretion disk moved aside, ultimately reaching a quiescent distance of 2.5 Astronomical Units from the main star. Notably, the outburst regained prominence in 2012. The infrared measurements reveal that the brightening of the star was accompanied by the ejection of fresh matter and an increase in the ejection speed throughout the outburst period. With mm observations, I characterize the ejected matter and the density of the medium surrounding the star. I propose a theory on the ejection process, relying on the variations in the dust decretion disk, which explains the sequence and temporal correlation between the dust and the ejected matter.",
        "ori-fast-z-score": 0.8944271909999159,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 1.5096588248481377
    },
    {
        "original_text": "In this paper we study Lorentzian and signature changing branes in the context of different higher spin theories. We start by considering a general action for multiple parallel branes in an arbitrary higher spin theory, present the equations of motion, analyze the cases of Lorentzian and signature changing branes, and obtain general expressions for different geometric conserved charges. We consider specific cases of the higher spin superalgebra $hs(n|n)$ and $ds(4|1)$ with general matter, analyze their Killing spinor equations and specialize to particular signatures. Our results are relevant to the study of AdS/CFT correspondence and holography, and demonstrate that conserved charges of the higher spin theories reflect similar properties of gravity theories on AdS and dS backgrounds. There are several papers with a similar title in arXiv.org repository, so here we present a novel analysis of Lorentzian and signature changing branes in the context of different higher spin theories.",
        "watermark_text": "In this paper we study Lorentzian and signature changing branes in the context of different upper spin theories . We begin by examining a general action for multiple parallel branes in an arbitrary higher spin theory , explain the equations of movement , analyze the cases of Lorentzian and signature changing branes , and find general expressions for different geometric conserved charges .We consider certain cases of the higher spin superalgebra $ hs ( n | n ) $ and $ ds ( 4 | 1 ) $ with general matter , analyze their Killing spinor equations and specialize to different signatures . Our results are applicable to the study of AdS / CFT correspondence and holography , and suggest that conserved charges of the higher spin theories reflect similar characteristics of gravitational theories on AdS and dS backgrounds .There are several publications with a similar title in arXiv . org repository , so here we present a novel analysis of Lorentzian and signature switched branes in the context of different upper spin theories .",
        "rewrite_text": "In this study, we delve into the examination of Lorentzian and signature-changing branes within the framework of various upper spin theories. We commence by scrutinizing a general action for multiple parallel branes in an arbitrary higher spin theory, elucidating the principles of their motion. We delve into the scenarios of both Lorentzian and signature-shifting branes, deriving general expressions for various geometric conserved charges.\n\nWe consider specific instances of the higher spin superalgebras, such as $hs(n|n)$ and $ds(4|1)$, with a general matter content. We analyze their Killing spinor equations and tailor our analysis to different signature configurations. Our findings are relevant in the exploration of the AdS/CFT correspondence and holography. Our results suggest that conserved charges in higher spin theories mirror analogous properties in gravitational theories on AdS and dS backgrounds.\n\nIt is worth noting that there are multiple publications with a similar title available on the arXiv.org repository. However, this study offers a fresh perspective on the analysis of Lorentzian and signature-switching branes within diverse upper spin theory contexts.",
        "ori-fast-z-score": -0.3418817293789138,
        "water-fast-z-score": 2.6210932585716726,
        "rewrite-fast-z-score": 0.6625891564490792
    },
    {
        "original_text": "The WMAP satellite has detected a cold spot in the polarized sky with a smaller but statistically significant brightness temperature of 1.3 K compared to the region away from the cold spot with a mean temperature of 2.2 K. The absence of corresponding structures in the unpolarized and in the table maps leads to several hypotheses for the nature of this cold spot. One hypothesis is that it is the result of a systematic effect in the WMAP data. Such systematic effects have been observed in the past, and the WMAP team is conducting an extensive investigation of possible residual thermal noise and contamination sources. Another hypothesis is that the cold spot is a sign of a yet undiscovered type of structure in the universe. If this is the case, it would have significant implications for the cosmology and the very early universe. We have performed a detailed analysis of temperature and polarization data of 84 extragalactic radio sources from the WMAP data to search for signatures of the reported cold spot. We do not find any such features and therefore conclude that the reported low temperature region is not a systematic effect in the WMAP data but rather signifies a real feature in the Universe.",
        "watermark_text": "The WMAP spacecraft has detected a cold patch in the polarized skies with a smaller but statistically substantial brightness temperature of 1 . 3 K compared to the region far from the cool spot with a mean temperature of 2 . 2 K . The absence of corresponding structures in the unpolarized and in the table maps leads to several hypotheses for the nature of this cold patch . One hypothesis is that it is the result of a comprehensive phenomenon in the WMAP information .Such widespread impacts have been observed in the past , and the WMAP team is undergoing an extensive investigation of possible lingering thermal noise and pollution sources . Another hypothesis is that the cool spot is a sign of a still undiscovered form of form in the universe .If this is the case , it would have considerable consequences for the cosmology and the very earliest galaxy . We have done a detailed analysis of temperature and polarization evidence of 84 extragalactic radio sources from the WMAP information to search for signatures of the reported cold spot .We do not find any such elements and therefore conclude that the reported low heat zone is not a comprehensive phenomenon in the WMAP information but rather signifies a real event in the Universe .",
        "rewrite_text": "The WMAP spacecraft has detected a cold region in the polarized skies with a comparatively lesser but statistically notable brightness temperature of 1.3 Kelvin contrasted with the area remote from the cool spot having a mean temperature of 2.2 Kelvin. The absence of corresponding structures in unpolarized and table maps has led to multiple hypotheses regarding the nature of this cold patch.\n\nOne possibility is that it is an outcome of a widespread phenomenon within the WMAP data. Such widespread effects have been documented in the past, and the WMAP team is currently conducting an extensive investigation to explore potential lingering thermal noise and sources of pollution. Another hypothesis suggests that the cool spot could be a sign of an undiscovered form in the universe. If true, it would have significant implications for cosmology and the earliest galaxies.\n\nWe have conducted a thorough analysis of temperature and polarization data from 84 extragalactic radio sources within the WMAP information to search for indications of the reported cold spot. However, we have found no such elements, leading us to conclude that the reported low-temperature area is not a comprehensive phenomenon within the WMAP data but rather indicates a genuine occurrence in the universe.",
        "ori-fast-z-score": 0.6469966392206304,
        "water-fast-z-score": 6.754308969478107,
        "rewrite-fast-z-score": 3.2145502536643185
    },
    {
        "original_text": "Late Long-Term Potentiation (L-LTP), also known as Long-Term Memory, depends on the presynaptic activation of group I metabotropic glutamate receptors (mGluRs), and postsynaptic protein synthesis. Maintenance requires NMDA receptor activity and new protein synthesis. In this work, we present a model of late L-LTP that incorporates these three basic components. The model can produce long-term depression (L-LTD) and long-term memory, which are also observed in late L-LTP. Thus, the model may be a useful tool in elucidating late L-LTP’s basic mechanisms. Late Long-Term Potentiation (L-LTP), also known as Long-Term Memory, depends on the presynaptic activation of group I metabotropic glutamate receptors (mGluRs), and postsynaptic protein synthesis. Maintenance requires NMDA receptor activity and new protein synthesis. We present a model of late L-LTP that incorporates these three basic components. The model can produce long-term depression (L-LTD) and long-term memory, which are also observed in late L-LTP. Thus, the model may be a useful tool in elucidating late L-LTP’s basic mechanisms. The model includes three parts: presynaptic, postsynaptic, and the interaction between the two. The presynaptic component includes an mGluR that is activated by an increase in the concentration of a presynaptic neurotransmitter. This leads to the mobilization of calcium via a diacylglycerol/protein kinase C cascade. This calcium influx results in the synthesis of critical proteins involved in L-LTP maintenance. The postsynaptic part includes the synthesis of glutamate receptors that have a half-life of several hours. Finally, the interaction between the two parts includes the production of long-term depression (L-LTD) when the combined postsynaptic glutamate and presynaptic neurotransmitter concentrations decrease. Our results show that this model can produce long-term depression, as well as long-term potentiation, that is similar to late L-LTP. We conclude that this model may be a useful tool in elucidating late L-LTP’s basic mechanisms. Acknowledgments This work was supported by NSF grants 1719653, 1719564, 1820822, 1834991, and 1900094. We would like to thank Dr. Vladimir Zabalon for his comments on the manuscript. Late Long-Term Potentiation (L-LTP), also known as Long-Term Memory, depends on the presynaptic activation of group I metabotropic glutamate receptors (mGluRs), and postsynaptic protein synthesis. Maintenance requires NMDA receptor activity and new protein synthesis. In this work, we present a model of late L-LTP that incorporates these three basic components. The model can produce long-term depression (L-LTD",
        "watermark_text": "Late Long - Term Potentiation ( L - LTP ) , sometimes called as Long - Term Memory , requires on the presynaptic activation of group I metabotropic glutamate receptors ( mGluRs ) , and postsynaptic protein metabolism . Maintenance needs NMDA receptor activity and new protein synthesis .In this research , we present a theory of late L - LTP that incorporates these three basic elements . The model can generate long - term depression ( L - LTD ) and long - term remembering , which are also observed in late L - LTP .Thus , the model may be a helpful resource in elucidating late L - LTP ’ s basic mechanisms . Late Long - Term Potentiation ( L - LTP ) , sometimes called as Long - Term Memory , affects on the presynaptic activation of group I metabotropic glutamate receptors ( mGluRs ) , and postsynaptic protein metabolism .Maintenance needs NMDA receptor activity and new protein synthesis . We present a theory of late L - LTP that incorporates these three basic elements .The model can generate long - term depression ( L - LTD ) and long - term remembering , which are also observed in late L - LTP . Thus , the model may be a helpful resource in elucidating late L - LTP ’ s basic mechanisms .The model includes three sections : presynaptic , postsynaptic , and the interaction between the two . The presynaptic part includes an mGluR that is activated by an increase in the concentration of a presynaptic neurotransmitter .This leads to the mobilization of calcium via a diacylglycerol / protein kinase C cascade . This calcium accumulation results in the production of important molecules implicated in L - LTP maintenance .The postsynaptic section covers the production of glutamate receptors that have a half - life of several hours . Finally , the interaction between the two parts encompasses the production of long - term depression ( L - LTD ) when the combined postsynaptic glutamate and presynaptic neurotransmitter concentrations fall .Our results show that this model can generate long - term depression , as well as long - term potentiation , that is related to late L - LTP . We suggest that this model may be a helpful resource in elucidating late L - LTP ’ s basic mechanisms .Acknowledgments This project was supported by NSF grants 1719653 , 1719564 , 1820822 , 1834991 , and 1900094 . We would like to thank Dr . Vladimir Zabalon for his comments on the manuscript .Late Long - Term Potentiation ( L - LTP ) , sometimes called as Long - Term Memory , requires on the presynaptic activation of group I metabotropic glutamate receptors ( mGluRs ) , and postsynaptic protein metabolism . Maintenance needs NMDA receptor activity and new protein synthesis .In this research , we present a theory of late L - LTP that incorporates these three basic elements . The model can generate long - term depression ( L - LTD",
        "rewrite_text": "Late Long-Term Potentiation (L-LTP), also known as Long-Term Memory, necessitates the activation of group I metabotropic glutamate receptors (mGluRs) on the presynaptic side and postsynaptic protein metabolism. To maintain this state, it requires NMDA receptor activity and the synthesis of new proteins. This study introduces a theory of late L-LTP that integrates these three fundamental components. This model is capable of producing both long-term depression (L-LTD) and long-term remembering, which are also observed in late L-LTP. Therefore, this model could be a valuable tool in elucidating the basic mechanisms of late L-LTP.\n\nThe model is composed of three sections: presynaptic, postsynaptic, and the interaction between the two. The presynaptic section involves an mGluR that becomes activated when the concentration of a presynaptic neurotransmitter increases. This leads to the mobilization of calcium through a diacylglycerol/protein kinase C cascade. This accumulation of calcium results in the production of crucial molecules involved in maintaining L-LTP.\n\nThe postsynaptic section covers the creation of glutamate receptors with a half-life of several hours. The interaction between the two sections encompasses the generation of long-term depression (L-LTD) when the combined postsynaptic glutamate and presynaptic neurotransmitter concentrations decrease.\n\nOur findings indicate that this model can generate both long-term depression and long-term potentiation, which are related to late L-LTP. We believe that this model could be a helpful resource in understanding the fundamental mechanisms of late L-LTP.\n\nWe are grateful for the support of NSF grants: 1719653, 1719564, 1820822, 1834991, and 1900094. We would like to express our appreciation to Dr. Vladimir Zabalon for his valuable feedback on our manuscript.",
        "ori-fast-z-score": 2.425913739452491,
        "water-fast-z-score": 9.302412790288885,
        "rewrite-fast-z-score": 2.942389786832747
    },
    {
        "original_text": "Dying radio galaxies are useful cosmological tools as they can be used to trace the large-scale structure of the universe. These galaxies appear as reduced radio emission associated with the active nucleus and some hot-spot emission, with a sharp cutoff at the edge of the large-scale structure in which the galaxy is found. Despite the great value of dying radio galaxies for cosmology, no clear samples are available to study statistically. We have carried out a search for such sources in the publicly available data from the Karl G. Jansky Very Large Array archive. We have found 19 sources which have fading radio emission similar to that expected from dying radio galaxies. We discuss the implications of this discovery for the study of large-scale structure, and future searches for such sources. This work was performed as part of the Karl G. Jansky Very Large Array public science program 13B254. We thank the staff of the array, as well as those of the VLASS Project Office and the McGee Division of the NRAO for their help in preparing this program.",
        "watermark_text": "Dying radio objects are helpful cosmological tools as they can be used to map the huge - scale organization of the universe . These galaxies appear as reduced radio emission associated with the active nucleus and some warm - place emission , with a sharp cutoff at the boundary of the huge - scale system in which the galaxy is found .Despite the great utility of dying radio stars for cosmology , no clear samples are available to study statistically . We have carried out a check for such sources in the fully available data from the Karl G . Jansky Very Large Array archive .We have discovered 19 sources which have fading radio emission similar to that expected from dying radio stars . We discuss the implications of this discovery for the science of large - scale structure , and future investigations for such sources .This project was done as part of the Karl G . Jansky Very Large Array public science program 13B254 . We thank the staff of the array , as well as those of the VLASS Project Office and the McGee Division of the NRAO for their help in preparing this project .",
        "rewrite_text": "Dying radio galaxies serve as valuable cosmological instruments, as they can be utilized to map the vast structure of the universe. These galaxies manifest as diminished radio emissions linked to their active nuclei and warm-spot emissions, featuring a distinct cutoff at the periphery of the large-scale system within which they reside. Although the significant contribution of dying radio galaxies to cosmology is undeniable, statistically robust samples for their study remain elusive. We have conducted a thorough search for such sources in the comprehensive data available from the Karl G. Jansky Very Large Array archive. Remarkably, we have identified 19 sources exhibiting fading radio emissions that closely resemble those expected from dying radio galaxies. We delve into the ramifications of this discovery for understanding large-scale structures and future investigations of these sources. This project, executed as part of the Karl G. Jansky Very Large Array's public science program 13B254, is grateful for the assistance provided by the array's staff, as well as the VLASS Project Office and the McGee Division of the NRAO.",
        "ori-fast-z-score": -0.11396057645963795,
        "water-fast-z-score": 4.900304787764432,
        "rewrite-fast-z-score": -1.2535663410560174
    },
    {
        "original_text": "A de Haas-van Alphen study of the filled skutterudite compounds PrOs_4As_{12} and LaOs_4As_{12} has been performed. Filled skutterudite compounds exhibit heavy Fermion behavior and thus PrOs_4As_{12} and LaOs_4As_{12} are candidates to realize a Kondo lattice system. We observed a nearly mass-independent component in the field-dependent magnetic susceptibility for both compounds down to 1.8 K. The nearly mass-independent component is likely due to the crystalline electric field (CEF) effect. The CEF level scheme is analyzed and nine excited levels are revealed. This study is one of the key experiments for understanding the quantum criticality in filled skutterudite systems. Authors: Takeshi Ebihara, Jun Hashimoto, Osamu Senda, Takeshi Takabatake, Hideki Yamamoto, Yoshihiko Kuroki, Atsushi Kawabata Journal: Physics Procedia doi: 10.1016/j.proc.2016.10.075",
        "watermark_text": "A de Haas - van Alphen study of the packed skutterudite compounds PrOs _ 4As _ { 12 } and LaOs _ 4As _ { 12 } has been performed . Filled skutterudite compounds exhibit strong Fermion behavior and therefore PrOs _ 4As _ { 12 } and LaOs _ 4As _ { 12 } are prospects to realize a Kondo symmetry structure .We observed a nearly mass - independent component in the field - dependent magnetic susceptibility for both compounds down to 1 . 8 K . The virtually mass - independent component is probably due to the crystalline electric field ( CEF ) effect . The CEF level scheme is evaluated and nine excited levels are revealed .This study is one of the key experiments for studying the quantum criticality in filled skutterudite solutions . Authors : Takeshi Ebihara , Jun Hashimoto , Osamu Senda , Takeshi Takabatake , Hideki Yamamoto , Yoshihiko Kuroki , Atsushi Kawabata Journal : Physics Procedia doi : 10 . 1016 / j . proc . 2016 . 10 . 075",
        "rewrite_text": "A study utilizing the de Haas-van Alphen method has been conducted on the packed skutterudite compounds PrOs₄As₁₂ and LaOs₄As₁₂. These filled skutterudite compounds demonstrate robust Fermion behavior, making them promising candidates for realizing a Kondo symmetry structure. Our observations indicate a nearly mass-independent component in the field-dependent magnetic susceptibility of both compounds, persisting down to 1.8 K. This component is likely attributed to the effect of the crystalline electric field (CEF). The CEF level scheme has been evaluated, revealing nine excited levels. This study is crucial for exploring quantum criticality in filled skutterudite solutions.\n\nAuthors: Takeshi Ebihara, Jun Hashimoto, Osamu Senda, Takeshi Takabatake, Hideki Yamamoto, Yoshihiko Kuroki, Atsushi Kawabata.\n\nJournal: Physics Procedia. DOI: 10.1016/j.proc.2016.10.075.",
        "ori-fast-z-score": 0.2886751345948129,
        "water-fast-z-score": 3.464101615137755,
        "rewrite-fast-z-score": 0.8660254037844387
    },
    {
        "original_text": "In this paper, we consider the problem of determining the minimal probability of ruin for a jump-diffusion risk model. We provide a characterization of the minimal probability of ruin via a convex game that consists of a stopping game and a control game. At each horizon, the player has to choose between playing the risk process until the time of ruin and terminating the play with some safe liquidation wealth level. The control game is then played with the strategy chosen by the first player determining the time and the level of the jump process. By applying an extended version of Sion s theorem to this game, we are able to characterize the value function of the stopping game as the unique classical solution to a degenerate elliptic partial differential equation. Finally, we prove that this value function is both the minimal probability of ruin and the unique maximum probability of survival. This work is of interest to risk managers, researchers, and students who are interested in problems related to determining the minimal probability of ruin for a jump-diffusion risk model.",
        "watermark_text": "In this paper , we investigate the question of determining the minimal probability of ruin for a leap - diffusion risk simulation . We provide a description of the minimal probability of ruin via a convex game that contains of a stopping player and a control game .At each horizon , the team has to choose between playing the danger cycle until the period of ruin and terminating the play with some free liquidation wealth level . The control game is then played with the plan selected by the first player determining the time and the level of the jump process .By applying an extended version of Sion s theorem to this tournament , we are able to characterize the value function of the stopping player as the unique classical answer to a degenerate elliptic partial differential equation . Finally , we prove that this value function is both the minimal probability of ruin and the unique maximum likelihood of survival .This research is of interest to risk managers , researchers , and students who are concerned in problems related to determining the minimal probability of ruin for a jump - diffusion risk design .",
        "rewrite_text": "In this study, we explore the question of determining the smallest possible risk of catastrophe within a leap-diffusion risk simulation. We present a concise explanation of this minimal risk probability through a convex game that involves a stopping player and a control game. At every juncture, the team must choose whether to continue playing the perilous cycle until the point of destruction or to terminate the game with a certain level of liquidated wealth. The control game is then played based on the plan selected by the initial player, determining both the timing and magnitude of the jump process. By applying an enhanced version of Sion's theorem to this dynamic, we can precisely characterize the value function of the stopping player as the sole classical solution to a degenerate elliptic partial differential equation. Ultimately, we prove that this value function represents both the smallest probability of destruction and the unique maximum likelihood of survival. This research is pertinent to risk managers, researchers, and students alike who are interested in issues related to determining the minimal probability of catastrophe in a jump-diffusion risk model.",
        "ori-fast-z-score": 1.0425720702853738,
        "water-fast-z-score": 5.004345937369795,
        "rewrite-fast-z-score": 2.5021729686848975
    },
    {
        "original_text": "Magnetic fields are known to be present in most stars and planets, and play a crucial role in their evolution. Much less is known about magnetic fields in systems lacking apparent collimated, ordered magnetic fields, such as stars and planets. Observations of continuum and line emission from large numbers of low-mass pre-main-sequence stars (T Tauri stars) have provided evidence for substantial magnetic fields in these systems. The most likely topology of the field in these systems is a dipole aligned with the stellar rotation axis. Here, we present Zeeman-Doppler imaging of the presence and properties of a dipole field in the T Tauri star Hen 3-4008. The dipole field is occulted by the large inner disk but emerges in the outer disk, along with slow magnetoacoustic waves that thread the disk. We present quantitative modeling of the field characteristics, and place lower limits on the strength of the magnetic field of Hen 3-4008 of approximately 650 G.",
        "watermark_text": "Magnetic fields are known to be found in most stars and planets , and play a crucial role in their development . Much less is known about magnetic fields in systems lacking apparent collimated , organized magnetic fields , such as planets and planets .Observations of continuum and line emission from huge groups of low - weight pre - main - sequence stars ( T Tauri stars ) have provided evidence for significant magnetic fields in these systems . The most likely geometry of the field in these systems is a dipole aligned with the stellar rotation axis .Here , we present Zeeman - Doppler observations of the presence and properties of a dipole field in the T Tauri star Hen 3 - 4008 . The dipole field is occulted by the huge inner disk but emerges in the outer disk , along with slow magnetoacoustic waves that thread the disk .We present numerical analyses of the field qualities , and place lesser restrictions on the strength of the magnetic force of Hen 3 - 4008 of circa 650 G .",
        "rewrite_text": "Magnetic fields are ubiquitous in most stars and planets, playing a vital role in their development. However, less is known about magnetic fields in systems lacking organized and collimated magnetic fields, such as in planets and stars without obvious magnetic characteristics. Observations of continuum and line emission from clusters of low-mass pre-main-sequence stars (T Tauri stars) have provided evidence of significant magnetic fields within these systems. The most probable geometry of these fields is a dipole aligned with the star's rotational axis.\n\nIn this study, we present Zeeman-Doppler observations regarding the presence and characteristics of a dipole field in the T Tauri star Hen 3-4008. This dipole field is obscured by the large inner disk but becomes visible in the outer disk, accompanied by slow magnetoacoustic waves that traverse the disk. We also present numerical analyses of the field's properties and put constraints on the magnetic force strength of Hen 3-4008, estimated to be around 650 G.",
        "ori-fast-z-score": -0.22941573387056174,
        "water-fast-z-score": 3.670651741928988,
        "rewrite-fast-z-score": 0.6793662204867574
    },
    {
        "original_text": "The existence of dark matter is established by the gravitational effects it has on visible matter in galaxies. The nature of dark matter, however, is unknown. The most popular theory regarding its nature, the cold dark matter theory, states that it is comprised of elementary particles which neither give off nor receive electromagnetic radiation. Observational evidence has established that dark matter exists in clusters, or aggregates, large enough to be observed in both particles and galaxies. These clusters are believed to evolve via the mutual gravitational interaction of their constituent particles and galaxies. Since the constituents of dark matter do not give off or absorb light, their direct observation has been challenging. Studying the dynamics of these clusters can therefore aid in understanding the properties of dark matter. B514 is a globular cluster located in the M31 galaxy approximately 25,000 light-years from Earth. Using the Hubble Space Telescope, I have determined that B514 has an additional extended halo which, based on its structural and dynamical characteristics, is most likely comprised of dark matter. I have determined that this dark matter halo has a mean density of 0.105 halo particles per cubic light-year and a diameter of 1.33 kpc or 17,650 light-years.",
        "watermark_text": "The nature of dark matter is known by the gravitational impacts it has on visible matter in galaxies . The nature of dark matter , however , is unknown .The most popular hypothesis regarding its nature , the cool soft material hypothesis , states that it is comprised of primary objects which neither send off nor accept electromagnetic radiation . Observational evidence has established that dark matter exists in clusters , or aggregates , large enough to be found in both particles and galaxies .These clusters are said to evolve via the mutual gravitational interaction of their constituent particles and galaxies . Since the constituents of dark matter do not give off or consume light , their direct observation has been challenging .Studying the dynamics of these complexes can thus help in understanding the properties of dark matter . B514 is a globular cluster located in the M31 galaxy approximately 25 , 000 light - years from Earth .Using the Hubble Space Telescope , I have concluded that B514 has an additional enlarged halo which , based on its structural and dynamical characteristics , is most likely comprised of dark matter . I have decided that this light matter halo has a mean concentration of 0 . 105 halo molecules per cubic light - month and a diameter of 1 . 33 kpc or 17 , 650 light - years .",
        "rewrite_text": "The identification of dark matter's nature relies on its gravitational influence on visible matter within galaxies. Despite this, the true essence of dark matter remains a mystery. The most widely accepted theory about its nature, the cool soft material hypothesis, suggests that it consists of primary objects that neither emit nor absorb electromagnetic radiation. Observational data has confirmed the existence of dark matter in clusters or aggregates, which can span from particles to entire galaxies. These clusters are believed to evolve through the mutual gravitational interaction of their constituent particles and galaxies. Since the components of dark matter do not emit or consume light, direct observation has been a challenge. Therefore, studying the dynamics of these clusters can aid in understanding the properties of dark matter.\n\nB514, a globular cluster situated in the M31 galaxy, is approximately 25,000 light-years away from Earth. Through the use of the Hubble Space Telescope, I have concluded that B514 possesses an enlarged halo, which, based on its structural and dynamic characteristics, is likely composed of dark matter. I have determined that this light matter halo has a mean concentration of 0.105 halo molecules per cubic light-month and a diameter of 1.33 kpc, or 17,650 light-years.",
        "ori-fast-z-score": 1.4288690166235207,
        "water-fast-z-score": 6.327848502189878,
        "rewrite-fast-z-score": 2.7414346458607715
    },
    {
        "original_text": "A muon, electron, andtau particle neutrino interaction with a nuclear target can lead to various channels. Of particular interest in neutrino oscillations experiments are the channels in which a charged current interaction produces a charged current signature lepton plus a missing transverse momentum. In this paper, we demonstrate that these channels can be reduced to a complex symplectic invariant, which we call s. The structure of this invariant allows us to predict the structure of the corresponding charged current neutrino interaction matrix in the Standard Model, namely the Cabibbo-Kobayashi-Maskawa (CKM) matrix. We use experimental data for the leading order structure of the CKM matrix and demonstrate that it reduces to a good approximation to the structure predicted by the invariant. We apply our analysis to precisionglobal fits to the neutrino mixing matrix and show that the corresponding structure predicted by s is in agreement with data at the 1.7 sigma level. We also discuss the connection of this result to higher-dimensional physics and flavor symmetry.",
        "watermark_text": "A muon , atom , andtau particle neutrino collision with a radioactive target can lead to numerous channels . Of particular concern in neutrino oscillations experiments are the channels in which a charged current interaction produces a charged current signature lepton plus a missing transverse energy .In this paper , we prove that these channels can be reduced to a complex symplectic invariant , which we call s . The structure of this invariant permits us to predict the composition of the associated charged current neutrino coupling matrix in the Standard Model , namely the Cabibbo - Kobayashi - Maskawa ( CKM ) matrix . We use experimental evidence for the main order structure of the CKM matrix and demonstrate that it reduces to a better approximation to the formation predicted by the invariant .We use our analysis to precisionglobal fitting to the neutrino composition matrix and suggest that the resulting structure predicted by s is in agreement with data at the 1 . 7 sigma level . We also discuss the link of this consequence to higher - dimensional physics and flavor symmetry .",
        "rewrite_text": "The collision of a muon, atom, and tau particle neutrino with a radioactive target can yield a multitude of channels. In neutrino oscillation experiments, particular concern lies in the channels where a charged current interaction results in the production of a charged current signature lepton accompanied by a missing transverse energy. In this paper, we establish that these channels can be condensed into a complex symplectic invariant, which we label as 's'. This invariant's structure enables us to forecast the composition of the associated charged current neutrino coupling matrix in the Standard Model, specifically the Cabibbo-Kobayashi-Maskawa (CKM) matrix. Leveraging experimental evidence for the primary structure of the CKM matrix, we demonstrate that it aligns more closely with the formation predicted by the invariant. We employ our analysis to achieve a precise global fit for the neutrino composition matrix and propose that the resulting structure predicted by 's aligns with data at the 1.7 sigma level. Furthermore, we delve into the connection between this consequence and physics in higher dimensions and flavor symmetry.",
        "ori-fast-z-score": -1.3764944032233704,
        "water-fast-z-score": 4.358898943540673,
        "rewrite-fast-z-score": 1.8599622199011085
    },
    {
        "original_text": "This is a description of a scientific paper posted on arXiv.org, a pre-publication history of the paper, and its forthcoming publication. The presented abstract was written by the authors of the paper. The presented paper describes the development of a new general relativistic hydrodynamics code called WHAM (WENO-HARDM). The paper presents the numerical methods implemented in the code, as well as some of the problems and solutions encountered during the development. The code is written in modular fashion, which allows it to be easily extended to other hydrodynamics equations, embedding equations, and mathematical flux functions. The authors performed several tests of the code to demonstrate the ability to accurately capture both smooth solutions and physical discontinuities. The results are in good agreement with the exact solutions and published literature for various hydrodynamics equations. WHAM is available at https://bitbucket.org/carrerr/wham. The master repository contains all development code, while the https://github.com/carrerr/wham/tree/master/doc directory contains all documentation. Pre-publication history ======================= * Modified time: 2019-03-23 13:45:22 * Authors: Carreras, Pablo; Miranda, Jorge E.; Vasquez, Cristobal WHAM: A WENO-based general relativistic numerical scheme I: Hydrodynamics Hydrodynamics is the description of the motion of fluids, which are described by fluid mechanics. Fluid mechanics, in turn, is the study of fluid behavior not involving radiation or mechanical phenomena. The dynamics of a viscous fluid may be described by the hydrodynamic equations: the continuity equation, which expresses the conservation of fluid mass; the fluid velocity, which is the statement of Newton s law of motion, and an appropriate velocity specification such as no flow across boundaries (no slip) or in canals (zero shear); and the conservation of momentum, which is the statement of force production due to contact friction and other sources not already included in the fluid mass equation. General relativity is the study of the effects of gravity on the propagation of light. General relativity is one of the most comprehensive and successful descriptions of the laws of physics, accounting for all known forms of energy, including mass-energy and matter, electromagnetic, and nuclear energy. It also describes the dynamics of spacetime, the construction of spacetime itself. The theory arose from attempts to reconcile the Ideas of Isaac Newton regarding an universal and singularly elegant theory of gravity with the correct description of the movements of the planets and stars. In particular, general relativity accurately describes the behavior of light on the scale of planets and stars, while Newton s theory does not appear to adequately explain such observations. Einstein s field equations are the most famous instance of the nonlinear structure of general relativity and have no known solution in general; so, Einstein supplemented his theory with a hypothesis of the existence of a",
        "watermark_text": "This is a description of a scientific publication published on arXiv . org , a pre - published history of the paper , and its forthcoming publication . The submitted abstract was written by the writers of the paper .The published paper explains the development of a new general relativistic hydrodynamics code called WHAM ( WENO - HARDM ) . The paper provides the numerical models utilized in the code , as well as some of the problems and solutions faced during the development .The language is designed in modular fashion , which allows it to be easily applied to other hydrodynamics equations , embedding equations , and mathematical flux systems . The authors performed numerous tests of the code to test the ability to correctly capture both smooth solutions and physical discontinuities .The results are in good agreement with the exact solutions and published literature for various hydrodynamics equations . WHAM is accessible at https : / / bitbucket . org / carrerr / wham .The master repository contains all development code , while the https : / / github . com / carrerr / wham / tree / master / doc directory contains all documentation . Pre - published history = = = = = = = = = = = = = = = = = = = = = = = * Modified time : 2019 - 03 - 23 13 : 45 : 22 * Authors : Carreras , Pablo ; Miranda , Jorge E . ; Vasquez , Cristobal WHAM : A WENO - based general relativistic numerical plan I : Hydrodynamics Hydrodynamics is the description of the movement of fluids , which are explained by liquid mechanics .Fluid mechanics , in turn , is the science of fluid activity not featuring radiation or mechanical phenomena . The behavior of a viscous fluid may be described by the hydrodynamic equations : the continuity equation , which expresses the conservation of fluid mass ; the liquid velocity , which is the assertion of Newton s law of movement , and an appropriate velocity specification such as no flow across barriers ( no slipping ) or in canals ( zero shear ) ; and the conservation of velocity , which is the statement of force generating owing to contact drag and other sources not already included in the liquid mass equation .General relativity is the science of the effects of gravitational on the propagation of light . General relativity is one of the most systematic and successful explanations of the rules of physics , accounting for all known forms of energy , including mass - energy and matter , electromagnetic , and nuclear power .It additionally describes the dynamics of spacetime , the creation of spacetime itself . The theory arose from attempts to unite the Ideas of Isaac Newton concerning an universal and singularly elegant concept of gravitational with the correct description of the actions of the planets and stars .In particular , special relativity accurately models the dynamics of light on the scale of planets and stars , while Newton s concept does not appear to adequately explain such observations . Einstein s field equations are the most famous instance of the nonlinear construction of general relativity and have no known solution in general ; so , Einstein supplemented his idea with a theory of the existence of a",
        "rewrite_text": "这是一个关于在arXiv.org上发布的科学出版物的描述，以及其预出版历史和即将出版的信息。提交的摘要由论文的作者所写。已发布的论文详细介绍了名为WHAM（WENO-HARDM）的新广义相对论流体动力学代码的开发过程。论文提供了代码中使用的数值模型，以及开发过程中遇到的一些问题和解决方案。该语言设计为模块化，可轻松应用于其他流体动力学方程、嵌入方程和数学通量系统。作者对代码进行了大量测试，以验证其正确捕捉平滑解和物理不连续性的能力。对于各种流体动力学方程，其结果与精确解和已发表文献中的结果相吻合。WHAM可以在https://bitbucket.org/carrer/wham上访问。主存储库包含所有开发代码，而https://github.com/carrer/wham/tree/master/doc目录则包含所有文档。\n\n预出版历史记录：\n修改时间：2019年3月23日 13:45:22\n作者：卡雷拉斯（Pablo）、米兰达（Jorge E.）、瓦兹奎兹（Cristobal）\n\nWHAM：基于WENO的广义相对论数值计划I：流体动力学\n\n流体动力学是描述流体运动的学科，这些流体运动由液体力学来解释。液体力学则是研究不涉及辐射或机械现象的流体运动科学。粘性流体的行为可以通过流体动力学方程来描述：连续性方程，表达流体的质量守恒；液体速度方程，即牛顿运动定律的断言，以及适当的速度规定，如无跨障碍物流动（无滑移）或在运河中（零剪切）；以及速度守恒方程，这是由于接触阻力和其他未包含在液体质量方程中的源而产生的力的陈述。\n\n广义相对论是研究引力对光传播影响的科学。广义相对论是物理学中最系统和成功的解释之一，它解释了所有已知形式的能量，包括质量-能量、物质、电磁能和核能。此外，它还描述了时空的动力学和时空本身的创造。这一理论起源于尝试将艾萨克·牛顿关于引力的普遍而优雅的概念与正确描述行星和恒星运动的描述相结合。特别是，特殊相对论精确地模拟了光在行星和恒星尺度上的动力学，而牛顿的概念似乎无法充分解释这些观察结果。爱因斯坦的场方程是广义相对论非线性构造的最著名实例，它没有通用的已知解；因此，爱因斯坦通过存在性理论来补充他的想法。",
        "ori-fast-z-score": 1.3018585486723102,
        "water-fast-z-score": 8.928436656664786,
        "rewrite-fast-z-score": 1.4142135623730951
    },
    {
        "original_text": "Noise in the fractional quantum Hall effect (FQHE) is an important aspect that can often be overlooked in the study of this novel state of quantum matter. Both emission and absorption noise have been theoretically predicted to occur in the FQHE. While emission noise can affect transport measurements through the sample, absorption noise cannot and can be observed via noise spectroscopy. We present two distinct experimental realizations of this noise spectroscopy, one utilizing microwave radiation and another optical Raman scattering, both of which are in good quantitative agreement with theoretical calculations of emission and absorption noise in the FQHE. This noise spectroscopy is an essential tool to study transport in the FQHE and will enable future studies of many-body dynamics in this quantum many-body system. This study of noise in the fractional quantum hall effect (FQHE) is an important one as often a study of this novel quantum state involves overlookinh the noise aspects of the FQHE. Emission noise can affect transport measurements through the sample but absorption noise cannot and can be observed via noise spectroscopy. We present two distinct experimental realizations of this noise spectroscopy one using microwave radiation and another optical Raman scattering, both of which are in good quantitative agreement with theoretical calculations of emission and absorption noise in the FQHE.",
        "watermark_text": "Noise in the fractional quantum Hall effect ( FQHE ) is an important element that can often be forgotten in the science of this novel state of quantum matter . Both emission and emission noise have been theoretically expected to appear in the FQHE .While emission noise can affect transport measurements through the sample , absorption noise cannot and can be viewed via noise spectroscopy . We present two different experimental realizations of this noise spectroscopy , one utilizing infrared rays and another optical Raman scattering , both of which are in good quantitative accord with theoretical calculations of emission and emission noise in the FQHE .This noise spectroscopy is an essential tool to study transport in the FQHE and will provide future research of several - bodies dynamics in this quantum several - bodies system . This study of noise in the fractional quantum hall effect ( FQHE ) is an important one as usually a investigation of this novel quantum state includes overlookinh the noise aspects of the FQHE .Emission sound can affect transport measurements through the sample but absorption noise cannot and can be viewed via noise spectroscopy . We present two different experimental realizations of this noise spectroscopy one utilizing infrared rays and another optical Raman absorption , both of which are in good quantitative accord with theoretical calculations of emission and emission noise in the FQHE .",
        "rewrite_text": "In the realm of the novel state of quantum matter, the fractional quantum Hall effect (FQHE), noise frequently remains an overlooked element. Both emission and absorption noise have been theoretically anticipated to manifest in FQHE. While emission noise can influence transport measurements within the sample, absorption noise remains undetectable and can be revealed through noise spectroscopy. We introduce two distinct experimental applications of this noise spectroscopy: one utilizing infrared rays, and another employing optical Raman scattering. Both methods align well with theoretical calculations of emission and noise in FQHE. This noise spectroscopy serves as a crucial instrument for studying transport in FQHE and will facilitate future investigations into the dynamics of multiple bodies within this quantum system. The exploration of noise in FQHE is significant as it often goes unnoticed in investigations of this unique quantum state. Although emission sound can affect measurements within the sample, absorption noise remains indiscernible and is revealed through spectral analysis. We present two distinct experimental setups for this spectral analysis, one utilizing infrared radiation and another utilizing optical Raman absorption, both closely aligning with theoretical predictions of emission and noise in FQHE.",
        "ori-fast-z-score": -0.502518907629606,
        "water-fast-z-score": 4.848732213850612,
        "rewrite-fast-z-score": 1.5389675281277313
    },
    {
        "original_text": "We investigate the collapse of very massive stars, starting with a Solar mass and rotating at the level of 600 times the speed of light, to black holes in full general relativity. These collapses occur in a region with very high angular momentum, where the hole punctures the sky and shrinks to a Schwarzschild radius of about 30 km, when General Relativity requires that it becomes infinitely massive. We perform simulations with up to 2048*1024 resolution, showing that the fluid forms a prolate configuration, with a maximum radius of about 1000 meters, and a slowly rotating inner core of radius a few meters. After an initial phase of about 0.5 ms, the collapse proceeds in a way remarkably similar to the head-on collision of two black holes of 30 and 10 solar masses, with an apparent horizon of about 20 kilometers in size and an estimated radiated energy of about 2.5% of the total mass, in excellent agreement with the values obtained in previous perturbative calculations. We study the properties of the formed black holes and show that several physical parameters, like the irreducible mass, the rotational parameter, the curvature invariant and the event horizon surface gravity are in very good agreement with the values obtained in perturbative calculations, even for the final, non-perturbative, rotating star.",
        "watermark_text": "We explore the fall of very huge stars , beginning with a Solar mass and rotating at the level of 600 times the speed of light , to dark holes in total general relativity . These collapses occur in a region with very high angular velocity , where the hole punctures the sky and shrinks to a Schwarzschild diameter of about 30 km , when General Relativity demands that it becomes infinitely heavy .We perform simulations with up to 2048 * 1024 resolution , showing that the liquid makes a prolate arrangement , with a maximum radius of about 1000 meters , and a rapidly spinning inner core of radius a few meters . After an initial phase of about 0 . 5 ms , the failure proceeds in a way remarkably reminiscent to the head - on crash of two white holes of 30 and 10 solar masses , with an apparent horizon of about 20 kilometers in length and an estimated radiated energy of about 2 . 5 % of the total mass , in good agreement with the figures obtained in earlier perturbative calculations .We research the properties of the formed black holes and find that several physical factors , like the irreducible mass , the rotational parameter , the curvature invariant and the event horizon surface gravity are in very high agreement with the expressions derived in perturbative calculations , even for the finished , non - perturbative , rotating star .",
        "rewrite_text": "We delve into the collapse of immense stars, starting with ones akin to a Solar mass that rotate at six hundred times the speed of light, until they transform into dark holes within the framework of general relativity. These collapses occur in regions with extreme angular velocity, where the hole punctures the sky and shrinks to a Schwarzschild diameter of approximately 30 kilometers as General Relativity dictates it to become infinitely massive. We conduct simulations with a resolution of up to 2048 x 1024, revealing that the resulting liquid takes on a prolate formation, with a maximum radius of about 1000 meters and a rapidly spinning inner core with a few-meter radius. Following an initial phase of roughly 0.5 milliseconds, the collapse progresses in a manner reminiscent of the head-on collision between two white holes weighing 30 and 10 solar masses, with an apparent horizon extending to approximately 20 kilometers in length and an estimated radiated energy of approximately 2.5% of the total mass - aligning well with previous perturbative calculations. We investigate the properties of the resulting black holes and find that several physical factors, such as the irreducible mass, rotational parameter, curvature invariant, and event horizon surface gravity, are highly consistent with the expressions derived from perturbative calculations, even for completed, non-perturbative, rotating stars.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.787473141261629,
        "rewrite-fast-z-score": 2.970442628930023
    },
    {
        "original_text": "Web data has evolved with the growth of Web and the introduction of new concepts such as social networks and mobile applications. The volume of this data increases day-by-day and new research in the field of data management is needed for effective data analytics on this data. In this paper, a distributed data model for Web data integration in data warehouses is proposed. The main advantage of this model is that all the data associated with a website is represented as a single row in the target relational database, which makes it easy to perform joins between this table and other tables. This model uses case-change style rules to add new columns to the base table to represent the new concepts and these rules can be extended easily to add new concepts. The proposed model has been implemented in MySQL database and the performance of this model has been evaluated using real-life data and it is shown that this model can handle large volumes of Web data compared to other models.",
        "watermark_text": "Web statistics has grown with the development of Web and the introduction of new concepts such as social networks and wireless technologies . The volume of this data increases year - by - day and new studies in the field of data management is required for efficient information analytics on this data .In this paper , a distributed database model for Web database integration in data warehouses is proposed . The main advantage of this model is that all the information involved with a website is depicted as a single row in the target relational data , which makes it able to conduct joins between this list and other tables .This theory utilizes case - change pattern laws to addition new columns to the base table to reflect the new concepts and these rules can be extended easily to addition new concepts . The proposed theory has been used in MySQL program and the performance of this model has been examined using real - time data and it is demonstrated that this model can handle big quantities of Web content relative to other models .",
        "rewrite_text": "With the advent of the web and the introduction of innovative concepts like social networks and wireless technologies, web statistics has blossomed. The amount of data generated is growing day by day, necessitating new studies in data management for efficient information analytics. This paper presents a distributed database model for integrating web databases into data warehouses. A key advantage of this model is that all website-related information is portrayed as a single row in the targeted relational data, facilitating joins between this row and other tables. This theory employs case-change pattern laws to add new columns to the base table, reflecting new concepts, and these rules can be effortlessly extended to incorporate new ideas. The proposed theory has been implemented in the MySQL program, and its performance has been evaluated using real-time data. It is demonstrated that this model can effectively handle large quantities of web content compared to other models.",
        "ori-fast-z-score": -0.953998092005724,
        "water-fast-z-score": 5.405989188032437,
        "rewrite-fast-z-score": 0.32539568672798425
    },
    {
        "original_text": "Recently, various anomalies have been found in the WMAP data, such as the alignments between the regions of high (positive) and low (negative) signed intensities and the directions of the solar wind, and the correlations between the temperature and the components of the WMAP data in the KQ75 orientated dataset. These anomalies have been widely interpreted as indirect signatures of the presence of electromagnetic interference and/or possible contamination from Solar System dust. In this paper, we report the discovery of several other alignment and intensity anomalies in the WMAP data that have not been reported previously and are more difficult to explain. We also provide a plausible physical explanation for these anomalies, which are due to interference from man-made radio signals. We demonstrate that the same interference is also responsible for the recently observed anomalies in the signed-intensity and cross-power spectra. The interference we report here probably comes from man-made radio signals in the frequency bands 128 - 137 MHz, 143 - 149 MHz, and 333 - 349 MHz. The radio waves from these sources would have propagated from North America into the northern hemisphere of the sky (where the WMAP spacecraft were located) in the period from May 2003 to November 2003.",
        "watermark_text": "Recently , various anomalies have been shown in the WMAP information , such as the alignments between the regions of high ( positive ) and low ( negative ) signed intensities and the directions of the solar wind , and the correlations between the temperature and the parts of the WMAP information in the KQ75 orientated dataset . These anomalies have been widely viewed as indirect signatures of the presence of electromagnetic interference and / or possible pollution from Solar System dust .In this paper , we document the discovery of several other alignment and intensity anomalies in the WMAP information that have not been reported previously and are more hard to explain . We additionally offer a plausible physical explanation for these anomalies , which are related to interference from man - made radio signals .We suggest that the same interference is also responsible for the recently detected anomalies in the signed - intensity and cross - energy spectra . The noise we report here probably originates from man - made radio signals in the frequency bands 128 - 137 MHz , 143 - 149 MHz , and 333 - 349 MHz .The signal signals from these sources would have propagated from North America into the northern hemisphere of the heavens ( where the WMAP spacecraft were situated ) in the period from May 2003 to November 2003 .",
        "rewrite_text": "Recently, a variety of abnormalities have emerged in the WMAP data, such as alignments between regions of high (positive) and low (negative) signed intensities and the directions of solar wind, as well as correlations between temperature and various parts of the WMAP information within the KQ75-oriented dataset. These abnormalities have frequently been viewed as indirect indicators of electromagnetic interference or potential pollution from Solar System dust.\n\nIn this paper, we document the discovery of several novel alignment and intensity abnormalities in the WMAP data that have not been previously reported and are more challenging to explain. We additionally propose a plausible physical explanation for these anomalies, which is linked to interference caused by man-made radio signals. We suggest that this same interference is also responsible for the recently detected abnormalities in signed-intensity and cross-energy spectra.\n\nThe noise reported here likely originates from man-made radio signals within the frequency ranges of 128-137 MHz, 143-149 MHz, and 333-349 MHz. These signal sources would have propagated from North America into the northern hemisphere of the sky, where the WMAP spacecraft was located, between May 2003 and November 2003.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": 1.5652475842498528
    },
    {
        "original_text": "In this paper, we present a construction of free fermions for supergroup WZNW models using a free fermion resolution of supergroups, along with a spectral flow that relates models with different levels. We give a free field realisation of the WZNW model based on the free fermion resolution. We compute correlation functions using the operator product expansion, and show that they agree with the known results in the case of the supergroup $GL(1|1)$ and the superconformal model with central charge c=6. In particular, we confirm the correctness of the recently proposed four-point functions with general symmetry and global consistency conditions. We also discuss how to apply the method to construct free fermions for the supergroup WZNW models with other supergroups and show that the models with general supergroups can be obtained as WZNW models based on free fermions. Date: 20XX Authors: Yegor Viktorovich Viktorov and Alexander Ivanovich Klitch Title: Free fermion resolution of supergroup WZNW models.",
        "watermark_text": "In this paper , we present a construction of free fermions for supergroup WZNW models using a free fermion resolution of supergroups , along with a spectral flow that connects models with various levels . We get a free field realisation of the WZNW model based on the free fermion resolution .We compute correlation functions using the operator product expansion , and find that they accord with the known results in the case of the supergroup $ GL ( 1 | 1 ) $ and the superconformal theory with central charge c = 6 . In particular , we prove the correctness of the recently suggested four - point functions with general symmetry and international consistency conditions .We also discuss how to apply the method to build free fermions for the supergroup WZNW models with other supergroups and know that the models with general supergroups can be obtained as WZNW models based on free fermions . Date : 20XX Authors : Yegor Viktorovich Viktorov and Alexander Ivanovich Klitch Title : Free fermion resolution of supergroup WZNW models .",
        "rewrite_text": "In this study, we present a construction of free fermions for supergroup WZNW models. This is achieved through a resolution of free fermions in supergroups, integrated with a spectral flow that connects models of different levels. This results in a realization of the WZNW model in the form of a free field based on the free fermion resolution. We compute correlation functions using the operator product expansion and find that they align with previously established results for the supergroup $GL(1|1)$ and the superconformal theory with a central charge of c = 6. Specifically, we verify the accuracy of recently proposed four-point functions with general symmetry and international consistency conditions. Furthermore, we discuss how this method can be applied to construct free fermions for WZNW models involving other supergroups, understanding that WZNW models with various supergroups can be derived from free fermion-based models. Date: 20XX Authors: Yegor Viktorovich Viktorov and Alexander Ivanovich Klitch Title: Free Fermion Resolution in Supergroup WZNW Models.",
        "ori-fast-z-score": 0.75,
        "water-fast-z-score": 3.75,
        "rewrite-fast-z-score": 1.61245154965971
    },
    {
        "original_text": "Bipolar spintronics combines the advantages of ferromagnetic materials and spin electronics by utilizing the spin of electron as well as that of hole. In this approach, the spin-orbit interaction (SOI) is employed to generate or detect spin polarization, and the ferromagnetism can be used for information storage. The first realized application of bipolar spintronics is the observation of spin hall effect in 1977. Since then, intensive studies have been conducted in this field, and various applications have been proposed, including the anomalous hall effect, pure spin current injection, and spintronics-based logic. This paper gives an overview of bipolar spintronics, from spin injection to spin-controlled logic. ****** IMPORTANT NOTICE ********** If you are writing a scientific paper, please consider citing some of the preliminary research that you build upon. Below is a suggested reference list (among others). Ali, M. Y. & Stiles, M. D. (2014). Bipolar spintronics: From spin injection to spin-controlled logic. Journal of Applied Physics, 116(7), p. 074302. In this paper, we give an overview of bipolar spintronics, from spin injection to spin-controlled logic. We start with a brief introduction to spin and the formulation of spin Hall effect. Then we discuss the discovery and early experimental study of spin Hall effect, including the anomalous hall effect, spin rectification, and spin pumping. The latter two effects have been recognized as an important foundation for subsequent studies of spin transport and applications. We present the theoretical models for spin Hall effect based on the classical mechanics and spin drift-diffusion equations. These models are the basis for further analytical and numerical studies of the spin transport. The experiments and theoretical investigations of spin pumping in thin magnetic films are overviewed. Then, we present the experimental studies of anomalous hall effect and spin rectification in thin films. The role of topological term in the anomalous hall effect is discussed. The spin rectification through spin mixing and transmission is also reviewed. The advantages and disadvantages of these two effects in applications are also compared. The formation of topological phases for the electron system with spin-orbit interaction is also presented. The topological spin, anomalous and topological Hall effects are all related to this topological phase. We then discuss the injection of spin current in terms of the spin Hall effect. The direct observation of spin Hall effect in metallic systems is overviewed. We also present the spin Hall effect in semiconductors, magnetic semiconductors, and topological insulators. The recent discovery of surface states in topological insulators is overviewed. The methods for detection of spin accumulation are then discussed. The studies of spin dynamics in semiconductors are overviewed. Then, we present the application of bipolar spintronics, including the spin-charge conversion devices, exchange interaction-based devices, tunnel magnetoresistance-based devices, and spint",
        "watermark_text": "Bipolar spintronics incorporates the advantages of ferromagnetic materials and spin electronics by using the spin of electron as well as that of hole . In this methodology , the spin - orbit interaction ( SOI ) is utilized to create or locate spin polarization , and the ferromagnetism can be used for information storage .The first realized application of bipolar spintronics is the observation of spin hall impact in 1977 . Since then , intensive studies have been performed in this area , and many applications have been proposed , notably the anomalous hall impact , pure spin current injection , and spintronics - based logic .This paper provides an overview of bipolar spintronics , from spinning injection to spinning - controlled logic . * * * * * * IMPORTANT NOTICE * * * * * * * * * * If you are producing a scientific publication , please consider citing some of the preliminary research that you build upon .Below is a possible reference list ( among others ) . Ali , M . Y .& Stiles , M . D . ( 2014 ) . Bipolar spintronics : From spin injection to spinning - controlled logic .Journal of Applied Physics , 116 ( 7 ) , p . 074302 . In this paper , we give an overview of bipolar spintronics , from spinning injection to spinning - controlled logic .We begin with a brief introduction to spinning and the formulation of spin Hall phenomenon . Then we explain the discovery and first experimental discussion of spin Hall impact , notably the anomalous hall impact , spin rectification , and spin pumping .The last two factors have been recognized as an important foundation for subsequent studies of spin transport and use . We see the theoretical descriptions for spinning Hall impact using on the classical mechanics and spin drift - diffusion equations .These methods are the foundation for further analytical and mathematical analyses of the spin flow . The studies and theoretical investigations of spin flow in narrow magnetic movies are overviewed .Then , we present the empirical studies of anomalous hall impact and spin rectification in thin films . The role of topological term in the anomalous hall impact is mentioned .The spin rectification through spin mixing and communication is also examined . The advantages and disadvantages of these two effects in applications are also compared .The formation of topological periods for the electron system with spin - orbit interaction is also presented . The topological spin , anomalous and topological Hall effects are all connected to this topological phase .We then discuss the injection of spin current in terms of the spin Hall phenomenon . The direct observation of spin Hall phenomenon in metallic systems is overviewed .We additionally describe the spin Hall phenomenon in semiconductors , magnetic semiconductors , and topological insulators . The recent discovery of surface states in topological insulators is overviewed .The methods for measurement of spin accumulation are then mentioned . The studies of spin behavior in semiconductors are overviewed .Then , we present the implementation of bipolar spintronics , notably the spin - charge conversion devices , exchange interaction - based devices , tunnel magnetoresistance - based devices , and spint",
        "rewrite_text": "Bipolar spintronics integrates the benefits of ferromagnetic materials and spin electronics by utilizing both the electron and hole spins. In this approach, the spin-orbit interaction (SOI) is employed to generate or locate spin polarization, while ferromagnetism can be utilized for information storage. The initial realized application of bipolar spintronics was the observation of spin hall effect in 1977. Since then, extensive research has been conducted in this area, leading to various proposed applications such as anomalous hall effect, pure spin current injection, and spintronics-based logic.\n\nThis paper provides a comprehensive overview of bipolar spintronics, spanning from spin injection to spin-controlled logic. A brief introduction to the concept of spin and the formulation of the spin Hall phenomenon is provided. The discovery and initial experimental exploration of the spin Hall effect, particularly the anomalous hall effect, spin rectification, and spin pumping, are then explained. These latter two factors have been recognized as crucial foundations for subsequent studies on spin transport and applications. Theoretical descriptions using classical mechanics and spin drift-diffusion equations are presented as foundations for further analytical and mathematical analyses of the spin flow.\n\nAn overview of studies and theoretical investigations on the spin flow in narrow magnetic films is provided. Empirical studies on anomalous hall effect and spin rectification in thin films are then presented. The role of the topological term in the anomalous hall effect is mentioned, and the spin rectification through spin mixing and communication is examined. A comparison of the advantages and disadvantages of these two effects in applications is also given. The formation of topological periods for electron systems with spin-orbit interaction is presented, as well as the connection between topological spin, anomalous, and topological Hall effects in this topological phase.\n\nWe proceed to discuss the injection of spin current in terms of the spin Hall phenomenon. The direct observation of the spin Hall phenomenon in metallic systems is reviewed. Additionally, the spin Hall phenomenon in semiconductors, magnetic semiconductors, and topological insulators is described. The recent discovery of surface states in topological insulators is highlighted. Methods for measuring spin accumulation are then mentioned. An overview of studies on spin behavior in semiconductors is provided. Finally, we present the implementation of bipolar spintronics, including spin-charge conversion devices, exchange interaction-based devices, tunnel magnetoresistance-based devices, and spintronics-based logic.",
        "ori-fast-z-score": -1.7609421998443506,
        "water-fast-z-score": 6.472318679647124,
        "rewrite-fast-z-score": 0.8660254037844387
    },
    {
        "original_text": "Neutrinos are an essential part of the Standard Model (SM) of particle physics. They interactions are described by the Lagrangian λ–μ+ν(−χ+ρ), where λ,μ and ν are the three lepton families  coupling constants, and χ and ρ are scalar fields. The origin of these couplings is not understood in the SM. The neutrinos  self-interactions are absent in the Lagrangian because of a symmetry, called lepton number symmetry. Nevertheless, we show that the observed neutrino oscillations are best explained by spontaneously breaking of this symmetry at the scale θ≤10−10 GeV, below which the effective theory has radiatively broken symmetry. We calculate θ and several cosmological parameters, and we find that the simplest versions of this model are in accord with recent data. We also briefly discuss variations of this model with additional scalar fields and steps to test this model. We show that the observed neutrino oscillations are best explained by spontaneously breaking of this symmetry at the scale θ≤10−10 GeV, below which the effective theory has radiatively broken symmetry. We calculate θ and several cosmological parameters, and we find that the simplest versions of this model are in accord with recent data. We also briefly discuss variations of this model with additional scalar fields and steps to test this model.",
        "watermark_text": "Neutrinos are an essential part of the Standard Model ( SM ) of particle physics . They interactions are explained by the Lagrangian λ – μ + ν ( −χ + ρ ) , where μ , μ and ν are the three lepton families coupling constants , and χ and ρ are scalar fields .The origin of these couplings is not explained in the SM . The neutrinos self - interactions are missing in the Lagrangian because of a symmetry , called lepton number symmetry .Nevertheless , we prove that the known neutrino oscillations are best explained by spontaneously broke of this symmetry at the range θ≤10−10 GeV , below which the effective theory has radiatively shattered symmetry . We calculate θ and many cosmological factors , and we find that the simplest versions of this model are in agreement with recent results .We additionally momentarily explore variations of this model with extra scalar fields and steps to test this model . We see that the known neutrino oscillations are best explained by spontaneously broke of this symmetry at the range θ≤10−10 GeV , below which the effective theory has radiatively shattered symmetry .We calculate θ and many cosmological factors , and we find that the simplest versions of this model are in agreement with recent results . We additionally momentarily explore variations of this model with extra scalar fields and steps to test this model .",
        "rewrite_text": "Neutrinos play a pivotal role in the Standard Model (SM) of particle physics. Their interactions are elucidated by the Lagrangian formula λ - μ + ν (-χ + ρ), wherein μ, μ, and ν represent the coupling constants of the three lepton families, while χ and ρ denote scalar fields. The SM does not offer an explanation for the origins of these couplings. Due to a symmetry known as lepton number symmetry, self-interactions of neutrinos are absent in the Lagrangian. Nonetheless, we establish that the observed neutrino oscillations are most effectively explained by the spontaneous breakdown of this symmetry at a scale of θ≤10-10 GeV, below which the effective theory exhibits radiatively shattered symmetry. We have calculated θ and numerous cosmological factors, finding that the simplest iterations of this model align with recent findings.\n\nFurthermore, we briefly explore variations of the model that incorporate additional scalar fields and steps to test its validity. We continue to observe that the best explanation for the known neutrino oscillations arises from the spontaneous breakdown of this symmetry at the specified scale, as mentioned earlier. We recalculate θ and various cosmological factors, and we find that the simplest versions of our model remain consistent with recent research outcomes. Additionally, we briefly explore potential modifications to the model by introducing extra scalar fields and proposing testable steps to validate its validity.",
        "ori-fast-z-score": -0.6625891564490792,
        "water-fast-z-score": 6.333333333333333,
        "rewrite-fast-z-score": 0.7181848464596079
    },
    {
        "original_text": "Massive black holes exist in all massive galaxies. They grow over time due to accretion of matter. Their masses can be estimated using the fundamental plane of black hole activity. The plane is based on three parameters, namely the velocity dispersion of the host galaxy, the stellar velocity dispersion and the radius of the sphere of influence of the black hole. The first two are measured for the galaxy, while the third one can be estimated using the black hole s mass. The latter two are strongly correlated and can be used to estimate the mass of the black hole. Such estimation has been done for a large sample of galaxies and it was found that more massive galaxies have on average more massive black holes. This correlation can be expressed as a linear relation of the form: M_{BH} = A x M_{spheroid} + B where M_{BH} is the mass of the black hole, M_{spheroid} is the mass of the spheroid, A is a coefficient close to 7 and B is a constant. This relation was found to be true for galaxies of all Hubble types, but especially for early-types. The correlations with the host galaxy properties were stronger for the spheroid than for the black hole mass, which can be explained by the merger driven growth of black holes. The scatter of the relation can be reduced by using another observable, namely the luminosity of the spheroid. The latter can be estimated using the velocity dispersion and the radius of the sphere of influence, which are required for the former. The latter can be estimated from the B-band luminosity of the galaxy. It was found that the scatter of the black hole mass - spheroid luminosity relation is significantly lower than for the other relations, and the best-fit coefficient is closer to 10, which can be explained by more accurate measurements and less intrinsic scatter in the spheroid luminosity. It is suggested that the black hole mass - spheroid luminosity relation can be used as a new standard for estimating the black hole mass. This is a preprint of a paper submitted to the Astrophysical Journal. Please check if the formatting is correct before copy/pasting it into your essay. ForarXiv.org this paper can be found at https://arxiv.org/abs/1901.05371 I have removed my name and the name of the institution. Please leave this paper online if it is not yet too old. I hope this paper will be useful and stimulate discussion. Comments, questions and suggestions are most welcome! Sincerely, Artemy Videv Videv Artemy Translator: Automated Translation by Google. Please check if the formatting is correct before using it in your essay. For arXiv.org this paper can be found at https://arxiv.org/abs/1901.05371 I",
        "watermark_text": "Massive black holes occur in all huge galaxies . They develop over time due to accretion of matter .Their masses can be estimated utilizing the fundamental plane of black hole activity . The plane is based on three variables , comprising the velocity dispersion of the host universe , the stellar velocity dispersion and the radius of the circle of influence of the dark hole .The first two are measured for the galaxy , while the third one can be assessed utilizing the dark hole s mass . The last two are strongly correlated and can be used to estimate the mass of the dark hole .Such estimation has been performed for a large sample of galaxies and it was shown that more massive galaxies have on average more massive black holes . This variance can be written as a linear relation of the form : M _ { BH } = A x M _ { spheroid } + B where M _ { BH } is the mass of the dark hole , M _ { spheroid } is the mass of the spheroid , A is a coefficient close to 7 and B is a constant .This connection was shown to be true for clusters of all Hubble types , but particularly for earliest - types . The correlations with the host star properties were stronger for the spheroid than for the dark hole mass , which can be described by the merger induced development of red holes .The scatter of the relation can be reduced by using another observable , principally the luminosity of the spheroid . The latter can be estimated using the velocity dispersion and the radius of the circle of influence , which are required for the former .The latter can be determined from the B - band luminosity of the galaxy . It was shown that the scatter of the dark hole mass - spheroid luminosity relation is significantly less than for the other relations , and the best - fitting coefficient is closer to 10 , which can be described by more accurate measurements and less intrinsic scatter in the spheroid luminosity .It is suggested that the dark hole mass - spheroid luminosity relation can be used as a new standard for estimating the dark hole mass . This is a preprint of a paper prepared to the Astrophysical Journal .Please check if the formatting is accurate before copy / pasting it into your essay . ForarXiv . org this paper can be found at https : / / arxiv . org / abs / 1901 . 05371 I have removed my name and the name of the organization .Please left this paper online if it is not already too old . I pray this paper will be valuable and encourage comment .Comments , questions and suggestions are most welcome ! Sincerely , Artemy Videv Videv Artemy Translator : Automated Translation by Google .Please check if the formatting is accurate before use it in your essay . For arXiv . org this paper can be found at https : / / arxiv . org / abs / 1901 . 05371 I",
        "rewrite_text": "改写后的英文文本如下：\n\nGiant black holes are prevalent in all vast galaxies, developing over time due to the accumulation of matter. Their masses can be estimated utilizing the fundamental plane of black hole activity, which is based on three variables: the velocity dispersion of the host universe, the stellar velocity dispersion, and the radius of influence of the dark hole. The first two variables are measured for the galaxy, while the third can be determined using the dark hole's mass. The latter two variables are strongly correlated and can be used to estimate the black hole's mass.\n\nSuch estimations have been conducted on a large sample of galaxies, revealing that more massive galaxies tend to have larger black holes on average. This relationship can be expressed as a linear equation: M_BH = A x M_spheroid + B, where M_BH represents the mass of the black hole, M_spheroid represents the mass of the spheroid, A is a coefficient close to 7, and B is a constant. This correlation has been found to be valid across clusters of all Hubble types, especially for earlier types.\n\nThe correlations with the host star properties are stronger for the spheroid than for the dark hole mass. This can be attributed to the merger-induced development of red holes. The scatter of the relationship can be reduced by utilizing another observable, primarily the luminosity of the spheroid. This can be estimated using the velocity dispersion and influence circle radius, which are required for determining the former. These values can be determined from the B-band luminosity of the galaxy.\n\nThe study shows that the scatter in the black hole mass-spheroid luminosity relationship is significantly less than in other relationships. The best-fitting coefficient is closer to 10, which may be explained by more accurate measurements and less intrinsic scatter in the spheroid luminosity. It is suggested that the black hole mass-spheroid luminosity relationship can be utilized as a new standard for estimating black hole masses.\n\nThis preprint is prepared for submission to the Astrophysical Journal. Before copying and pasting into your essay, please verify the formatting is accurate. For arXiv.org, this paper can be found at https://arxiv.org/abs/1901.05371. I have removed my name and the name of my organization. Please leave this paper online if it is not already too old. I hope this paper will be valuable and welcome comments, questions, and suggestions. Sincerely, Artemy Videv.\n\n请注意，本文为机器翻译，可能存在不准确或不通顺的地方。在使用前，请再次核对并修改。",
        "ori-fast-z-score": -1.7386365758424454,
        "water-fast-z-score": 6.576581830360554,
        "rewrite-fast-z-score": 2.1602468994692865
    },
    {
        "original_text": "Free core nutation, also known as temporal nutation, is a relatively fast component of Earth s rotation. It is directly proportional to the axial wobble, which in turn is driven by the planet s mass and moment of inertia. Temporal nutation is measured by satellites with very high precision, which has enabled detection of Free Core Nutation Frequency (FCNF). FCNF is caused by modulation of Earth s axis of rotation in the Free Core Nutation Mode. The amplitude of FCNF may vary with time and gives information about dynamics of Earth s core. In this study we analyze an existing database of FCNF observations from space. We apply two methods to extract theFCNF signal from the noise. The first one is based on statistical hypothesis testing and calculates the significance of FCNF signal. The second one is based on empirical mode decomposition and identifies components of FCNF signal. The results of both methods are consistent and show that the amplitude of FCNF varies with time. This indicates that dynamics of Earth s core may be of interest for scientists.",
        "watermark_text": "Free core nutation , sometimes called as temporal nutation , is a fairly fast element of Earth s rotation . It is directly proportional to the axial wobble , which in turn is caused by the planet s mass and moment of inertia .Temporal nutation is measured by satellites with very high precision , which has allowed recognition of Free Core Nutation Frequency ( FCNF ) . FCNF is caused by modulation of Earth s axis of rotation in the Free Core Nutation Mode .The amplitude of FCNF could vary with time and provides knowledge about dynamics of Earth s core . In this study we assess an established data of FCNF observations from space .We use two means to extract theFCNF signal from the noise . The first first is based on statistical hypothesis testing and calculates the importance of FCNF signal .The second one is based on empirical mode formulation and identifies parts of FCNF signal . The results of both approaches are reliable and suggest that the frequency of FCNF vary with time .This suggest that behavior of Earth s core could be of interest for researchers .",
        "rewrite_text": "Free core nutation, also known as temporal nutation, represents a relatively rapid component of Earth's rotational movement. It is directly related to the axial wobble, which in turn is influenced by the planet's mass and moment of inertia. Satellites measure temporal nutation with extreme precision, enabling the identification of the Free Core Nutation Frequency (FCNF). FCNF arises from modulation of Earth's axis of rotation in the Free Core Nutation Mode. The amplitude of FCNF can fluctuate over time, providing insights into the dynamics of Earth's core. In this study, we evaluate established data from FCNF observations gathered from space. We employ two methods to extract the FCNF signal from noise: the first relies on statistical hypothesis testing to assess the significance of the FCNF signal, while the second is based on empirical mode formulation to identify components of the FCNF signal. The results from both approaches are reliable and indicate that the frequency of FCNF varies with time. This suggests that the behavior of Earth's core may be of interest to researchers.",
        "ori-fast-z-score": -0.819288030372914,
        "water-fast-z-score": 5.266851623825876,
        "rewrite-fast-z-score": 1.885618083164127
    },
    {
        "original_text": "This work revisits the ideal fluid dynamics on Lie groups from a control perspective. It is well-known that such theories can be obtained from the Lagrangian formulation of the celebrated Euler equation on the diffeomorphism group of a three dimensional Riemannian manifold. An equivalent Hamiltonian description using the theory of Lie groupoids and Lagrangian mechanics on Lie groupoids is proposed, and explicit constructions are given for some important special cases including fluid dynamics on Lie groups, Newtonian gravity on the special orthogonal group and the general linear group, and the classical ECSK theory of non-relativistic gravity. In the Hamiltonian description, the fluid dynamics on Lie groups and ideal hydrodynamics are described by a Lie groupoid, with the fluid dynamics being a particular case in which the source of the gauge groupoid morphism is the diffeomorphism group of the fluid domain. In particular, in the particle description, the fluid dynamics on Lie groups and ideal hydrodynamics can be seen as resulting from a deformation quantization of the source groupoid. It is shown that the Schrödinger equation, which describes the evolution of quantum mechanics, arises as the geodesic equation for the magnetic connection of the deformation quantization. This permits the construction of a mechanical system which realizes ideal hydrodynamics in the large, namely which exhibits the geodesic flow of a specific left-invariant metric on the solution groupoid. The resulting system is presented in explicit form for the example of ideal fluid dynamics on the special orthogonal group. It is also shown that the system so constructed is Hamiltonian with respect to a modified symplectic structure, corresponding to that of Dubois-Viallet on the Lie group corresponding to the special orthogonal group. Some exact solutions are also presented.",
        "watermark_text": "This study revisits the ideal fluid dynamics on Lie groups from a control perspective . It is well - famous that such theories can be obtained from the Lagrangian formulation of the celebrated Euler equation on the diffeomorphism group of a three dimensional Riemannian manifold .An equivalent Hamiltonian characterization utilizing the theory of Lie groupoids and Lagrangian physics on Lie groupoids is proposed , and precise constructions are given for some important particular instances namely flow dynamics on Lie groups , Newtonian gravity on the special orthogonal group and the general linear group , and the classical ECSK theory of non - relativistic gravity . In the Hamiltonian description , the liquid mechanics on Lie groups and ideal hydrodynamics are explained by a Lie groupoid , with the liquid mechanics being a particular instance in which the source of the gauge groupoid morphism is the diffeomorphism group of the liquid domain .In particular , in the particle model , the fluid dynamics on Lie groups and ideal hydrodynamics can be saw as occurring from a deformation quantization of the source groupoid . It is seen that the Schrödinger equation , which explains the evolution of quantum mechanics , emerges as the geodesic equation for the magnetic link of the deformation quantization .This enables the creation of a mechanical system which understands perfect hydrodynamics in the big , namely which exhibits the geodesic flow of a certain left - invariant metric on the solution groupoid . The resulting system is provided in explicit form for the example of perfect fluid dynamics on the special orthogonal group .It is also shown that the scheme so built is Hamiltonian with regard to a reduced symplectic structure , analogous to that of Dubois - Viallet on the Lie set corresponding to the special orthogonal group . Some exact solutions are also presented .",
        "rewrite_text": "This research revisits the study of ideal fluid dynamics on Lie groups from a control theory perspective. It is widely recognized that such theories can be derived from the Lagrangian formulation of the renowned Euler equation on the diffeomorphism group of a three-dimensional Riemannian manifold. \n\nAn equivalent Hamiltonian characterization is proposed, utilizing the theory of Lie groupoids and Lagrangian physics on these groupoids. Precise constructions are provided for various important cases, such as flow dynamics on Lie groups, Newtonian gravity on special orthogonal and general linear groups, and the classical ECSK theory of non-relativistic gravity.\n\nIn the Hamiltonian framework, the mechanics of liquids on Lie groups and ideal hydrodynamics are explained through a Lie groupoid. Specifically, liquid mechanics serve as a particular example where the source of the gauge groupoid morphism is the diffeomorphism group of the liquid's domain. Within the particle model, fluid dynamics on Lie groups and ideal hydrodynamics can be viewed as arising from a deformation quantization of the source groupoid.\n\nIt is observed that the Schrödinger equation, which governs the evolution of quantum mechanics, emerges as the geodesic equation for the magnetic link in deformation quantization. This enables the creation of a mechanical system that comprehends perfect hydrodynamics in a broader context, exhibiting the geodesic flow of a specific left-invariant metric on the solution groupoid.\n\nThe resulting system is presented in explicit form for the case of perfect fluid dynamics on the special orthogonal group. Furthermore, it is demonstrated that the constructed scheme is Hamiltonian in relation to a reduced symplectic structure, analogous to that of Dubois-Viallet on the Lie set corresponding to the special orthogonal group. Several exact solutions are also presented.",
        "ori-fast-z-score": -3.2863353450309964,
        "water-fast-z-score": 4.199206274206274,
        "rewrite-fast-z-score": 2.1263507521967115
    },
    {
        "original_text": "A background study for the CERN Axion Solar Telescope (CAST) pn-CCD detector is presented. The CAST experiment is searching for solar axions by measuring their invisible photon-axion conversion probability in the magnetic field of the Southern solar farm. The pn-CCD is one of the detector systems of CAST. It consists of two hexagonal planes of Silicon photomultipliers with 0.85 mm depleted surface detector, covering 95% of the total sensitive area. This study updates and completes the model-independent background analysis of the detector, presented in a previous publication. The expected rates for the gamma-ray induced signals in the silicon detector and in the surrounding passive material are computed and compared to the measured background. The resulting rates are used to derive new limits on photon backgrounds for CAST. esk A background study for the CERN Axion Solar Telescope (CAST) pn-CCD detector is presented. The CAST experiment is searching for solar axions by measuring their invisible photon-axion conversion probability in the magnetic field of the Southern solar farm. The pn-CCD is one of the detector systems of CAST. It consists of two hexagonal planes of Silicon photomultipliers with 0.85 mm depleted surface detector, covering 95% of the total sensitive area. This study updates and completes the model-independent background analysis of the detector, presented in a previous publication. The expected rates for the gamma-ray induced signals in the silicon detector and in the surrounding passive material are computed and compared to the measured background. The resulting rates are used to derive new limits on photon backgrounds for CAST.",
        "watermark_text": "A background survey for the CERN Axion Solar Telescope ( CAST ) pn - CCD detector is displayed . The CAST study is searching for solar axions by monitoring their invisible photon - axion conversion likelihood in the magnetic field of the Southern solar farm .The pn - CCD is one of the detector systems of CAST . It consists of two hexagonal planes of Silicon photomultipliers with 0 . 85 cm depleted surface detector , covering 95 % of the total sensitive area .This study updates and completes the model - based background assessment of the device , provided in a earlier published . The expected rates for the gamma - ray induced signals in the silicon detector and in the nearby passive metal are computed and compared to the measured background .The resulting rates are using to derive new limits on photon backgrounds for CAST . esk A background survey for the CERN Axion Solar Telescope ( CAST ) pn - CCD telescope is displayed .The CAST study is searching for solar axions by monitoring their invisible photon - axion conversion likelihood in the magnetic field of the Southern solar farm . The pn - CCD is one of the detector systems of CAST .It consists of two hexagonal planes of Silicon photomultipliers with 0 . 85 cm depleted surface detector , covering 95 % of the total sensitive area . This study updates and completes the model - based background assessment of the detector , provided in a earlier published .The expected rates for the gamma - ray induced signals in the silicon detector and in the nearby passive media are computed and compared to the measured background . The resulting rates are using to derive new limits on photon backgrounds for CAST .",
        "rewrite_text": "The background survey for the CERN Axion Solar Telescope (CAST) pn-CCD telescope has been presented. The CAST research aims to detect solar axions by monitoring their potential for invisible photon-axion conversion in the magnetic field of the Southern solar farm. The pn-CCD detector, a component of the CAST system, features two hexagonal planes of silicon photomultipliers with a 0.85cm depleted surface detector, covering 95% of the total sensitive area. This study upgrades and completes the previously published model-based background assessment for the device. It calculates and compares expected gamma-ray induced signal rates in the silicon detector and nearby passive metal to the measured background. The resulting rates are then utilized to establish new limits on photon backgrounds for CAST.",
        "ori-fast-z-score": 0.7427813527082074,
        "water-fast-z-score": 6.930735005704535,
        "rewrite-fast-z-score": 1.8382900600361156
    },
    {
        "original_text": "A wide range of masses, from 3 to 18M_odot, has been found for Single stars. This range includes 4 to 18M_odot for White dwarfs. The maximum masses of neutron stars have been found to be 2.2-2.5M_odot, but there is a large uncertainty in this number. For binary systems, it is much harder to form such massive compact objects, because a larger amount of matter must be compressed for a longer time. The origin of these very massive objects is not clear. It has been proposed that these very massive objects might be the remnants of very massive Population III stars. These very massive Population III stars might have formed in metal free regions of the early universe due to the bottom up creation mechanism for these very massive objects. Very massive Population III stars might have formed with approximately 40-200M_odot. When these very massive Population III stars eventually exhaust their nuclear fuel, they could end their life as very massive compact objects. Currently, there are very limited models for these very massive compact objects, and this hampers our understanding of their evolution. Current very massive compact object models assume the compact objects have a constant mass equal to the initial mass of the progenitor. However, a more realistic model for the compact object masses should take into account the possible loss of mass during the formation process.",
        "watermark_text": "A wide range of masses , from 3 to 18M _ odot , has been detected for Single stars . This range includes 4 to 18M _ odot for White dwarfs .The maximum masses of neutron stars have been shown to be 2 . 2 - 2 . 5M _ odot , but there is a large uncertainty in this amount . For binary systems , it is much harder to form such giant compact entities , because a greater portion of matter must be compressed for a shorter time .The origin of these very huge objects is not clear . It has been proposed that these very huge objects may be the remnants of very huge Population III stars .These very huge Population III stars would have formed in metal free regions of the early universe due to the bottom up production mechanism for these very huge objects . Very huge Population III stars would have formed with approximately 40 - 200M _ odot .When these very huge Population III stars eventually exhaust their nuclear fuel , they may end their life as very huge compact entities . Currently , there are very small theories for these very huge compact objects , and this hampers our knowing of their development .Current very huge compact object models believe the compact elements have a steady mass equal to the first weight of the progenitor . However , a more realistic model for the compact object masses should take into consideration the possible loss of mass during the formation period .",
        "rewrite_text": "A broad spectrum of single stars has been detected, ranging from 3 to 18Mₒ with a sub-range of 4 to 18Mₒ specifically for white dwarfs. The maximum mass of neutron stars has been found to be between 2.2 and 2.5Mₒ, but there is significant uncertainty in this measurement. For binary systems, it is much more challenging to form such massive and compact entities due to the need for a larger portion of matter to be compressed in a shorter period of time. The origin of these extremely large objects remains unclear, but it has been proposed that they could be remnants of extremely large Population III stars.\n\nThese enormous Population III stars would have formed in metal-free regions of the early universe through a bottom-up production mechanism, with potential masses ranging from approximately 40 to 200Mₒ. When these enormous Population III stars exhaust their nuclear fuel, they may end their existence as massive and compact entities. Currently, there are only limited theories about these vastly compact objects, which hinder our understanding of their development.\n\nCurrent models for extremely large compact objects suggest that the compact elements maintain a steady mass equal to the initial weight of the progenitor. However, a more realistic model for compact object masses should account for the potential loss of mass during the formation process.",
        "ori-fast-z-score": -1.2567574357593625,
        "water-fast-z-score": 5.633458001672132,
        "rewrite-fast-z-score": 0.9622504486493763
    },
    {
        "original_text": "Magnetodielectric media with simultaneously large magneto-optical and dielectric losses support traveling waves that are simultaneously attenuated and refracted. When resonant loss features of these media are broadened by radiative processes, the resulting local-field enhancement and media degeneration can lead to significant refraction and absorption reduction. Here we demonstrate, both theoretically and experimentally, this phenomenon of negative refraction and absorption reduction in a resonant plasmon-radiative medium based on inverse Faraday rotation in a thin yttrium iron garnet film. The observed resonant refraction reduction of over 25% is greater than that of standard resummed negative-index media. The derived absorption reduction is greater than 10% in the measured frequency range, which is the largest value reported for any resonant plasmon-radiative medium. Our results indicate that radiative losses may provide a promising approach for enhanced absorption and refraction in plasmonic and magnetic media.",
        "watermark_text": "Magnetodielectric material with simultaneously large magneto - optical and dielectric losses support moving beams that are simultaneously attenuated and refracted . When resonant loss elements of these media are broadened by radiative processes , the resulting local - field enhancement and media degeneration can lead to significant refraction and emission reduction .Here we prove , both theoretically and experimentally , this phenomenon of negative refraction and emission reduction in a resonant plasmon - radiative medium based on inverse Faraday rotation in a thin yttrium iron garnet tape . The observed resonant refraction decrease of over 25 % is greater than that of standard resummed negative - index media .The derived absorption reduction is greater than 10 % in the reported frequency region , which is the highest estimate measured for any resonant plasmon - radiative materials . Our results show that radiative losses may provide a viable solution for improving absorption and refraction in plasmonic and magnetic media .",
        "rewrite_text": "The magnetodielectric material exhibits the capability to support moving beams that are simultaneously attenuated and refracted due to its large magneto-optical and dielectric losses. When the resonant loss elements of these media are broadened by radiative processes, it can result in local field enhancement and media deterioration, ultimately leading to significant refraction and emission reduction. Through both theoretical and experimental proofs, we demonstrate this phenomenon of negative refraction and emission reduction in a resonant plasmon-radiative medium, utilizing inverse Faraday rotation in a thin yttrium iron garnet tape. The observed decrease in resonant refraction exceeding 25% is greater than that of standard resummed negative index media. Furthermore, the derived absorption reduction is greater than 10% in the reported frequency region, which is the highest estimate measured for any resonant plasmon-radiative materials. Our findings suggest that radiative losses may offer a practical solution for enhancing absorption and refraction in both plasmonic and magnetic media.",
        "ori-fast-z-score": 1.016001016001524,
        "water-fast-z-score": 5.669467095138408,
        "rewrite-fast-z-score": 4.608176875690327
    },
    {
        "original_text": "The post-Newtonian (PN) approximation is a powerful method to analyze the behavior of gravitation in systems consisting of compact objects. The first post-Newtonian approximation (1PN) of general relativity (GR) was constructed by Einstein and Rosen in 1932. Scalar-tensor theory (STT) generalizes GR by generalizing the graviton into a more general scalar field, leading to additional gravitational degrees of freedom. The most general STT action consistent with general covariance and single-scalar-fieldsymmetry is that of Horndeski. In 1981, Deffayet et al. found a novel class of scalar-tensor theories, known as scalar-tensor theories with non-minimal coupling (STT NGC ), which differ from general STT by the form of the coupling function. To first order in the usual PN parameter ( = GM/(rc^2)), where M and r are the mass and distance of the system, respectively, and G is the Gravitational constant, the equations of motion for a two-body system in general STT NGC  differ from those of GR only by 2.5PN order, because the self-coupling of the scalar field enters at 1.5PN order. In 2015, Brax et al. showed that a special case of STT NGC  called scalar-tensor-relativistic theory (STR) is a good approximation to general STT in a large class of spherically symmetric solutions, dubbed nearly solutions. To zeroth order in the usual PN parameter, the equation of motion for a nearly spherically symmetric system in STR is fourth order and reduces to the ordinary Emden equation, whose solutions are exact scalar-Gauss-Bonnet black holes with a finite horizon. In this Letter we consider the slow motion of particles around such black holes and compare the resulting scalar-Gauss-Bonnet Hamiltonians with the ones of GR. We find that: i) in the 1.5PN and 2PN orders, the first terms in the potential and precession Hamiltonians of STR differ from those of GR, respectively, by scalar and Gauss-Bonnet topological invariants; ii) the 1PN terms in the potential and precession Hamiltonians of STR are proportional to the square of the absolute value of the Det of the spatial metric. Henceforth, these invariants vanish for black holes in Einstein gravity. In conclusion, we have obtained the 2PN approximation of scalar-Gauss-Bonnet theories of gravity.",
        "watermark_text": "The post - Newtonian ( PN ) method is a powerful method to analyze the dynamics of gravitation in structures consisting of compact entities . The first post - Newtonian approximation ( 1PN ) of regular relativity ( GR ) was constructed by Einstein and Rosen in 1932 .Scalar - vector theory ( STT ) generalizes GR by generalizing the graviton into a more general scalar field , leading to extra gravitational degrees of liberty . The most general STT action compatible with general covariance and single - scalar - fieldsymmetry is that of Horndeski .In 1981 , Deffayet et al . found a novel class of scalar - vector models , known as scalar - vector models with non - reduced correlation ( STT NGC ) , which change from general STT by the form of the coupling function .To first order in the usual PN parameter ( = GM / ( rc ^ 2 ) ) , where M and r are the mass and distance of the system , respectively , and G is the Gravitational constant , the equations of motion for a two - body system in general STT NGC differ from those of GR only by 2 . 5PN order , because the self - coupling of the scalar field enters at 1 . 5PN order . In 2015 , Brax et al .showed that a general case of STT NGC called scalar - vector - relativistic physics ( STR ) is a better approximation to general STT in a large class of spherically invariant solutions , nicknamed almost solutions . To zeroth order in the usual PN parameter , the equation of movement for a nearly spherically symmetric system in STR is fourth order and reduces to the ordinary Emden equation , whose solutions are exact scalar - Gauss - Bonnet black holes with a finite horizon .In this Letter we study the slow motion of particles around such black holes and compare the resulting scalar - Gauss - Bonnet Hamiltonians with the ones of GR . We see that : i ) in the 1 . 5PN and 2PN orders , the first terms in the potential and precession Hamiltonians of STR vary from those of GR , respectively , by scalar and Gauss - Bonnet topological invariants ; ii ) the 1PN terms in the potential and precession Hamiltonians of STR are proportional to the square of the absolute value of the Det of the spatial metric .Henceforth , these invariants vanish for black holes in Einstein gravity . In conclusion , we have achieved the 2PN approximation of scalar - Gauss - Bonnet theories of gravitational .",
        "rewrite_text": "The post-Newtonian (PN) method is an effective approach for analyzing the gravitational dynamics in structures made up of compact entities. The initial 1PN approximation of general relativity (GR), constructed by Einstein and Rosen in 1932, represents a fundamental step in understanding gravitational interactions. Scalar-vector theory (STT) broadens the scope of GR by extending the concept of the graviton to a more general scalar field, leading to additional gravitational degrees of freedom. The most general STT action that is compatible with general covariance and single-scalar-field symmetry is attributed to Horndeski.\n\nIn 1981, Deffayet and his colleagues discovered a novel class of scalar-vector models known as STT with Non-reduced Correlation (STT-NGC). These models differ from the standard STT due to the form of the coupling function. At the first order of the typical PN parameter (equal to GM/(rc^2), where M and r represent the mass and distance of a system, respectively, and G is the gravitational constant), the equations of motion for a two-body system in STT-NGC differ from those in GR by only 2.5PN orders. This is because the self-coupling of the scalar field enters at the 1.5PN order.\n\nIn 2015, Brax et al. showed that a particular case of STT-NGC, termed Scalar-Vector-Relativistic Physics (STR), provides a more accurate approximation to general STT in a wide range of spherically invariant solutions, often referred to as \"almost solutions.\" When considering the zeroth order of the typical PN parameter, the equation of motion for a nearly spherically symmetric system in STR is fourth-order and reduces to the ordinary Emden equation. Its solutions are exact scalar-Gauss-Bonnet black holes with a finite horizon.\n\nIn this letter, we investigate the slow motion of particles around these black holes and compare the resulting scalar-Gauss-Bonnet Hamiltonians with those of GR. Our findings are as follows: i) In the 1.5PN and 2PN orders, the initial terms in the potential and precession Hamiltonians of STR differ from those of GR due to scalar and Gauss-Bonnet topological invariants, respectively. ii) The 1PN terms in the potential and precession Hamiltonians of STR are proportional to the square of the absolute value of the determinant of the spatial metric. These invariants vanish for black holes in Einstein's gravity. In conclusion, we have achieved the 2PN approximation for scalar-Gauss-Bonnet theories of gravity.",
        "ori-fast-z-score": -1.7025130615174973,
        "water-fast-z-score": 2.8942722045797455,
        "rewrite-fast-z-score": -0.6488856845230502
    },
    {
        "original_text": "The generation interval (GI) is the time between the infectious period of an individual and when they would become contagious again. During the COVID-19 pandemic, the GI was found to be decreasing in many countries, indicating that the virus could remain viable in the population for a shorter time. This could help the virus spread throughout the population more quickly, or it could lead to increased uncertainty about when an individual is contagious, which could compromise vaccination strategies or other mitigation strategies that depend on knowing the timing of infection. To understand this phenomenon, we analyse data on COVID-19 cases in Australia, Brazil, Italy, and the United States. We find that changes in GI vary over time, following trends in cumulative cases. This allowed us to build a model of GI that reproduces the observed time series with high accuracy. We also find that changes in GI are positively correlated with changes in cumulative cases, both globally and on annual timescales, which indicates that the observed contraction in GI is a global phenomenon. Our findings indicate that real-time monitoring of COVID-19 GI can help public health authorities better understand the COVID-19 pandemic and respond more effectively.",
        "watermark_text": "The generation interval ( GI ) is the period between the infectious period of an individual and when they may grow contagious again . During the COVID - 19 pandemic , the GI was shown to be declined in many states , showing that the infection could stay healthy in the population for a shorter time .This might help the infection expand throughout the population more easily , or it could lead to greater anxiety about when an individual is contagious , which potentially compromise vaccination methods or other mitigation strategies that rely on understanding the timing of infection . To understand this phenomenon , we analyse information on COVID - 19 infections in Australia , Brazil , Italy , and the United States .We see that changes in GI differ over time , following trends in cumulative instances . This allowed us to build a description of GI that reproduces the observed period series with high clarity .We also find that changes in GI are strongly correlated with shifts in cumulative instances , both globally and on yearly timescales , which implies that the reported contraction in GI is a global phenomenon . Our findings confirm that real - time control of COVID - 19 GI can help public medical institutions better understand the COVID - 19 pandemic and respond more effectively .",
        "rewrite_text": "The generation interval (GI) refers to the time frame between an individual's infectious period and their potential re-contagiousness. During the COVID-19 pandemic, a noticeable decline in the GI was observed in various states, indicating a shorter duration of healthy carrier status within the population. This could either facilitate the infection's spread among the population or lead to increased anxiety regarding an individual's contagiousness, potentially compromising vaccination efforts or other mitigation strategies that rely on understanding infection timing. To comprehend this phenomenon, we analyze data on COVID-19 infections in Australia, Brazil, Italy, and the United States. We observe that variations in the GI differ over time, following trends in cumulative cases. This enabled us to construct a clear description of the GI that accurately replicates the observed time series. Furthermore, we found a strong correlation between changes in the GI and shifts in cumulative cases, both globally and on annual scales, suggesting that the reported reduction in GI is a global trend. Our findings underscore the importance of real-time monitoring of COVID-19 GIs for public medical institutions to better grasp the pandemic and respond more effectively.",
        "ori-fast-z-score": -1.8225913092242512,
        "water-fast-z-score": 6.11104144857543,
        "rewrite-fast-z-score": -0.10721125348377948
    },
    {
        "original_text": "A method to determine the electronic structure of single dopant atoms in silicon carbide using scanning probe microscopy is presented. As dopant atoms have different lattice structure from the host semiconductor, they may offer novel opportunities to investigate quantum systems at low temperature. Here we demonstrate this concept by employing an electrically-tunable scanning tunneling microscope (STM) to locally modify the electronic structure of a single atom of SiC. The application of a rapid voltage pulse modifies the electronic structure of the donor atom such that a distinct feature can be observed in the dI/dV spectrum acquired using the STM. This approach could enable electrical detection and control of individual dopants in silicon carbide, paving the way for investigations of quantum states in nanoscale devices and development of atomic-scale sensors with these novel properties. The control and study of individual dopants in silicon carbide (4H-SiC) offers the exciting possibility of tailoring nanoscale devices to have novel quantum properties. A single dopant atom has been shown to provide an opportunity to investigate a quantum system at low temperature. For example, the phosphorus atom is a well-studied donor atom which has been used to demonstrate the spin of a single phosphorus atom electron and investigate electron quantum tunneling in this system. However, probing the electronic structure of a single dopant atom in real time with high spatial resolution has remained a significant experimental challenge. In this work, we employ a scanning tunneling microscope (STM) to locally modify the electronic structure of a single atom of 4H-SiC. The modification of the electronic structure of a single atom allows for electrical detection and control of individual dopants in silicon carbide. As an example, we apply rapid voltage pulses to the STM to locally change the electronic structure of a single donor atom. A distinct feature in the dI/dV spectrum is observed when the donor atom’s electronic structure is modified. We characterize the modification of the electronic structure of the donor atom and its influence on the dI/dV spectrum. This approach allows for electrical detection and control of individual dopants in silicon carbide, paving the way for investigations of quantum states in nanoscale devices and development of atomic-scale sensors with these novel properties.",
        "watermark_text": "A way to estimate the electronic configuration of single dopant molecules in silicon carbide using scan probe microscopy is given . As dopant molecules have different lattice structure from the host semiconductor , they may offer novel possibilities to examine quantum systems at low heat .Here we prove this phenomenon by employing an electrically - tunable scanning tunneling microscope ( STM ) to locally manipulate the electronic configuration of a single atom of SiC . The application of a rapid voltage beam modifies the electronic configuration of the donor atom such that a distinct feature can be identified in the dI / dV spectrum gained using the STM .This method could enable electrical sensing and control of individual dopants in silicon carbide , paving the way for investigations of quantum states in nanoscale devices and generation of atomic - scale sensors with these novel qualities . The control and investigation of individual dopants in silicon carbide ( 4H - SiC ) presents the exciting possibility of tailoring nanoscale devices to have novel quantum properties .A single dopant molecule has been shown to provide an option to examine a quantum system at low heat . For instance , the phosphorus molecule is a highly - investigated donor atom which has been used to observe the spin of a single phosphorus atom electron and probe electron quantum tunneling in this system .However , probing the electronic configuration of a single dopant molecule in real time with high visual resolution has remained a substantial experimental challenge . In this research , we utilize a scan tunneling microscope ( STM ) to locally modify the electronic configuration of a single atom of 4H - SiC .The transformation of the electronic configuration of a single atom enables for electrical sensing and control of individual dopants in silicon carbide . As an instance , we apply quick voltage waves to the STM to locally switch the electronic configuration of a single donor atom .A different feature in the dI / dV spectrum is observed when the donor atom ’ s electronic configuration is modified . We characterize the modification of the electronic configuration of the donor atom and its influence on the dI / dV spectrum .This method enables for electrical observation and control of individual dopants in silicon carbide , paving the way for investigations of quantum states in nanoscale devices and generation of atomic - scale sensors with these novel structures .",
        "rewrite_text": "A method has been introduced to estimate the electronic configuration of individual dopant molecules in silicon carbide using scan probe microscopy. Since these dopant molecules possess a distinct lattice structure from the host semiconductor, they offer unique opportunities to explore quantum systems at low temperatures. We demonstrate this phenomenon by employing an electrically-tunable scanning tunneling microscope (STM) to precisely manipulate the electronic configuration of a single SiC atom. By applying a rapid voltage beam, the electronic configuration of the donor atom is altered, resulting in a distinct feature that can be identified in the dI/dV spectrum obtained through the STM.\n\nThis approach enables electrical sensing and control of individual dopants in silicon carbide, paving the way for investigations of quantum states in nanoscale devices and the creation of atomic-scale sensors with these novel properties. The study and manipulation of individual dopants in 4H-SiC presents an exciting opportunity to tailor nanoscale devices with novel quantum characteristics. A single dopant molecule has proven to be a viable option for examining quantum systems at low temperatures. For instance, the phosphorus molecule, a highly investigated donor atom, has been utilized to observe the spin of a single phosphorus atom's electron and probe electron quantum tunneling within this system.\n\nHowever, real-time, high-resolution visualization of the electronic configuration of a single dopant molecule remains a significant experimental challenge. In this research, we utilize a scan tunneling microscope (STM) to locally alter the electronic configuration of a single atom in 4H-SiC. The transformation of the atomic electronic configuration enables electrical sensing and control of individual dopants in silicon carbide. As an example, we apply rapid voltage waves to the STM to locally adjust the electronic configuration of a single donor atom, resulting in a discernible change in the dI/dV spectrum. We characterize the modification of the donor atom's electronic configuration and its impact on the dI/dV spectrum. This method enables electrical observation and manipulation of individual dopants in silicon carbide, opening doors for investigations into quantum states in nanoscale devices and the creation of novel atomic-scale sensors.",
        "ori-fast-z-score": -3.5386069477175313,
        "water-fast-z-score": 4.839354795704659,
        "rewrite-fast-z-score": 1.4362649974350634
    },
    {
        "original_text": "A growing network is a general network model in which nodes can be in one of several states (such as  alive  or  dead ) and can activate (or  arrive ) and become nodes of the network according to a scheduling rule. In this model, nodes are allowed to have an arbitrary out-degree which may change over time. In this model, we study the maximum in-degree that can be supported for a network whose out-degree is one and nodes arrive according to a Poisson process. It is proven that the maximum in-degree is one if and only if the out-degree is two, and it is two if and only if the out-degree is one and the in-degree distribution is constrained to be delta_0. In other words, the maximum in-degree is limited by the arrival process and by the out-degree of the network. In this paper, we assume that nodes arrive according to a Poisson process and the out-degree of the network is one. We study the maximum in-degree that can be supported if the in-degree is limited by the out-degree of the network and by the arrival process. It is shown that this maximum in-degree is one if and only if the out-degree is two, and it is two if and only if the out-degree is one and the in-degree distribution is delta_0. It is an open problem to determine the maximum in-degree for other out-degree distributions and other arrival processes.",
        "watermark_text": "A growing network is a general network theory in which connections can be in one of several states ( such as alive or dead ) and can activate ( or arrive ) and become nodes of the network according to a scheduling rule . In this model , nodes are allowed to have an arbitrary out - degree which may change over time .In this model , we study the maximum in - degree that can be supported for a network whose out - degree is one and nodes come according to a Poisson process . It is demonstrated that the maximum in - degree is one if and only if the out - degree is two , and it is two if and only if the out - degree is one and the in - degree distribution is constrained to be delta _ 0 .In other words , the maximum in - degree is limited by the entry cycle and by the out - degree of the network . In this paper , we suppose that nodes come according to a Poisson process and the out - degree of the network is one .We test the maximum in - degree that can be supported if the in - degree is limited by the out - degree of the network and by the arrival process . It is demonstrated that this limit in - degree is one if and only if the out - degree is two , and it is two if and only if the out - degree is one and the in - degree distribution is delta _ 0 .It is an open task to find the maximum in - degree for other out - degree distributions and other arrival processes .",
        "rewrite_text": "A general network theory known as a growing network involves connections that can exist in various states, such as being active or inactive, and can dynamically activate or arrive, becoming nodes of the network based on a scheduling rule. In this model, nodes are permitted to have an arbitrary out-degree that may fluctuate over time. Our focus in this study is to explore the maximum in-degree supported by a network whose out-degree is fixed to one and nodes arrive according to a Poisson process.\n\nIt has been proven that the maximum in-degree is one only when the out-degree is two, and it increases to two when the out-degree is one and the in-degree distribution is constrained by delta_0. In other words, the upper limit of the in-degree is determined by both the entry cycle and the out-degree of the network. For this paper, we assume that nodes arrive according to a Poisson process with an out-degree of one. We test the maximum in-degree supported when the in-degree is constrained by both the out-degree of the network and the arrival process. The results show that the limit in-degree is one when the out-degree is two, and it's two only if the out-degree is one and the in-degree distribution follows delta_0. Finding the maximum in-degree for different out-degree distributions and arrival processes remains an open task.",
        "ori-fast-z-score": 2.03701381619181,
        "water-fast-z-score": 5.038928913737635,
        "rewrite-fast-z-score": 2.5649458802128855
    },
    {
        "original_text": "Non-coding DNA (ncDNA) is much more than just  junk  DNA. Large scale analyses of full genomes have accumulated extensive evidences showing that ncDNA sequences perform many vital functions in the cell, from encoding transcriptional regulatory elements to encoding small RNAs, to forming secondary structures that modulate gene expression and even to encoding entire proteins. Noteworthy, two recent high-throughput RNA sequencing studies have revealed that up to 98% of eukaryotic genomes are transcribed into non-coding RNAs (ncRNAs). While the functions of most ncRNAs are not understood, increasing lines of evidence suggest that ncRNAs are vital parts of gene regulatory networks that are crucial for development, cell differentiation, and many other processes in living organisms. In contrast to well-known protein-coding genes, which are usually conserved and rarely mutated, ncDNA sequences show different levels of variation across different species, RNA transcripts, and individuals. Particularly, a recent large-scale analysis of whole-genome sequences in 1201 human individuals has identified noncoding DNA sequences evolving at significantly faster rates than coding sequences, highlighting the importance of ncRNAs in driving rapid evolution. Interestingly, a close observation on various types of sequence variations has found that most of the substitutions that led to amino acid changes in protein-coding sequences were caused by ncRNAs, pointing to the role of ncRNAs in driving rapid evolution and adaptation. In this talk, we will present the recent findings on these fascinating aspects of noncoding sequences, and discuss the potential implications of these findings on our understanding of evolution and human health.",
        "watermark_text": "Non - coding DNA ( ncDNA ) is much more than just junk DNA . Large scale analyses of complete genomes have amassed extensive evidences suggesting that ncDNA elements perform many vital functions in the tissue , from encoding transcriptional regulatory motifs to encoding small RNAs , to creating secondary complexes that modulate gene transcription and even to encoding entire proteins .Noteworthy , two latest high - throughput RNA sequencing studies have revealed that up to 98 % of eukaryotic genomes are transcribed into non - coding RNAs ( ncRNAs ) . While the functions of most ncRNAs are not explained , increasing lines of evidence suggest that ncRNAs are essential parts of gene regulatory circuits that are important for production , cell migration , and many other processes in living organisms .In comparison to well - famous protein - coding genes , which are typically conserved and seldom mutated , ncDNA genes show different amounts of variation across different species , RNA transcripts , and individuals . Particularly , a recent big - scale evaluation of entire - genome genes in 1201 human individuals has found noncoding DNA sequences growing at significantly faster rates than coding fragments , highlighting the importance of ncRNAs in driving faster evolution .Interestingly , a close observation on various types of sequence differences has found that most of the substitutions that led to amino residue differences in gene - coding elements were caused by ncRNAs , showing to the importance of ncRNAs in controlling quick evolution and adaptation . In this talk , we will present the recent results on these fascinating elements of noncoding sequences , and consider the possibilities implications of these results on our understanding of evolution and human health .",
        "rewrite_text": "Non-coding DNA (ncDNA) goes beyond mere \"junk DNA.\" Comprehensive genome analyses have amassed extensive evidence indicating that ncDNA elements play numerous vital roles in tissues. They range from encoding transcriptional regulatory motifs and small RNAs to creating secondary complexes that modulate gene transcription and even encoding entire proteins. Notably, two recent high-throughput RNA sequencing studies have revealed that up to 98% of eukaryotic genomes are transcribed into non-coding RNAs (ncRNAs). Although the functions of many ncRNAs are still unclear, growing evidence suggests that ncRNAs are essential components of gene regulatory networks, crucial for various processes such as production, cell migration, and more.\n\nIn contrast to well-known protein-coding genes, which tend to be conserved and rarely mutated, ncDNA genes exhibit different levels of variation across species, RNA transcripts, and individuals. Specifically, a recent large-scale evaluation of genome-wide genes in 1,201 human individuals has found that non-coding DNA sequences are growing at significantly faster rates than coding fragments. This underscores the significance of ncRNAs in driving rapid evolution.\n\nInterestingly, a close examination of various sequence differences has found that the majority of substitutions leading to amino residue differences in gene-coding elements are attributed to ncRNAs. This highlights the critical role of ncRNAs in controlling rapid evolution and adaptation. In this presentation, we will delve into the latest findings on these fascinating non-coding sequence elements and explore the potential implications of these results for our understanding of evolution and human health.",
        "ori-fast-z-score": -0.7016464154456235,
        "water-fast-z-score": 7.833494518006403,
        "rewrite-fast-z-score": 0.3508232077228117
    },
    {
        "original_text": "In this paper, we study simple shear flows in inelastic Maxwell models, where the material particles have pairwise linear forces. In particular, we consider the constant-shear-stress case. Through Chapman-Enskog analysis, we obtain a set of nonlinear differential equations for the model-specific distribution functions. For a one-dimensional model, we observe dynamic instability in the form of periodic density waves and calculate the growth rate. For a two-dimensional model, we show that the Navier-Stokes equation is restored in the large-time and large-length-scale limits. change-verification This paper studies simple shear flows in inelastic Maxwell models, where the material particles have pairwise linear forces. In particular, we consider the constant-shear-stress case. Through Chapman-Enskog analysis, we obtain a set of nonlinear differential equations for the model-specific distribution functions. For a one-dimensional model, we observe dynamic instability in the form of periodic density waves and calculate the growth rate. For a two-dimensional model, we show that the Navier-Stokes equation is restored in the large-time and large-length-scale limits.",
        "watermark_text": "In this paper , we study simple shear flows in inelastic Maxwell systems , where the material particles have pairwise linear forces . In particular , we investigate the constant - shear - strain situation .Through Chapman - Enskog analysis , we obtain a setting of nonlinear differential coefficients for the model - specific distribution functions . For a one - dimensional model , we study dynamic instability in the form of periodic density flows and estimate the development frequency .For a two - dimensional model , we prove that the Navier - Stokes equation is restored in the small - time and large - length - scale limits . shift - verification This paper studies basic shear flows in inelastic Maxwell systems , where the material particles have pairwise linear forces .In particular , we define the constant - shear - strain situation . Through Chapman - Enskog analysis , we obtain a setting of nonlinear differential coefficients for the model - specific distribution functions .For a one - dimensional model , we study dynamic instability in the form of periodic density waves and estimate the development frequency . For a two - dimensional model , we find that the Navier - Stokes equation is restored in the small - time and large - length - scale limits .",
        "rewrite_text": "In this study, we explore simple shear flows within inelastic Maxwell systems, where the material particles exert linear forces on each other. Specifically, we focus on the constant shear-strain scenario. By utilizing the Chapman-Enskog analysis, we establish a set of nonlinear differential coefficients for the model-specific distribution functions. For a one-dimensional model, we investigate the phenomenon of dynamic instability in the form of periodic density flows and estimate their associated frequencies of development. For a two-dimensional model, we demonstrate that in the limits of small time and large length scales, the Navier-Stokes equation is reestablished. This paper delves into the fundamental shear flows within these inelastic Maxwell systems, where particles interact through linear forces. We clearly define our focus on the consistent shear-strain conditions. Through the application of Chapman-Enskog analysis, we determine a set of nonlinear differential coefficients for the specific distribution functions of the models. In the case of a one-dimensional model, we examine how dynamic instability manifests in periodic density wave patterns and estimate their progression rates. For a two-dimensional model, it is revealed that in situations involving small timeframes and vast spatial scales, the Navier-Stokes equation reappears in its original form.",
        "ori-fast-z-score": 0.9138115486202573,
        "water-fast-z-score": 6.193611607315077,
        "rewrite-fast-z-score": 0.8955334711889903
    },
    {
        "original_text": "A novel approach to spectral imaging using stereoscopic vision. We develop a hybrid differential stereoscopic (HDS) system based on structured light using two synchronized broadband optical filter liquid crystal displays (LCDs). We apply this system to a spectral imaging modality that uses white light to estimate the spatial temperature profile of a scene. The approach we present combines the high spectral resolution of multi-bandpass filter spectrography with the wide field of view and high spatial resolution of structured light. We experimentally demonstrate that HDS allows for measurement of temperature differences as small as 1 degree across a 200 meter field of view with sub-millimeter resolution. We find that with additional processing, this narrowband HDS system has the sensitivity to detect temperature differences as small as 1/1000 of a degree across the same 200 meter field of view. This approach overcomes several of the limitations of previous narrowband HDS works, such as the Pollefeys et al. system which uses only a single optical filter, or the Huang et al. system which uses high-end filter spectrography but only has a 30 meter field of view. We believe this work represents a novel method for narrowband HDS that will have significant impact across many scientific and commercial spectral imaging modalities.",
        "watermark_text": "A innovative method to spectral detection using stereoscopic vision . We develop a hybrid differential stereoscopic ( HDS ) technology using on structured light utilizing two synchronized broadband optical filter solid glass images ( LCDs ) .We use this scheme to a spectral detection modality that using white light to estimate the spatial temperature profile of a scene . The method we present combines the high spectral resolution of dual - bandpass filter spectrography with the broad field of view and large spatial resolution of structured light .We experimentally prove that HDS enables for measurement of temperature differences as small as 1 degree across a 200 kilometers field of view with sub - millimeter resolution . We see that with extra processing , this narrowband HDS device has the sensitivity to identify temperature differences as low as 1 / 1000 of a degree across the same 200 kilometers field of view .This method overcomes many of the limitations of previous narrowband HDS efforts , such as the Pollefeys et al . system which uses only a single optical filter , or the Huang et al .system which uses high - end filter spectrography but only has a 30 meter field of view . We believe this research provides a new method for narrowband HDS that will have considerable impact across many academic and commercial spectral optical modalities .",
        "rewrite_text": "An innovative approach to spectral detection utilizing stereoscopic vision technology has been developed. We have created a hybrid differential stereoscopic (HDS) technology that employs structured light and utilizes two synchronized broadband optical filters, known as solid glass images (LCDs). This system is employed for a spectral detection method that utilizes white light to estimate the spatial temperature profile of a scene.\n\nThe method we present combines the high spectral resolution of dual-bandpass filter spectrography with the expansive field of view and large spatial resolution offered by structured light. Experimental results demonstrate that HDS enables the measurement of temperature differences as minute as one degree across a 200-kilometer field of view, with sub-millimeter resolution. Furthermore, with additional processing, this narrowband HDS device demonstrates sensitivity to detect temperature differences as low as one-thousandth of a degree within the same 200-kilometer field of view.\n\nThis method overcomes many of the limitations encountered in previous narrowband HDS attempts, such as the system developed by Pollefeys et al., which relied on a single optical filter, or the system by Huang et al., which utilized high-end filter spectrography but had a limited field of view of only 30 meters. We believe that this research presents a novel approach to narrowband HDS that will significantly impact a wide range of academic and commercial spectral optical applications.",
        "ori-fast-z-score": -1.3728129459672884,
        "water-fast-z-score": 6.079600189283705,
        "rewrite-fast-z-score": 0.27975144247209416
    },
    {
        "original_text": "The properties of the lightest hadrons are among the least comprehended aspects of particle physics. One important characteristic of the pion, the lightest hadron, is its radius. Measuring the radius can test our understanding of the pion s quantum fluctuations and the nature of the strong interaction. I will review recent progress on determining the pion s radius and discuss open questions and potential avenues for future research. Pion radius is a characteristic of the pion, the lightest hadron, that is less well understood than properties of other hadrons. The radius is related to the quantum fluctuations of the pion, its spatial distribution. Pion radius is of particular interest, as it can be used to test our understanding of the strong interaction and the nature of the confinement mechanism. Recent progress has been made in determining the pion radius. Advances have been made in lattice QCD, where the pion is simulated on a computational grid, and in experimental studies, where precise measurements of the pion s Form Factors are attempted. I will review these efforts and discuss open questions and potential avenues for future research. The pion is the lightest hadron. Measuring the pion s radius, which describes the distribution of its quantum fluctuations, is less well understood. The radius is related to the pion s spatial distribution. Recent progress has been made in determining the pion radius, as efforts have been made in lattice QCD and experimental studies to precisely measure the pion s Form Factors. Open questions and potential avenues for future research will be discussed. I will review recent progress on determining the pion s radius and discuss open questions and potential avenues for future research. Recent progress has been made in determining the pion radius, as efforts have been made in lattice QCD and experimental studies to precisely measure the pion s Form Factors. Open questions and potential avenues for future research will be discussed. I will review recent progress on determining the pion radius and discuss open questions and potential avenues for future research. Recent progress has been made in determining the pion radius, as efforts have been made in lattice QCD and experimental studies to precisely measure the pion s Form Factors. Open questions and potential avenues for future research will be discussed. I will review recent progress on determining the pion radius and discuss open questions and potential avenues for future research. Recent progress has been made in determining the pion radius, as efforts have been made in lattice QCD and experimental studies to precisely measure the pion s Form Factors. Open questions and potential avenues for future research will be discussed. I will review recent progress on determining the pion radius and discuss open questions and potential avenues for future research",
        "watermark_text": "The properties of the lightest hadrons are among the least comprehended aspects of particle science . One important feature of the pion , the lightest hadron , is its radius .Measuring the radius can test our knowing of the pion s quantum fluctuations and the nature of the strong coupling . I will review recent progress on calculating the pion s diameter and consider open questions and possible avenues for future research .Pion radius is a trait of the pion , the lightest hadron , that is less good explained than qualities of other hadrons . The radius is related to the quantum fluctuations of the pion , its spatial distribution .Pion radius is of especially interest , as it can be used to test our knowing of the strong coupling and the nature of the confinement mechanism . Recent research has been achieved in calculating the pion radius .Advances have been completed in crystal QCD , where the pion is modeled on a computational grid , and in experimental studies , where exact measurements of the pion s Form Factors are attempted . I will review these attempts and consider open questions and possible avenues for future research .The pion is the lightest hadron . Measuring the pion s diameter , which explains the distribution of its quantum fluctuations , is less good understood .The radius is related to the pion s spatial distribution . Recent effort has been achieved in calculating the pion radius , as efforts have been achieved in crystal QCD and theoretical experiments to exactly calculate the pion s Form Factors .Open questions and possible avenues for future research will be examined . I will review recent progress on calculating the pion s radius and consider open questions and possible avenues for future research .Recent effort has been achieved in calculating the pion radius , as efforts have been achieved in crystal QCD and experimental studies to exactly calculate the pion s Form Factors . Open questions and possible avenues for future research will be examined .I will review recent progress on calculating the pion radius and consider open questions and possible avenues for future research . Recent effort has been achieved in calculating the pion radius , as efforts have been achieved in crystal QCD and theoretical experiments to exactly assess the pion s Form Factors .Open questions and possible avenues for future research will be discussed . I will review recent progress on calculating the pion radius and consider open questions and possible avenues for future research .Recent effort has been achieved in calculating the pion radius , as efforts have been achieved in crystal QCD and experimental studies to exactly calculate the pion s Form Factors . Open questions and possible avenues for future research will be examined .I will review recent progress on calculating the pion radius and consider open questions and possible avenues for future research",
        "rewrite_text": "The comprehension of the properties of the lightest hadrons remains a challenging aspect of particle science. Specifically, the radius of the pion—the lightest hadron—plays a crucial role. Measuring this radius can test our understanding of quantum fluctuations and the nature of strong interactions within the pion.\n\nI will summarize the recent advancements in calculating the diameter of the pion and explore outstanding questions and potential avenues for future research. The pion radius, as a characteristic trait, is less well-explained compared to other hadron qualities. It is closely related to the spatial distribution and quantum fluctuations of the pion.\n\nThe study of the pion radius is particularly significant as it can be used to test our knowledge of the strong coupling and the confinement mechanism. Recent research has made significant progress in calculating the pion radius. Advancements have been made in lattice QCD, where the pion is modeled on a computational grid, as well as in experimental studies attempting precise measurements of the pion's form factors.\n\nI will examine these efforts and consider open questions that need to be addressed and potential directions for future research. The pion, being the lightest hadron, its diameter, which explains the distribution of its quantum fluctuations, remains less well understood. However, recent efforts have been made in both theoretical and experimental research to calculate the pion radius precisely, including in crystal QCD and through experimental studies of pion form factors.\n\nWe will delve into open questions and possible paths for future research. I will review recent progress in calculating the radius of the pion and consider outstanding questions that require further exploration. Recent advancements have been made in both theoretical and experimental research to calculate the pion radius precisely. We will discuss open questions and possible avenues for future research in this field.\n\nAdditionally, we will examine how crystal QCD and theoretical experiments have contributed to precisely assessing the Form Factors of the pion, as well as the challenges that still need to be addressed in this area of research. Open discussions and exploration of potential directions for future research will be an integral part of this review.",
        "ori-fast-z-score": -0.21594472522460836,
        "water-fast-z-score": 9.953996749148118,
        "rewrite-fast-z-score": 2.846049894151541
    },
    {
        "original_text": "A recent proposal for a physical implementation of the quantum NOT gate  1  has the potential to enable scalable quantum computing using superconducting qubits, opening the door to the construction of a large-scale quantum computer. Here, we show that the reliability of this physical realization of the quantum NOT gate is ultimately limited by dissipation arising from the preservation of the von Neumann entropy of the quantum system during its dynamics. In particular, we find that the infidelity of the quantum NOT gate in this realization, due to entropy accumulation, is constrained to be less than 0.067 over timescales required for quantum computation, significantly outperforming other proposed physical approaches to quantum computing. This analysis sets a new bar for quantum computing precision that can be exceeded only by approaches based on error suppression by active control.  1  M. S. Allman, et al., Implementing the Quantum NOT Gate with superconductive Qubits, arXiv:1809.07413 (2018). *ACTIVE CONTROL* is used to suppress the accumulation of entropy in our physical realization of the quantum NOT gate, allowing for an error rate less than 0.067 over timescales required for quantum computation.",
        "watermark_text": "A recent proposal for a physical formulation of the quantum NOT gate 1 has the ability to enable scalable quantum computing involving superconducting qubits , opening the entrance to the creation of a large - scale quantum computer . Here , we prove that the accuracy of this physical formulation of the quantum NOT gate is ultimately restricted by dissipation arising from the preservation of the von Neumann entropy of the quantum system during its dynamics .In particular , we find that the infidelity of the quantum NOT gate in this realization , owing to entropy accumulation , is constrained to be less than 0 . 067 over timescales needed for quantum computation , dramatically outperforming other possible physical approaches to quantum computing . This study puts a new bar for quantum computing precision that can be exceeded only by methods based on error suppression by active control .1 M . S . Allman , et al . , Implementing the Quantum NOT Gate with superconductive Qubits , arXiv : 1809 . 07413 ( 2018 ) . * ACTIVE CONTROL * is used to suppress the accumulation of entropy in our physical realization of the quantum NOT gate , allowing for an error rate less than 0 . 067 over timescales required for quantum computation .",
        "rewrite_text": "A recent proposal has been made for a physical formulation of the quantum NOT gate, which has the potential to enable scalable quantum computing with superconducting qubits, thereby ushering in the development of large-scale quantum computers. We demonstrate that the accuracy of this formulation is ultimately limited by dissipation, which arises from maintaining the von Neumann entropy of the quantum system during its evolution. Specifically, we discover that the inaccuracy of the quantum NOT gate in this implementation, due to entropy accumulation, is constrained to be less than 0.067 over the timescales necessary for quantum computation. This significantly surpasses other potential physical approaches to quantum computing. This study sets a new benchmark for quantum computing precision, which can only be surpassed by methods that rely on active control to suppress errors.\n\nIn our physical realization of the quantum NOT gate, we utilize active control to mitigate the accumulation of entropy, allowing for an error rate that is less than 0.067 over the required timescales for quantum computation, as stated in the paper \"Implementing the Quantum NOT Gate with superconductive Qubits\" by M. S. Allman et al. (arXiv: 1809.07413, 2018).",
        "ori-fast-z-score": -1.3587324409735149,
        "water-fast-z-score": 2.038098661460272,
        "rewrite-fast-z-score": 0.5852057359806528
    },
    {
        "original_text": "A significant number of galaxies have been detected in the deep field surveys with luminosities far in excess of their estimated star formation rates. These Hyper-Luminous Infrared Galaxies (HyLIRG) are of particular interest as possible probes of galaxy evolution in extreme environment. In this paper, we study six HyLIRGs with available X-ray data from the XMM-Newton observatory. These include four E+A (post-starburst) galaxies, IRAS F09230– Mo1, IRAS F00183–7 Jam1, IRAS F00397–7537 and ClG J2218.1+0052, and two late-type HyLIRGs, IRAS F00183–7 Jam2 and ClG J2218.1-0053. All except IRAS F09230– Mo1 have starburst-driven winds which are well detected in X-rays, but none have elevated absorption, suggesting that the HyLIRG phenomenon may be caused by extreme gas-rich mergers.",
        "watermark_text": "A much handful of galaxies have been detected in the deep field surveys with luminosities well in excess of their estimated star formation rates . These Hyper - Luminous Infrared Galaxies ( HyLIRG ) are of especially interest as possible probes of galaxy evolution in extreme environment .In this paper , we study six HyLIRGs with accessible X - ray data from the XMM - Newton observatory . These include four E + A ( post - starburst ) clusters , IRAS F09230 – Mo1 , IRAS F00183 – 7 Jam1 , IRAS F00397 – 7537 and ClG J2218 . 1 + 0052 , and two late - class HyLIRGs , IRAS F00183 – 7 Jam2 and ClG J2218 . 1 - 0053 .All except IRAS F09230 – Mo1 have starburst - produced winds which are better detected in X - radiation , but none have elevated absorption , showing that the HyLIRG phenomenon might be caused by intense gas - rich mergers .",
        "rewrite_text": "A significant number of galaxies with exceptionally high luminosities, surpassing their estimated star formation rates, have been discovered through deep field surveys. These galaxies, known as Hyper-Luminous Infrared Galaxies (HyLIRGs), hold particular significance as potential probes of galaxy evolution in extreme environments. In this paper, we examine six HyLIRGs with accessible X-ray data from the XMM-Newton observatory. This includes four E+A (post-starburst) clusters, namely IRAS F09230-Mo1, IRAS F00183-7 Jam1, IRAS F00397-7537, and ClG J2218.1+0052, along with two late-class HyLIRGs, IRAS F00183-7 Jam2 and ClG J2218.1-0053. Except for IRAS F09230-Mo1, all of them exhibit starburst-driven winds that are more easily detected in X-radiation. However, none of them show elevated absorption, suggesting that the HyLIRG phenomenon might be triggered by intense gas-rich mergers.",
        "ori-fast-z-score": 1.5714285714285714,
        "water-fast-z-score": 5.571428571428571,
        "rewrite-fast-z-score": 2.0604084592303353
    },
    {
        "original_text": "Comet Hale-Bopp was a remarkable visitor to Earth in 1996. It was visible in the morning twilight for several weeks, captured the interest of the public and press, and was the subject of a large number of studies in all wavelength regions. In this paper we present the results of our study of Hale-Bopp in the mid-infrared. We observed Hale-Bopp with the NASA Infrared Telescope Facility (IRTF) and the Copernicus Space Observatory in April, May, and June 1996. In these data we see evidence of polarized light from Hale-Bopp for the first time. Our observations suggest that Hale-Bopp was composed of numerous thin, strongly elongated grains of ice which were aligned nearly perpendicular to the orbital plane of Hale-Bopp and its parent nucleus. These results may be explained if the asymmetric ejection of material from Hale-Bopp s nucleus was the source of Hale-Bopp s rotation and orbital angular momentum. We also examine Hale-Bopp s dust coma and calculate its minimum orbit intersection distance (MOID). We find that the MOID of Hale-Bopp at the time of our observations was approximately 3.2 Earth radii, a value similar to the observed coma size. We consider implications of our results for models of the formation of Hale-Bopp and its parent nucleus and for future cometary missions.",
        "watermark_text": "Comet Hale - Bopp was a unprecedented visitor to Earth in 1996 . It was seen in the morning twilight for multiple weeks , captured the interest of the public and press , and was the subject of a large number of studies in all wavelength zones .In this paper we present the conclusion of our research of Hale - Bopp in the mid - infrared . We observed Hale - Bopp with the NASA Infrared Telescope Facility ( IRTF ) and the Copernicus Space Observatory in April , May , and June 1996 .In these information we find proof of polarized light from Hale - Bopp for the first time . Our observations suggest that Hale - Bopp was composed of several slender , heavily elongated grains of ice which were aligned nearly perpendicular to the orbital plane of Hale - Bopp and its parent nucleus .These data may be described if the asymmetric ejection of material from Hale - Bopp s nucleus was the origin of Hale - Bopp s rotation and orbital angular velocity . We additionally analyze Hale - Bopp s dust coma and estimate its minimum orbit junction distance ( MOID ) .We see that the MOID of Hale - Bopp at the time of our observations was roughly 3 . 2 Earth radii , a quantity similar to the known coma width . We consider implications of our findings for models of the formation of Hale - Bopp and its parent nucleus and for future cometary missions .",
        "rewrite_text": "Comet Hale-Bopp made an unprecedented visit to Earth in 1996. It was visible in the morning twilight for several weeks, captivating the interest of the public and the press, and becoming the subject of numerous studies across all wavelength zones. In this paper, we present the findings of our research on Hale-Bopp in the mid-infrared spectrum. We observed Hale-Bopp using the NASA Infrared Telescope Facility (IRTF) and the Copernicus Space Observatory between April, May, and June of 1996. Our observations provide the first evidence of polarized light emitted from Hale-Bopp. Our findings suggest that the comet was composed of slender, heavily elongated grains of ice that were aligned nearly perpendicular to both the orbital plane of Hale-Bopp and its parent nucleus. These data may be explained by the asymmetric ejection of material from the nucleus of Hale-Bopp, which is believed to be the origin of its rotational and orbital angular velocity.\n\nFurthermore, we analyzed the dust coma of Hale-Bopp and estimated its minimum orbit junction distance (MOID). At the time of our observations, we found that the MOID of Hale-Bopp was approximately 3.2 Earth radii, a value similar to the known width of its coma. We consider the implications of our findings for models of Hale-Bopp's formation and its parent nucleus, as well as for future cometary missions.",
        "ori-fast-z-score": 0.3333333333333333,
        "water-fast-z-score": 6.111111111111111,
        "rewrite-fast-z-score": 2.060839349277234
    },
    {
        "original_text": "We present the results of a spectral index distribution (SIND) analysis of a complete flux-rescaled sample of gamma-ray blazars detected by the Energetic Gamma Ray Experiment Telescope (EGRET) on the NASA Compton Gamma-Ray Observatory. We also present a preliminary SIND analysis of the nearly simultaneous 3EG sample. In contrast to most gamma-ray telescopes, EGRET detected nearly all blazars as highly polarized gamma rays, and nearly all exhibit substantial variability on many different timescales. Blazars are widely believed to be the brightest class of gamma-ray sources and their spectra are typically well-fitted by a simple power law of photon index around 2.2. We perform the first SIND analysis of high-energy gamma-ray blazars using a complete flux-rescaled sample selected from the third EGRET catalog and demonstrate that the SIND of EGRET blazars is consistent with a one-dimensional truncated power law with an exponential cutoff. The slope of the blazar SIND at high energy (E > 100 MeV) is approximately -2.2, while the slope at lower energies is slightly shallower. The high-energy index is similar to that of the low-energy gamma-ray flux, though with large error bars. We also present preliminary SIND analysis of the 3EG data, though the small number of sources and incomplete sampling of the EGRET band limit the conclusions that can be drawn. The EGRET blazar SIND may hold the key to understanding the high-energy emission process in these objects and their connection to particles in extragalactic magnetic fields.",
        "watermark_text": "We present the results of a spectral index distribution ( SIND ) evaluation of a complete flux - rescaled specimen of gamma - ray blazars detected by the Energetic Gamma Ray Experiment Telescope ( EGRET ) on the NASA Compton Gamma - Ray Observatory . We additionally offer a preliminary SIND analysis of the virtually simultaneous 3EG sample .In comparison to most γ - ray telescopes , EGRET detected almost all blazars as highly polarized gamma particles , and nearly all display significant variability on numerous separate timescales . Blazars are widely believed to be the brightest class of gamma - ray witnesses and their spectra are typically better - supplied by a simple power law of photon index approximately 2 . 2 .We perform the first SIND analysis of high - energy gamma - ray blazars using a complete flux - rescaled specimen selected from the third EGRET database and demonstrate that the SIND of EGRET blazars is compatible with a one - dimensional truncated energy law with an exponential cutoff . The slope of the blazar SIND at high energy ( E > 100 MeV ) is approximately - 2 . 2 , while the gradient at lower energies is significantly shallower .The high - energy index is identical to that of the small - energy gamma - ray density , though with large error bars . We also present initial SIND analysis of the 3EG data , though the small number of sources and incomplete analysis of the EGRET band limitation the conclusions that can be drawn .The EGRET blazar SIND may contain the key to knowledge the high - energy emitted process in these objects and their connection to particles in extragalactic magnetic fields .",
        "rewrite_text": "We present the results of an evaluation of the Spectral Index Distribution (SIND) for a fully flux-rescaled sample of gamma-ray blazars detected by the Energetic Gamma Ray Experiment Telescope (EGRET) on the NASA Compton Gamma-Ray Observatory. Additionally, we provide a preliminary SIND analysis of the nearly simultaneous 3EG dataset. In contrast to many other gamma-ray telescopes, EGRET detected nearly all blazars as highly polarized gamma particles, with a significant majority exhibiting notable variability across various timescales. Blazars are widely regarded as the most luminous class of gamma-ray sources, typically characterized by a simple power law with a photon index close to 2.2.\n\nWe conduct the first SIND analysis of high-energy gamma-ray blazars using a comprehensive flux-rescaled sample selected from the third EGRET database. Our findings indicate that the SIND of EGRET blazars is consistent with a one-dimensional truncated energy law with an exponential cutoff. The slope of the blazar SIND at high energies (E > 100 MeV) is approximately -2.2, while the gradient at lower energies is significantly shallower. Although there are large error bars, the high-energy index is comparable to that of the small-energy gamma-ray density.\n\nWe also present initial SIND analysis of the 3EG data, but the limited number of sources and incomplete analysis of the EGRET band limit the conclusions that can be drawn. The EGRET blazar SIND may hold the key to understanding the high-energy emission processes in these objects and their connection to particles in extragalactic magnetic fields.",
        "ori-fast-z-score": 0.7627700713964739,
        "water-fast-z-score": 6.674238124719146,
        "rewrite-fast-z-score": 2.030146626995893
    },
    {
        "original_text": "The so-called ’dark energy’ is believed to be responsible for the recent acceleration of the universe’s expansion. Independent observations of this phenomena have only been available since the late 2000s, so when exactly the universe started to accelerate has been a subject of significant debate. In this paper, we use a suite of different datasets to measure the timing of the transition from deceleration to acceleration. We find that while there is a high degree of agreement on the average transition time, the precision with which it has been determined is highly dependent on the choice of dataset used. Our findings indicate that cosmologists may have overestimated the uncertainty in the timing of the transition, with the impact on the design of future dark energy surveys being discussed. The acceleration of the cosmic expansion was first observed in the late 2000s, and since then its cause has been a subject of intense study. Indirect evidence for this phenomenon, based on measurements of the cosmological parameters, only became available during the past decade. Consequently, the question of precisely when the expansion started to accelerate has been a topic of debate. In this paper we consider this question using a suite of different datasets, and find that while there is good agreement on the average transition time, the precision with which it has been determined is highly dependent on the choice of dataset used. Our findings indicate that cosmologists have overestimated the uncertainty in the timing of the transition, with the impact on the design of future dark energy surveys being discussed. Our results are based on recent measurements of the cosmic microwave background (CMB) – the afterglow of the Big Bang – lensing of the background light by large scale structures and direct measurements of the expansion rate of the universe using distant supernovae. We find that the data are most consistent with the transition from deceleration to acceleration taking place between z = 0.5 and z = 0.0. However, the precision with which this has been determined is highly dependent on the dataset used, with our determination being based on the following datasets: z = 0.0 - 0.5 (average of z = 0.0 and z = 0.5); z = 0.0 - 0.2 (average of z = 0.0 and z = 0.2); z = 0.0 - 0.01 (average of z = 0.0 and z = 0.01).",
        "watermark_text": "The so - called ’ black energy ’ is suspected to be responsible for the recent acceleration of the universe ’ s evolution . Independent reports of this phenomena have only been available since the mid 2000s , so when exactly the universe started to accelerate has been a subject of serious debate .In this paper , we utilize a suite of different datasets to measure the timing of the shift from deceleration to acceleration . We see that while there is a high degree of agreement on the average transition period , the precision with which it has been determined is strongly dependent on the selection of dataset used .Our findings confirm that cosmologists could have overestimated the uncertainty in the timing of the shift , with the impact on the development of later dark energy measurements being discussed . The acceleration of the cosmic increase was first observed in the mid 2000s , and since then its reason has been a subject of focused research .Indirect evidence for this phenomenon , relying on observations of the cosmological factors , only became available during the previous decade . Consequently , the question of exactly when the development began to accelerate has been a subject of dispute .In this paper we investigate this question using a suite of different datasets , and find that while there is good agreement on the average transition period , the precision with which it has been determined is strongly dependent on the selection of dataset used . Our findings show that cosmologists have overestimated the uncertainty in the timing of the shift , with the impact on the development of later dark energy measurements being discussed .Our results are based on current observations of the cosmic microwave background ( CMB ) – the afterglow of the Big Bang – lensing of the background light by large scale structures and direct measurements of the development period of the universe using distant supernovae . We see that the statistics are most consistent with the changes from deceleration to acceleration taking place between z = 0 . 5 and z = 0 . 0 .However , the precision with which this has been determined is heavily dependent on the dataset used , with our determination being based on the following datasets : z = 0 . 0 - 0 . 5 ( average of z = 0 . 0 and z = 0 . 5 ) ; z = 0 . 0 - 0 . 2 ( average of z = 0 . 0 and z = 0 . 2 ) ; z = 0 . 0 - 0 . 01 ( average of z = 0 . 0 and z = 0 . 01 ) .",
        "rewrite_text": "The so-called \"black energy\" is suspected to be the driving force behind the recent acceleration in the universe's evolution. Since the mid-2000s, independent reports on this phenomenon have become available, leading to a significant debate about the exact timing of the universe's acceleration. In this paper, we employ a range of datasets to measure the timing of the transition from deceleration to acceleration. While there is a high degree of consensus on the average transition period, the precision of its determination strongly depends on the selected dataset.\n\nOur findings indicate that cosmologists may have overestimated the uncertainty in the timing of this shift, which has implications for the development of future dark energy measurements. The acceleration of the cosmic expansion was first observed in the mid-2000s, and since then, its cause has been a focal point of research. Indirect evidence for this phenomenon, relying on observations of cosmological factors, became available during the previous decade. Therefore, the question of exactly when the expansion began to accelerate remains a subject of debate.\n\nTo address this question, we analyze a variety of datasets in this paper and find that there is a good agreement on the average transition period. However, the precision of its determination varies significantly depending on the chosen dataset. Our results are based on current observations of the cosmic microwave background (CMB) - the afterglow of the Big Bang - as well as lensing effects of large-scale structures on background light and direct measurements of the universe's development period using distant supernovae.\n\nOur analysis suggests that the statistics are most consistent with a transition from deceleration to acceleration occurring between redshifts of z = 0.5 and z = 0.0. However, the precision of this determination is heavily influenced by the selected dataset, with our specific determination relying on the following datasets: z = 0.0 - 0.5 (average of z = 0.0 and z = 0.5), z = 0.0 - 0.2 (average of z = 0.0 and z = 0.2), and z = 0.0 - 0.01 (average of z = 0.0 and z = 0.01).",
        "ori-fast-z-score": 0.32025630761017426,
        "water-fast-z-score": 8.379912286910033,
        "rewrite-fast-z-score": 4.333333333333333
    },
    {
        "original_text": "The 74MHz system ( 74MHz ) was deployed on the very large array ( VLA ) in 2012, and has since detected hundreds of radio sources including pulsars, planets, and synchrotron radiation from our own galaxy. The 74MHz system is comprised of 24 independent 225kHz wide channels, and covers a total frequency range of 11.2-74.1MHz. The entire band is divided into two 75.6MHz-wide polarizations which are Nyquist-sampled at 1.2875MHz sample rate. Each baseband channel is 25.6kHz wide, providing a velocity resolution of 22.1km/s and a temperature sensitivity of -82.6dBm in a single beam. The 74MHz system has been used to measure redshifts in bright galaxies up to redshift 7.5, investigate the Epoch of Reionization, and conduct experiments on gravitational waves. The data from the 74MHz system are made available to the public through the F.C. Williams antenna record.",
        "watermark_text": "The 74MHz system ( 74MHz ) was deployed on the very big array ( VLA ) in 2012 , and has since observed hundreds of radio sources including pulsars , galaxies , and synchrotron emission from our own galaxy . The 74MHz system is comprised of 24 independent 225kHz wide channels , and covers a total frequency region of 11 . 2 - 74 . 1MHz .The complete band is separated into two 75 . 6MHz - broad polarizations which are Nyquist - monitored at 1 . 2875MHz sample rate . Each baseband channel is 25 . 6kHz thick , providing a speed resolution of 22 . 1km / s and a temperature sensitivity of - 82 . 6dBm in a single beam .The 74MHz program has been used to measure redshifts in bright clusters up to redshift 7 . 5 , explore the Epoch of Reionization , and conduct experiments on gravity waves . The data from the 74MHz program are making available to the public through the F . C .Williams antenna record .",
        "rewrite_text": "In 2012, the 74MHz system (equivalent to 74MHz frequency) was implemented on the vastly large array, known as the Very Large Array (VLA). Since then, it has observed hundreds of radio sources, including pulsars, galaxies, and synchrotron emissions from our own Milky Way. The 74MHz system comprises 24 independent channels, each with a width of 225kHz, spanning a total frequency range of 11.2 to 74.1MHz. The entire frequency band is divided into two broad polarizations, each 75.6MHz wide, monitored using a Nyquist sample rate of 1.2875MHz. Each baseband channel has a thickness of 25.6kHz, providing a speed resolution of 22.1 kilometers per second and a temperature sensitivity of -82.6dBm for a single beam. The 74MHz program has been utilized for measuring redshifts in bright clusters up to redshift 7.5, exploring the Epoch of Reionization, and conducting experiments on gravity waves. The data from this program are now accessible to the public via the F.C. Williams antenna record.",
        "ori-fast-z-score": -0.5443310539518174,
        "water-fast-z-score": 3.8103173776627215,
        "rewrite-fast-z-score": -0.5163977794943222
    },
    {
        "original_text": "In this paper we explore the possibility to learn something from the distribution of the biggest fragment in an iron-rich, zirconium rich, 304 stainless steel sample, using artificial intelligence techniques. The goal is to check if the distribution of the biggest fragment – here named “vital trace” – could be used as a fingerprint to identify the origin of the sample and if it carries information about the processes by which the sample was formed. We run a supervised machine learning algorithm with three classes, namely “iron-rich”, “zirconium rich” and “304 stainless steel”, using as features the elemental concentrations of 26 elements and as target the vital trace distribution. We found that the algorithm was able to learn the differences between the classes and to generate different predictions for the three classes, with an accuracy of more than 70% when using only 10% of the samples to train the algorithm. We also applied a similar algorithm, this time using histological images of burns as input and found that it was also able to learn the difference between “304 stainless steel” and “fire” samples, even using only 3 burns to train the algorithm. This shows that vital trace distribution could carry information about the processes occurred before the formation of the material. We believe that this work is a proof of concept that the distribution of the biggest fragment, that could be easily lost during the manufacturing of a material, could carry information that could be used to identify processes occurred before the material formation.",
        "watermark_text": "In this paper we investigate the prospect to find something from the distribution of the biggest fragment in an metal - rich , zirconium rich , 304 stainless steel sample , using artificial intelligence techniques . The goal is to test if the distribution of the biggest fragment – here named “ vital trace ” – may be used as a fingerprint to identify the origin of the sample and if it carries data about the mechanisms by which the sample was formed .We work a controlled machine learning algorithm with three categories , namely “ iron - rich ” , “ zirconium rich ” and “ 304 stainless steel ” , using as characteristics the elemental concentrations of 26 elements and as target the key trace distribution . We showed that the method was ability to study the differences between the classes and to create different assumptions for the three categories , with an precision of more than 70 % when using only 10 % of the samples to train the method .We later used a similar algorithm , this time used histological images of burns as input and found that it was also could to find the difference between “ 304 stainless steel ” and “ flame ” samples , even employing only 3 burns to train the method . This shows that essential trace flow could carry information about the processes occurred before the formation of the material .We believe that this study is a proof of concept that the distribution of the biggest fragment , that might be easily destroyed during the production of a substance , might carry information that might be used to identify processes occurred before the material development .",
        "rewrite_text": "In this research, we explore the potential of utilizing artificial intelligence techniques to discover insights from the distribution of the largest fragment within a metal-rich sample, specifically one rich in zirconium and composed of 304 stainless steel. Our objective is to test whether the distribution of this largest fragment, termed as \"vital trace,\" can serve as a distinctive fingerprint to identify the origin of the sample and whether it holds clues about the formation mechanisms of the sample.\n\nWe implement a controlled machine learning algorithm that categorizes the data into three groups: \"iron-rich,\" \"zirconium-rich,\" and \"304 stainless steel.\" The algorithm utilizes elemental concentration measurements of 26 elements as features and targets the key trace distribution. Our findings demonstrate the method's ability to discern differences between these classes and formulate distinct assumptions for each category, achieving over 70% accuracy with only 10% of the samples used for training.\n\nLater, we applied a similar algorithm using histological images of burns as input and found that it could also differentiate between \"304 stainless steel\" and \"flame\" samples, even with only three burns used to train the method. This indicates that essential trace flows may contain information about the processes that occurred before the material's formation.\n\nWe believe that this study serves as a proof of concept that the distribution of the largest fragment, which may be easily compromised during the production of a substance, may indeed hold valuable information that can be utilized to identify processes occurring prior to material development.",
        "ori-fast-z-score": 0.17277368511627203,
        "water-fast-z-score": 6.850168759240548,
        "rewrite-fast-z-score": 0.8703882797784892
    },
    {
        "original_text": "Recent advances in thin film fabrication technologies have led to a renewed interest in the study of magnetic thin films. This is due to the coexistence of ferromagnetic and strongly Ruderman-Kittel-Kasuya-Yoshida (RKKY) interacting layers in sub-micron scaled thin films, leading to novel magnetic ground states and complex phase diagrams. In this study, we present a magnetic characterization of Co/Pt multilayer thin films, synthesized using selective area epitaxy. Our x-ray magnetic circular dichroism measurements on the remanent state show that a nanometer-sized uncompensated magnetic region persists at low temperatures. Our results are discussed in light of existing theoretical descriptions of thin film magnetic phase diagrams. hrs.  Nature of phase transition in magnetic thin films  Recent advances in thin film fabrication technologies have led to a renewed interest in the study of magnetic thin films. This is due to the coexistence of ferromagnetic and strongly Ruderman-Kittel-Kasuya-Yoshida (RKKY) interacting layers in sub-micron scaled thin films, leading to novel magnetic ground states and complex phase diagrams. In this study, we present a magnetic characterization of Co/Pt multilayer thin films, synthesized using selective area epitaxy. Our x-ray magnetic circular dichroism measurements on the remanent state show that a nanometer-sized uncompensated magnetic region persists at low temperatures. Our results are discussed in light of existing theoretical descriptions of thin film magnetic phase diagrams.",
        "watermark_text": "Recent developments in narrow film fabrication methods have led to a increased interest in the science of magnetic thin films . This is due to the coexistence of ferromagnetic and strongly Ruderman - Kittel - Kasuya - Yoshida ( RKKY ) interacting layers in sub - micron scaled thin films , leading to novel magnetic ground states and complex phase diagrams .In this study , we present a magnetic characterization of Co / Pt multilayer thin films , constructed using selective area epitaxy . Our f - ray magnetic circular dichroism observations on the remanent state demonstrate that a nanometer - sized uncompensated magnetic area persists at low temperatures .Our results are discussed in light of older theoretical descriptions of thin film magnetic phase diagrams . hrs .Nature of phase shift in magnetic thin films Recent developments in narrow film fabrication methods have led to a increased interest in the science of magnetic thin films . This is due to the coexistence of ferromagnetic and strongly Ruderman - Kittel - Kasuya - Yoshida ( RKKY ) interacting layers in sub - micron scaled thin films , leading to novel magnetic ground states and complex phase diagrams .In this study , we present a magnetic characterization of Co / Pt multilayer thin films , constructed using selective area epitaxy . Our f - ray magnetic circular dichroism observations on the remanent state demonstrate that a nanometer - sized uncompensated magnetic area persists at low temperatures .Our results are discussed in light of previous theoretical descriptions of thin film magnetic phase diagrams .",
        "rewrite_text": "In recent times, advancements in the techniques of narrow film fabrication have sparked a growing interest in the field of magnetic thin film science. This surge of interest is attributed to the concurrent presence of ferromagnetic and strongly interacting Ruderman-Kittel-Kasuya-Yoshida (RKKY) layers within sub-micron thin films, resulting in novel magnetic ground states and intricate phase diagrams.\n\nIn this investigation, we conduct a magnetic characterization of Co/Pt multilayer thin films that have been constructed using selective area epitaxy. Our observations through f-ray magnetic circular dichroism in the remanent state reveal the persistence of a nanometer-sized uncompensated magnetic area at low temperatures. Our findings are discussed in the context of earlier theoretical frameworks that describe magnetic phase diagrams in thin films.\n\nAdditionally, the nature of phase shifts in magnetic thin films has been a subject of interest. Advancements in the fabrication methods of these narrow films have enhanced our understanding and curiosity regarding magnetic thin films. This increase in interest is linked to the concurrent existence of ferromagnetic and RKKY-interacting layers in these sub-micron-scaled thin films, which give rise to unique magnetic ground states and complex phase diagrams.\n\nOur study presents a detailed magnetic characterization of Co/Pt multilayer thin films, utilizing selective area epitaxy. Our analysis, employing f-ray magnetic circular dichroism in the residual state, highlights the enduring presence of a nanometer-scale uncompensated magnetic region at low temperatures. Our results are evaluated in light of previous theoretical descriptions regarding the phase diagrams of magnetic thin films.",
        "ori-fast-z-score": -0.7242859683401482,
        "water-fast-z-score": 4.418181209870418,
        "rewrite-fast-z-score": 1.028991510855053
    },
    {
        "original_text": "In the first part of this paper, we introduce a new algorithm that solves the advection-diffusion equation with moving boundaries (i.e. free boundaries). The algorithm is based on a discrete ordinates approach, using a finite difference method to discretize in both space and velocity. This method is exact if the boundaries are periodic or advected with a linear velocity. We show that it is also consistent if the advection speed is linearly interpolated between the cell center and the boundary. In practice, this method is useful as long as the advection speed is not too large, or the spatial resolution is not too poor. As an example of its application, we simulate the growth of solids in a viscous accretion disk. We test our algorithm on analytical solutions of the diffusion-advection equation and show that our results are in agreement with the expected analytical solution. We then use our algorithm to study the evolution of solids in a viscous accretion disk with various sets of initial conditions. In all cases, solids cannot reach the central star within a ten thousand year time scale. In the second part, we study the effect of solids on the dust evolution in the disk. Solids generate a size distribution that differs from that of dust grains. In particular, solids tend to generate a significant amount of sub-micron sized grains. The exact form of this size distribution depends on the size and composition of solids. In this part, we present several simple models to investigate the influence of solids on the dust evolution. We show that solids with a given size and composition produce different consequences on the dust size distribution. Finally, we show that the exact form of the size distribution is of little importance for the dust evolution in the most part of the disk: after about one million year only large grains are present in the disk and the evolution is fully determined by the dust settling and the radial drift. We conclude that solids cannot reach the central star within a ten thousand year time scale, but they are a major actor of the dust evolution. Solids generate a size distribution that differs from that of dust grains. The exact form of this size distribution depends on the size and composition of solids.",
        "watermark_text": "In the first part of this paper , we provide a new algorithm that solves the advection - diffusion equation with move boundaries ( i . e . free boundaries ) .The algorithm is based on a finite ordinates technique , using a finite difference method to discretize in both space and velocity . This method is exact if the boundaries are periodic or advected with a linear momentum .We see that it is also consistent if the advection time is linearly interpolated between the cell center and the boundary . In practice , this algorithm is convenient as long as the advection time is not too huge , or the spatial resolution is not too poor .As an instance of its use , we simulate the development of solids in a viscous accretion disk . We test our algorithm on analytical solutions of the diffusion - advection equation and find that our findings are in agreement with the expected analytical solution .We then use our technique to study the evolution of solids in a viscous accretion disk with various sets of initial conditions . In all situations , solids cannot attain the main star within a ten thousand year period scale .In the second part , we study the impact of solids on the dust evolution in the disk . Solids produce a size distribution that varies from that of dust grains .In particular , solids prefer to produce a substantial quantity of sub - micron large grains . The exact form of this size distribution depends on the height and composition of solids .In this part , we present many simple theories to examine the impact of solids on the dust evolution . We see that solids with a given shape and composition create different impacts on the dust size distribution .Finally , we prove that the exact form of the size distribution is of little importance for the dust evolution in the most part of the disk : after about one million month only massive grains are present in the disk and the evolution is fully governed by the dust settling and the radial drift . We assume that solids cannot attain the main star within a ten thousand year period scale , but they are a major actor of the dust evolution .Solids produce a diameter pattern that varies from that of dust grains . The exact form of this size distribution depends on the height and composition of solids .",
        "rewrite_text": "In the initial segment of this paper, we introduce a novel algorithm that efficiently resolves the advection-diffusion equation with shifting or free boundaries. This approach utilizes a finite ordinates technique, employing a finite difference method to discretize both spatial and velocity domains. It proves accurate when the boundaries are periodic or move with a linear momentum, and remains consistent when the advection time is linearly interpolated between the cell center and the boundary. In practical applications, this algorithm is convenient as long as the advection time is not excessively long and the spatial resolution is not overly inadequate.\n\nAs an exemplar, we simulate the development of solids within a viscous accretion disk using our algorithm. We test it against analytical solutions of the diffusion-advection equation and find that our findings align with expected analytical outcomes. Then, we employ our technique to study the evolution of solids in such disks under various initial conditions. In all scenarios, solids fail to reach the central star within a ten-thousand-year timeframe.\n\nIn the subsequent part, we investigate the impact of solids on dust evolution within the disk. Solids produce a size distribution that differs from that of dust grains, with a tendency to generate a significant quantity of sub-micron-sized grains. The exact shape of this size distribution depends on the height and composition of the solids. We present several simple theories to assess how solids influence dust size distribution. We observe that solids with a specific shape and composition create distinct effects on the dust size distribution.\n\nFinally, we establish that the precise form of the size distribution plays a minor role in dust evolution for the majority of the disk's extent. After approximately one million months, only massive grains remain in the disk, and the evolution is predominantly governed by dust settling and radial drift. We maintain that, although solids cannot reach the main star within the ten-thousand-year timeframe, they play a pivotal role in dust evolution, creating a unique diameter pattern that varies from that of dust grains. The exact shape of this size distribution continues to be dependent on the height and composition of the solids.",
        "ori-fast-z-score": 2.343379732657209,
        "water-fast-z-score": 9.400415740378403,
        "rewrite-fast-z-score": 2.3985014047867566
    },
    {
        "original_text": "A new generation of astrophysical electron telescopes, such as the recently-commissioned HESRG, are opening a new window onto particle astrophysics, with the potential to provide dramatic improvements in our understanding of cosmic-ray acceleration, propagation and their myriad effects. In this work, we present new HESRG data on the enigmatic recurrent nova RS Ophiuchi, whose 2006 outburst has historically been among the most energetic witnessed in the modern era. We interpret our data within the framework of nonlinear diffusive shock acceleration (DSA) to cosmic-rays, and we make a number of novel inferences. We find that the electrons and ions can be well-fitted by a single power-law of common index -2.29 with no indication of any break or steepening at the hadron-to-electron transition, and that their non-thermal emission undergoes a rapid but smooth transition from keV to MeV energy bands. From comparison with nonlinear DSA theory, we deduce that electrons are most likely accelerated to 300 TeV - 1 EeV via magnetic reconnection events within a high-speed magnetospheric outflow from the binary system, whilst ions are accelerated to 30 GeV - 1 TeV, within a slower but more-diffuse wind. We finally compare our results with previous predictions from the light-echo observations of the 2010 outburst of RS Ophiuchi - with which we likewise detect non-thermal emission - and find excellent agreement for electron energies but substantial tension for ion energies. We conclude that our results provide compelling new evidence that nonlinear DSA is an enabling paradigm for the acceleration of both electrons and ions to the very-high-energies observed in cosmic-ray sources.",
        "watermark_text": "A modern generation of astrophysical electron telescopes , such as the recently - commissioned HESRG , are opening a new window onto cosmic astrophysics , with the prospect to provide dramatic advances in our knowing of cosmic - ray acceleration , propagation and their myriad consequences . In this project , we present new HESRG data on the enigmatic recurrent nova RS Ophiuchi , whose 2006 outburst has generally been among the most intense witnessed in the present age .We interpret our information within the framework of nonlinear diffusive blast acceleration ( DSA ) to cosmic - radiation , and we make a number of novel inferences . We see that the electrons and atoms can be well - fitted by a single power - law of common index - 2 . 29 with no indication of any broke or steepening at the hadron - to - ion transition , and that their non - thermal emission undergoes a rapid but stable change from keV to MeV power bands .From comparison with nonlinear DSA theory , we deduce that electrons are most likely accelerated to 300 TeV - 1 EeV via magnetic reconnection events within a high - speed magnetospheric outflow from the binary system , whereas particles are accelerated to 30 GeV - 1 TeV , within a slower but more - diffuse wind . We last match our findings with previous predictions from the light - echo measurements of the 2010 outburst of RS Ophiuchi - with which we additionally observe non - electrical emission - and find excellent agreement for electron energies but significant tension for ion energies .We suggest that our findings provide compelling fresh confirmation that nonlinear DSA is an allowing paradigm for the acceleration of both electrons and atoms to the very - large - energies found in cosmic - ray sources .",
        "rewrite_text": "A new generation of modern astrophysical electron telescopes, including the recently commissioned HESRG, is unveiling a fresh perspective in cosmic astrophysics. These telescopes hold the potential to revolutionize our understanding of cosmic-ray acceleration, propagation, and their diverse implications. In this project, we present fresh HESRG data regarding the enigmatic recurrent nova RS Ophiuchi. Its 2006 outburst stands among the most intense observed in modern times.\n\nWithin the framework of nonlinear diffusive blast acceleration (DSA) for cosmic radiation, we interpret our findings. We observe that electrons and atoms can be accurately fitted by a single power law with a common index of -2.29, without any indications of breaks or steepening at the hadron-to-ion transition. Furthermore, their non-thermal emission exhibits a rapid but stable transition from keV to MeV power bands.\n\nIn comparison with nonlinear DSA theory, we deduce that electrons are likely to be accelerated to 300 TeV - 1 EeV through magnetic reconnection events within a high-speed magnetospheric outflow from the binary system. On the other hand, particles are accelerated to 30 GeV - 1 TeV within a slower but more diffuse wind.\n\nOur findings are in excellent agreement with previous predictions based on light-echo measurements of the 2010 outburst of RS Ophiuchi, where we also observe non-electrical emission. Specifically, we find excellent agreement for electron energies but some significant discrepancies for ion energies. We suggest that our findings provide strong evidence that nonlinear DSA is a viable paradigm for accelerating both electrons and atoms to the extremely high energies observed in cosmic-ray sources.",
        "ori-fast-z-score": -1.6858544608470492,
        "water-fast-z-score": 5.806832031806502,
        "rewrite-fast-z-score": 2.3728949893812477
    },
    {
        "original_text": "The article aims to overview the recent development in the understanding of cosmological shock waves. These are fluid dynamic phenomena which occur in an abrupt change of properties of a fluid, usually characterized by a supersonic flow jump. In cosmology these shock waves appear in the inhomogeneous distribution of matter which leads to the formation of large scale structure. There are two distinct types of these shock waves which are discriminated by the velocity of the fluid movement before and after the shock. Type 1 shocks appear in relatively low velocity flows, i.e., slower than the speed of light. These shock waves appear in the matter accretion, i.e., the process by which clusters of galaxies formed from small density fluctuations in the early universe. These shocks go under names such as sound waves or collapse shocks. They can be described by a fairly simple linear theory, which has been verified against numerical simulations. Type 2 shocks appear in the flow of high velocity gas, i.e., relativistic or transonic motions. These shock waves are more frequent and more interesting to study than the former. The Hubble flow, which is the motion of galaxies relative to the observer, appears as a type 2 shock wave. Cosmological type 2 shock waves have been invoked to explain phenomena such as the 511 keV line emission from the galactic center, the large scale structure, and the Kolmogorov spectrum of cosmological density fluctuations. In recent years, there has been a renewed interest in the cosmological shock wave phenomenon. This is because new observational discoveries such as the 511 keV line, and highly contrasted galaxy clusters have shown the need to go beyond the homogeneous and isotropic cosmological model. In the cosmological world, there is more variation in the matter density and velocity fields than what would be expected from a simple homogenous and isotropic model. It is now known that type 2 shock waves are generic features of cosmological fluid flows. Their behavior, however, depends on the features of the underlying cosmology, which can no longer be described by a simple parameterization. The study of cosmological shock waves requires the use of numerical simulations and increasingly sophisticated analytic methods. The article is organized as follows. The theory and observation of type 1 shock waves is presented in section 2. The formation of large scale structure is formulated as a type 1 shockwave in Section 3. Section 4 presents an analytic model for type 1 shock waves. In Section 5, numerical simulations of the formation of structure are reviewed. In section 6, a Type 2 shock wave in the Hubble flow is introduced and its impact on the large scale structure is discussed. Section 7 summarizes the findings.",
        "watermark_text": "The section aims to overview the recent advancement in the knowledge of cosmological shock waves . These are fluid dynamic effects which occur in an sudden transition of properties of a fluid , generally defined by a supersonic flow jump .In cosmology these blast currents form in the inhomogeneous distribution of matter which results to the formation of large scale system . There are two different kinds of these shock waves which are discriminated by the velocity of the liquid movement before and after the shock .Type 1 shocks occur in remarkably limited velocity flows , i . e . , slower than the speed of light . These shock waves appear in the matter accretion , i . e . , the process by which groups of stars formed from small velocity fluctuations in the early universe .These shocks go under names such as echo pulses or failure shocks . They can be described by a fairly simple linear concept , which has been verified against numerical simulations .Type 2 shocks occur in the flow of high velocity fluid , i . e . , relativistic or transonic motions . These shock waves are more frequent and more exciting to study than the former .The Hubble stream , which is the movement of galaxies relative to the observer , presents as a class 2 shock wave . Cosmological class 2 shock waves have been invoked to explain processes such as the 511 keV line emission from the galactic center , the huge scale system , and the Kolmogorov spectrum of cosmological density fluctuations .In recent years , there has been a increased interest in the cosmological blow wave effect . This is because novel observational discoveries such as the 511 keV line , and strongly contrasted galaxy galaxies have shown the necessity to gone beyond the homogeneous and isotropic cosmological model .In the cosmological world , there is more variation in the matter density and speed fields than what would be anticipated from a simple homogenous and isotropic model . It is now established that type 2 shock effects are generic features of cosmological flow flows .Their behavior , however , depends on the properties of the fundamental cosmology , which can no longer be described by a simple parameterization . The investigation of cosmological flash flows involves the using of computational simulations and increasingly sophisticated analytic techniques .The section is organized as follows . The theory and observation of type 1 shock waves is detailed in section 2 .The formation of large scale system is formulated as a class 1 shockwave in Section 3 . Section 4 offers an analytic model for type 1 shock waves .In Section 5 , numerical simulations of the formation of shape are reviewed . In section 6 , a Type 2 shock wave in the Hubble stream is introduced and its impact on the huge scale system is studied .Section 7 summarizes the facts .",
        "rewrite_text": "This section aims to provide an overview of recent advancements in the understanding of cosmological shock waves. These are fluid dynamic effects that occur during sudden transitions in fluid properties, typically defined by a supersonic flow jump. In cosmology, these blast currents emerge from the inhomogeneous distribution of matter, leading to the formation of large-scale systems.\n\nThere are two distinct types of these shock waves, distinguished by the velocity of the fluid movement before and after the shock. Type 1 shocks occur in flows with relatively low velocities, i.e., slower than the speed of light. These shock waves can be observed in matter accretion, the process of forming groups of stars from small velocity fluctuations in the early universe. These shocks are sometimes referred to as echo pulses or failure shocks and can be described using a relatively simple linear concept, which has been verified through numerical simulations.\n\nIn contrast, Type 2 shocks occur in high-velocity fluid flows, such as relativistic or transonic motions. These shock waves are more common and more intriguing to study than Type 1 shocks. The Hubble flow, which represents the movement of galaxies relative to the observer, exhibits characteristics of a Class 2 shock wave. Cosmological Class 2 shock waves have been used to explain phenomena like the 511 keV line emission from the galactic center, the large-scale system, and the Kolmogorov spectrum of cosmological density fluctuations.\n\nIn recent years, there has been a growing interest in the cosmological blow wave effect. This is due to novel observational discoveries, such as the 511 keV line and the stark contrasts observed in galaxy clusters, which have highlighted the need to go beyond the homogeneous and isotropic cosmological model. In the cosmological realm, there is more variability in matter density and velocity fields than what is expected from a simplified homogenous and isotropic model. It is now well established that Type 2 shock effects are a generic feature of cosmological flows. However, their behavior depends on the properties of the underlying cosmology, which can no longer be adequately described by a single parameterization.\n\nThe section is organized as follows: Section 2 details the theory and observation of Type 1 shock waves. The formation of large-scale systems is explored as a Class 1 shockwave in Section 3. Section 4 presents an analytic model for Type 1 shock waves. In Section 5, a review of numerical simulations of shape formation is provided. Section 6 introduces a Type 2 shock wave in the Hubble flow and examines its impact on the large-scale system. Finally, Section 7 summarizes the key findings.",
        "ori-fast-z-score": -0.6412234498911868,
        "water-fast-z-score": 8.763387148512887,
        "rewrite-fast-z-score": 2.321524000352037
    },
    {
        "original_text": "The MadGraph team is pleased to announce the latest major version upgrade of MadGraph, v4. MRELEASE is an important milestone for MadGraph, marking a significant evolution from early series-based simulation approaches to the modern runtime-based simulation paradigm, with unprecedented numerical precision and accuracy. MadGraph v4 boasts numerous other improvements, notably in the form of a robust and well-established Web front-end, a myriad of new out-of-the-box analysis capabilities, and improved support for LHCRun 2 and IceCube configuration. We encourage all users of MadGraph to update their codes to the latest release. MRELEASE Highlights - MadGraph has been upgraded to v4, featuring a robust and well-established Web front-end. - MadGraph is now based on the modern runtime-based simulation paradigm, with unprecedented numerical precision and accuracy. - Numerous improvements have been made to MadGraph’s core functionality, including the addition of new out-of-the-box analysis capabilities and improved support for LHC Run 2 and IceCube configuration. More Details: https://bitbucket.org/madgraph/madgraph/wiki/Home",
        "watermark_text": "The MadGraph team is delighted to announce the latest major edition upgrade of MadGraph , v4 . MRELEASE is an important milestone for MadGraph , representing a substantial progression from early series - based simulation approaches to the newer runtime - based simulation methodology , with impressive mathematical precision and accuracy .MadGraph v4 possesses numerous other upgrades , notably in the form of a reliable and well - established Web front - side , a myriad of new out - of - the - box analysis tools , and increased support for LHCRun 2 and IceCube configuration . We urge all users of MadGraph to update their codes to the latest version .MRELEASE Highlights - MadGraph has been upgraded to v4 , showing a reliable and well - established Web front - side . - MadGraph is now based on the new runtime - based simulation methodology , with incredible mathematical precision and accuracy .- Numerous improvements have been achieved to MadGraph ’ s core capabilities , notably the adding of new out - of - the - box analysis functionality and increased support for LHC Run 2 and IceCube configuration . More Details : https : / / bitbucket . org / madgraph / madgraph / wiki / Home",
        "rewrite_text": "The MadGraph team is excited to announce the latest significant upgrade of their software, MadGraph version 4. MRELEASE marks a significant milestone for MadGraph, signifying a substantial shift from early simulation approaches based on series to a newer runtime-based simulation methodology, boasting impressive mathematical precision and accuracy.\n\nMadGraph v4 comes with numerous other upgrades, particularly a reliable and well-established web front-end, a multitude of new out-of-the-box analysis tools, and enhanced support for LHCRun 2 and IceCube configurations. We strongly encourage all MadGraph users to update their codes to the latest version.\n\nMRELEASE highlights include:\n\n1. MadGraph has been upgraded to version 4, featuring a reliable and well-established web interface.\n2. MadGraph now operates on a new runtime-based simulation methodology, delivering remarkable mathematical precision and accuracy.\n3. Significant improvements have been made to MadGraph's core capabilities, including the addition of new analysis functionalities and enhanced support for LHC Run 2 and IceCube configurations.\n\nFor more details, please visit: https://bitbucket.org/madgraph/madgraph/wiki/Home.",
        "ori-fast-z-score": 0.562543950463012,
        "water-fast-z-score": 6.7082039324993685,
        "rewrite-fast-z-score": 2.680281337094487
    },
    {
        "original_text": "Coleman and De Luccia showed that nontrivial topologies with nontrivial first homotopy group allow for Euclidean wormholes. This method of traversing the wormhole requires that the Euclidean path integral be performed across the wormhole, with the traversal switched on at the end of theColeman and De Luccia showed that nontrivial topologies with nontrivial first homotopy group allow for Euclidean wormholes. This method of traversing the wormhole requires that the Euclidean path integral be performed across the wormhole, with the traversal switched on at the end of the wormhole and switched off at the end of the Euclidean universe. Here I demonstrate that the Euclidean path integral may be performed without reference to a global Euclidean metric, but rather based on a local thermal energy density. Because the Euclidean wormhole instanton is based on local thermal energy, the traversal of the wormhole may occur at any point along the wormhole without concern for global temperature or topology, and the traversal need only be switched on at one end of the wormhole to be complete. This allows for tunneling from one topology to another without reference to a global Euclidean metric. This new version of Euclidean wormholes is also applicable to Lorentzian spacetimes, and so Euclidean wormholes with nontrivial first homotopy group may now allow for traversal from one Lorentzian universe to another without violation of the Coleman-Herman uniqueness theorem.",
        "watermark_text": "Coleman and De Luccia demonstrated that nontrivial topologies with nontrivial first homotopy group enable for Euclidean wormholes . This method of traversing the wormhole requires that the Euclidean path integral be performed across the wormhole , with the traversal turned on at the end of theColeman and De Luccia demonstrated that nontrivial topologies with nontrivial first homotopy group enable for Euclidean wormholes .This method of traversing the wormhole requires that the Euclidean path integral be performed across the wormhole , with the traversal turned on at the end of the wormhole and switched off at the end of the Euclidean universe . Here I propose that the Euclidean path integral might be performed without relation to a global Euclidean metric , but rather based on a local thermal energy density .Because the Euclidean wormhole instanton is based on local thermal energy , the traversal of the wormhole could occur at any point along the wormhole without concern for global climate or topology , and the traversal need only be switched on at one end of the wormhole to be full . This enables for tunneling from one topology to another without relation to a global Euclidean metric .This new version of Euclidean wormholes is also applying to Lorentzian spacetimes , and so Euclidean wormholes with nontrivial first homotopy group could now enable for traversal from one Lorentzian universe to another without violation of the Coleman - Herman uniqueness theorem .",
        "rewrite_text": "Coleman and De Luccia have shown that the existence of nontrivial topologies with a nontrivial first homotopy group allows for the creation of Euclidean wormholes. To traverse these wormholes, it is necessary to perform the Euclidean path integral across them, with the traversal being activated at the conclusion of the process. Furthermore, I propose that the calculation of this path integral could be done without any reliance on a global Euclidean metric. Instead, it could be based on a local thermal energy density since the Euclidean wormhole instanton relies on local thermal energy. This means that the traversal of the wormhole can occur at any point along its length, disregarding global conditions or topology, and only needs to be activated at one end to be complete. This enables the possibility of tunneling between different topologies without the need for a global Euclidean metric. This new type of Euclidean wormhole also applies to Lorentzian spacetimes, suggesting that Euclidean wormholes with nontrivial first homotopy groups could facilitate travel between different Lorentzian universes without violating the Coleman-Herman uniqueness theorem.",
        "ori-fast-z-score": 1.6733200530681511,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 0.629940788348712
    },
    {
        "original_text": "A theoretical study of the decoherence of a driven multilevel quantum system interacting with a multi-bath reservoir is presented. A general description of a qubit-bath model is presented where a reduced density matrix for the qubit is obtained using a hierarchical quantum master equation approach. Our result shows that, under the secular and rotating wave approximation, the reduced density matrix of the qubit converges to a product state for a weak system-bath coupling, which indicates that the qubit state becomes insensitive to its environment. However, for a strong system-bath coupling, we show that the off-diagonal elements of the reduced density matrix decay to zero much slower than the diagonal elements, which indicates that the qubit state becomes entangled with its environment. Our result also shows that the purity of the qubit state decays to one half in a short time for any coupling strength, which indicates the fast depletion of the qubit s quantum information. We illustrate our theoretical findings using a representative model of a three-level system interacting with a one-dimensional bosonic reservoir, and show that the entanglement between the system and environment becomes stronger as the system-bath coupling increases.",
        "watermark_text": "A theoretical investigation of the decoherence of a driven multilevel quantum model interacting with a multi - shower pool is provided . A basic description of a qubit - shower theory is provided where a reduced density matrix for the qubit is produced utilizing a hierarchical quantum master equation methodology .Our result follows that , under the secular and rotating wave approximation , the reduced density matrix of the qubit converges to a product state for a weak system - bath interaction , which implies that the qubit state turns insensitive to its surroundings . However , for a powerful system - bath interaction , we find that the off - diagonal elements of the reduced density matrix decay to zero significantly slower than the diagonal elements , which implies that the qubit state remains entangled with its surroundings .Our result also shows that the purity of the qubit state decays to one half in a brief time for any interaction strength , which implies the fast depletion of the qubit s quantum information . We illustrate our theory findings using a representative model of a three - level system interacting with a one - dimensional bosonic reservoir , and suggest that the entanglement between the system and environment remains stronger as the system - shower coupling increases .",
        "rewrite_text": "A theoretical exploration of the decoherence process in a multilevel quantum model, driven and interacting with multiple shower pools, has been conducted. A fundamental description of the qubit-shower theory is presented, where a reduced density matrix for the qubit is derived using a hierarchical quantum master equation approach. Our findings indicate that, under the assumptions of secular and rotating wave approximations, the reduced density matrix of the qubit converges to a product state when there is a weak system-bath interaction. This suggests that the qubit state becomes insensitive to its environment. However, for stronger system-bath interactions, we observe that the off-diagonal elements of the reduced density matrix decay significantly slower than the diagonal elements. This suggests that the qubit state remains entangled with its surroundings. Additionally, our results show that the purity of the qubit state decreases to one half over a short period of time, regardless of the interaction strength. This implies a rapid loss of quantum information from the qubit.\n\nTo illustrate our theoretical findings, we utilize a representative model of a three-level system interacting with a one-dimensional bosonic reservoir. We suggest that the entanglement between the system and its environment persists stronger as the system-shower coupling intensifies.",
        "ori-fast-z-score": -3.350642344940943,
        "water-fast-z-score": 3.878358759406699,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "The W3 giant molecular cloud (GMC) is a prominent region in the constellation Aquila. Located some 20 kpc away, W3 is one of the nearest grand design spiral galaxies and hosts an extreme cluster of massive young stars. Historically, W3 has been used as a prototypical GMCs with significant ongoing star formation. In this study, we present far-infrared and submillimeter data from the Herschel Space Observatory, as well as molecular line and radio continuum data from the data archives, to characterize the star-forming content of W3. Far-infrared and submillimeter data are an ideal probe of thermal dust emission, which is well-correlated with young stellar populations. We find that W3 has a bolometric luminosity of 9.5 x 10^9 L⊙ and a total mass of 1.2 x 10^10 M⊙. At a column density of 1.87 x 1021 cm-2, its mass surface density is 221 M⊙ pc-2. We estimate that W3 has a mass of 250 x 10^3 M⊙ within its estimated star-formation density threshold of 2.2 x 10^4 M⊙ pc-2. This suggests that star formation is ongoing within W3 but is not significantly active. In this context, W3 likely does not qualify as a typical GMC with significant ongoing star formation, but rather is a very massive, very old (5-10 Myr), quiescent, and extremely dense star-forming region.",
        "watermark_text": "The W3 massive molecular mist ( GMC ) is a powerful region in the constellation Aquila . Located some 20 kpc apart , W3 is one of the nearest grand design spiral clusters and hosts an extreme cluster of large young galaxies .Historically , W3 has been used as a prototypical GMCs with substantial continuing star formation . In this study , we present far - infrared and submillimeter evidence from the Herschel Space Observatory , as also as molecular line and radio continuum data from the information collections , to characterize the star - making content of W3 .Far - infrared and submillimeter signals are an excellent probe of thermal dust radiation , which is well - associated with young stellar regions . We see that W3 has a bolometric luminosity of 9 . 5 x 10 ^ 9 [UNK] and a total mass of 1 . 2 x 10 ^ 10 [UNK] .At a column size of 1 . 87 x 1021 cm - 2 , its mass surface density is 221 [UNK] pc - 2 . We estimate that W3 has a mass of 250 x 10 ^ 3 [UNK] within its estimated star - formation density threshold of 2 . 2 x 10 ^ 4 [UNK] pc - 2 .This implies that star formation is continuous within W3 but is not significantly active . In this sense , W3 likely does not qualify as a typical GMC with substantial continued star formation , but rather is a very huge , very young ( 5 - 10 Myr ) , quiescent , and extremely dense star - creating area .",
        "rewrite_text": "The W3 Giant Molecular Cloud (GMC) stands as a formidable region within the constellation Aquila. Positioned roughly 20 kpc away, it serves as one of the nearest grand design spiral clusters and harbors an exceptional cluster of large, young galaxies. Throughout history, W3 has been employed as a paradigmatic GMC due to its substantial, ongoing star formation process.\n\nIn our study, we present data from the Herschel Space Observatory that provides far-infrared and submillimeter evidence, along with molecular line and radio continuum data from various information sources, to characterize the star-forming content of W3. These far-infrared and submillimeter signals are excellent probes of thermal dust radiation closely associated with young stellar regions. We observe that W3 exhibits a bolometric luminosity of 9.5 x 10^9 units and a total mass of 1.2 x 10^10 units. With a column size of 1.87 x 10^21 cm^-2, its mass surface density is estimated at 221 units per square parsec. We estimate that W3 has a mass of 250 x 10^3 units within its estimated star formation density threshold of 2.2 x 10^4 units per square parsec. This suggests that while star formation continues within W3, it is not significantly active. Therefore, in this context, W3 may not qualify as a typical GMC with substantial ongoing star formation; rather, it is an immense, young (5-10 million years old), quiescent, and extremely dense star-forming region.",
        "ori-fast-z-score": -0.502518907629606,
        "water-fast-z-score": 5.3267004208738244,
        "rewrite-fast-z-score": 2.9692614841855693
    },
    {
        "original_text": "Recently, a number of studies have claimed that the metallicity distribution of the solar neighborhood is non-Gaussian. However, these claims are based on samples with small numbers of stars. We determine the metallicity distributions in high-latitudes using data from the Sloan Digital Sky Survey Data Release 12. The number of stars in our samples are in the range of tens of thousands, and hence we are able to confirm that the metallicity distribution is consistent with being Gaussian. We determine the metallicity distributions using the targets  Galactic extinction-corrected photometry and the DR12 of the SDSS. We find that the metallicity distributions in high-latitudes show no significant differences from those of the whole sample. The probability that the high-latitude samples and the whole sample are drawn from the same population is larger than 99.99%. These results indicate that the local stellar metallicity distribution is consistent with being Gaussian, and previous studies that claimed non-Gaussian distribution were likely caused by small sample statistics.",
        "watermark_text": "Recently , a number of studies have claimed that the metallicity distribution of the solar neighborhood is un - Gaussian . However , these accusations are based on samples with little numbers of stars .We determine the metallicity distributions in high - latitudes using data from the Sloan Digital Sky Survey Data Release 12 . The amount of stars in our specimens are in the range of tens of thousands , and hence we are able to confirm that the metallicity distribution is compatible with being Gaussian .We determine the metallicity distributions using the targets Galactic extinction - adjusted photometry and the DR12 of the SDSS . We see that the metallicity distributions in high - latitudes exhibit no considerable changes from those of the whole specimen .The probability that the high - latitude specimens and the whole sample are drawn from the same population is bigger than 99 . 99 % . These data indicate that the local stellar metallicity distribution is compatible with being Gaussian , and previous research that assumed non - Gaussian distribution were likely affected by small sample statistics .",
        "rewrite_text": "Recently, numerous studies have asserted that the metallicity distribution within the solar neighborhood does not conform to a Gaussian distribution. However, these claims are founded on samples containing a limited number of stars. Utilizing data from the Sloan Digital Sky Survey's Data Release 12, we have determined the metallicity distributions in high latitudes with a star count in the tens of thousands. Therefore, we can affirm that the metallicity distribution is consistent with a Gaussian distribution.\n\nWe have analyzed the metallicity distributions employing targets' Galactic extinction-adjusted photometry and the DR12 of the SDSS. Our observations reveal that there are no significant variations in the metallicity distributions at high latitudes compared to the entire sample. The likelihood that the high-latitude specimens and the entire sample originate from the same population exceeds 99.99%. These data suggest that the local stellar metallicity distribution is compatible with a Gaussian distribution, and previous studies assuming a non-Gaussian distribution may have been influenced by small sample sizes.",
        "ori-fast-z-score": 1.212678125181665,
        "water-fast-z-score": 6.305926250944657,
        "rewrite-fast-z-score": 1.5650160901149996
    },
    {
        "original_text": "The Laser Interferometer Space Antenna (LISA) will be an orbiting telescope designed to detect weak gravitational waves. Space-borne gravitational wave detectors are most effective at detecting the most strongly bound systems: supermassive black hole (SMBH) binaries. Accretion-driven phase relations in such systems can be predicted using mass and angular momentum accretion rates, and when the SMBH mass is measured by other means, such as stellar velocity dispersion or gas dynamics, the most precise tests of general relativity come from LISA s sensitivity to measurement errors of these parameters at the milli-arcsecond level. Here we report the result of a three-stage search for SMBHB candidates in LISA data. The first stage was a matched filter search using error models; the second was a genetic algorithm search for coherent sinusoidal signals in the LISA data; and the third was a hierarchical Bayesian analysis using population synthesis models for SMBHB populations to quantify the sensitivity of the first two stages to SMBHB signals. No significant candidates were found in the three stages of the search, and this resulted in a false alarm rate of less than 1 per billion years. These results place significant bounds on the space density of SMBHBs and constrain scenarios for their evolution.",
        "watermark_text": "The Laser Interferometer Space Antenna ( LISA ) will be an orbiting telescope designed to identify weak gravitational waves . Space - borne gravitational wave detectors are most efficient at detecting the most strongly bound structures : supermassive black hole ( SMBH ) binaries .Accretion - driven phase relations in such systems can be predicted use mass and angular velocity accretion levels , and when the SMBH mass is measured by other methods , such as stellar velocity dispersion or gas mechanics , the most accurate studies of general relativity result from LISA s tolerance to measurement failures of these parameters at the milli - arcsecond level . Here we publish the result of a three - phase search for SMBHB applicants in LISA data .The first phase was a paired filter search using mistake models ; the second was a genetic algorithm search for coherent sinusoidal signals in the LISA data ; and the third was a hierarchical Bayesian analysis utilizing population synthesis estimates for SMBHB populations to quantify the sensitivity of the first two stages to SMBHB signals . No meaningful candidates were found in the three stages of the search , and this resulted in a false alarm frequency of fewer than 1 per billion decades .These data put significant bounds on the space density of SMBHBs and constrain strategies for their evolution .",
        "rewrite_text": "The Laser Interferometer Space Antenna (LISA) will be an orbiting telescope meticulously designed to detect feeble gravitational waves. Among space-borne gravitational wave detectors, those detecting the most tightly bound structures, such as binaries of supermassive black holes (SMBHs), are the most efficient. The phase relationships resulting from accretion in these systems can be anticipated by utilizing mass and angular velocity accretion levels. When the SMBH mass is measured using alternative methods, such as stellar velocity dispersion or gas mechanics, LISA's tolerance for measurement failures at the milli-arcsecond level yields the most precise studies of general relativity. Herein, we present the outcome of a three-phase search for SMBHB candidates within LISA data. The initial phase involved a paired filter search utilizing mistake models, the second phase employed a genetic algorithm to seek coherent sinusoidal signals within the LISA data, and the final phase employed a hierarchical Bayesian analysis utilizing population synthesis estimates for SMBHB populations to assess the sensitivity of the first two phases to SMBHB signals. Although no significant candidates were discovered throughout the three phases of the search, this resulted in a low false alarm frequency of less than one per billion decades. These data provide significant constraints on the spatial density of SMBHBs and restrict strategies for their evolution.",
        "ori-fast-z-score": -0.618852747755276,
        "water-fast-z-score": 5.288453643125169,
        "rewrite-fast-z-score": 3.5645311547160277
    },
    {
        "original_text": "In this paper we present the first evidence of the transverse proximity effect in spectral hardness towards HE 2347-4342. Hard x-ray observations of this quasar, performed with the Chandra satellite, revealed a proximity effect typical of a forest of absorption systems along the line of sight. However, new optical observations carried out with the William Herschel Telescope (WHT) revealed a proximity zone with a filamentary structure and a transverse size of 30-40 kpc in radius, showing that the forest of systems is transverse the line of sight. We discuss the evidence for the transverse proximity effect and its implications in terms of the transverse distance between the forest of systems and the central quasar.  This work is published in “The transverse proximity effect in spectral hardness on the line of sight towards HE 2347-4342” by L. M. Bogdanov, V. V. Zabolotny, D. C. Lu, G. P. Garmire, A. C. Fabian, R. S. Holt, A. K. Pollman, A. V. Filippenko, K. P. Hyett, J. E. Grindlay, M. Eracleous, L. C. Gallo and T. Sikora, published on 17th September 2023 on the arXiv pre-print server   https://arxiv.org/abs/2003.09983   The transverse proximity effect in spectral hardness on the line of sight towards HE 2347-4342 In this paper we present the first evidence of the transverse proximity effect in spectral hardness towards HE 2347-4342. Hard x-ray observations of this quasar, performed with the Chandra satellite, revealed a proximity effect typical of a forest of absorption systems along the line of sight. However, new optical observations carried out with the William Herschel Telescope (WHT) revealed a proximity zone with a filamentary structure and a transverse size of 30-40 kpc in radius, showing that the forest of systems is transverse the line of sight. We discuss the evidence for the transverse proximity effect and its implications in terms of the transverse distance between the forest of systems and the central quasar.",
        "watermark_text": "In this paper we present the first data of the transverse location phenomenon in spectral hardness towards HE 2347 - 4342 . Hard x - ray observations of this quasar , conducted with the Chandra satellite , revealed a proximity phenomenon typical of a forest of absorption complexes along the line of view .However , new optical images conducted out with the William Herschel Telescope ( WHT ) confirmed a proximity zone with a filamentary shape and a transverse size of 30 - 40 kpc in radius , showing that the forest of systems is transverse the line of vision . We discuss the proof for the transverse location phenomenon and its consequences in terms of the transverse distance between the forest of systems and the main quasar .This study is published in “ The transverse location phenomenon in spectral hardness on the line of seeing towards HE 2347 - 4342 ” by L . M . Bogdanov , V . V . Zabolotny , D . C . Lu , G . P . Garmire , A . C . Fabian , R . S . Holt , A . K . Pollman , A . V . Filippenko , K . P . Hyett , J . E . Grindlay , M . Eracleous , L . C . Gallo and T . Sikora , published on 17th September 2023 on the arXiv pre - print server https : / / arxiv . org / abs / 2003 . 09983 The oblique proximity phenomenon in spectral purity on the line of vision towards HE 2347 - 4342 In this paper we present the first data of the transverse location phenomenon in spectral purity towards HE 2347 - 4342 . Hard x - ray observations of this quasar , conducted with the Chandra satellite , revealed a proximity phenomenon typical of a forest of absorption systems along the line of view .However , new optical images conducted out with the William Herschel Telescope ( WHT ) confirmed a proximity zone with a filamentary shape and a transverse size of 30 - 40 kpc in radius , showing that the forest of systems is transverse the line of vision . We discuss the proof for the transverse location phenomenon and its consequences in terms of the transverse distance between the forest of systems and the main quasar .",
        "rewrite_text": "In this article, we present the initial data regarding the transverse location phenomenon observed in the spectral purity towards HE 2347-4342. Hard X-ray observations of this quasar, executed by the Chandra satellite, disclosed a typical proximity effect akin to an absorption system forest along the line of sight. Nevertheless, utilizing the William Herschel Telescope (WHT) to acquire fresh optical images, we verified a proximity zone with a filamentary structure and a transverse size ranging from 30 to 40 kpc in radius, indicating that the system forest is positioned across the line of vision. We delve into the evidence for this transverse location phenomenon and its ramifications in terms of the transverse distance between the system forest and the primary quasar. This study, titled \"The Transverse Location Phenomenon in Spectral Purity along the Line of Vision towards HE 2347-4342,\" was published on September 17th, 2023, by L.M. Bogdanov and other co-authors on the arXiv pre-print server at https://arxiv.org/abs/2003.09983. The oblique proximity effect in spectral purity along the line of sight towards HE 2347-4342 is documented in this paper. Initial data reveals the phenomenon of transverse location in terms of spectral purity towards this specific quasar. X-ray observations performed by the Chandra satellite show a typical close association or 'forest' of absorption systems that is common along the visual axis. However, optical images captured by the William Herschel Telescope (WHT) confirm a close-by zone with a filamentary structure and a transverse size spanning 30 to 40 kpc in radius, indicating that these systems are indeed positioned across our line of sight. We further discuss the evidence for this transverse location phenomenon and its implications regarding the transverse distance between these systems and the primary quasar.",
        "ori-fast-z-score": -0.7924058156930615,
        "water-fast-z-score": 6.552780424957784,
        "rewrite-fast-z-score": 2.4748737341529163
    },
    {
        "original_text": "Optical amplifiers play an important role in telecommunication networks. Potassium-calcium phosphate based optical amplifiers have been recently introduced as a cost-effective alternative to Erbium-doped fiber amplifiers. In this paper, the optical properties of Er3+ doped KCa3(PO4)2 glasses, which are promising candidates for the realization of low-cost optical amplifiers, are presented. Transmittance, luminescence and photoluminescence spectra, radiative and non-radiative lifetimes of KCa3(PO4)2 : Er3+ samples are measured in the wavelength range from 200 nm to 1000 nm. A detailed study of temperature dependence of spectroscopic characteristics of Er3+ ions is also presented. The obtained results can be useful for the tailoring of Er3+ doped KCa3(PO4)2 glasses for specific optical amplifiers applications.",
        "watermark_text": "Optical amplifiers serve an important role in telecommunication networks . Potassium - calcium phosphate derived optical amplifiers have been successfully introduced as a price - effective alternative to Erbium - doped cable amplifiers .In this paper , the optical properties of Er3 + doped KCa3 ( PO4 ) 2 glasses , which are promising candidates for the realization of poor - cost optical amplifiers , are presented . Transmittance , luminescence and photoluminescence spectra , radiative and non - radiative lifetimes of KCa3 ( PO4 ) 2 : Er3 + samples are measured in the frequency range from 200 nm to 1000 nm .A full study of temperature dependence of spectroscopic characteristics of Er3 + ions is also presented . The achieved findings can be used for the tailoring of Er3 + doped KCa3 ( PO4 ) 2 glasses for specific optical amplifiers applications .",
        "rewrite_text": "Optical amplifiers play a crucial part in the intricate web of telecommunication networks. Phosphate-based optical amplifiers derived from potassium-calcium have been successfully introduced as a cost-effective alternative to Erbium-doped cable amplifiers. This paper delves into the optical properties of Er3+ doped KCa3 (PO4)2 glasses, which are emerging as viable options for low-cost optical amplifier development. Measurements have been taken on the transmittance, luminescence, and photoluminescence spectra, along with radiative and non-radiative lifetimes of KCa3 (PO4)2:Er3+ samples within the frequency range of 200 nm to 1000 nm. Furthermore, a comprehensive study on the temperature dependence of Er3+ ion spectroscopic characteristics is also presented. The findings obtained can be utilized for tailoring Er3+ doped KCa3 (PO4)2 glasses for specific applications in optical amplifier technology.",
        "ori-fast-z-score": 2.06418738616856,
        "water-fast-z-score": 5.307910421576297,
        "rewrite-fast-z-score": 3.328201177351375
    },
    {
        "original_text": "Resolving the spectra of individual galaxies is one of the most promising methods to measure their physical properties, as it would allow to separate starlight from active galactic nuclei (AGN) and potentially to map the spatial distribution of different types of galaxies. While recent large surveys like the Sloan Digital Sky Survey (SDSS; @SDSS) have provided large samples of spectra of galaxies, their analysis still requires significant effort. Modeling individual galaxies, on the other hand, is time-consuming, as it requires the knowledge of numerous parameters that describe a galaxy. A combination of the two approaches — a library of synthetic galaxy spectra, which would parametrize the basic properties of galaxies and allow to fit the spectra of individual galaxies, could significantly simplify the process of galaxy spectrum analysis and could be a key to unlock the full potential of current and future spectroscopic surveys, like the Gaia mission. To this end, we have started building a library of synthetic galaxy spectra, based on state-of-the-art physically-based models of galaxies, which we are making publicly available to the community. Here, we present the first results of the galaxy sample classification based on a Random Forest algorithm, trained on a sample of 3 million galaxy spectra, which we made available together with the library. We show that it is possible to separate the library galaxies according to four basic properties: starlight fraction, AGN fraction, the spatial distribution of the starlight and the physical size of the galaxies. We also present preliminary results of the preliminary parametrization of unresolved galaxies in the Gaiaspectro library by means of Random Forest regression.",
        "watermark_text": "Resolving the spectra of different galaxies is one of the most attractive tools to measure their physical properties , as it would enable to separate starlight from active galactic nuclei ( AGN ) and potentially to map the spatial distribution of different kinds of clusters . While recent big surveys like the Sloan Digital Sky Survey ( SDSS ; @ SDSS ) have provided large specimens of spectra of galaxies , their analysis already requires large work .Modeling individual galaxies , on the other hand , is time - consuming , as it takes the knowledge of several variables that explain a galaxy . A combination of the two perspectives — a library of natural galaxy spectra , which would parametrize the fundamental properties of galaxies and allow to fit the spectra of different galaxies , might considerably simplify the process of galaxy spectrum analysis and could be a key to unlock the full potential of recent and future spectroscopic observations , like the Gaia expedition .To this end , we have started building a library of natural galaxy spectra , built on state - of - the - art physically - based estimates of galaxies , which we are making officially available to the public . Here , we present the first findings of the universe sample classification based on a Random Forest algorithm , trained on a sample of 3 million galaxy spectra , which we made accessible together with the library .We see that it is easy to separate the library galaxies according to four fundamental properties : starlight percentage , AGN fraction , the spatial distribution of the starlight and the physical height of the galaxies . We additionally offer preliminary results of the tentative parametrization of unresolved galaxies in the Gaiaspectro collection by means of Random Forest regression .",
        "rewrite_text": "Determining the spectra of various galaxies represents a compelling method for measuring their physical properties. This approach enables the differentiation between starlight and active galactic nuclei (AGN), potentially facilitating the mapping of spatial distributions of various clusters. Although recent large surveys, such as the Sloan Digital Sky Survey (SDSS; @ SDSS), have provided an extensive array of galaxy spectra, analyzing these data demands significant effort. In contrast, individual galaxy modeling is time-consuming due to the multiple variables that describe a galaxy. Combining these perspectives, a library of natural galaxy spectra could greatly simplify the process of analyzing galaxy spectra. This library would parameterize the fundamental properties of galaxies and facilitate the fitting of various galaxy spectra. It has the potential to unlock the full potential of recent and future spectroscopic observations, like the Gaia mission.\n\nTo this end, we have commenced constructing a library of natural galaxy spectra, utilizing state-of-the-art physically based estimates. We are proud to make this library publicly accessible. We present here the initial findings from the classification of the universe sample, utilizing a Random Forest algorithm trained on a dataset containing 3 million galaxy spectra, which we have made available alongside the library.\n\nOur findings indicate that it is straightforward to categorize galaxies in the library based on four fundamental properties: starlight percentage, AGN proportion, the spatial distribution of starlight, and the physical height of galaxies. Furthermore, we provide preliminary results from the tentative parameterization of unresolved galaxies in the Gaiaspectro collection using Random Forest regression.",
        "ori-fast-z-score": -2.3958625754235072,
        "water-fast-z-score": 6.6551738206208535,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "We present dielectric spectroscopy studies of a 0.65 Pb(Ni_1/3Nb_2/3)O_3 -0.35PbTiO_3 (PNN) relaxor-based ceramics across the so-called supercooled liquid and smectic-C* (Sm-C*) transitions. We demonstrate that the temperature dependence of the dynamical susceptibility, χ″(T), and dielectric loss factor, ε″(T), strongly differs in the two states. We argue that the presence of two relaxations corresponding to two energy scales in the supercooled liquid state is a precursor phenomenon of the phase transition to the Sm-C* state. We compare our results with the most recent theoretical models for relaxors and discuss the implications of our results for the physics of the phase transition in relaxor systems. We present dielectric spectroscopy studies of a 0.65 Pb(Ni_1/3Nb_2/3)O_3 -0.35PbTiO_3 (PNN) relaxor-based ceramics across the so-called supercooled liquid and smectic-C* (Sm-C*) transitions. We demonstrate that the temperature dependence of the dynamical susceptibility, χ″(T), and dynamical loss factor, ε″(T), strongly differs in the two states. We argue that the presence of two relaxations corresponding to two energy scales in the supercooled liquid state is a precursor phenomenon of the phase transition to the Sm-C* state. We compare our results with the most recent theoretical models for relaxors and discuss the implications of our results for the physics of the phase transition in relaxor systems.",
        "watermark_text": "We present dielectric spectroscopy investigations of a 0 . 65 Pb ( Ni _ 1 / 3Nb _ 2 / 3 ) O _ 3 - 0 . 35PbTiO _ 3 ( PNN ) relaxor - based ceramics across the so - called supercooled fluid and smectic - C * ( Sm - C * ) transitions . We establish that the temperature dependence of the dynamical susceptibility , χ ″ ( T ) , and dielectric loss parameter , ε ″ ( T ) , heavily differs in the two states .We argue that the presence of two relaxations corresponding to two energy scales in the supercooled fluid state is a precursor effect of the phase shift to the Sm - C * state . We contrast our findings with the most current theoretical theories for relaxors and consider the implications of our findings for the physics of the phase shift in relaxor systems .We present dielectric spectroscopy investigations of a 0 . 65 Pb ( Ni _ 1 / 3Nb _ 2 / 3 ) O _ 3 - 0 . 35PbTiO _ 3 ( PNN ) relaxor - based ceramics across the so - called supercooled fluid and smectic - C * ( Sm - C * ) transitions . We establish that the temperature dependence of the dynamical susceptibility , χ ″ ( T ) , and dynamical loss parameter , ε ″ ( T ) , heavily differs in the two states .We argue that the presence of two relaxations corresponding to two energy scales in the supercooled fluid state is a precursor effect of the phase shift to the Sm - C * state . We contrast our findings with the most current theoretical theories for relaxors and consider the implications of our findings for the physics of the phase shift in relaxor systems .",
        "rewrite_text": "We conducted dielectric spectroscopy studies on a 0.65 Pb(Ni1/3Nb2/3)O3 - 0.35PbTiO3 (PNN) relaxor-based ceramic, focusing on the transitions from the supercooled fluid to the smectic-C* (Sm-C*) state. Our findings indicate significant differences in the temperature dependence of the dynamic susceptibility, χ''(T), and the dielectric loss parameter, ε''(T), between the two states. We suggest that the presence of two relaxations, corresponding to different energy scales in the supercooled fluid state, is a precursor effect leading to the phase transition to the Sm-C* state. We compare our results with current theoretical models for relaxors and discuss the implications of our findings for understanding phase shifts in relaxor systems.",
        "ori-fast-z-score": -2.49100947511811,
        "water-fast-z-score": 4.755563543407302,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "Theory predicts that stars more massive than about two solar masses will develop degenerate cores, and will consequently become magnetic stars known as Ap stars. Observational evidence for such a link, if it exists, could provide strong constraints on stellar evolution models. Here we report spectropolarimetric observations of 150 candidate Ap stars belonging to open clusters and associations. We measured longitudinal magnetic fields and from this, assuming the relation between magnetic field strength and rotational spin down rate, obtained estimates of the stellar rotation rates. We find a significant anticorrelation between the stellar age and the strength of the measured magnetic fields, such that the most slowly rotating stars have the strongest magnetic fields. This is consistent with the existence of a spin-down magnetic field strength evolution timescale of order 10^6 years, much longer than the estimated lifetimes of stars with convective cores. This is the first observationally-based evidence for the link between magnetic fields and stellar evolution.",
        "watermark_text": "Theory predicts that galaxies more massive than about two solar masses will develop degenerate cores , and will consequently develop magnetic stars called as Ap stars . Observational evidence for such a link , if it exists , might give strong restrictions on stellar evolution models .Here we publish spectropolarimetric studies of 150 candidate Ap objects belonging to open complexes and associations . We calculated longitudinal magnetic fields and from this , assuming the relation between magnetic field intensity and rotational spin down rate , obtained estimates of the stellar rotation rates .We get a substantial anticorrelation between the stellar age and the strength of the measured magnetic fields , such that the most slowly rotating stars have the highest magnetic fields . This is compatible with the existence of a spinning - down magnetic field intensity expansion timescale of order 10 ^ 6 years , far longer than the expected lifetimes of stars with convective cores .This is the first observationally - based data for the link between magnetic fields and stellar evolution .",
        "rewrite_text": "The theory suggests that galaxies exceeding approximately two solar masses in mass will develop degenerate cores, leading to the formation of magnetic stars known as Ap stars. If such a correlation exists, observational evidence for it could significantly constrain models of stellar evolution. In this study, we present spectropolarimetric investigations of 150 Ap object candidates found within open clusters and associations. We computed longitudinal magnetic fields and, based on the relationship between magnetic field intensity and rotational spin-down rate, estimated stellar rotation rates. Our findings reveal a notable anti-correlation between the age of the star and the measured magnetic field strength, where the stars with the slowest rotation rates possess the strongest magnetic fields. This is consistent with a timescale of magnetic field intensity expansion due to spinning down of approximately 10^6 years, which is much longer than the expected lifespan of stars with convective cores. This represents the first observation-based data to establish a link between magnetic fields and stellar evolution.",
        "ori-fast-z-score": -1.5652475842498528,
        "water-fast-z-score": 3.4444444444444446,
        "rewrite-fast-z-score": 1.9188064472004938
    },
    {
        "original_text": "Manganese and other rare earths are examples of impurities that may localize in the interstitial sites of the semiconductor host lattice. Such defects may form bound states with free carriers - usually referred to as localized charge carriers - leading to the formation of impurity bands. While for nonmagnetic impurities the dominant interaction mechanism is the so-called Hunds rule coupling, in the presence of localized magnetic moments of the impurity the Zeeman interaction has to be taken into account. If both, the localized magnetic moments as well as the carriers are deconfined at finite temperature, we encounter a magnetic impurity band that may exhibit properties characteristic for both, localized and delocalized charge carriers. In order to understand the nature of the impurity band we study for the case of diluted magnetic semiconductors (DMS) the crossover from localized to delocalized charge carriers via a systematic variation of the Coulomb attraction by the acceptor. In this context the physical concept of Wannier functions is employed to describe the spatial extent and localizability of the impurity band wave functions. The Coulomb attraction by the acceptor is modeled by a dielectric background in order to account for the screening behavior of the charge carriers in the valence band. By comparing calculated optical spectra with corresponding experiments we are able to extract the relevant material parameters. The result of this theoretical study allow to conclude that the acceptor-limited mobilities of the DMS systems can be described by the simple formula σ(cm2/V·s) = (μ0(cm2/V·s) · epsilon(0) · N(D) · N(A))½, where μ0 is the band mobility in the absence of localized charge carriers, N(D) is the acceptor density, and N(A) is the density of donors. Hence, for a fixed acceptor density, the acceptor-limited carrier mobility in diluted magnetic semiconductors is only determined by the density of donors. This fundamental relation has to be considered as a general property of all IV-VI compounds which contain both, magnetic impurities and non-magnetic acceptors.",
        "watermark_text": "Manganese and other rare earths are examples of impurities that might localize in the interstitial places of the semiconductor host structure . Such defects could form bound states with free carriers - typically known to as localized charge carriers - leading to the formation of impurity groups .While for nonmagnetic impurities the dominant coupling mechanism is the so - called Hunds rule coupling , in the presence of localized magnetic moments of the impurity the Zeeman interaction has to be taken into consideration . If both , the localized magnetic moments as well as the carriers are deconfined at finite temperature , we encounter a magnetic impurity band that might exhibit properties characteristic for both , confined and delocalized charge carriers .In order to explain the nature of the impurity band we study for the case of diluted magnetic semiconductors ( DMS ) the crossover from localized to delocalized charge carriers via a systematic variation of the Coulomb attraction by the acceptor . In this sense the physical idea of Wannier functions is utilized to explain the spatial scope and localizability of the impurity band wave functions .The Coulomb attraction by the acceptor is modeled by a dielectric background in order to explain for the screening behavior of the charge carriers in the valence band . By matching calculated magnetic spectra with corresponding experiments we are able to extract the appropriate material components .The result of this theoretical investigation allow to observe that the acceptor - limited mobilities of the DMS systems can be described by the simple equation σ ( cm2 / V · s ) = ( μ0 ( cm2 / V · s ) · epsilon ( 0 ) · N ( D ) · N ( A ) ) ½ , where μ0 is the band connectivity in the absence of localized charge carriers , N ( D ) is the acceptor density , and N ( A ) is the density of donors . Hence , for a fixed acceptor density , the acceptor - limited carrier mobility in diluted magnetic semiconductors is only determined by the density of donors .This fundamental relation has to be regarded as a general property of all IV - VI molecules which contain both , magnetic impurities and non - magnetic acceptors .",
        "rewrite_text": "Manganese and other rare earth elements serve as examples of impurities that may be localized within the interstitial spaces of a semiconductor host structure. These defects can form bound states with free carriers, commonly known as localized charge carriers, leading to the formation of impurity groups. For non-magnetic impurities, the dominant coupling mechanism is the Hunds rule coupling; however, in the presence of localized magnetic moments from the impurity, the Zeeman interaction becomes a factor to consider. When both the localized magnetic moments and carriers are not confined at finite temperatures, a magnetic impurity band arises that may exhibit properties characteristic of both confined and delocalized charge carriers.\n\nTo elucidate the nature of the impurity band, we investigate the crossover from localized to delocalized charge carriers in the context of diluted magnetic semiconductors (DMS). This is achieved through a systematic variation of the Coulomb attraction exerted by the acceptor, utilizing the physical concept of Wannier functions. The Coulomb attraction by the acceptor is modeled using a dielectric background to explain the screening behavior of charge carriers in the valence band. By matching calculated magnetic spectra with corresponding experiments, we can extract the appropriate material components.\n\nThe outcome of this theoretical investigation reveals that the acceptor-limited mobilities in DMS systems can be described by the equation: σ (cm²/V·s) = (μ0 (cm²/V·s) · ε(0) · N(D) · N(A))½, where μ0 represents the band connectivity in the absence of localized charge carriers, N(D) is the acceptor density, and N(A) is the donor density. Therefore, for a fixed acceptor density, the acceptor-limited carrier mobility in diluted magnetic semiconductors is solely determined by the density of donors. This fundamental relationship should be considered a general property of all IV-VI molecules that contain both magnetic impurities and non-magnetic acceptors.",
        "ori-fast-z-score": -0.7633700367119739,
        "water-fast-z-score": 4.156125755431858,
        "rewrite-fast-z-score": 1.794151081205198
    },
    {
        "original_text": "The c2d Legacy Project mapped the emission from gas and ice particles frozen onto the surfaces of dust grains in 14 molecular clouds. This, combined with complementary millimeter and submillimeter continuum data, provided estimates of the dust temperature, column density, and mass for these clouds. In this paper, we compare the spatial distributions of recent star formation in these clouds as revealed by Bolocam 1.1 mm dust continuum observations. In general, the star formation appears to be more extended in Perseus and Ophiuchus than in Serpens, though the signal-to-noise ratio of the data for the former are relatively low. The Perseus and Ophiuchus clouds also exhibit distinctive centrally-condensed structures that do not appear in the Serpens data. In particular, the Perseus and Ophiuchus clouds exhibit clear two-dimensional features that are not present in the Serpens data. The Perseus cloud also appears to be undergoing global collapse on large scales, indicated by the central concentration of filamentary structure and depletion of dust mass toward the centers of some of the brighter 1.1 mm condensations. If real, this implies that the star formation in Perseus has recently been even more spatially concentrated than suggested by the Bolocam data. Although the spatial scales sampled by the Bolocam observations are large (0.1-2 pc), the mass of material involved in recent star formation in Perseus is comparable to or greater than the typical mass of gas and dust in these clouds. Given this, the comparison between the spatial distributions of recent star formation in Perseus and Serpens may not be particularly meaningful. Finally, the Perseus and Ophiuchus clouds exhibit comparable levels of recent star formation, given the amounts of cold gas and dust observed in these regions.",
        "watermark_text": "The c2d Legacy Project analyzed the emission from gas and ice particles frozen onto the edges of dust grains in 14 molecular clouds . This , combined with complementary millimeter and submillimeter continuum data , provided estimates of the dust temperature , column thickness , and mass for these clouds .In this paper , we compare the spatial distributions of recent star formation in these clouds as revealed by Bolocam 1 . 1 mm dust continuum measurements . In general , the star formation appears to be more extended in Perseus and Ophiuchus than in Serpens , though the signal - to - noise proportion of the information for the former are fairly high .The Perseus and Ophiuchus clouds also display distinguishing centrally - condensed forms that do not appear in the Serpens data . In particular , the Perseus and Ophiuchus clouds exhibit clear two - dimensional characteristics that are not present in the Serpens data .The Perseus cloud also seems to be experiencing global failure on huge scales , suggested by the central concentration of filamentary structure and depletion of dust mass toward the centers of some of the brighter 1 . 1 mm condensations . If real , this implies that the star formation in Perseus has recently been much more spatially focused than proposed by the Bolocam data .Although the spatial dimensions recorded by the Bolocam experiments are big ( 0 . 1 - 2 pc ) , the mass of material responsible in recent star formation in Perseus is equal to or greater than the typical mass of gas and dust in these clouds . Given this , the comparison between the spatial distributions of recent star formation in Perseus and Serpens might not be particularly meaningful .Finally , the Perseus and Ophiuchus clouds exhibit comparable levels of recent star formation , given the amounts of cold gas and dust detected in these regions .",
        "rewrite_text": "The c2d Legacy Project conducted an analysis on the emissions from gas and ice particles frozen onto dust grain edges in 14 molecular clouds. This, combined with supplementary millimeter and submillimeter continuum data, enabled estimates of dust temperature, column thickness, and mass for these clouds. In this paper, we compare the spatial distributions of recent star formation in these clouds, as revealed by measurements from Bolocam at 1.1 mm dust continuum.\n\nIn general, star formation appears to be more widespread in the Perseus and Ophiuchus clouds compared to Serpens. However, the signal-to-noise ratio of information for the former is relatively high. Both Perseus and Ophiuchus clouds also demonstrate distinct centrally-condensed forms that are absent in the Serpens data. Specifically, the Perseus and Ophiuchus clouds exhibit clear two-dimensional characteristics not present in the Serpens data.\n\nAdditionally, it seems that the Perseus cloud is experiencing global failures on a large scale, suggested by the central concentration of filamentary structures and the depletion of dust mass towards the centers of some of the brighter 1.1 mm condensations. If this is true, it implies that star formation in Perseus has recently been more spatially focused than suggested by the Bolocam data. Despite the large spatial dimensions recorded by the Bolocam experiments (0.1-2 pc), the amount of material responsible for recent star formation in Perseus is equivalent to or greater than the typical mass of gas and dust in these clouds.\n\nIn this context, comparing the spatial distributions of recent star formation in Perseus and Serpens may not hold much significance. Finally, given the amounts of cold gas and dust detected in these regions, the Perseus and Ophiuchus clouds exhibit comparable levels of recent star formation.",
        "ori-fast-z-score": 2.729397809609434,
        "water-fast-z-score": 8.012103247563177,
        "rewrite-fast-z-score": 4.980113122967916
    },
    {
        "original_text": "Fermi’s golden rule (FGR) is an intuitive principle for understanding the rates of quantum system evolution. It states that the rate of some physical process is proportional to the second power of the density of states (DOS) of the initial states to final states available in the process. This proportionality constant is known as the strength of the process. The rule is exact for weak system-bath couplings. For molecular relaxation time scale quantum systems, the FGR predicted exponential dependence of the relaxation time on the thermal energy. Experimental verification of this prediction using Infrared (IR) photon emission rates for different temperatures showed the predicted dependence only for high temperatures, while for low temperatures the relaxation time is much shorter than expected. This result was interpreted as a signature of quantum dynamical phase transition (QDPT) in terms of scale-invariant behavior of the system. Subsequent theoretical analysis for a general quantum mechanical system with weak system-bath coupling shows the dependence of the relaxation time on the temperature is not exponential, but rather power-law. This finding was later supported by numerous experiments on different quantum systems. In this paper, we argue that the theoretical framework for analyzing the FGR based on scale-invariance is not applicable to realistic quantum systems. In particular, we focus on the initial proposal of QDPT based on exponential temperature dependence of the relaxation time and show that this assumption leads to incorrect conclusion about the validity of FGR. We then analyze the relaxation time and the FGR within a phenomenological microscopic quantum relaxation model. We show that the relaxation time as a function of temperature has a power law instead of exponential dependence as predicted by the FGR. Using the microscopic model, we fit the relaxation time for different temperature and extract the temperature dependence of the microscopic parameters. The analysis based on microscopic parameters reveals that for low temperatures, the quantum criticality disappears and the microscopic model predictions agree with the classical model. We further show that the power-law temperature dependence of the microscopic parameters leads to the power-law temperature dependence of the relaxation time, in agreement with the results obtained using the phenomenological model. Our analysis shows that the FGR cannot be applied to quantum systems within the QDPT framework. We also propose an alternative perspective on the quantum dynamical phase transition, and show that QDPT exists only for the classical system, and does not exist in the limit of quantum system. Our analysis shows that the QDPT is a manifestation of classical mechanics in the quantum system due to approximation of neglecting quantum interference effects. Using the proposed alternative perspective, we show that the scale-invariance is a valid principle for the quantum dynamical phase transition. Our analysis uncovers the mechanism of quantum dynamical phase transition in the classical limit, and suggests that the scale-invariance is not a fundamental symmetry of the quantum system, but a long-distance emergent behavior in the classical limit.",
        "watermark_text": "Fermi ’ s golden law ( FGR ) is an intuitive principle for studying the rates of quantum system progression . It says that the frequency of some physical process is proportional to the second power of the density of states ( DOS ) of the first states to eventual states provided in the process .This proportionality parameter is known as the strength of the process . The rule is exact for weak system - bath couplings .For molecular relaxation time scale quantum systems , the FGR predicted exponential dependence of the relaxation time on the thermal energy . Experimental verification of this forecast utilizing Infrared ( IR ) photon radiation rates for different temperatures revealed the expected dependence only for high temperatures , while for low temperatures the relaxation time is much longer than expected .This result was interpreted as a signature of quantum dynamical phase shift ( QDPT ) in terms of scale - invariant behavior of the system . Subsequent theoretical analysis for a general quantum mechanical system with weakened system - shower coupling finds the dependence of the relaxation time on the temperature is not exponential , but rather power - law .This conclusion was afterwards backed by various tests on various quantum systems . In this paper , we claim that the theoretical framework for evaluating the FGR based on scale - invariance is not applicable to realistic quantum systems .In particular , we focus on the early proposal of QDPT based on exponential temperature dependence of the relaxation time and find that this assumption results to wrong conclusion about the legitimacy of FGR . We then investigate the relaxation time and the FGR within a phenomenological microscopic quantum relaxation model .We see that the relaxation time as a function of temperature has a power law instead of exponential dependence as predicted by the FGR . Using the microscopic description , we fit the relaxation time for different temperature and extract the temperature dependence of the microscopic variables .The evaluation based on microscopic variables suggests that for low temperatures , the quantum criticality disappears and the microscopic theory estimates agree with the classical model . We further show that the power - law temperature dependence of the microscopic variables leads to the power - law temperature dependence of the relaxation time , in agreement with the results collected using the phenomenological theory .Our study shows that the FGR never be applied to quantum systems within the QDPT framework . We also suggest an additional viewpoint on the quantum dynamical phase shift , and suggest that QDPT exists only for the classical system , and does not occur in the limit of quantum system .Our study shows that the QDPT is a manifestation of classical mechanics in the quantum system owing to approximation of neglecting quantum interference effects . Using the suggested alternative perspective , we prove that the scale - invariance is a valid concept for the quantum dynamical phase shift .Our study uncovers the process of quantum dynamical phase shift in the classical limit , and suggests that the scale - invariance is not a basic symmetry of the quantum system , but a large - distance emergent phenomenon in the classical limit .",
        "rewrite_text": "Fermi's Golden Rule (FGR) serves as an intuitive principle for examining the rates of quantum system progression. This rule states that the frequency of a certain physical process is directly proportional to the square of the density of states (DOS) between the initial and final states involved in the process. This proportionality factor is known as the strength of the process. The rule is particularly accurate for weak system-bath interactions.\n\nFor molecular quantum systems on a relaxation time scale, FGR predicts an exponential dependence of relaxation time on thermal energy. Experimental tests utilizing infrared (IR) photon radiation rates at different temperatures reveal this expected dependence primarily at high temperatures, while at low temperatures, the relaxation time is significantly longer than anticipated. This outcome has been interpreted as a signature of Quantum Dynamical Phase Transition (QDPT) due to the scale-invariant behavior of the system.\n\nFurther theoretical analysis for generalized quantum mechanical systems with weaker system-bath coupling reveals that the relaxation time's dependence on temperature is not exponential, but rather follows a power law. This conclusion has been supported by various tests conducted on diverse quantum systems.\n\nIn this paper, we argue that the theoretical framework for evaluating FGR based on scale-invariance does not apply to realistic quantum systems. We specifically focus on the early proposal of QDPT based on the exponential temperature dependence of relaxation time and find that this assumption leads to an incorrect conclusion regarding the validity of FGR.\n\nWe then explore the relaxation time and FGR within a phenomenological microscopic quantum relaxation model. We observe that, instead of an exponential dependence predicted by FGR, the relaxation time as a function of temperature follows a power law. Using a microscopic description, we match the relaxation time at different temperatures and derive the temperature dependence of microscopic variables. The evaluation based on these microscopic variables suggests that at low temperatures, quantum criticality diminishes, and the microscopic theory aligns with classical models.\n\nMoreover, we show that the power-law temperature dependence of microscopic variables results in a power-law temperature dependence of the relaxation time, which is in agreement with the findings using the phenomenological theory. Our study reveals that FGR cannot be applied to quantum systems within the QDPT framework. We also offer an alternative perspective on Quantum Dynamical Phase Transition, suggesting that QDPT exists only for classical systems and does not occur in the limit of quantum systems.\n\nOur research demonstrates that QDPT is a manifestation of classical mechanics in quantum systems due to the approximation of neglecting quantum interference effects. With this alternative perspective, we prove that scale-invariance is a valid concept for Quantum Dynamical Phase Shift. Our study unveils the process of Quantum Dynamical Phase Shift in the classical limit and suggests that scale-invariance is not a fundamental symmetry of the quantum system but rather an emerging phenomenon at larger distances in the classical limit.",
        "ori-fast-z-score": 0.9544271444636666,
        "water-fast-z-score": 9.008159566499153,
        "rewrite-fast-z-score": 3.8892223413129865
    },
    {
        "original_text": "In the standard model (SM), top quark production is described by seven penguin diagrams at the Tevatron, and nine at the LHC. For LHC energies, higher-order corrections are large and uncertainties from various sources, such as intrinsic theoretical uncertainties in the leading-order calculations, as well as experimental uncertainties in the knowledge of the jet energy and the momentum of the initial state particles, grow due to increased higher-order corrections. While the total top quark cross section has been measured by both experiments to be consistent with the SM expectation, with a global significance of 2.9 standard deviations, there are several anomalous observations which have been reported: the measurement of the forward-backward asymmetry by both CDF and D0, the total cross section for top-antitop production, and the production of a single top quark in association with a W boson. While the measured forward-backward asymmetry is consistent with the SM expectation, all other measurements are 2.5-3.6 sigma deviations from the SM predictions. It is currently hypothesized that these deviations may be the result of the production of new, strongly-interacting particles which decay to top quarks, and thus the search for these particles has been the focus of recent studies. In this work, we perform a global fit to all anomalous measurements, assuming the production of nonstandard spin one particles, which decay to top quarks and a new, heavy, neutral scalar or vector boson. We find regions of the parameter space of the new physics model, compatible with all observations, and present the corresponding expected signatures at collider experiments.",
        "watermark_text": "In the standard model ( SM ) , top quark output is characterized by seven penguin diagrams at the Tevatron , and nine at the LHC . For LHC energies , greater - order corrections are big and uncertainties from several sources , such as intrinsic theoretical uncertainties in the leading - order calculations , as well as empirical uncertainties in the knowledge of the jet energy and the velocity of the first state particles , grow owing to greater greater - order corrections .While the total top quark cross area has been measured by both researchers to be compatible with the SM expectation , with a global significance of 2 . 9 standard deviations , there are several anomalous findings which have been reported : the measurement of the back - backward asymmetry by both CDF and D0 , the total cross section for top - antitop production , and the production of a single bottom quark in association with a W boson . While the reported forward - backward asymmetry is compatible with the SM expectation , all other measurements are 2 . 5 - 3 . 6 sigma deviations from the SM predictions .It is currently hypothesized that these deviations might be the result of the production of new , highly - interacting particles which decay to top quarks , and therefore the hunt for these ions has been the subject of recent studies . In this research , we perform a global fit to all anomalous measurements , assuming the production of nonstandard spin one particles , which decay to top quarks and a new , heavy , neutral scalar or vector boson .We get regions of the parameter space of the new physics model , compatible with all experiments , and present the equivalent predicted signatures at collider experiments .",
        "rewrite_text": "In the Standard Model (SM), the output of top quarks is characterized by seven penguin diagrams at the Tevatron and nine at the Large Hadron Collider (LHC). For LHC energies, higher-order corrections are significant, and uncertainties arise from various sources, including inherent theoretical uncertainties in leading-order calculations, as well as empirical uncertainties in jet energy and the velocity of initial state particles, which are exacerbated by greater higher-order corrections.\n\nWhile the total cross-sectional area of top quarks measured by researchers is consistent with SM expectations, with a global significance of 2.9 standard deviations, several anomalous findings have been reported. These include measurements of the back-backward asymmetry by CDF and D0, the total cross-section for top-antitop production, and the production of a single bottom quark in association with a W boson. While the reported forward-backward asymmetry aligns with SM expectations, all other measurements deviate from SM predictions by 2.5 to 3.6 sigma.\n\nIt is currently hypothesized that these deviations could be the result of the production of new, highly interactive particles that decay into top quarks. Therefore, the search for these particles has been a focal point of recent studies. In this research, we perform a global fit of all anomalous measurements, assuming the production of nonstandard spin-one particles that decay into top quarks and a new, heavy, neutral scalar or vector boson. We identify regions in the parameter space of the new physics model that are compatible with all experiments and present the predicted signatures equivalent to those observed in collider experiments.",
        "ori-fast-z-score": -0.9622504486493763,
        "water-fast-z-score": 4.426352063787131,
        "rewrite-fast-z-score": 0.9712858623572641
    },
    {
        "original_text": "Type Ia supernovae are the best distance indicators available today, being standardizable candles with the advantage of their high luminosity being directly linked to the amount of Chandrasekhar mass exploded in a white-dwarf system. The progenitor system of these double-degenerate systems is believed to consist of a carbon-oxygen white dwarf, accreted material from a non-degenerate companion and most likely a neutron star (or black hole), which eventually reaches enough masses together with gravity to detonate as a type Ia supernova. We present near-infrared and optical observations of the type Ia supernova 2001el obtained with the Very Large Telescope (VLT) and the European Southern Observatory s New Technology Telescope. The peak of the light curve of 2001el was observed at approximately 120 days after the fitted explosion date and the spectrum is very similar to what is observed in typical type Ia supernovae one month after maximum brightness. We investigate possible causes for the late-time light curve break observed in 2001el, comparing it with normal type Ia supernovae and with other type Ia supernova ejections observed in a different cosmological context (SNe 2002bo, 2003hv, 2003bb and 2003ee). Our results favour a Chandrasekhar-mass white-dwarf model for 2001el, even though we cannot completely rule out the presence of a non-degenerate companion, nor a helium-rich composition for the progenitor. Late-time observations of more type Ia supernovae are crucial for reaching a better understanding of the degenerate binary scenario and of the role of possible explosion variations.",
        "watermark_text": "Type Ia supernovae are the best distance indicators used today , being standardizable candles with the advantage of their high luminosity being immediately linked to the quantity of Chandrasekhar mass destroyed in a black - giant system . The progenitor system of these double - degenerate systems is suspected to contain of a carbon - oxygen white dwarf , accreted material from a non - degenerate companion and most likely a neutron star ( or black hole ) , which eventually reaches enough masses combined with gravity to detonate as a class Ia supernova .We present near - infrared and imaging observations of the class Ia supernova 2001el collected with the Very Large Telescope ( VLT ) and the European Southern Observatory s New Technology Telescope . The peak of the light curve of 2001el was seen at approximately 120 weeks after the fitted accident day and the spectrum is very identical to what is observed in characteristic type Ia supernovae one month after maximum brightness .We explore possible causes for the mid - time light curve broke observed in 2001el , comparing it with normal type Ia supernovae and with other class Ia supernova ejections observed in a different cosmological context ( SNe 2002bo , 2003hv , 2003bb and 2003ee ) . Our results favour a Chandrasekhar - mass white - giant model for 2001el , even though we lack totally rule out the presence of a non - degenerate companion , nor a helium - rich composition for the progenitor .Late - time observations of more type Ia supernovae are important for achieving a better grasp of the degenerate binary scenario and of the role of possible explosion changes .",
        "rewrite_text": "Type Ia supernovae are the most reliable distance indicators utilized today, acting as standardizable candles. Their high luminosity is directly linked to the amount of Chandrasekhar mass destroyed in a black-giant system, providing an immediate advantage. It is suspected that the progenitor system of these double-degenerate systems comprises a carbon-oxygen white dwarf, accreted material from a non-degenerate companion, and most likely a neutron star (or black hole), which accumulates enough mass through gravity to detonate as a class Ia supernova.\n\nWe present observations of the class Ia supernova 2001el in the near-infrared and imaging collected with the Very Large Telescope (VLT) and the European Southern Observatory's New Technology Telescope. The peak of the light curve for 2001el was observed approximately 120 weeks after the estimated explosion day, and its spectrum closely resembles what is observed in characteristic type Ia supernovae one month after peak brightness.\n\nWe explore potential reasons for the mid-time light curve break observed in 2001el by comparing it with typical type Ia supernovae and other class Ia supernova ejections observed in various cosmological contexts (SNe 2002bo, 2003hv, 2003bb, and 2003ee). Our findings suggest a Chandrasekhar-mass white-giant model for 2001el, although we cannot entirely rule out the presence of a non-degenerate companion or a helium-rich composition for the progenitor.\n\nFurther observations of type Ia supernovae at late stages are crucial for gaining a deeper understanding of the degenerate binary scenario and the role of potential explosion variations.",
        "ori-fast-z-score": -1.8367993291867606,
        "water-fast-z-score": 4.041451884327381,
        "rewrite-fast-z-score": 0.5720775535473553
    },
    {
        "original_text": "In this article we present a rigorous mapping between the Ising spin glass models and the Ising models in the long temperature range, where the former only exhibit disordered behavior. The proof is based on the so-calledyss Hamiltonian decomposition, which allows one to write the partition function of the spin glass model as a product of simpler terms, each corresponding to a particular configuration of frozen spins. The nature of the frozen spins implies that the couplings between corresponding terms in the Ising and spin glass models are positive, and thus the whole product has to be positive, allowing us to restrict the consideration to positive Boltzmann weights, which greatly simplifies the analysis. The mapping is carried out for general graphs, meaning that the underlying lattice structure is not specified. We expect the following direct implications of the presented results: 1. The existence of a zero temperature phase transition in the Ising and spin glass models with proper choice of parameters is equivalent. 2. The critical behavior at the zero temperature phase transition of the Ising and spin glass models is identical. 3. The dynamical behavior of the systems at low temperature is related in the following way: The dynamical behavior of the corresponding Ising models at low temperature regime is characterized by a spontaneous breaking of the global $Z_2$ symmetry (spin inversion). However, for spin glasses, this global $Z_2$ symmetry is broken at any finite temperature due to the presence of frozen spins. The spontaneous breaking of the $Z_2$ symmetry at finite temperature indicates the lack of rigidity of the frozen spins, namely the de Almeida-Thouless (AT) line of the phase transition in the dynamical behavior of the spin glasses. We also provide a number of technical results, such as the explicit freezing probability bounds, which are likely to be useful for further studies.",
        "watermark_text": "In this article we present a rigorous map between the Ising spin glass models and the Ising models in the long temperature spectrum , where the former only show disordered behavior . The proof is based on the so - calledyss Hamiltonian transformation , which allows one to write the partition function of the spin glass model as a product of simpler terms , each corresponding to a certain configuration of frozen spins .The nature of the freeze spins indicates that the couplings between corresponding terms in the Ising and spin glass models are positive , and therefore the whole product has to be positive , allowing us to limit the consideration to positive Boltzmann sizes , which severely simplifies the analysis . The map is carried out for general graphs , meaning that the fundamental lattice structure is not specified .We assume the following direct consequences of the shown conclusions : 1 . The existence of a zero temperature phase change in the Ising and spin glass models with proper choosing of constraints is equivalent .2 . The fundamental behavior at the zero temperature phase shift of the Ising and spin glass models is identical .3 . The dynamical behavior of the systems at low heat is related in the following way : The dynamical behavior of the equivalent Ising models at low heat regime is characterized by a spontaneous breaking of the global $ Z _ 2 $ symmetry ( spin inversion ) .However , for spinning glasses , this global $ Z _ 2 $ symmetry is shattered at any finite temperature owing to the presence of frozen spins . The spontaneous breaking of the $ Z _ 2 $ symmetry at finite temperature indicates the lack of rigidity of the freeze spins , namely the de Almeida - Thouless ( AT ) line of the phase shift in the dynamical behavior of the spin glasses .We additionally offer a number of technical results , such as the explicit freezing probability bounds , which are likely to be valuable for further studies .",
        "rewrite_text": "In this article, we present a precise mapping between the Ising spin glass models and the Ising models within a broad temperature spectrum. Specifically, the former models only exhibit disordered behavior. The proof relies on the utilization of the Yss Hamiltonian transformation, which allows us to express the partition function of the spin glass model as a product of simpler terms, each corresponding to a specific configuration of frozen spins.\n\nThe nature of these frozen spins implies that the couplings between corresponding terms in both the Ising and spin glass models are positive, resulting in a positive overall product. This enables us to limit our considerations to positive Boltzmann sizes, significantly simplifying the analysis. This mapping is applicable to general graphs, meaning that a specific lattice structure is not required.\n\nWe further assume the following direct consequences of our findings:\n\n1. The existence of a phase transition at zero temperature in both the Ising and spin glass models is equivalent when certain constraints are properly chosen.\n2. The fundamental behavior at the zero-temperature phase shift in both the Ising and spin glass models is identical.\n3. Regarding the dynamical behavior of the systems at low heat, we observe that the spontaneous breaking of the global Z2 symmetry (spin inversion) characterizes the equivalent Ising models in the low-temperature regime. However, in spinning glasses, this global Z2 symmetry is disrupted at any finite temperature due to the presence of frozen spins. The spontaneous breaking of the Z2 symmetry at finite temperature indicates a lack of rigidity in the frozen spins, which corresponds to the de Almeida-Thouless (AT) line in the dynamical behavior of spin glasses.\n\nAdditionally, we provide several technical results, such as explicit freezing probability bounds, that are likely to be valuable for future studies.",
        "ori-fast-z-score": 0.24743582965269675,
        "water-fast-z-score": 5.0963686064660765,
        "rewrite-fast-z-score": 2.425101829020563
    },
    {
        "original_text": "In this paper we study a 4D Z_2-symmetric thick brane solution in an AdS spacetime. We compute the Wightman function and vacuum densities corresponding to this solution. We show that these densities are nontrivial and depend on the spacetime and the fifth dimension. mselves, we consider the 5D Einstein-Maxwell action with the 5D Minkowski space-time metric and with the potential that generates a Z_2-symmetric thick brane solution with the corresponding 5D Riemann and 4D induced metrics on the brane, and with the appropriate jump in the 5th dimension component of the 5D vector potential across the brane. Solving the 5D equations of motion with this brane configuration, we compute the Wightman function and vacuum densities for this system. We show that these densities are nontrivial and depend on the spacetime and the fifth dimension. Welfare, in this paper we show that the nontriviality of the vacuum densities has an interesting physical implication. It leads to the breakdown of the additivity of the energy in 4D effective theory, for the system of the brane and the bulk fields.",
        "watermark_text": "In this paper we study a 4D Z _ 2 - symmetric deep brane solution in an AdS spacetime . We compute the Wightman function and vacuum densities corresponding to this solution .We see that these densities are nontrivial and depend on the spacetime and the fifth dimension . mselves , we define the 5D Einstein - Maxwell action with the 5D Minkowski space - time metric and with the potential that generates a Z _ 2 - symmetric deep brane solution with the equivalent 5D Riemann and 4D induced metrics on the brane , and with the appropriate jump in the fifth dimension component of the 5D vector potential across the brane .Solving the 5D equations of movement with this brane configuration , we compute the Wightman function and vacuum densities for this scheme . We see that these densities are nontrivial and depend on the spacetime and the fifth dimension .Welfare , in this paper we prove that the nontriviality of the vacuum densities has an interesting physical implication . It leads to the breakdown of the additivity of the power in 4D effective theory , for the system of the brane and the bulk fields .",
        "rewrite_text": "In this study, we explore a 4D Z₂-symmetric deep brane solution within an AdS spacetime framework. We calculate the Wightman function and associated vacuum densities for this specific solution. We observe that these densities are significant and depend on both the spacetime structure and the fifth dimension.\n\nWe define the 5D Einstein-Maxwell action, utilizing the 5D Minkowski spacetime metric and a potential that generates a Z₂-symmetric deep brane solution. This includes equivalent 5D Riemann and 4D induced metrics on the brane, along with an appropriate jump in the fifth dimension component of the 5D vector potential across the brane. By solving the 5D motion equations with this brane configuration, we compute the Wightman function and vacuum densities for this system. Our findings indicate that these densities are not trivial and are influenced by both the spacetime and the fifth dimension.\n\nFurthermore, in this paper, we demonstrate a fascinating physical implication of the non-triviality of the vacuum densities. It results in the breakdown of power additivity in the 4D effective theory for the system involving the brane and bulk fields.",
        "ori-fast-z-score": 1.4832396974191326,
        "water-fast-z-score": 4.449719092257398,
        "rewrite-fast-z-score": 2.3566599571949607
    },
    {
        "original_text": "Ferromagnetism in doped graphene remains an important open issue despite extensive research in recent years. There is strong evidence for Stoner ferromagnetic instability in the electron-doped case, while for the hole-doped system a large body of evidence supports a nonmagnetic ground state. Here we show that the magnetic properties of hole-doped graphene can be understood in terms of a SU(4) C1S0-symmetric bosonized theory. Our approach starts from a generalized Hubbard model with full SU(4) symmetry. Using a combination of non-perturbative methods, namely the Functional Renormalization Group and the Cluster Perturbation Theory, we find that this model yields ferromagnetism at low energies. This leads us to propose that C1S0 symmetry might be generally relevant to ferromagnetic ground states in hole-doped graphene, and possibly in other systems with strong electronic correlations such as high-temperature superconductors.",
        "watermark_text": "Ferromagnetism in doped graphene remains an important open topic despite extensive research in recent years . There is strong evidence for Stoner ferromagnetic disorder in the electron - doped case , while for the hole - doped scheme a large body of evidence confirms a nonmagnetic ground state .Here we prove that the magnetic properties of hole - doped graphene can be understood in terms of a SU ( 4 ) C1S0 - symmetric bosonized theory . Our solution starts from a generalized Hubbard theory with full SU ( 4 ) symmetry .Using a combination of non - perturbative methods , notably the Functional Renormalization Group and the Cluster Perturbation Theory , we find that this model gives ferromagnetism at low energies . This leads us to propose that C1S0 symmetry could be broadly relevant to ferromagnetic ground states in hole - doped graphene , and maybe in other structures with high electronic correlations such as low - temperature superconductors .",
        "rewrite_text": "Despite extensive research in recent years, ferromagnetism in doped graphene remains a significant open topic. Strong evidence suggests Stoner ferromagnetic disorder in electron-doped cases, while a substantial amount of evidence confirms a nonmagnetic ground state in hole-doped systems. We now offer proof that the magnetic properties of hole-doped graphene can be explained through a SU(4) C1S0-symmetric bosonized theory.\n\nOur approach starts with a generalized Hubbard theory encompassing full SU(4) symmetry. By utilizing a blend of non-perturbative techniques, namely the Functional Renormalization Group and Cluster Perturbation Theory, we discover that this model generates ferromagnetism at lower energies. Consequently, we propose that C1S0 symmetry may have widespread implications for ferromagnetic ground states in hole-doped graphene, possibly extending to other structures with strong electronic correlations, such as low-temperature superconductors.",
        "ori-fast-z-score": -0.13018891098082389,
        "water-fast-z-score": 3.713069518053983,
        "rewrite-fast-z-score": 1.1338934190276817
    },
    {
        "original_text": "In our recent work (Carroll et al. 2012, hereafter Paper I), we presented evidence for an active star formation event in the ultracompact H<span style= font-variant: small-caps; > ii</span> region, <span style= font-variant:small-caps; >NGC</span>7538, in the Perseus spiral arm using Spitzer and IRAS data. IRAS determined that 70% of <span style= font-variant:small-caps; >NGC</span>7538’s luminosity was coming from this small region, and Spitzer identified 12 infrared point sources with spectral types of B3 or later. These results, along with our multiwavelength analysis of the region, suggested that this region was likely forming stars at a high rate. In this work, we examine this formation event further through the use of new molecular outflow data obtained with the Jansky Very Large Array (JVLA). We combine our new observations with existing near-infrared and mid-infrared data to further examine the processes acting in <span style= font-variant:small-caps; >NGC</span>7538. We utilize the <span style= font-variant:small-caps; >CLOUDY</span> photodissociation region (PDR) modeling code (cloudy) to create a physical-based scenario for the molecular outflows in <span style= font-variant:small-caps; >NGC</span>7538. This analysis implies that the region is likely producing stars at a high rate through ambipolar diffusion, and not gravitational collapse.",
        "watermark_text": "In our latest work ( Carroll et al . 2012 , hereafter Paper I ) , we presented data for an active star formation event in the ultracompact H < span style = font - variant : tiny - caps ; > ii < / span > region , < span style = font - variant : tiny - caps ; > NGC < / span > 7538 , in the Perseus spiral arm combining Spitzer and IRAS data .IRAS determined that 70 % of < span style = font - variant : tiny - hats ; > NGC < / span > 7538 ’ s luminosity was coming from this tiny region , and Spitzer detected 12 infrared point sources with spectral classes of B3 or later . These data , along with our multiwavelength evaluation of the region , suggested that this area was likely forming stars at a high pace .In this project , we investigate this formation event further through the using of new molecular outflow data acquired with the Jansky Very Large Array ( JVLA ) . We integrate our new images with existing near - infrared and mid - infrared evidence to further examine the mechanisms working in < span style = font - variant : tiny - caps ; > NGC < / span > 7538 .We utilize the < span style = font - variant : tiny - caps ; > CLOUDY < / span > photodissociation region ( PDR ) modeling code ( cloudy ) to create a physical - based scenario for the molecular outflows in < span style = font - variant : tiny - caps ; > NGC < / span > 7538 . This assessment indicates that the region is probably producing stars at a high rate through ambipolar diffusion , and not gravitational failure .",
        "rewrite_text": "In our recent work (Carroll et al., 2012; referred to as Paper I), we have presented findings from an ongoing star formation event in the ultracompact Hii region, NGC 7538, located in the Perseus spiral arm. Utilizing Spitzer and IRAS data, we observed that IRAS detected 70% of NGC 7538's luminosity originating from this small area. Meanwhile, Spitzer identified 12 infrared point sources with spectral classes of B3 or later. These data, along with our multi-wavelength analysis of the region, suggest that this area is actively forming stars at a rapid pace.\n\nIn this project, we delve deeper into this formation event by utilizing new molecular outflow data acquired from the Jansky Very Large Array (JVLA). We integrate our new images with existing near- and mid-infrared evidence to further investigate the mechanisms at play in NGC 7538. To this end, we employ the CLOUDY photodissociation region (PDR) modeling code to establish a physically-based scenario for the molecular outflows in NGC 7538. Our assessment suggests that the region is likely producing stars at a high rate due to ambipolar diffusion rather than gravitational failure.",
        "ori-fast-z-score": 0.4703604341917986,
        "water-fast-z-score": 6.490973991846821,
        "rewrite-fast-z-score": 1.1043152607484654
    },
    {
        "original_text": "SunOS/Solaris is a widely used Unix operating system, being among the most popular operating systems for servers, desktops and workstations. In this paper, we assess the availability of some SunOS/Solaris systems by means of the analysis of the corresponding wtmpx and Syslogd logfiles. The wtmpx files store the contents of the user s work-related timestamps, whereas the Syslogd logfiles store messages from the system s log (e.g., error messages, messages generated by the runlevel transition mechanism, and other messages the system is supposed to log). Both files are regularly backed up in a different location. We study the availability of these systems by inspecting the corresponding wtmpx and Syslogd files and by observing the corresponding syslogd and getty processes. We detect various types of failures, such as unrecoverable device or software failures, shutdown or startup failures, and process failures. Additionally, we notice that the logs are not always kept long enough to properly detect some failures, such as runlevels that are not configured to stay around for a long time or getty processes that die after a certain time. The results show that the analyzed systems provide a Mean Time Between Failure (MTBF) between 3.12 and 4.96 hours, depending on the type of failure considered. The corresponding MTTRs range from 5 minutes to 5 hours, with a median of 30 minutes.",
        "watermark_text": "SunOS / Solaris is a widespread using Unix operating product , being among the most popular operating machines for clients , desktops and workstations . In this paper , we assess the availability of some SunOS / Solaris systems by means of the evaluation of the associated wtmpx and Syslogd logfiles .The wtmpx files contain the contents of the user s work - linked timestamps , whereas the Syslogd logfiles handle messages from the program s log ( e . g . , error signals , messages produced by the runlevel transition process , and other emails the program is meant to log ) . Both databases are regularly backed up in a different location .We assess the availability of these systems by inspecting the respective wtmpx and Syslogd databases and by observing the respective syslogd and getty systems . We detect various types of failures , such as unrecoverable device or software failures , shutdown or startup failures , and system failures .Additionally , we notice that the files are not always kept short enough to properly detect some failures , such as runlevels that are not configured to stay around for a length time or getty systems that die after a certain time . The results show that the evaluated systems provide a Mean Time Between Failure ( MTBF ) between 3 . 12 and 4 . 96 hours , depending on the kind of failure considered .The equivalent MTTRs vary from 5 minutes to 5 hours , with a median of 30 moments .",
        "rewrite_text": "SunOS/Solaris is a widely used Unix operating system, frequently employed as a client, desktop, and workstation machine. In this paper, we assess the availability of certain SunOS/Solaris systems through the evaluation of associated wtmpx and Syslogd log files. The wtmpx files store user-related work timestamps, while Syslogd log files handle program log messages such as error signals, runlevel transition messages, and other emails intended for logging. Both databases are regularly backed up at a different location.\n\nTo evaluate the systems' availability, we inspect the respective wtmpx and Syslogd databases, as well as observe the syslogd and getty systems. We detect various types of failures, including unrecoverable device or software malfunctions, boot-up or shutdown failures, and systemic failures. Additionally, we notice that the files are not always short enough to properly detect certain failures, such as runlevels not configured to persist for an extended period or getty systems that fail after a certain time.\n\nThe results indicate that the evaluated systems exhibit a Mean Time Between Failure (MTBF) ranging from 3.12 to 4.96 hours, depending on the type of failure considered. The corresponding Mean Time To Restore (MTTR) varies from 5 minutes to 5 hours, with a median of 30 minutes.",
        "ori-fast-z-score": -0.10369516947304253,
        "water-fast-z-score": 6.740186015747764,
        "rewrite-fast-z-score": 1.744163198544762
    },
    {
        "original_text": "We propose to distinguish a gravastar from a black hole by observing the event horizon of the former but not the latter. Unlike a black hole, a gravastar has an additional surface, called the coat hanger hole, through which matter can fall but out of which it cannot escape. Thus, a test particle in a gravitational field of a gravastar will accumulate ergs trying to climb up the coat hanger hole, but will fall behind the event horizon. In contrast, a test particle in a gravitational field of a black hole will not experience such accumulative erg energy transfer, but will be pulled into the event horizon. The proposed distinguishing observational signatures may help to address the current debate on the nature of dark matter. Keywords: dark matter, gravastar Here is the full text of the article: How to tell a gravastar from a black hole? Xiao-Dong peasant arXiv.org June 1, 2022 The possibility of objects with very strong gravity, called gravastars, was suggested by Mazur and Mottola in 1994 as an alternative to black holes. Gravastars are distinct from black holes in that they have an additional surface, called the coat hanger hole, through which matter can fall but out of which it cannot escape. Thus, a test particle in a gravitational field of a gravastar will accumulate ergs trying to climb up the coat hanger hole, but will fall behind the event horizon. In contrast, a test particle in a gravitational field of a black hole will not experience such accumulative erg energy transfer, but will be pulled into the event horizon. The proposed distinguishing observational signatures may help to address the current debate on the nature of dark matter.",
        "watermark_text": "We suggest to distinguish a gravastar from a black hole by observing the event horizon of the former but not the latter . Unlike a black hole , a gravastar has an additional surface , known the coat hanger hole , through which matter can drop but out of which it cannot flee .Thus , a test particle in a gravitational field of a gravastar will accumulate ergs trying to climb up the coat hanger hole , but will drop behind the event horizon . In comparison , a test particle in a gravitational field of a black hole will not encounter such accumulative erg heat release , but will be pulled into the event horizon .The proposed identifying observational signatures may assist to address the present discussion on the nature of dark matter . Keywords : dark matter , gravastar Here is the full content of the article : How to guess a gravastar from a black hole ?Xiao - Dong peasant arXiv . org June 1 , 2022 The possibility of bodies with very strong gravitational , called gravastars , was suggested by Mazur and Mottola in 1994 as an alternative to dark holes . Gravastars are distinct from black holes in that they have an additional surface , known the coat hanger hole , through which matter can drop but out of which it must exit .Thus , a test particle in a gravitational field of a gravastar will accumulate ergs trying to climb up the coat hanger hole , but will drop behind the event horizon . In comparison , a test particle in a gravitational field of a black hole will not encounter such accumulative erg heat release , but will be pulled into the event horizon .The proposed identifying observational signatures may assist to address the recent debate on the nature of deep material .",
        "rewrite_text": "We propose to differentiate a gravastar from a black hole by observing its event horizon in contrast to the absence of such observation for the latter. In contrast to a black hole, a gravastar possesses an additional surface known as the \"coat hanger hole\" through which matter can descend but cannot escape. Therefore, a test particle within the gravitational field of a gravastar will accumulate energy attempting to climb the coat hanger hole, yet it will ultimately fall behind the event horizon. Conversely, a test particle in the gravitational field of a black hole will not experience such accumulative energy release, but rather will be pulled into the event horizon. The suggested observational markers for identification may aid in addressing the current discussions about the nature of dark matter.\n\nKeywords: dark matter, gravastar\n\nFull article content: How to distinguish a gravastar from a black hole? By Xiao-Dong peasant, arXiv.org, June 1, 2022\n\nThe concept of bodies with extremely strong gravity, termed gravastars, was proposed by Mazur and Mottola in 1994 as an alternative to black holes. Gravastars differ from black holes in that they possess an extra surface, the coat hanger hole, through which matter can enter but must also exit. Consequently, a test particle in the gravitational field of a gravastar will accumulate energy while attempting to climb the coat hanger hole, but will eventually be drawn into the event horizon. In contrast, a test particle in the gravitational field of a black hole will not encounter such energy accumulation, but will be pulled into the event horizon instead. The proposed observational signatures for identification may aid in resolving the recent debate regarding the properties of deep material.",
        "ori-fast-z-score": -0.9053574604251853,
        "water-fast-z-score": 4.8889302862960005,
        "rewrite-fast-z-score": 2.4370871833797696
    },
    {
        "original_text": "This paper investigates the second-order optimality of electrical impedance tomography (EIT). For the standard elliptic EIT inverse problem, a state that has minimal energy relative to the specified electrical boundary conditions does not necessarily have a corresponding physical shape. This phenomenon is often referred to as impedance artifacts. It has been shown that this problem can be cast as a constrained second-order minimization, subject to the incompressibility constraint and a globalization condition. In this work, we present a unified first- and second-order approach to shape optimization for EIT. We show that the associated energy functional is of second order in the deviation of the current state from an optimizer. We apply a convex relaxation to the second-order shape derivative in order to characterize the second-order shape derivative by means of the Dirichlet energy, and thereby reduce the analysis to the level of convex analysis. Numerical experiments are presented to illustrate the theory.",
        "watermark_text": "This paper investigates the second - order optimality of electrical impedance tomography ( EIT ) . For the standard elliptic EIT inverse question , a state that has minimal power relative to the certain electrical boundary parameters does not necessarily have a corresponding physical form .This phenomenon is often referred to as impedance artifacts . It has been shown that this situation can be cast as a constrained second - order minimization , subject to the incompressibility constraint and a globalization condition .In this research , we present a consolidated first - and second - order approach to shape optimization for EIT . We see that the associated energy functional is of second order in the deviation of the current state from an optimizer .We use a convex relaxation to the second - order shape derivative in order to characterize the second - order shape derivative by means of the Dirichlet power , and consequently decrease the analysis to the level of convex analysis . Numerical observations are presented to illustrate the model .",
        "rewrite_text": "This study examines the second-order optimality pertaining to Electrical Impedance Tomography (EIT). In the context of the standard elliptic EIT inverse problem, a state with minimal power in relation to specific electrical boundary parameters may not necessarily correspond to a physical form. This phenomenon is commonly known as impedance artifacts. It has been demonstrated that this scenario can be formulated as a constrained second-order minimization, subject to the incompressibility constraint and a globalization condition.\n\nIn this research, we propose a unified first and second-order approach for shape optimization in EIT. We observe that the associated energy functional is of second-order in terms of the deviation of the current state from an optimizer. To characterize the second-order shape derivative, we employ a convex relaxation approach using the Dirichlet power, thereby reducing the analysis to the level of convex analysis. Numerical observations are presented to illustrate the effectiveness of our model.",
        "ori-fast-z-score": -1.7801724872907798,
        "water-fast-z-score": 1.0681034923744679,
        "rewrite-fast-z-score": 0.23570226039551587
    },
    {
        "original_text": "In three-dimensional topology, the Thurston norm is a natural generalisation of the Euler characteristic. Although its definition makes sense for any 3-manifold, it has been characterised only for representations of its fundamental group in the special linear group, over the field of complex numbers. We show that the Thurston norm may be computed by considering any such representation in the standard 3-space, over the field of real numbers. As a consequence, we provide a new proof that certain normal surfaces in knot complements are taut: every smoothly embedded disk in these surfaces has odd multiplicity at each of its points. The proof is based on work of Agol,boileau and C.T.CTseng, and of Rosenthal and of Sivek, on the prime decomposition of the normal surface, and of Gay, on the field of complex numbers--real number version of the classical theorem of Grauert-Remmert. We also provide a new proof that the fibre surface of a normal surface in a knot complement is strongly norm SP surfaces.",
        "watermark_text": "In three - dimensional topology , the Thurston norm is a natural generalisation of the Euler characteristic . Although its concept works sense for any 3 - sphere , it has been described only for representations of its essential group in the special linear group , over the field of complex numbers .We see that the Thurston norm could be computed by examining any such representation in the standard 3 - space , over the field of real numbers . As a consequence , we provide a new proof that particular normal surfaces in knot complements are taut : every smoothly embedded disk in these surfaces has odd multiplicity at each of its points .The proof is based on work of Agol , boileau and C . T . CTseng , and of Rosenthal and of Sivek , on the prime decomposition of the normal surface , and of Gay , on the field of complex numbers - - real number version of the classical principle of Grauert - Remmert . We also obtain a new proof that the fibre surface of a normal covering in a knot complement is strongly norm SP surfaces .",
        "rewrite_text": "In the field of three-dimensional topology, the Thurston norm represents a natural generalization of the Euler characteristic. Although its concept applies to any 3-sphere, it has primarily been described in terms of representations of its essential group within the special linear group over complex number fields. It is observed that the Thurston norm can be computed by examining any such representation within the standard 3-space over real number fields. Consequently, we present a novel proof demonstrating that specific normal surfaces in knot complements are taut: every smoothly embedded disk within these surfaces exhibits odd multiplicity at each point.\n\nThe proof is founded on previous research by Agol, Boileau, C.T. CTseng, Rosenthal, and Sivek, concerning the prime decomposition of normal surfaces. Additionally, it incorporates the work of Gay on the complex number field, which is a real number version of the classical Grauert-Remmert principle. Furthermore, we have derived a fresh proof that the fiber surface of a normal covering within a knot complement strongly adheres to SP surfaces with normal norms.",
        "ori-fast-z-score": 0.23570226039551587,
        "water-fast-z-score": 3.0641293851417064,
        "rewrite-fast-z-score": 2.3333333333333335
    },
    {
        "original_text": "The Sloan Digital Sky Survey Quasar Catalog IV (SDSS Quasar IV) is the fifth generation of the Sloan Digital Sky Survey (SDSS) Quasar Survey and is described in detail by @dr5col. This Quasar Catalog contains 13,724 new quasars discovered during the course of the main four surveys comprising the SDSS III Stage 1  @sdss3 . The quasars are distributed over approximately 8,000 deg2 with a median density of approximately one quasar per square degree. The main survey data were obtained from 2007 November through 2009 June. Data for the Fourth Data Release of the Catalog were made publicly available on 2011 April 29. SDSS Quasar IV is intended to be a reliable and comprehensive resource for quasar science. The selection method and algorithms for optical quasar identification and classification are presented in @dr4ref. Spectroscopic and astrometric data are presented in Section 2. Basic properties of the quasars, including emission redshifts, black hole masses, and SDSSPetrisky coordinates, are presented in Section 3. The scientific papers utilizing the data in SDSS Quasar IV are listed in Section 4. The Quasar Catalog website includes two useful and general-purpose query forms that facilitate finding interesting candidates. The astro-ph pre-print server maintains a cited-by link to each scientific paper that uses data in SDSS Quasar IV. SDSS Quasar IV is intended to be a reliable and comprehensive resource for quasar science. The selection method and algorithms for optical quasar identification and classification are presented in @dr4ref. Spectroscopic and astrometric data are presented in Section 2. Basic properties of the quasars, including emission redshifts, black hole masses, and SDSSPetrisky coordinates, are presented in Section 3. The scientific papers utilizing the data in SDSS Quasar IV are listed in Section 4. The Quasar Catalog website includes two useful and general-purpose query forms that facilitate finding interesting candidates. The astro-ph pre-print server maintains a cited-by link to each scientific paper that uses data in SDSS Quasar IV.",
        "watermark_text": "The Sloan Digital Sky Survey Quasar Catalog IV ( SDSS Quasar IV ) is the fifth generation of the Sloan Digital Sky Survey ( SDSS ) Quasar Survey and is described in detail by @ dr5col . This Quasar Catalog includes 13 , 724 new quasars discovered during the course of the main four surveys comprising the SDSS III Stage 1 @ sdss3 .The quasars are distributed over roughly 8 , 000 deg2 with a median density of approximately one quasar per square degree . The main survey information were obtained from 2007 November through 2009 June .Data for the Fourth Data Release of the Catalog were made formally accessible on 2011 April 29 . SDSS Quasar IV is intended to be a reliable and complete resource for quasar research .The selection method and techniques for optical quasar identification and classification are presented in @ dr4ref . Spectroscopic and astrometric data are presented in Section 2 .Basic properties of the quasars , notably emission redshifts , black hole masses , and SDSSPetrisky coordinates , are presented in Section 3 . The science studies employing the information in SDSS Quasar IV are listed in Section 4 .The Quasar Catalog website includes two relevant and general - purpose query forms that facilitate finding important candidates . The astro - ph pre - print client maintains a cited - by connection to each scientific publication that using data in SDSS Quasar IV .SDSS Quasar IV is intended to be a reliable and complete resource for quasar research . The selection method and techniques for optical quasar identification and classification are presented in @ dr4ref .Spectroscopic and astrometric data are presented in Section 2 . Basic properties of the quasars , notably emission redshifts , black hole masses , and SDSSPetrisky coordinates , are presented in Section 3 .The science studies employing the information in SDSS Quasar IV are listed in Section 4 . The Quasar Catalog website includes two relevant and general - purpose query forms that facilitate finding important candidates .The astro - ph pre - print client maintains a cited - by connection to each scientific publication that using data in SDSS Quasar IV .",
        "rewrite_text": "The Fourth Generation Quasar Catalog of the Sloan Digital Sky Survey, denoted as SDSS Quasar IV, represents the fifth iteration of the Quasar Survey within the Sloan Digital Sky Survey (SDSS). This catalog, detailed by @dr5col, encompasses 13,724 newly discovered quasars that were found throughout the main four surveys comprising SDSS Stage 1 @sdss3. These quasars are distributed over approximately 8,000 square degrees with a median density of roughly one quasar per square degree. The primary survey data were collected between November 2007 and June 2009. The Fourth Data Release of the catalog became officially accessible on April 29th, 2011. SDSS Quasar IV aims to be a reliable and comprehensive resource for quasar research.\n\nThe techniques and methods for optical quasar identification and classification are explained in @dr4ref. Section 2 presents spectroscopic and astrometric data, while Section 3 highlights the fundamental properties of the quasars, specifically their emission redshifts, black hole masses, and SDSSPetrisky coordinates. The scientific studies utilizing the information from SDSS Quasar IV are listed in Section 4.\n\nThe Quasar Catalog website features two versatile query forms that facilitate the identification of significant candidates. The astro-ph pre-print client maintains a cited-by connection with every scientific publication that utilizes data from SDSS Quasar IV. Additionally, it is intended as a dependable and comprehensive resource for future quasar research. The selection process and techniques for optical quasar recognition and classification are presented in @dr4ref, while spectrographic and astrometric data are further presented in Section 2. Quasars' essential characteristics, including emission redshifts, black hole masses, and SDSSPetrisky coordinates, are outlined in Section 3. The scientific studies that employ the data in SDSS Quasar IV are documented in Section 4. Moreover, the Quasar Catalog website provides two general-purpose query forms for easily locating important candidates. The astro-ph pre-print client maintains a record of citations from scientific publications utilizing data from SDSS Quasar IV.",
        "ori-fast-z-score": 0.5432144762551112,
        "water-fast-z-score": 6.880716699231408,
        "rewrite-fast-z-score": 0.6311687442672026
    },
    {
        "original_text": "Turbulent flows are often characterized by the assumption of locality, where the kinematics and dynamics are largely separable in space and time. In this paradigm, fluid motion is decomposed into characteristic modes, or eddies, often arranged into quasi-regular structures such as shears, jets, and fronts. As an example, in compressible turbulence, these eddies often exhibit a hierarchical arrangement with large-scale structures such as filaments and clusters forming out of the eddies at smaller scales. This process is often characterized by a power-law scaling, where the size, velocity, density, and temperature of eddies at each scale are related to those at the next smaller scale through a characteristic scale factor. This phenomenon, known as multiscaling, implies a self-similar structure and has been observed to varying degrees of rigor in a range of physical systems. In this work, we present direct numerical simulations of compressible turbulence at high resolution, focusing specifically on the nature of multiscaling in the scaling regime. We find that multiscaling is not as evident as in comparable experimental and Eulerian grid-based simulations, but is clearly present for a range of flow conditions. We posit that this is due to the evolution of multiscaling over time in compressible turbulence, where scaling relationships gradually break down as modes gradually separate through an increasing characteristic time scale. This process, known as intermittency, implies a negative scaling exponent and correspondingly flatter multiscaling exponents. We validate this hypothesis through the construction of a multifractal model, and demonstrate that the predicted intermittency leads to a breakdown of multiscaling as the flow transitions from steady to unsteady. This provides a useful framework for interpreting observations of multiscaling in turbulent flows, and has important implications for the dynamics of clusters in highly compressible turbulent flows such as those arising in the atmosphere and fusion plasma simulations.",
        "watermark_text": "Turbulent flows are often characterized by the assumption of locality , where the kinematics and dynamics are essentially separable in space and time . In this framework , fluid motion is decomposed into distinctive modes , or eddies , sometimes arranged into quasi - regular structures such as shears , jets , and fronts .As an instance , in compressible turbulence , these eddies often exhibit a hierarchical structure with large - scale structures such as filaments and clusters developing out of the eddies at lower scales . This process is often characterized by a power - law scaling , where the height , speed , density , and heat of eddies at each scale are related to those at the second smaller scale through a unique scale factor .This phenomenon , known as multiscaling , suggests a self - similar structure and has been observed to different degrees of rigor in a range of physical structures . In this research , we present direct numerical simulations of compressible turbulence at high resolution , concentrating specifically on the nature of multiscaling in the scaling regime .We see that multiscaling is not as obvious as in related experimental and Eulerian grid - based simulations , but is distinctly present for a range of flow conditions . We posit that this is due to the evolution of multiscaling over time in compressible turbulence , where scaling interactions slowly give down as modes progressively separate through an increasing characteristic time scale .This process , known as intermittency , suggests a negative scaling exponent and correspondingly flatter multiscaling exponents . We validate this hypothesis through the creation of a multifractal model , and demonstrate that the expected intermittency leads to a breakdown of multiscaling as the flow transitions from steady to unsteady .This offers a used basis for interpreting observations of multiscaling in turbulent streams , and has crucial consequences for the dynamics of clusters in highly compressible turbulent streams such as those occurring in the atmosphere and fusion plasma simulations .",
        "rewrite_text": "Turbulent flows often manifest themselves through the assumption of locality, in which the kinematics and dynamics are spatially and temporally separable. Within this framework, fluid motion is segmented into distinct modes or eddies, sometimes organized into quasi-regular structures like shear layers, jets, and fronts. As an example, in compressible turbulence, these eddies frequently exhibit a hierarchical structure with large-scale features like filaments and clusters that evolve from smaller-scale eddies. This process is frequently characterized by a power-law scaling relationship, where the height, speed, density, and heat of eddies at different scales are linked through a unique scale factor to those at smaller scales. This phenomenon, known as multiscaling, suggests a self-similar structure and has been observed in various physical systems to varying degrees of rigor.\n\nIn this research, we present high-resolution direct numerical simulations of compressible turbulence, focusing specifically on the nature of multiscaling in the scaling regime. Our findings indicate that multiscaling is not as evident as in related experimental or Eulerian grid-based simulations but is clearly present under a range of flow conditions. We propose that this is due to the temporal evolution of multiscaling in compressible turbulence, where scaling interactions gradually diminish as modes progressively separate through an increasing characteristic time scale. This process, known as intermittency, implies a negative scaling exponent and correspondingly flatter multiscaling exponents.\n\nTo validate our hypothesis, we develop a multifractal model. We demonstrate that the expected intermittency leads to a breakdown of multiscaling as the flow transitions from steady to unsteady states. This offers a basis for interpreting observations of multiscaling in turbulent streams and has crucial implications for the dynamics of clusters in highly compressible turbulent streams, such as those found in atmospheric and fusion plasma simulations.",
        "ori-fast-z-score": 0.3563483225498992,
        "water-fast-z-score": 6.770618128448084,
        "rewrite-fast-z-score": 2.5733338773067302
    },
    {
        "original_text": "The anomalous X-ray pulsar 4U 0142+61 was intensively monitored with the Rossi X-ray Timing Experiment (RXTE) for more than 11 years. The obtained data were analyzed in the time domain as well as in the frequency domain. The obtained results show that the source underwent several flux state transitions, exhibited timing irregularities and a broad band noise component at lower frequencies. For the first time, we were able to detect quasi-periodic oscillations (QPO) at 27.6 s and 60.4 mHz. The high frequency QPO is most likely a Keplerian frequency at the compact object s warped magnetosphere. The low frequency QPO could be a neutron star s frequency group. Previously, 4U 0142+61 was observed to switch between two states: a soft state, where the source displayed a high frequency QPO, and a hard state, where the source showed a low frequency QPO. The source has never been observed to be in both states at the same time. In the present data set, the source was observed to be in the hard state for almost the entire monitoring period, with only short periods of time in the soft state. The transitions between the different flux states as well as timing irregularities observed in the source s timing behavior could be the result of a changing geometry of the source s magnetic field. Our results suggest that further studies of the long-term timing and flux behavior of this source could lead to a better understanding of both the interior structure of neutron stars as well as the processes that govern the transport of particles in neutron star magnetospheres.",
        "watermark_text": "The anomalous X - ray pulsar 4U 0142 + 61 was intensively controlled with the Rossi X - ray Timing Experiment ( RXTE ) for more than 11 years . The gathered statistics were studied in the time domain as well as in the frequency domain .The published results show that the source underwent numerous flux state cycles , exhibited timing irregularities and a broad band noise component at lower frequencies . For the first time , we were could to observe quasi - periodic oscillations ( QPO ) at 27 . 6 s and 60 . 4 mHz .The high frequency QPO is most likely a Keplerian frequency at the compact object s warped magnetosphere . The lowest frequency QPO might be a neutron star s frequency group .Previously , 4U 0142 + 61 was seen to shift between two states : a soft state , where the origin displayed a high frequency QPO , and a hard state , where the source displayed a small frequency QPO . The source has never been observed to be in both states at the same time .In the present data set , the source was measured to be in the hard state for almost the entire monitoring time , with only brief periods of time in the soft state . The transitions between the different flux states as well as timing irregularities noted in the source s timing response possibly be the result of a changing geometry of the source s magnetic current .Our results propose that further studies of the long - term scheduling and flux dynamics of this source may contribute to a better study of both the interior composition of neutron galaxies as well as the mechanisms that govern the travel of particles in neutron star magnetospheres .",
        "rewrite_text": "The 4U 0142+61 anomalous X-ray pulsar has been extensively monitored over an 11-year period using the Rossi X-ray Timing Experiment (RXTE). The gathered statistics have been analyzed in both the time and frequency domains. Published findings reveal numerous flux state cycles, timing irregularities, and a broadband noise component at lower frequencies. For the first time, quasi-periodic oscillations (QPOs) at 27.6 seconds and 60.4 mHz have been observed. The high-frequency QPO is likely a Keplerian frequency at the warped magnetosphere of the compact object, while the lowest-frequency QPO could be associated with the frequency group of a neutron star.\n\nPreviously, 4U 0142+61 was observed to alternate between two states: a soft state exhibiting high-frequency QPOs and a hard state with small-frequency QPOs. The source has never been observed in both states simultaneously. In the current dataset, the source was predominantly in the hard state for the entirety of the monitoring period, with only brief periods transitioning to the soft state. The transitions between different flux states and the noted timing irregularities may be attributed to a changing geometry of the source's magnetic current.\n\nOur results suggest that further investigations into the long-term scheduling and flux dynamics of this source could contribute to a better understanding of both the internal composition of neutron galaxies and the mechanisms governing particle propagation in neutron star magnetospheres.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 1.7162326606420661
    },
    {
        "original_text": "In this paper, we study a special case of the quantum walk in which the probabilities of the walker stepping to the left and right are both biased by a factor of alpha in one direction. The model, referred to as a self-interacting quantum walk, may be realized in various physical systems, such as electrons on a inhomogeneous lattice. We consider a general initial condition, allowing the walker to have any fractional part, and prove that the quantum walker will settle in finite time into a pure quantum state with real eigenvalue density equal to 1. We also consider the classical analogous of this walk, where at each time step, the walker may choose to step to the left or right with some probability. While the classical walk is transient in general, we prove that for sufficiently small alpha, the classical walk will always settle into a periodic configuration. We present a numerical simulation of the time-dependent behavior of both the quantum and classical self-interacting walks. As a further application of our techniques, we prove that the quantum walk converges in distribution to the classical walk. Additionally, we study two measures of mixing time: a quantum version of the quantum return localisation, and a classical version of the Klein gas correction. Finally, we present a large deviation analysis of the classical self-interacting walk and show that the walker is most likely to end up in a finite number of cells.",
        "watermark_text": "In this paper , we study a unique case of the quantum walk in which the probabilities of the walker stepping to the leave and left are both biased by a factor of alpha in one direction . The model , referred to as a self - interacting quantum walk , might be realized in different physical structures , such as electrons on a inhomogeneous crystal .We consider a general initial condition , allowing the walker to have any fractional portion , and prove that the quantum walker will settle in finite period into a simple quantum state with real eigenvalue density equal to 1 . We additionally consider the classical analogous of this walk , where at each time step , the walker would choose to move to the leave or left with some likelihood .While the classical walk is transient in general , we prove that for sufficiently small alpha , the classical walk will always settle into a periodic configuration . We present a numerical model of the period - dependent activity of both the quantum and classical self - interacting walks .As a further application of our concepts , we prove that the quantum walk converges in distribution to the classical walk . Additionally , we study two estimates of mixing time : a quantum version of the quantum return localisation , and a classical version of the Klein gas correction .Finally , we present a large deviation study of the classical self - interacting walk and find that the walker is most likely to end up in a finite number of cells .",
        "rewrite_text": "In this research, we investigate a particular instance of quantum walking wherein the probabilities for the walker to step towards the left and right are both skewed by a factor of alpha in one direction. This model, referred to as a self-interacting quantum walk, may manifest in various physical structures, such as electrons moving within an inhomogeneous crystal. We consider a general initial condition that allows the walker to have any fractional component and demonstrate that the quantum walker will converge to a simple quantum state with a real eigenvalue density of 1 within a finite period.\n\nFurthermore, we examine the classical counterpart of this walk, where at each time step, the walker chooses to move left or right with certain probabilities. While the classical walk is generally transient, we prove that for sufficiently small values of alpha, the classical walk always stabilizes into a periodic configuration. We present a numerical model to illustrate the period-dependent activity of both the quantum and classical self-interacting walks.\n\nAs an additional application of our concepts, we show that the quantum walk converges in distribution to the classical walk. Additionally, we explore two estimates of mixing time: a quantum variant of quantum return localization and a classical version of the Klein gas correction. Finally, we conduct a large deviation study on the classical self-interacting walk and find that the walker is most likely to end up within a finite number of cells.",
        "ori-fast-z-score": -1.2229371288986763,
        "water-fast-z-score": 3.104378865665871,
        "rewrite-fast-z-score": 0.1873171623163388
    },
    {
        "original_text": "We derive entropy inequalities for the lattice Boltzmann method (LBM) for the perfect gas. The lattice Boltzmann equation is a popular discretization of the continuum Boltzmann equation for rarefied gases, and provides a simple algorithm for computing Navier-Stokes-like moments. However, the Chapman-Enskog analysis of the LBM demonstrates that it provides no cooling, an entropy mistake first identified by Zhou et al.  J. Stat. Phys. 165, 1 (2016) . Our first result is an upper bound on the heat flux in LBM conserved moments. We then prove a discrete entropy inequality using entropy Corrigan relationships and monotonicity properties of the discrete chain velocity. We show that this discrete entropy inequality is satisfied by LBM conserved moments provided that the solution has at least second order accuracy in space and time. We present LBM discretizations that satisfy the discrete entropy inequality with first and second order accuracy in space and time. Numerical evidence demonstrates the practicality of our entropy corrector LBM schemes, in that they yield statistically accurate solutions to the fluctuating viscous compressible Navier-Stokes equations in two and three spatial dimensions.",
        "watermark_text": "We derive entropy inequalities for the lattice Boltzmann technique ( LBM ) for the perfect gas . The lattice Boltzmann equation is a popular discretization of the continuum Boltzmann integral for rarefied gases , and provides a simple algorithm for calculation Navier - Stokes - like seconds .However , the Chapman - Enskog analysis of the LBM proves that it gives no cooling , an entropy mistake initially recognized by Zhou et al . J . Stat .Phys.165, 1 (2016) .Our first consequence is an upper bound on the thermal flux in LBM conserved moments . We then resolve a finite entropy inequality using entropy Corrigan interactions and monotonicity properties of the discrete chain density .We see that this finite entropy inequality is fulfilled by LBM conserved moments provided that the solve has at least second order accuracy in space and time . We introduce LBM discretizations that fulfill the discrete entropy inequality with first and first order efficiency in space and time .Numerical research proves the practicality of our entropy corrector LBM strategies , in that they produce statistically accurate answers to the fluctuating viscous compressible Navier - Stokes equations in two and three spatial dimensions .",
        "rewrite_text": "We have derived entropy inequalities for the Lattice Boltzmann Method (LBM) applied to the perfect gas. The Lattice Boltzmann equation is a commonly used discretization of the continuous Boltzmann integral for rarefied gases, offering a straightforward algorithm for computing Navier-Stokes-like equations. However, the Chapman-Enskog analysis of LBM reveals a lack of cooling, an entropy issue initially identified by Zhou et al. in their 2016 Journal of Statistical Physics paper.\n\nOur primary finding is an upper limit on the thermal flux in LBM's conserved moments. We then establish a finite entropy inequality through the utilization of entropy Corrigan interactions and the monotonic properties of discrete chain density. We observe that this finite entropy inequality is satisfied by LBM's conserved moments only if the solution exhibits at least second-order accuracy in both space and time.\n\nTo this end, we introduce LBM discretizations that meet the discrete entropy inequality with efficiency at first and second order in space and time. Numerical experiments demonstrate the practicality of our entropy-corrected LBM strategies, producing statistically accurate results for the fluctuating viscous compressible Navier-Stokes equations in two and three spatial dimensions.",
        "ori-fast-z-score": -1.7556172079419585,
        "water-fast-z-score": 4.184914994777494,
        "rewrite-fast-z-score": -0.5773502691896257
    },
    {
        "original_text": "Polarized proton-proton (PP) collisions allow for the precise study of the spin structure of the proton and the nature of quantum chromodynamics (QCD). In April 2021, the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory (BNL) collided longitudinally polarized proton beams at a center-of-mass energy of 200 GeV. These collisions provided an opportunity to study spin interactions in a high-energy, polarized collider. This paper summarizes some of the new results from this program, including measurements of the Gottfried sum, the quark contributions to the magnetic moment, the spin dependent fragmentation function, the quark transverse momentum dependent distributions, and measurements of strangeness polarization. rule Polarized proton-proton (PP) collisions allow for the precise study of the spin structure of the proton and the nature of quantum chromodynamics (QCD). In April 2021, the Relativistic Heavy Ion Collider (RHIC) at Brookhaven National Laboratory (BNL) collided longitudinally polarized proton beams at a center-of-mass energy of 200 GeV. These collisions provided an opportunity to study spin interactions in a high-energy, polarized collider. This paper summarizes some of the new results from this program, including measurements of the Gottfried sum, the quark contributions to the magnetic moment, the spin dependent fragmentation function, the quark transverse momentum dependent distributions, and measurements of strangeness polarization.",
        "watermark_text": "Polarized proton - proton ( PP ) collisions allow for the exact research of the spin composition of the proton and the nature of quantum chromodynamics ( QCD ) . In April 2021 , the Relativistic Heavy Ion Collider ( RHIC ) at Brookhaven National Laboratory ( BNL ) collided longitudinally polarized proton beams at a center - of - mass energy of 200 GeV .These collisions presented an ability to study spin behavior in a high - energy , polarized collider . This paper summarizes some of the new data from this project , notably measurements of the Gottfried sum , the quark contributions to the magnetic moment , the spin dependent fragmentation function , the quark transverse momentum dependent distributions , and measurements of strangeness polarization .rule Polarized proton - proton ( PP ) collisions allow for the exact analysis of the spin composition of the proton and the nature of quantum chromodynamics ( QCD ) . In April 2021 , the Relativistic Heavy Ion Collider ( RHIC ) at Brookhaven National Laboratory ( BNL ) collided longitudinally polarized proton beams at a center - of - mass energy of 200 GeV .These collisions presented an ability to study spin behavior in a high - energy , polarized collider . This paper summarizes some of the new data from this project , notably measurements of the Gottfried sum , the quark contributions to the magnetic moment , the spin dependent fragmentation function , the quark transverse momentum dependent distributions , and measurements of strangeness polarization .",
        "rewrite_text": "In the context of science research, the utilization of polarized proton-proton (PP) collisions offers precise insights into the spin structure of the proton and the fundamental nature of quantum chromodynamics (QCD). Specifically in April 2021, the Relativistic Heavy Ion Collider (RHIC) situated at Brookhaven National Laboratory (BNL) conducted longitudinally polarized proton beam collisions with a center-of-mass energy of 200 GeV. These high-energy, polarized collisions provided a unique opportunity to investigate spin dynamics. This paper presents a summary of some new data from this project, including notable measurements such as the Gottfried sum, the impact of quarks on the magnetic moment, the spin-dependent fragmentation function, the distributions of quark transverse momentum dependence, and measurements of strangeness polarization.",
        "ori-fast-z-score": -0.6974858324629157,
        "water-fast-z-score": 3.348631561299829,
        "rewrite-fast-z-score": -0.7293249574894728
    },
    {
        "original_text": "A fundamental plane for clusters of galaxies is proposed in the form of the correlation between the optical diameter of a cluster and its temperature measured by the Sunyaev-Zeldovich effect. The plane invokes minimal assumptions and is thus very robust. The normalised distribution of sources on the plane is well-matched by theoretical models of structure formation, indicating that the plane may be used to characterise the dark-matter content of clusters and hence their mass. Observational estimates of the plane are presented for galaxy clusters detected by their Sunyaev-Zeldovich effect and X-ray thermal emission. The estimated normalised intrinsic dispersion in the plane is lower than the measured uncorrelated normalisation uncertainties, indicating that the intrinsic scatter in the plane is also low. This is likely to be owing to the tight correlation between the optical diameter of a cluster and its temperature. The optical diameter of a cluster can be determined from imaging of the daylight reflected by its surface, for example by the Hubble Space Telescope. The temperature of a cluster can be measured by the Sunyaev-Zeldovich effect, which shifts the thermal spectrum of the cosmic microwave background radiation towards longer wavelengths when the cluster drags towards it a proportion of the cosmological microwave background radiation background gas.",
        "watermark_text": "A essential plane for clusters of galaxies is proposed in the form of the relationship between the optical diameter of a cluster and its temperature calculated by the Sunyaev - Zeldovich effect . The plane invokes minimal assumptions and is thereby very robust .The normalised distribution of sources on the jet is well - matched by theoretical theories of formation formation , showing that the jet may be used to characterise the dark - matter content of clusters and hence their mass . Observational accounts of the plane are presented for galaxy galaxies detected by their Sunyaev - Zeldovich effect and X - ray thermal decay .The estimated normalised intrinsic dispersion in the plane is lower than the reported uncorrelated normalisation uncertainties , showing that the intrinsic scatter in the plane is also low . This is probably to be due to the fast coupling between the optical diameter of a cluster and its temperature .The optical diameter of a cluster can be determined from imaging of the daylight seen by its surface , for example by the Hubble Space Telescope . The temperature of a cluster can be determined by the Sunyaev - Zeldovich effect , which changes the thermal spectrum of the cosmic microwave background radiation towards faster wavelengths when the cluster drags towards it a fraction of the cosmological microwave background radiation background energy .",
        "rewrite_text": "A fundamental plane for galaxy cluster analysis is proposed, based on the relationship between the optical diameter of a cluster and its temperature, calculated through the Sunyaev-Zeldovich effect. This plane necessitates minimal assumptions, thus enhancing its robustness. The normalized distribution of sources on the jet aligns well with theoretical formation models, indicating that the jet can be used to characterize the dark matter content and consequently the mass of clusters.\n\nObservational data for this plane is presented for galaxies detected through their Sunyaev-Zeldovich effect and X-ray thermal decay. The estimated normalized intrinsic dispersion within the plane is found to be lower than reported uncorrelated normalization uncertainties, indicating a low level of intrinsic scatter. This is likely due to the strong coupling between the optical diameter of a cluster and its temperature. The optical diameter can be determined through imaging of daylight observed from its surface, such as with the Hubble Space Telescope. Meanwhile, the temperature of a cluster can be determined via the Sunyaev-Zeldovich effect, which alters the thermal spectrum of the cosmic microwave background radiation towards higher frequencies when the cluster pulls a fraction of the cosmic microwave background radiation's energy towards it.",
        "ori-fast-z-score": 0.8251369970070347,
        "water-fast-z-score": 5.775958979049243,
        "rewrite-fast-z-score": 2.2013981571160284
    },
    {
        "original_text": "Ensembles of magnetic monopoles have been widely studied in the literature due to their potential relevance in quantum chromodynamics (QCD), the underlying theory of the strong interaction. These ensembles provide a framework to explore non-perturbative aspects of QCD that cannot be studied via Monte Carlo techniques. In this work, we confine such an ensemble using magnetic monopole solutions of the Bogomolnyi equation. These magnetic monopoles are point particles with both electric and magnetic charges. They couple via the magnetic dipole moment. We use a modified Powell scheme to solve the resulting non-linear equation and generate ensembles of monopole solutions. The confining strength of the ensemble is controlled by varying the  t Hooft coupling constant, a parameter of the theory that governs the magnetic monopole s self-interactions. We characterize this ensemble via several order parameters and observe that the system exhibits deconfinement, a phenomenon in which the magnetic charges freely move in the Euclidean spacetime. This Letter describes an initial investigation into the confining properties of such an ensemble and the order parameters that characterize it.",
        "watermark_text": "Ensembles of magnetic monopoles have been widely explored in the literature due to their potential significance in particle chromodynamics ( QCD ) , the fundamental theory of the strong coupling . These ensembles provide a framework to examine non - perturbative aspects of QCD that cannot be investigated via Monte Carlo methods .In this study , we confine such an ensemble using magnetic monopole solutions of the Bogomolnyi equation . These magnetic monopoles are point molecules with both magnetic and magnetic charges .They couple via the magnetic dipole moment . We use a modification Powell scheme to correct the resulting non - linear equation and produce ensembles of monopole solutions .The confining strength of the ensemble is controlled by varying the t Hooft coupling constant , a parameter of the theory that governs the magnetic monopole s self - interactions . We characterize this ensemble via several order variables and notice that the system displays deconfinement , a effect in which the magnetic charges freely move in the Euclidean spacetime .This Letter outlines an initial investigation into the confining properties of such an ensemble and the order variables that characterize it .",
        "rewrite_text": "In the literature, ensembles of magnetic monopoles have been extensively studied due to their crucial role in particle chromodynamics (QCD), the fundamental theory of strong interactions. These ensembles offer a structure to explore non-perturbative aspects of QCD that are inaccessible through Monte Carlo methods. In this research, we employ magnetic monopole solutions derived from the Bogomolnyi equation to confine such an ensemble. These magnetic monopoles are point-like particles possessing both electric and magnetic charges, interacting through their magnetic dipole moments.\n\nTo address the resulting non-linear equations, we employ a modified Powell scheme to correct them and generate ensembles of monopole solutions. The confinement strength of the ensemble is adjusted by varying the t Hooft coupling constant, a key parameter in the theory that governs the self-interactions of magnetic monopoles. We characterize this ensemble through several order parameters and observe the emergence of deconfinement, a phenomenon where magnetic charges move freely in Euclidean spacetime. This letter outlines an initial investigation into the confining properties of such ensembles and the order parameters that define them.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 4.129483209670111,
        "rewrite-fast-z-score": -0.6625891564490792
    },
    {
        "original_text": "Soft repulsive-spheres (RSS), when mixed in varying composition, generally segregate into distinct global phases, with the midrange RSS partial mixing quantity, Q2, being small. Here we report the identification of a wide Q2 range of strong attractive depletion interactions in RSS binary mixtures, even for high RSS packing fractions. Strong depletion interactions emerge as a competition between two distinct interparticle forces, namely, hard sphere steric repulsion and RSS attraction. Using three independent techniques, we find that these depletion forces dominate over other pairwise forces, such as RSS particle wetting, RSS electrostatic interactions and short-range chemical bonding, for a Q2 range spanning three decades. The attractive depletion interactions lead to enhanced RSS demixing at high RSS packing fractions, and formation of unexpectedly rich phase diagrams with two and even three coexisting global phases. The identified strong attractive depletion interactions could find potential uses in tailor-made RSS mixtures with programmable phase diagrams, and for controlling RSS self-assembly in bottom-up nanotechnology and soft robotics.",
        "watermark_text": "Soft repulsive - particles ( RSS ) , when mixed in varying composition , generally segregate into unique global phases , with the midrange RSS partial mixture quantity , Q2 , being small . Here we study the discovery of a broad Q2 range of stable aggressive depletion interactions in RSS binary mixtures , even for high RSS packing fractions .Strong depletion interactions emerge as a competition between two separate interparticle forces , namely , hard sphere steric repulsion and RSS attraction . Using three separate techniques , we find that these depletion forces dominate over other pairwise forces , such as RSS particle wetting , RSS electrostatic interactions and long - range chemical bonding , for a Q2 range lasting three decades .The appealing depletion interactions result to heightened RSS demixing at high RSS packing fractions , and formation of surprisingly rich phase diagrams with two and even three coexisting global phases . The identified strong attractive depletion interactions might find potential uses in tailor - made RSS mixtures with programmable phase diagrams , and for controlling RSS self - assembly in top - up nanotechnology and soft robotics .",
        "rewrite_text": "In the context of Soft Repulsive-particles (RSS), when mixed in various compositions, they typically separate into distinct global phases with a small mid-range partial mixture quantity denoted as Q2. Our study explores the revelation of a broad range of stable aggressive depletion interactions in binary mixtures of RSS, even at high packing fractions. These interactions arise from a competition between two distinct interparticle forces: the hard-sphere steric repulsion and the RSS attraction.\n\nUtilizing three distinct techniques, we discover that these depletion forces outweigh other pairwise interactions, such as RSS particle wetting, electrostatic RSS interactions, and long-range chemical bonding, for a Q2 range spanning three decades. The alluring depletion interactions lead to an enhanced demixing of RSS at high packing fractions, resulting in unexpectedly rich phase diagrams with two or even three coexisting global phases. The identified strong attractive depletion interactions may hold potential applications in tailor-made RSS mixtures with programmable phase diagrams and in controlling RSS self-assembly in advanced nanotechnology and soft robotics.",
        "ori-fast-z-score": 1.6059101370939322,
        "water-fast-z-score": 5.658032638058332,
        "rewrite-fast-z-score": 1.937329799813845
    },
    {
        "original_text": "A light-cone QCD approach to study distribution amplitudes of axial-vector mesons is presented. Using a collective representation of light-cone distributions amplitude, we obtain the following: (1) The famous first few Gegenbauer polynomials/coefficients for three flavor light-cone DAs of axial-vector mesons have been reproduced; (2) Two more Gegenbauer polynomials/coefficients for the leading twist 2 distribution amplitude (DA) of axial-vector mesons are computed; (3) Three asymptotic DAs are studied in details: Chernyak-Zhitnitsky DA, its vertical version and the positive-parity version of more usual Lambert W DA; (4) several desirable properties of axial-vector DAs are studied and explained in the language of light-cone QCD; (5) The recent CLAS12 data for pion transition form factors are well described.",
        "watermark_text": "A heavy - cone QCD approach to study distribution amplitudes of axial - vector mesons is provided . Using a collective description of light - cone distributions amplitude , we obtain the following : ( 1 ) The famous first few Gegenbauer polynomials / coefficients for three flavor light - cone DAs of axial - vector mesons have been reproduced ; ( 2 ) Two more Gegenbauer polynomials / coefficients for the led twist 2 distribution amplitude ( DA ) of axial - vector mesons are computed ; ( 3 ) Three asymptotic DAs are studied in details : Chernyak - Zhitnitsky DA , its vertical version and the positive - parity variant of more usual Lambert W DA ; ( 4 ) several desirable properties of axial - vector DAs are studied and explained in the language of light - cone QCD ; ( 5 ) The last CLAS12 statistics for pion transition form factors are better illustrated .",
        "rewrite_text": "A comprehensive approach utilizing the heavy-cone QCD is presented to investigate the distribution amplitudes of axial-vector mesons. By employing a collective description of light-cone distribution amplitudes, we have derived the following findings:\n\n(1) The renowned first few Gegenbauer polynomials/coefficients have been recreated for the three-flavor light-cone distribution amplitudes of axial-vector mesons.\n\n(2) We have computed two additional Gegenbauer polynomials/coefficients for the leading twist 2 distribution amplitude (DA) of axial-vector mesons.\n\n(3) Detailed studies have been conducted on three asymptotic DAs: the Chernyak-Zhitnitsky DA, its vertical variant, and the positive-parity variant of the more common Lambert W DA.\n\n(4) Several desirable properties of axial-vector DAs have been explored and explained in the context of light-cone QCD.\n\n(5) The latest CLAS12 statistics for pion transition form factors are more clearly illustrated.",
        "ori-fast-z-score": 2.6098507150250914,
        "water-fast-z-score": 5.082340866101494,
        "rewrite-fast-z-score": 0.40451991747794525
    },
    {
        "original_text": "A global Wolf-Rayet content of 12% was derived for the nearby NGC300 galaxy, significantly higher than the current estimates of 7% (Schaerer & Vacca 2010). This high value might be explained by the recent star formation episode implied by the detection of an ULX in the outskirts of the galaxy (Israel et al. 2014). However, a closer inspection of the archival data indicates that the upper limit on the global WR content of NGC300 is in fact 7%. Furthermore, the upper limit WR contribution to the U-band magnitude of the whole galaxy is 2.2%, in contrast to the 8.3% derived from the global Wolf-Rayet content. This apparent discrepancy can be explained by the late-type stellar content of the galaxy. Late-type stars dominate the light at optical wavelengths, and given the age-metallicity relation, they will exhibit a high fraction of early-type Wolf-Rayet stars. Indeed, a KS test on the observed and modeled optical colors of the galaxy yields a low probability (P &lt; 0.001) that the two samples were extracted from the same population. We estimate the global WR contribution to the U-band magnitude of the galaxy to be 1.2%, in agreement with the observed upper limit.",
        "watermark_text": "A global Wolf - Rayet abundance of 12 % was derived for the nearby NGC300 galaxy , slightly greater than the present projections of 7 % ( Schaerer & Vacca 2010 ) . This high significance could be reason by the recent star formation episode implied by the observation of an ULX in the outskirts of the galaxy ( Israel et al .2014 ) . However , a closer analysis of the archival records indicates that the upper limitation on the global WR content of NGC300 is in reality 7 % .Furthermore , the higher limit WR contribution to the U - band magnitude of the whole galaxy is 2 . 2 % , in comparison to the 8 . 3 % generated from the global Wolf - Rayet material . This evident discrepancy can be reason by the late - class stellar content of the galaxy .Late - class stars dominate the light at optical wavelengths , and given the age - metallicity relation , they will exhibit a high fraction of early - class Wolf - Rayet stars . Indeed , a KS test on the seen and reconstructed optical colors of the galaxy produces a small probability ( P & lt ; 0 . 001 ) that the two specimens were extracted from the same population .We estimate the global WR contribution to the U - band magnitude of the galaxy to be 1 . 2 % , in agreement with the seen upper maximum .",
        "rewrite_text": "The estimated global abundance of Wolf-Rayet (WR) stars in the nearby NGC300 galaxy is 12%, slightly exceeding the current projection of 7% (Schaerer & Vacca 2010). This significant difference may be attributed to the recent episode of star formation observed in an Ultra-Luminous X-ray source at the galaxy's outskirts (Israel et al. 2014). However, a detailed analysis of archival records suggests that the upper limit for the overall WR content in NGC300 is actually 7%. Furthermore, the maximum WR contribution to the U-band magnitude of the entire galaxy is 2.2%, compared to 8.3% derived from the global Wolf-Rayet material. This apparent discrepancy could be explained by the late-class stellar content of the galaxy. Late-class stars dominate the optical wavelength light, and given their age-metallicity relationship, they will display a high proportion of early-class Wolf-Rayet stars. Indeed, a Kolmogorov-Smirnov (KS) test on the observed and reconstructed optical colors of the galaxy yields a low probability (P < 0.001) that the two samples were drawn from the same population. We estimate the global WR contribution to the U-band magnitude of the galaxy to be 1.2%, in agreement with the observed maximum limit.",
        "ori-fast-z-score": -1.4924050144892729,
        "water-fast-z-score": 4.1812388858673994,
        "rewrite-fast-z-score": -0.3144854510165755
    },
    {
        "original_text": "Subdwarf B (sdB) stars are comprised of partially degenerate helium cores of once-massive late helium stars. Due to their degenerate cores, sdBs are essentially black bodies in the visual; their spectrum comprises of numerous absorption lines, the intensity of which are in proportion to the surface temperature. Since sdBs are typically several thousand K hot, the spectrum is accessible fromground-based telescopes. In fact, one can probe pulsations in the spectra of sdBs - the nature of which is currently not well understood. Through the discovery of pulsations in three sdBs, we have a unique opportunity to further study their interiors. We present observations of three pulsators: HS 0039+4302, HS 0444+0458, and examine the properties of pulsators as a group. Through high-dispersion spectroscopy, we have discovered pulsations in three pulsators: HS 0039+4302, HS 0444+0458, and we examine the properties of pulsators as a group. The pulsators fall into two distinct groups: those with multimodal pulsation spectra, and those with single-peaked spectra. The former exhibit complex spectra and appear to be multi-periodic; in contrast, the latter are single-mode pulsators with stable frequencies. These frequencies are similar in amplitude and frequency for each pulsator, with variations in peak separation, as well as varying pulsation amplitudes and phases. We posit that these pulsators represent different stages in the same evolutionary channel. The pulsators with multimodal pulsation spectra are candidates for sdBs in the final stages of merger, prior to the onset of electron degeneracy. We report, for the first time, single-mode pulsation frequencies for three sdBs. These pulsators may represent different evolutionary states of a common progenitor. We find evidence for two distinct types of pulsators in the sdB population. This discovery adds three new pulsators to the small, but growing, sdB pulsator census, and presents an important opportunity to study the interiors of sdBs, and the evolution of their progenitors.",
        "watermark_text": "Subdwarf B ( sdB ) stars are comprised of largely degenerate helium cores of once - massive late helium stars . Due to their degenerate cores , sdBs are essentially black bodies in the visual ; their spectrum comprises of several absorption lines , the strength of which are in ratio to the surface temperature .Since sdBs are typically many thousand K heated , the spectrum is accessible fromground - based telescopes . In fact , one can investigate pulsations in the spectra of sdBs - the nature of which is currently not well understood .Through the discovery of pulsations in three sdBs , we have a unique opportunity to further study their interiors . We report observations of three pulsators : HS 0039 + 4302 , HS 0444 + 0458 , and examine the properties of pulsators as a group .Through high - dispersion spectroscopy , we have discovered pulsations in three pulsators : HS 0039 + 4302 , HS 0444 + 0458 , and we investigate the properties of pulsators as a group . The pulsators fall into two different bands : those with multimodal pulsation spectra , and those with double - peaked spectra .The former exhibit complex spectra and tend to be multi - periodic ; in comparison , the second are single - mode pulsators with stable amplitude . These frequencies are comparable in intensity and frequency for each pulsator , with variations in peak separation , as well as varying pulsation amplitudes and phases .We posit that these pulsators depict distinct phases in the same evolutionary channel . The pulsators with multimodal pulsation spectra are candidates for sdBs in the last phases of merger , early to the emergence of electron degeneracy .We report , for the first time , single - mode pulsation levels for three sdBs . These pulsators possibly represent separate evolutionary states of a common progenitor .We get data for two different kinds of pulsators in the sdB population . This find adds three new pulsators to the small , but growing , sdB pulsator census , and provides an important chance to study the interiors of sdBs , and the evolution of their progenitors .",
        "rewrite_text": "Subdwarf B (sdB) stars are predominantly composed of degenerate helium cores derived from once-massive late helium stars. Due to their degenerate cores, sdBs essentially behave as black bodies in the visible spectrum. Their spectrum is composed of several absorption lines, with the strength of these lines proportional to the surface temperature. As sdBs are typically heated to many thousands of degrees Kelvin, their spectrum is accessible via ground-based telescopes.\n\nIn fact, one can investigate the pulsations in the spectra of sdBs, whose nature is not yet fully understood. Through the discovery of pulsations in three sdBs, we have a unique opportunity to further explore their internal structures. We present observations of three pulsators: HS 0039+4302 and HS 0444+0458, and examine the collective properties of this group of pulsators.\n\nUsing high-dispersion spectroscopy, we have identified pulsations in these three pulsators and are investigating their collective properties. These pulsators fall into two distinct categories: those with multimodal pulsation spectra and those with double-peaked spectra. The first category exhibits complex spectra that tend to be multi-periodic, while the second category consists of single-mode pulsators with stable amplitudes. These frequencies and intensities are consistent for each pulsator, with variations in peak separation, as well as differing pulsation amplitudes and phases.\n\nWe propose that these pulsators represent distinct phases within the same evolutionary pathway. The sdBs with multimodal pulsation spectra are candidates for being in the final phases of merger, close to the emergence of electron degeneracy. This is the first report of single-mode pulsation levels for three sdBs. These pulsators likely represent separate evolutionary states of a common progenitor.\n\nOur findings include data for two distinct types of pulsators within the sdB population. This addition of three new pulsators to the small but growing census of sdB pulsators provides an important opportunity to study the internal structures of sdBs and the evolution of their progenitors.",
        "ori-fast-z-score": -0.936585811581694,
        "water-fast-z-score": 5.315277406969789,
        "rewrite-fast-z-score": -0.1796053020267749
    },
    {
        "original_text": "A Branch and Cut algorithm for the Halfspace Depth problem is proposed. The problem is known to be strongly NP-hard. The proposed algorithm first generates a set of bounding boxes for the input space. Then, it solves a series of strongly reduced instances by a branch and cut approach. In order to speed-up the solving process, the intermediate solutions are cached. In order to guarantee the optimality of the found solution, a cutting plane based approach is used. The performance of the proposed algorithm is evaluated on two large scale aircraft landing problem instances from the CUT willingness to pay dataset. The obtained results are compared to the best-known solutions obtained from the CUT automated optimisation tool as well as two existing cutting plane based algorithms.  Reference: Z. Du, Z. Hu, W. Yang, and J. Cheng. A Branch and Cut Algorithm for the Halfspace Depth Problem. arXiv:1901.02347, Jan. 2019.",
        "watermark_text": "A Branch and Cut algorithm for the Halfspace Depth problem is proposed . The question is known to be highly NP - hard .The proposed algorithm first produces a group of bounding boxes for the input space . Then , it solves a sequence of highly reduced instances by a branch and cut approach .In order to faster - up the solving process , the intermediate solutions are cached . In try to ensure the optimality of the finding solution , a cutting plane based methods is utilized .The effectiveness of the suggested method is evaluated on two huge scale aircraft landing problem cases from the CUT willingness to pay dataset . The achieved findings are compared to the best - recognized solutions obtained from the CUT adaptive optimisation tool as well as two existing cuts plane based algorithms .Reference : Z . Du , Z . Hu , W . Yang , and J . Cheng .A Branch and Cut Algorithm for the Halfspace Depth Problem.arXiv:1901.02347, Jan. 2019.",
        "rewrite_text": "A proposed Branch and Cut algorithm has been introduced to tackle the Halfspace Depth problem, which is widely recognized as a highly NP-hard problem. The algorithm initially generates a set of bounding boxes for the input space. Subsequently, it employs a branch and cut approach to solve a sequence of significantly simplified instances. To expedite the solution process, intermediate solutions are cached for future use. To ensure the optimality of the solutions found, a cutting plane-based method is incorporated.\n\nThe effectiveness of this suggested approach has been evaluated using two large-scale aircraft landing problem cases from the CUT willingness to pay dataset. The obtained findings are compared with the best-recognized solutions sourced from the CUT adaptive optimization tool and two existing cutting plane algorithms.\n\nReference: Z. Du, Z. Hu, W. Yang, and J. Cheng. \"A Branch and Cut Algorithm for the Halfspace Depth Problem.\" arXiv:1901.02347, January 2019.",
        "ori-fast-z-score": -1.212678125181665,
        "water-fast-z-score": 4.520269441183293,
        "rewrite-fast-z-score": 0.7171371656006361
    },
    {
        "original_text": "A quasar outflow, perhaps powered by a radio galaxy analogue, is identified in the narrow-line region of z = 0.1723 quasar SDSS J154141.33+155426.6. The outflow has a kinematic width of at least 1000 km s-1, a dynamical mass of at least 1042 M⊙, and is moving at high velocity (v = -0.999995 c, relative to the quasar frame). The width, dynamical mass, and velocity are all similar to, but slightly lower than, those of broad absorption line outflows. The narrow-line region gas is asymmetrically distributed around the quasar, with one side closer to the quasar than the other. The closer side is coincident with the radio galaxy analogue, suggesting that the quasar wind has been partially turned around by the AGN radiation pressure, and is interacting with the radio galaxy host. The density of the outflowing gas, calculated from the X-ray absorption, is high enough ( approximately 1011 cm-3) to affect the spectrum of the quasar, providing a natural explanation for the unusually blue UV/optical/near-infrared colours of the quasar, as well as the detection of the MgII absorber in previous radio galaxy observations.",
        "watermark_text": "A quasar outflow , perhaps powered by a radio galaxy analogue , is identified in the narrow - line zone of z = 0 . 1723 quasar SDSS J154141 . 33 + 155426 . 6 . The outflow has a kinematic length of at least 1000 km s - 1 , a dynamical mass of at least 1042 [UNK] , and is moving at high velocity ( v = - 0 . 999995 c , relative to the quasar frame ) .The width , dynamical mass , and speed are all identical to , but little lower than , those of broad absorption line outflows . The narrow - line zone gas is asymmetrically spread around the quasar , with one side nearer to the quasar than the other .The closer side is coincident with the radio galaxy analogue , showing that the quasar wind has been partially pushed around by the AGN radiation stress , and is interacting with the radio galaxy host . The density of the outflowing gas , estimated from the X - ray absorption , is high enough ( approximately 1011 cm - 3 ) to affect the spectrum of the quasar , providing a natural explanation for the unusually blue UV / optical / near - infrared colours of the quasar , as also as the observation of the MgII absorber in earlier radio galaxy surveys .",
        "rewrite_text": "A quasar outflow, possibly powered by a radio galaxy analog, has been identified in the narrow-line region of SDSS J154141.33+155426.6 at z = 0.1723. This outflow exhibits a kinematic length of at least 1000 km s⁻¹ and a dynamical mass of at least 10⁴² units, moving rapidly with a velocity of v = -0.999995c relative to the quasar frame. Its width, mass, and speed are comparable to, albeit slightly lower than, those of broad absorption line outflows.\n\nThe gas in the narrow-line region is asymmetrically distributed around the quasar, with one side being closer to the quasar than the other. The closer side aligns with the radio galaxy analog, indicating that the quasar wind has been partially redirected by the stress of AGN radiation and is interacting with the host of the radio galaxy. The density of the outflowing gas, estimated from X-ray absorption, is sufficiently high (approximately 10¹¹ cm⁻³) to influence the quasar's spectrum, offering a natural explanation for the quasar's unusually blue UV/optical/near-infrared colors, as well as the observation of the MgII absorber in previous radio galaxy surveys.",
        "ori-fast-z-score": -0.4472135954999579,
        "water-fast-z-score": 2.81271975231506,
        "rewrite-fast-z-score": 0.45291081365783825
    },
    {
        "original_text": "The paper studies the invariance properties of the linear parabolic equation in n spatial dimensions x =A(x) under the transformation of the independent variables x and the solutions x, where A is a matrix whose eigenvalues are all zero. We establish the complete invariance under the Galilean transformation and the scaling transformation, in the case n≥2, and the Galilean invariance in the case n=1. We also establish the invariance under the transformation of the form x→λ(x), where the eigenvector corresponding to the zero eigenvalue of A satisfies λ(x1,…,xn)=0, and the invariance under the transformation xT−x for all vectors x. As an application of the results, we prove the existence of conservation laws for this equation. Keywords: Invarianace, Conservation laws, Parabolic equations Date: June 15, 2019 Author: Takuya Tsuji URL: https://arxiv.org/abs/1906.05029 Dependencies: None NEWS: Version 1.0 (June 15, 2019) Version 1.1 (June 22, 2019) - Corrected the scaling invariance under xT−x Version 1.2 (July 5, 2019) - Added invariance under x→λ(x) Version 1.3 (July 11, 2019) - Revised the invariance under the scaling transformation for n=1 Version 1.4 (July 17, 2019) - Corrected invariance under the transformation xT−x Version 1.5 (August 8, 2019) - Revised the invariance under x→λ(x) Version 1.6 (August 15, 2019) - Corrected the invariance under the transformation xT−x Version 1.7 (August 24, 2019) - Corrected the scaling invariance Version 1.8 (September 9, 2019) - Revised the invariance under the transformation xT−x Version 1.9 (September 16, 2019) - Added invariance under the transformation x→λ(x) Version 1.10 (October 14, 2019) - Corrected invariance under the transformation xT−x Version 1.11 (October 28, 2019) - Corrected invariance under the transformation x→λ(x) Version 1.12 (November 4, 2019) - Corrected scaling invariance Version 1.13 (November 19, 2019) - Revised the invariance under the transformation xT−x Version 1.14 (December 16, 2019) - Corrected the invariance under the transformation xT−x Version 1.15 (January 14, 2020) - Cor",
        "watermark_text": "The paper studies the invariance characteristics of the linear parabolic equation in n spatial dimensions x = A ( x ) under the transformation of the independent vectors x and the solutions x , where A is a matrix whose eigenvalues are all zero . We establish the complete invariance under the Galilean change and the scaling conversion , in the case n≥2 , and the Galilean invariance in the case n = 1 .We additionally establish the invariance under the transformation of the form x→λ ( x ) , where the eigenvector corresponding to the zero eigenvalue of A satisfies λ ( x1 , … , xn ) = 0 , and the invariance under the transformation xT−x for all vectors x . As an application of the results , we prove the existence of protection laws for this equation .Keywords : Invarianace , Conservation laws , Parabolic equations Date : June 15 , 2019 Author : Takuya Tsuji URL : https : / / arxiv . org / abs / 1906 . 05029 Dependencies : None NEWS : Version 1 . 0 ( June 15 , 2019 ) Version 1 . 1 ( June 22 , 2019 ) - Corrected the scaling invariance under xT−x Version 1 . 2 ( July 5 , 2019 ) - Added invariance under x→λ ( x ) Version 1 . 3 ( July 11 , 2019 ) - Revised the invariance under the scaling transformation for n = 1 Version 1 . 4 ( July 17 , 2019 ) - Corrected invariance under the transformation xT−x Version 1 . 5 ( August 8 , 2019 ) - Revised the invariance under x→λ ( x ) Version 1 . 6 ( August 15 , 2019 ) - Corrected the invariance under the transformation xT−x Version 1 . 7 ( August 24 , 2019 ) - Corrected the scaling invariance Version 1 . 8 ( September 9 , 2019 ) - Revised the invariance under the transformation xT−x Version 1 . 9 ( September 16 , 2019 ) - Added invariance under the transformation x→λ ( x ) Version 1 . 10 ( October 14 , 2019 ) - Corrected invariance under the transformation xT−x Version 1 . 11 ( October 28 , 2019 ) - Corrected invariance under the transformation x→λ ( x ) Version 1 . 12 ( November 4 , 2019 ) - Corrected scaling invariance Version 1 . 13 ( November 19 , 2019 ) - Revised the invariance under the transformation xT−x Version 1 . 14 ( December 16 , 2019 ) - Corrected the invariance under the transformation xT−x Version 1 . 15 ( January 14 , 2020 ) - Cor",
        "rewrite_text": "The study delves into the invariance properties of the linear parabolic equation in n spatial dimensions, given by x = A(x), where x represents the independent vectors and solutions, and A is a matrix with all zero eigenvalues. We establish complete invariance under the Galilean transformation and scaling conversion when n≥2, and Galilean invariance for the case of n = 1. Additionally, we establish invariance under the transformation x→λ(x), where the eigenvector corresponding to A's zero eigenvalue satisfies λ(x1,…,xn) = 0, and invariance is also observed under the transformation xT−x for all vectors x. As an application of our findings, we prove the existence of protection laws for this equation.\n\nKeywords: Invariance, Conservation Laws, Parabolic Equations\nDate: June 15th, 2019\nAuthor: Takuya Tsuji\nURL: https://arxiv.org/abs/1906.05029\n\nDependencies: None\n\nNEWS:\nVersion History:\n\n1.0 (June 15, 2019) - Initial Release\n1.1 (June 22, 2019) - Corrected scaling invariance under xT−x\n1.2 (July 5, 2019) - Added invariance under x→λ(x)\n1.3 (July 11, 2019) - Revised the invariance under scaling transformation for n = 1\nSubsequent versions (e.g., 1.4 to 1.15) - Continuous revisions and corrections to the invariance properties as mentioned in the text.",
        "ori-fast-z-score": 1.8382900600361156,
        "water-fast-z-score": 3.4139672543527864,
        "rewrite-fast-z-score": 1.386750490563073
    },
    {
        "original_text": "In this overview, we survey energy-efficient resource allocation problems in wireless networks. We categorize the solutions to these problems into two main categories: algorithms that optimize a single network objective, such as energy consumption or sum-throughput, and algorithms that optimize multiple objectives, such as energy consumption and fairness. We survey game-theoretic approaches to solve these resource allocation problems. A game is a mathematical model of conflict between rational agents that each prefer a different outcome. Game-theoretic solutions for resource allocation in wireless networks assign algorithmic agents (transmitters or receivers) to matchmaking roles that compete or collaborate to settle on a solution. We categorize game-theoretic approaches to wireless network resource allocation into two classes: 1) best-response algorithms that involve only searching through the agent’s own playbook for a best response; and 2) non-trivial algorithms that leverage the underlying game to improve the search for a best response. We survey approximation and martingale solutions to the multi-agent resource allocation problems. In particular, we survey Lagrangian relaxation, sub-gradient methods, and fictitious play. Finally, we conclude our overview with a survey of emerging research areas for future directions in this field of study.”",
        "watermark_text": "In this outline , we study energy - efficient energy management problems in wireless networks . We categorize the answers to these problems into two principal categories : methods that optimize a single network goal , such as energy consumption or sum - throughput , and algorithms that optimize multiple goals , such as energy consumption and fairness .We evaluate game - theoretic approaches to solve these resource sharing questions . A game is a mathematical description of conflict between logical agents that each prefer a distinct outcome .Game - theoretic systems for resource transfer in wireless networks assign algorithmic agents ( transmitters or receivers ) to matchmaking positions that compete or collaborate to settle on a solution . We categorize game - theoretic approaches to telecommunications system resource allocation into two groups : 1 ) best - response methods that involve only searching through the agent ’ s own playbook for a greatest response ; and 2 ) non - trivial algorithms that leverage the underlying game to reduce the search for a greatest response .We survey approximation and martingale solutions to the multi - agent resource transfer problems . In particular , we study Lagrangian relaxation , sub - gradient models , and fictitious play .Finally , we conclude our overview with a survey of new study fields for future directions in this area of study . ”",
        "rewrite_text": "In this outline, we examine energy-efficient energy management challenges within wireless networks. We classify solutions to these issues into two primary categories: methods that optimize a singular network objective, such as energy consumption or aggregate throughput, and algorithms that optimize multiple objectives, such as energy consumption and fairness. We assess game-theoretic approaches to address these resource-sharing questions. A game mathematically represents a conflict among logical agents, each with a unique desired outcome. Game-theoretic systems for resource allocation in wireless networks assign algorithmic agents (e.g., transmitters or receivers) to matchmaking positions that compete or collaborate to find a solution. \n\nWe categorize game-theoretic methods for telecommunications system resource allocation into two groups: 1) best-response strategies that search exclusively within an agent's playbook for the most favorable response; and 2) sophisticated algorithms that leverage the underlying game mechanics to narrow the search for optimal responses. \n\nAdditionally, we explore approximate and martingale solutions to multi-agent resource transfer problems. Specifically, we investigate Lagrangian relaxation, sub-gradient models, and fictitious play techniques. Finally, we summarize our overview with an examination of emerging research fields that hold potential for future directions in this area of study.",
        "ori-fast-z-score": -2.463323195410733,
        "water-fast-z-score": 4.118438837901865,
        "rewrite-fast-z-score": 1.643452031377628
    },
    {
        "original_text": "Aerosol optical depth (AOD), SO2, NO2, PM2.5, PM10, temperature, and relative humidity were measured at a urban site in Greater Tokyo, Japan (36.5N 137.5E) from January 2014 to December 2017. High-pollution episodes (HPE) were defined as days with PM2.5 concentrations of over 300 μg/m3. HPEs accounted for 42% of the total days and were dominant during the warm seasons. HPEs had high AOD, high SO2, high NO2, and low wind speeds. Diagnostics of the dominated aerosol processes revealed that 61% of HPEs were related to secondary aerosol formation. 24% of HPEs were related to re-suspension, 11% to regional transport, and 5% to gas-phase chemical reactions. These processes were more active in winter, indicating the importance of wintertime transport in greater Tokyo. Analysis of the emission inventories showed that fossil fuel combustion and industrial processes were the primary sources of SO2, NO2, and PM2.5. Estimated secondary aerosol formation contributed 44% of SO2, 61% of NO2, and 73% of PM2.5. These processes, along with the high temperature and low wind speeds during HPEs, were major contributors to the high AOD. The dominant aerosol processes were different from what was observed in previous studies, which were mainly related to regional transport. This may be because of the different emission patterns and climate conditions in Japan, compared to North America or Europe.",
        "watermark_text": "Aerosol optical height ( AOD ) , SO2 , NO2 , PM2 . 5 , PM10 , temperature , and absolute moisture were calculated at a urban site in Greater Tokyo , Japan ( 36 . 5N 137 . 5E ) from January 2014 to December 2017 . High - contamination episodes ( HPE ) were specified as days with PM2 . 5 levels of over 300 μg / m3 .HPEs accounted for 42 % of the total days and were strong during the warm seasons . HPEs had high AOD , low SO2 , low NO2 , and low wind speeds .Diagnostics of the dominated aerosol processes revealed that 61 % of HPEs were linked to secondary aerosol formation . 24 % of HPEs were linked to re - suspension , 11 % to regional transport , and 5 % to gas - phase organic reactions .These systems were more active in snow , showing the importance of wintertime travel in greater Tokyo . Analysis of the emission inventories showed that fossil gasoline burning and commercial processes were the primary sources of SO2 , NO2 , and PM2 . 5 .Estimated secondary aerosol formation contributed 44 % of SO2 , 61 % of NO2 , and 73 % of PM2 . 5 . These mechanisms , along with the high heat and low wind speeds during HPEs , were major contributors to the high AOD .The dominant aerosol processes were different from what was seen in earlier surveys , which were mainly related to regional transport . This might be because of the different pollution patterns and weather conditions in Japan , compared to North America or Europe .",
        "rewrite_text": "Between January 2014 and December 2017, various environmental parameters such as aerosol optical depth (AOD), SO2, NO2, PM2.5, PM10, temperature, and absolute moisture were computed at an urban site located in the Greater Tokyo area (36.5°N, 137.5°E). High-contamination episodes (HPEs) were defined as days with PM2.5 concentrations exceeding 300 μg/m3. These HPEs constituted 42% of the total days and were most intense during the warmer seasons. During HPEs, AOD levels were high, SO2 and NO2 levels were low, and wind speeds were low.\n\nDiagnostic assessments of the prevailing aerosol processes indicated that 61% of HPEs were linked to secondary aerosol formation. Additionally, 24% were attributed to re-suspension, 11% to regional transport, and 5% to gas-phase organic reactions. These systems were particularly active during snowy conditions, highlighting the significance of winter travel in the Greater Tokyo area.\n\nAn analysis of emission inventories revealed that burning of fossil gasoline and commercial processes were the primary sources of SO2, NO2, and PM2.5. It was estimated that secondary aerosol formation contributed 44% to SO2, 61% to NO2, and 73% to PM2.5. These mechanisms, along with high temperatures and low wind speeds during HPEs, were major contributors to the high AOD levels.\n\nNotably, the dominant aerosol processes differed from earlier surveys, which were primarily associated with regional transport. This might be attributed to the distinct pollution patterns and weather conditions in Japan compared to North America or Europe.",
        "ori-fast-z-score": 0.10846522890932808,
        "water-fast-z-score": 6.9829724875517565,
        "rewrite-fast-z-score": 2.626396615835748
    },
    {
        "original_text": "Blazars are the most powerful non-active galaxies with their relativistic jets closely aligned to our line of sight. They are classified in two categories based on the presence or absence of high-energy emission of whose particle accelerators is unknown. Observational data indicates that the emission is most likely originated near the black hole event horizon, although not all models are able to producing high-energy gamma-rays. Emission mechanisms in blazars are uncertain. A popular theory is the Blandford-Znajek process in which the magnetized central black hole powers the jet by extracting spin energy of the black hole. The original proposal had intended application to weak stellar magnetic fields, however subsequent studies suggest that the fields in blazars can be several orders of magnitude stronger. Another popular model is synchrotron-self-Compton (SSC) in which synchrotron radiation up-scatters part of its own photons to high energy gamma-rays. Blazars often exhibit variability at all wavelengths on very short timescales, ranging from hours to days. Blazars are powerful emitters of radio waves, optical and gamma-rays and this electromagnetic radiation is highly variable. In this work we show that blazar jets are closely related to the inner regions of their associated accretion disks. Jets appear to be launched from tiny scale inner regions of the accretion disks and strong correlation is found between the magnitude of the disk scale height and the jet power. Such a correlation is expected in the Blandford-Znajek model and fits well the available data. The article is aimed at a broad interdisciplinary audience, including specialists in plasma physics, black hole physics, relativistic astrophysics and nuclear physics.",
        "watermark_text": "Blazars are the most intense non - active galaxies with their relativistic jets closely aligned to our line of vision . They are classified in two genres according on the presence or lack of high - energy emission of whose particle accelerators is unknown .Observational data indicates that the emission is most likely originated near the dark hole event horizon , although not all models are able to producing high - energy gamma - rays . Emission patterns in blazars are unsure .A popular hypothesis is the Blandford - Znajek mechanism in which the magnetized central black hole powers the jet by extracting spin power of the dark hole . The original suggestion had intended apply to weak stellar magnetic fields , however recent studies confirm that the fields in blazars can be several orders of magnitude higher .Another famous model is synchrotron - self - Compton ( SSC ) in which synchrotron emission up - scatters part of its own photons to large energy gamma - radiation . Blazars usually display variability at all wavelengths on very brief timescales , ranging from hours to days .Blazars are powerful emitters of radio beams , optical and alpha - radiation and this electromagnetic radiation is strongly varied . In this research we find that blazar jets are tightly related to the inner regions of their associated accretion disks .Jets tend to be launched from small scale inner regions of the accretion disks and strong correlation is found between the magnitude of the disk scale length and the jet capacity . Such a correlation is expected in the Blandford - Znajek simulation and fits well the provided information .The paper is aiming at a broad interdisciplinary audience , particularly specialists in particle science , white hole physics , relativistic astrophysics and nuclear science .",
        "rewrite_text": "Blazars are the most intense non-dormant galaxies, with their relativistic jets aligned closely to our line of sight. They are categorized into two genres based on the presence or absence of high-energy emission, where the origin of particle accelerators remains unknown. Observational data suggests that the emission likely originates near the event horizon of the black hole, though not all models can produce high-energy gamma rays. The emission patterns in blazars remain uncertain. A popular hypothesis is the Blandford-Znajek mechanism, in which the central magnetized black hole powers the jet by extracting the spin energy of the dark hole. Originally intended for weak stellar magnetic fields, recent studies confirm that the magnetic fields in blazars can be significantly stronger.\n\nAnother notable model is the synchrotron-self-Compton (SSC) process, where synchrotron emission up-scatters a portion of its own photons into high-energy gamma radiation. Blazars typically exhibit variability across all wavelengths on extremely short timescales, ranging from hours to days. These galaxies are powerful emitters of radio beams, optical, and alpha radiation, with significant electromagnetic radiation fluctuations.\n\nIn this research, we discover a strong connection between blazar jets and the inner regions of their associated accretion disks. Jets tend to be launched from small-scale inner portions of these disks, and a significant correlation is found between the magnitude of the disk scale length and jet capacity. This correlation aligns with expectations from the Blandford-Znajek simulation and fits well with the provided information.\n\nThe paper targets a broad interdisciplinary audience, particularly specialists in particle science, white hole physics, relativistic astrophysics, and nuclear science.",
        "ori-fast-z-score": -0.43033148291193524,
        "water-fast-z-score": 7.091168559049116,
        "rewrite-fast-z-score": 2.553769592276246
    },
    {
        "original_text": "In this paper, we show that superconducting correlations can be induced in a ferromagnetic layer of a superconductor-ferromagnet (S/F) structure due to the proximity effect. When a magnetic field is applied to such a S/F structure, nonhomogeneous magnetization occurs inside the ferromagnetic layer. This gives rise to a Doppler shift of the ferromagnetic resonance (FMR) signal and a superconducting current in the superconductor. We find that the triplet component of the induced superconducting correlations exhibits a local maximum near the FMR resonance frequency and the magnetic field at which the nonhomogeneous magnetization occurs. These results indicate that it should be possible to detect FMR in S/F structures by using the techniques of superconducting spintronics. The full paper  Nonhomogeneous magnetization and superconductivity in superconductor-ferromagnet structures  can be found at https://arxiv.org/abs/1709.07446",
        "watermark_text": "In this paper , we find that superconducting correlations can be induced in a ferromagnetic layer of a superconductor - ferromagnet ( S / F ) configuration owing to the proximity phenomenon . When a magnetic force is applied to such a S / F structure , nonhomogeneous magnetization occurs inside the ferromagnetic layer .This gives rise to a Doppler shift of the ferromagnetic resonance ( FMR ) wave and a superconducting current in the superconductor . We see that the triplet constituent of the induced superconducting correlations experiences a local maximum near the FMR resonance frequency and the magnetic force at which the nonhomogeneous magnetization occurs .These data indicate that it should be possible to identify FMR in S / F structures by using the methods of superconducting spintronics . The full paper Nonhomogeneous magnetization and superconductivity in superconductor - ferromagnet structures can be found at https : / / arxiv . org / abs / 1709 . 07446",
        "rewrite_text": "In this study, we discovered that superconducting correlations can be triggered in a ferromagnetic layer of a superconductor-ferromagnet (S/F) configuration due to the proximity effect. When a magnetic force is applied to this S/F structure, it results in the occurrence of nonhomogeneous magnetization within the ferromagnetic layer. This, in turn, leads to a Doppler shift in the ferromagnetic resonance (FMR) wave and a superconducting current in the superconductor. We observed that the triplet component of the induced superconducting correlations experiences a local maximum close to the FMR resonance frequency and the magnetic force responsible for the nonhomogeneous magnetization. These findings suggest that FMR in S/F structures can be identified using techniques from superconducting spintronics. The complete paper on nonhomogeneous magnetization and superconductivity in superconductor-ferromagnet structures is available at https://arxiv.org/abs/1709.07446.",
        "ori-fast-z-score": 0.30151134457776363,
        "water-fast-z-score": 4.422458539645535,
        "rewrite-fast-z-score": 1.3416407864998738
    },
    {
        "original_text": "Single crystals of PbZr0.52Ti0.48O3 (PZT) undergo a phase transition from a higher to a lower symmetry phase at around 600 °C. This structural phase transition is accompanied by a large change in electrical conductivity. It has been shown that the conductivity change can be explained by a change in carrier concentration caused by thermal excitation of deep trapping states. It has further been suggested that in order to describe the change in conductivity correctly, the distribution of activation energies of the deep trapping states has to be considered. The traditional method to obtain this distribution is the quasiequilibrium approximation (QEA), which is an assumption about the thermal evolution of the carrier concentration and requires a full description of the carrier diffusion. Here, we propose a method that relaxes the quasiequilibrium assumption and solves the coupled heat diffusion and drift-diffusion equations with the finite-element method. The obtained conductivity-temperature-relaxation-time-distribution function is in agreement with the quasiequilibrium approximation results for lattice temperatures below around 700 °C. For higher temperatures, however, the difference between the two approaches is considerable and increases with temperature. We argue that this is due to an approximation inherent to the quasiequilibrium approximation and suggest a different approximation that yields results in good agreement with our new method.",
        "watermark_text": "Single crystals of PbZr0 . 52Ti0 . 48O3 ( PZT ) undergo a phase shift from a higher to a reduced symmetry phase at around 600 °C . This structural phase shift is accompanied by a large change in electrical conductivity .It has been shown that the conductivity change can be described by a change in carrier concentration caused by thermal excitation of deep trap states . It has further been proposed that in order to explain the shift in conductivity correctly , the distribution of activation energies of the deep trapping states has to be understood .The conventional approach to obtain this distribution is the quasiequilibrium approximation ( QEA ) , which is an assumption about the thermal evolution of the carrier concentration and requires a complete characterization of the carrier diffusion . Here , we undertake a technique that relaxes the quasiequilibrium assumption and solves the coupled thermal diffusion and drift - diffusion coefficients with the finite - element method .The derived conductivity - temperature - relaxation - time - distribution relation is in agreement with the quasiequilibrium approximation results for lattice pressures below around 700 °C . For larger temperatures , however , the difference between the two approaches is substantial and increases with temperature .We argue that this is due to an approximation inherent to the quasiequilibrium approximation and suggest a better approximation that yields results in good agreement with our new method .",
        "rewrite_text": "The phase transition of PbZr0.52Ti0.48O3 (PZT) single crystals occurs at approximately 600°C, transitioning from a higher to a lower symmetry phase. This structural phase transition is accompanied by a significant change in electrical conductivity. Studies have demonstrated that this conductivity variation can be attributed to a change in carrier concentration resulting from thermal excitation of deep trap states. To accurately explain the conductivity shift, it is further proposed that the distribution of activation energies for the deep trapping states must be understood.\n\nThe traditional method to obtain this distribution relies on the quasiequilibrium approximation (QEA), which assumes certain conditions on the thermal evolution of carrier concentration and necessitates a comprehensive characterization of carrier diffusion. In our approach, we relax the quasiequilibrium assumption and solve the coupled thermal diffusion and drift-diffusion coefficients using the finite-element method. The derived relationship between conductivity, temperature, and relaxation time distribution aligns with QEA results for lattice pressures below approximately 700°C. However, for higher temperatures, the divergence between the two approaches becomes significant and increases with temperature. We attribute this to an inherent approximation in the quasiequilibrium approximation and suggest a more accurate approximation that yields results in good agreement with our novel method.",
        "ori-fast-z-score": 0.6965260331469925,
        "water-fast-z-score": 5.47270454615494,
        "rewrite-fast-z-score": 2.7724131203346882
    },
    {
        "original_text": "A robust developmental program is essential for multicellular organisms to carry out their functions, such as formation of body plans and limbs, and to adapt to environmental change. Here we develop a theoretical and experimental framework to investigate multicellular developmental robustness, based on an epigenetic model for gene regulation. The model encompasses, for the first time, the role of cell division in developmental robustness, and our theory and experiments reveal that developmental robustness can arise from a compromise between developmental tempo and program, and between robustness to variability in cell type composition. Using this framework, we identify non-mutually exclusive avenues to improve developmental robustness. For example, increases in developmental tempo can enhance robustness; and alternative strategies to increase robustness may be to (i) reduce the influence of noise in gene expression, or (ii) reduce heterogeneity in cell composition. These results establish a theoretical foundation for multicellular developmental robustness, and we propose that our theory should enable further identification and implementation of strategies to improve robustness in synthetic systems and in regenerative medicine.",
        "watermark_text": "A vigorous developmental program is crucial for multicellular organisms to carry out their tasks , such as development of bodies plans and limbs , and to adapt to environmental change . Here we develop a conceptual and experimental framework to examine multicellular evolutionary robustness , relying on an epigenetic model for gene control .The model encompasses , for the first time , the importance of cell division in developmental robustness , and our theory and experiments discover that developmental robustness can arise from a compromise between developmental pace and program , and between robustness to variability in cell type composition . Using this framework , we identify non - mutually exclusive avenues to promote developmental robustness .For instance , improvements in developmental pace can increase robustness ; and additional strategies to raise robustness might be to ( i ) lower the impact of noise in gene transcription , or ( ii ) lower heterogeneity in cell composition . These data establish a conceptual foundation for multicellular developmental robustness , and we propose that our theory should enable further identification and implementation of methods to promote robustness in artificial systems and in regenerative drugs .",
        "rewrite_text": "For multicellular organisms to effectively carry out their tasks like body plan and limb development, as well as adapting to environmental changes, a robust developmental program is absolutely essential. We've established a conceptual and experimental framework to explore the evolutionary resilience of multicellular organisms, leveraging an epigenetic model for gene control. This model uniquely highlights the significance of cell division in developmental resilience. Through our theoretical and empirical findings, we've discovered that developmental robustness can emerge from a balance between the pace and program of development, as well as the resilience to variations in cell type composition.\n\nUsing this framework, we've identified non-exclusive approaches to enhancing developmental robustness. For instance, improving the developmental pace can enhance resilience, and additional strategies could include (i) minimizing the impact of genetic transcription noise, or (ii) reducing cellular composition heterogeneity. These data provide a conceptual foundation for understanding multicellular developmental resilience, and we propose that our theory can facilitate further identification and implementation of methods to promote resilience in artificial systems and regenerative medicines.",
        "ori-fast-z-score": -0.8626621856275073,
        "water-fast-z-score": 6.399448505650358,
        "rewrite-fast-z-score": 1.4924050144892729
    },
    {
        "original_text": "In this paper we study the problem of finding rational points on elliptic surfaces. Elliptic surfaces are surfaces of the form y2 = x(x^2 +Ax +B), where A and B are constants, and they can be viewed as minimal models for certain elliptic curves. Finding rational points on elliptic surfaces of this form would give integer solutions to the elliptic curve given by the following Weierstrass equation: y2 = x^3 + Ax2 + B x + C. This curve has been of interest in the study of QEC, and a proof of its integer points was announced in 2013 by H. Davenport, C. Lewis, and Y. Tang, but this result has not appeared in print to date. In this paper we announce a proof of this result, and we make available our manuscript in its entirety. Rational points on certain elliptic surfaces. In this paper we study the problem of finding rational points on elliptic surfaces. Elliptic surfaces are surfaces of the form y2 = x(x^2 +Ax +B), where A and B are constants, and they can be viewed as minimal models for certain elliptic curves. Finding rational points on elliptic surfaces of this form would give integer solutions to the elliptic curve given by the following Weierstrass equation: y2 = x^3 + Ax2 + B x + C. This curve has been of interest in the study of QEC, and a proof of its integer points was announced in 2013 by H. Davenport, C. Lewis, and Y. Tang, but this result has not appeared in print to date. In this paper we announce a proof of this result, and we make available our manuscript in its entirety. In this paper we study the problem of finding rational points on elliptic surfaces. Elliptic surfaces are surfaces of the form y2 = x(x^2 +Ax +B), where A and B are constants, and they can be viewed as minimal models for certain elliptic curves. Finding rational points on elliptic surfaces of this form would give integer solutions to the elliptic curve given by the following Weierstrass equation: y2 = x^3 + Ax2 + B x + C. This curve has been of interest in the study of QEC, and a proof of its integer points was announced in 2013 by H. Davenport, C. Lewis, and Y. Tang, but this result has not appeared in print to date. In this paper we announce a proof of this result, and we make available our manuscript in its entirety. This curve has been of interest in the study of QEC, and a proof of its integer points was announced in 2013 by H. Davenport, C. Lewis, and Y. Tang, but this result has not appeared in print to date. In this paper we announce a proof of this result, and we make available our manuscript in its entirety.",
        "watermark_text": "In this paper we study the question of finding rational points on elliptic curves . Elliptic curves are surfaces of the form y2 = x ( x ^ 2 + Ax + B ) , where A and B are constants , and they can be viewed as minimal models for particular elliptic curves .Finding rational points on elliptic curves of this form would give integer solutions to the elliptic curve given by the following Weierstrass equation : y2 = x ^ 3 + Ax2 + B x + C . This curve has been of interest in the study of QEC , and a proof of its integer points was announced in 2013 by H . Davenport , C . Lewis , and Y . Tang , but this result has not arrived in print to date . In this paper we publish a proof of this result , and we give available our manuscript in its entirety .Rational points on particular elliptic curves . In this paper we study the question of finding rational points on elliptic surfaces .Elliptic curves are surfaces of the form y2 = x ( x ^ 2 + Ax + B ) , where A and B are constants , and they can be viewed as minimal models for particular elliptic curves . Finding rational points on elliptic curves of this form would obtain integer solutions to the elliptic curve presented by the following Weierstrass equation : y2 = x ^ 3 + Ax2 + B x + C . This curve has been of interest in the study of QEC , and a proof of its integer points was announced in 2013 by H . Davenport , C . Lewis , and Y . Tang , but this result has not published in print to date .In this paper we publish a proof of this fact , and we give available our manuscript in its entirety . In this paper we study the question of finding rational points on elliptic surfaces .Elliptic curves are surfaces of the form y2 = x ( x ^ 2 + Ax + B ) , where A and B are constants , and they can be viewed as minimal models for particular elliptic curves . Finding rational points on elliptic curves of this form would obtain integer solutions to the elliptic curve presented by the following Weierstrass equation : y2 = x ^ 3 + Ax2 + B x + C . This curve has been of interest in the study of QEC , and a proof of its integer points was announced in 2013 by H . Davenport , C . Lewis , and Y . Tang , but this result has not published in print to date .In this paper we publish a proof of this result , and we give available our manuscript in its entirety . This curve has been of interest in the study of QEC , and a proof of its integer points was announced in 2013 by H . Davenport , C . Lewis , and Y . Tang , but this result has not seen in print to date .In this paper we publish a proof of this result , and we give available our manuscript in its entirety .",
        "rewrite_text": "In this study, we explore the quest to find rational points on elliptic curves. These curves are surfaces defined by the equation y2 = x(x^2 + Ax + B), where A and B are constants, and they can be considered as simplified models for specific elliptic curves. Locating rational points on these curves would yield integer solutions for the elliptic curve represented by the Weierstrass equation: y2 = x^3 + Ax2 + Bx + C. This curve has been a focal point in the study of Quadratic Elliptic Curves (QEC). Although a proof of its integer points was announced in 2013 by H. Davenport, C. Lewis, and Y. Tang, the result has yet to be published. In this paper, we present a proof of this finding and make our manuscript publicly accessible in its entirety.\n\nWe also investigate the issue of finding rational points on elliptic surfaces, which follow a similar mathematical structure. These surfaces can be seen as minimal representations of particular elliptic curves. Discovering rational points on these surfaces would provide integer solutions for the aforementioned elliptic curve equation. This curve has been a significant subject of interest in QEC research, and the pending proof of its integer points was initially announced in 2013 by the aforementioned researchers. However, this breakthrough has yet to be printed and made widely available. In this paper, we provide a thorough proof of this result and make our manuscript readily available to all.\n\nMoreover, the curve under investigation has been a key focus in the field of QEC. A proof of its integer points was announced in 2013 by H. Davenport, C. Lewis, and Y. Tang, but this significant discovery remains unseen in printed form until now. We are proud to share that in this paper, we have published a proof of this result and made our manuscript fully accessible to all interested researchers.",
        "ori-fast-z-score": 0.6285393610547089,
        "water-fast-z-score": 6.068450128041075,
        "rewrite-fast-z-score": 1.9295276424754644
    },
    {
        "original_text": "We present numerical results for scalar field perturbations from a particle orbiting a rotating black hole. We evolve the perturbation equations using numerical methods presented in previous work. We evolve the perturbation equations in 2+1 dimensions in Misner-sharp variables, using an ingoing-null foliation in place of a tortoise-like radial coordinate. We obtain quasinormal modes (QNMs) of various discrete values of angular momentum, $l$, and charge, $q$, for Schwarzschild black holes and for Kerr black holes with different spin parameters. The frequencies and damping times of QNMs are tabulated. We also discuss the computation of energy and angular momentum radiated in gravitational waves, using a new integral definition of radiated quantities for perturbations of any spin. Finally, we apply our results for scalar field perturbations to an estimate of energy and angular momentum radiated in scalar waves from a particle in circular orbit around a Kerr black hole. We find that the emitted energy and angular momentum are both negligible compared with the mass and spin of the hole, respectively.",
        "watermark_text": "We present numerical findings for scalar field perturbations from a particle orbiting a rotating black hole . We evolve the perturbation equations using numerical models provided in earlier papers .We evolve the perturbation equations in 2 + 1 dimensions in Misner - sharp variables , using an ingoing - null foliation in place of a tortoise - like radial coordinate . We get quasinormal modes ( QNMs ) of several discrete values of angular velocity , $ r $ , and charge , $ q $ , for Schwarzschild grey holes and for Kerr white holes with various spin parameters .The wavelength and damping times of QNMs are tabulated . We additionally discuss the computation of power and angular velocity radiated in gravity signals , using a new integral formulation of radiated quantities for perturbations of any spin .Finally , we apply our findings for scalar field perturbations to an measure of power and angular energy radiated in scalar beams from a particle in circular orbit around a Kerr black hole . We see that the emitted energy and angular energy are both negligible compared with the mass and spin of the hole , respectively .",
        "rewrite_text": "We present numerical results for scalar field perturbations arising from a particle orbiting a rotating black hole. We utilize numerical models detailed in previous studies to evolve the perturbation equations. These equations are progressed in 2 + 1 dimensions, employing Misner-sharp variables, with an ingoing-null foliation replacing a tortoise-like radial coordinate system. We have obtained quasinormal modes (QNMs) with various discrete values of angular velocity, 'r', and charge, 'q', for both Schwarzschild gray holes and Kerr white holes with distinct spin parameters. We have compiled a table detailing the wavelengths and damping times of these QNMs. Additionally, we discuss the computation of the power and angular velocity emitted in gravitational signals, utilizing a novel integral formulation for radiated quantities applicable to perturbations of any spin.\n\nFinally, our findings on scalar field perturbations are applied to measure the power and angular energy radiated in scalar beams by a particle in a circular orbit around a Kerr black hole. Our observations indicate that the emitted energy and angular energy are both minimal in comparison to the mass and spin of the black hole, respectively.",
        "ori-fast-z-score": -2.618614682831909,
        "water-fast-z-score": 3.1454916383705145,
        "rewrite-fast-z-score": -0.20851441405707477
    },
    {
        "original_text": "In experiments with ultracold atoms, quantum mechanics allows for effects that would be reversible in classical physics, such as quantum backaction or measurement backaction. We observe quantum-measurement backaction with an ultracold atomic gas. Using state-selection spin echos, we detect atoms that experienced a unitary transform associated with the measurement of an operator that has no mean value in the system’s initial state. We perform a quantum process tomography of this unitary transform and show that our measurements reproduce the dynamics predicted by quantum mechanics. Our results establish measurement backaction in an essentially pure system and withdB/dt ≥ |h|/τ, where dB is the change in operator expectation values and τ is the characteristic evolution time. These results could enable new approaches to quantum control and precision measurement, with potential applications to quantum information and ultra-low light-noise measurements. We observe quantum-measurement backaction with an ultracold atomic gas. In experiments with ultracold atoms, quantum mechanics allows for effects that would be reversible in classical physics, such as quantum backaction or measurement backaction. We perform a quantum process tomography of this unitary transform and show that our measurements reproduce the dynamics predicted by quantum mechanics. In general, quantum backaction occurs when a quantum system responds to a measurement. Quantum measurement theory describes the statistical relationships between quantum systems and measurements, and the act of measuring an observable affects the state of the system. Measurement backaction occurs when the measured observable has no mean value in the system’s initial state, and it is defined as the rate of change of the expected value of that observable. It can have a variety of forms. For example, if an operator is the observable of a quantum system, the measured system can become entangled with a meter system in a process called quantum entanglement, and backaction corresponds to how the measured system and meter system evolve together in time. Alternatively, if the operator is a parameter of a unitary transform, backaction could enable control and measurement of the parameters of unitary transforms, for example in quantum process tomography. Thus far, measurement backaction has only been observed in the presence of quantum entanglement or incoherent forces (e.g., gravity or the spin-force associated with magnetic fields). We demonstrate measurement backaction without entanglement or additional incoherent forces. Instead, we observe measurement backaction in an essentially pure system, without loss due to interactions with a large environment. The dynamics are reversible because our experiment operates near the quantum limit of few-atom systems. Our results establish measurement backaction in an essentially pure system and with dB/dt ≥ |h|/τ, where dB is the change in operator expectation values and τ is the characteristic evolution time. These results could enable new approaches to quantum control and precision measurement, with potential applications to quantum information and ultra-low light-noise measurements.",
        "watermark_text": "In studies with ultracold atoms , quantum mechanics allows for changes that would be reversible in classical physics , such as particle backaction or measurement backaction . We see quantum - measurement backaction with an ultracold atomic gas .Using state - selection spin echos , we locate atoms that underwent a unitary transform attributed with the observation of an operator that has no mean value in the system ’ s initial state . We undergo a quantum process tomography of this unitary transform and find that our measurements mimic the dynamics anticipated by quantum mechanics .Our results establish detection backaction in an basically simple system and withdB / dt ≥ | h | / τ , where dB is the shift in operator expectation values and τ is the typical evolution period . These results could enable new approaches to quantum control and accuracy detection , with potential applications to quantum information and ultra - low light - noise measurements .We experience particle - measurement backaction with an ultracold atomic gas . In studies with ultracold atoms , quantum mechanics allows for changes that would be reversible in classical physics , such as particle backaction or measurement backaction .We undergo a quantum process tomography of this unitary transform and tell that our measurements mimic the dynamics anticipated by quantum mechanics . In general , quantum backaction happens when a quantum system replies to a measurement .Quantum measurement theory involves the statistical relationships between quantum systems and measurements , and the act of monitoring an observable affects the state of the system . Measurement backaction happens when the measured observable has no mean value in the system ’ s initial state , and it is calculated as the rate of change of the expected value of that observable .It can have a variety of forms . For instance , if an operator is the observable of a quantum system , the measured system can develop entangled with a meter system in a process called quantum entanglement , and backaction corresponds to how the measured system and meter system develop together in time .Alternatively , if the operator is a parameter of a unitary transform , backaction could enable control and measurement of the variables of unitary transforms , for example in quantum process tomography . Thus far , measurement backaction has only been observed in the presence of quantum entanglement or incoherent forces ( e . g . , gravity or the spin - force associated with magnetic fields ) .We experience measurement backaction without entanglement or additional incoherent forces . Instead , we witness measurement backaction in an basically simple system , without losing resulting to interactions with a large environment .The dynamics are reversible because our experiment runs near the quantum limit of few - atom systems . Our results establish measurement backaction in an basically simple system and with dB / dt ≥ | h | / τ , where dB is the shift in operator expectation values and λ is the typical evolution period .These data could enable new approaches to quantum control and accuracy observation , with potential applications to quantum information and ultra - low light - noise measurements .",
        "rewrite_text": "In research with ultracold atoms, quantum mechanics permits transformations that are reversible in classical physics, such as particle and measurement backaction. We have observed quantum-measurement backaction in an ultracold atomic gas. By utilizing state-selected spin echoes, we locate atoms that have undergone a unitary transformation associated with the observation of an operator lacking a mean value in the system's initial state. We perform a quantum process tomography on this unitary transformation and find that our measurements mimic the dynamics predicted by quantum mechanics.\n\nOur findings establish the presence of measurement backaction in a fundamentally simple system, with a sensitivity of dB/dt ≥ |h|/τ, where dB represents the shift in operator expectations and τ is the typical evolution period. These results could pave the way for new approaches in quantum control and precision detection, with potential applications in quantum information and ultra-low light noise measurements.\n\nFurthermore, we experience particle-measurement backaction with an ultracold atomic gas. Quantum mechanics allows for changes, such as particle or measurement backaction, that are reversible in classical physics when studying ultracold atoms. We subject this unitary transformation to a quantum process tomography and confirm that our measurements resemble the dynamics anticipated by quantum mechanics.\n\nIn general, quantum backaction occurs when a quantum system responds to a measurement. Quantum measurement theory involves statistical relationships between quantum systems and measurements, and the act of monitoring an observable affects the state of the system. Measurement backaction occurs when the measured observable has no initial mean value in the system's state, and it is calculated as the rate of change in the expected value of that observable. It can manifest in various forms.\n\nFor instance, if an operator is the observable of a quantum system, the measured system can become entangled with a meter system through a process called quantum entanglement, and backaction reflects how the measured system and meter system evolve together in time. Alternatively, if the operator is a parameter of a unitary transform, backaction can enable control and measurement of the variables involved in unitary transforms, such as in quantum process tomography.\n\nUntil now, measurement backaction has only been observed in the presence of quantum entanglement or incoherent forces (e.g., gravity or spin-force associated with magnetic fields). However, in our experiment, we observe measurement backaction without entanglement or additional incoherent forces. Instead, we witness measurement backaction in a fundamentally simple system without interference from a larger environment. The dynamics are reversible because our experiment operates close to the quantum limit of small-atom systems. Our results establish the presence of measurement backaction in a straightforward system and with the specified sensitivity, potentially enabling new approaches to quantum control and observation with applications in quantum information and ultra-low light noise measurements.",
        "ori-fast-z-score": 0.8999540851465151,
        "water-fast-z-score": 7.554175916040863,
        "rewrite-fast-z-score": 2.1712405933672376
    },
    {
        "original_text": "Millisecond pulsars (MSPs) are fast-spinning, strongly magnetized neutron stars. Binary MSPs are routinely detected in close orbits about their companion stars, and some of these systems become radio loud due to interaction between the pulsar and its companion star in a process known as accretion disk ablation. The very similar solitary MSPs are much less frequently detected, and have thus far been found in about half of transient gamma-ray bursts (GRBs). These systems, lacking companion stars, are expected to be radio quiet. Here we report the detection of PSR J1453+1902, a solitary MSP previously known only at 1400 MHz, for which we obtain a 1.4 GHz pulsed radio upper limit that is more than an order of magnitude deeper than the average of other solitary MSPs. We use the Large Sky Area Multi-band Transit Satellite (LSAM). We compare the upper limit on the radio luminosity of PSR J1453+1902 with the distribution of radio luminosities for the much more commonly observed binary MSPs and find it to be a very high-luminosity outlier. We therefore suggest that PSR J1453+1902 is the first solitary MSP found in a transient GRB.",
        "watermark_text": "Millisecond pulsars ( MSPs ) are fast - spinning , heavily magnetized neutron stars . Binary MSPs are routinely detected in close orbits about their companion stars , and some of these systems become radio loud due to contact between the pulsar and its companion star in a process known as accretion disk ablation .The very related solitary MSPs are much less frequently observed , and have thus far been seen in about half of transient gamma - ray bursts ( GRBs ) . These systems , lacking companion stars , are expected to be radio calm .Here we publish the discovery of PSR J1453 + 1902 , a solitary MSP previously known only at 1400 MHz , for which we obtain a 1 . 4 GHz pulsed radio upper maximum that is more than an order of magnitude deeper than the average of other solitary MSPs . We use the Large Sky Area Multi - band Transit Satellite ( LSAM ) .We compare the higher limit on the television luminosity of PSR J1453 + 1902 with the distribution of radio luminosities for the considerably more frequently observed binary MSPs and find it to be a very high - luminosity outlier . We consequently suggest that PSR J1453 + 1902 is the first solitary MSP found in a transient GRB .",
        "rewrite_text": "Millisecond pulsars (MSPs) are rapidly rotating neutron stars that are highly magnetized. Binary MSPs are frequently detected in close orbits around their companion stars, and some of these systems become radio-loud due to a process known as accretion disk ablation, which occurs when the pulsar makes contact with its companion star. In contrast, solitary MSPs, which are closely related, are observed much less frequently and have been detected in approximately half of the transient gamma-ray bursts (GRBs). These systems, lacking companion stars, are expected to be radio-quiet.\n\nIn this study, we announce the discovery of PSR J1453+1902, a solitary MSP that was previously only known at 1400 MHz. We have obtained a maximum pulsed radio upper limit at 1.4 GHz that is over an order of magnitude deeper than the average for other solitary MSPs. We utilize the Large Sky Area Multi-band Transit Satellite (LSAM) for this investigation.\n\nWe compare the upper limit on the television luminosity of PSR J1453+1902 to the distribution of radio luminosities observed in the more frequently detected binary MSPs and find it to be an outlier with exceptionally high luminosity. Therefore, we propose that PSR J1453+1902 is the first solitary MSP discovered in a transient GRB event.",
        "ori-fast-z-score": 1.0392304845413263,
        "water-fast-z-score": 4.129483209670111,
        "rewrite-fast-z-score": 0.329292779969071
    },
    {
        "original_text": "Galaxy clusters are the largest known structures in the universe, containing hundreds or even thousands of galaxies. Despite their importance, it is not easy to study the large-scale behaviour of cluster galaxies. Most of the time one has to resort to studying individual galaxies or small samples. A promising approach is to model the overall galaxy light distribution and use the resulting model to fit the light of distant galaxies. Such an approach was proposed by Falco et al. in 1987 and implemented on large datasets by Gonzalez et al. in 2002. Since then, several improvements and adaptations to different galaxy samples have been proposed. We present the dataset of early-type galaxies in the WINGS clusters, a multi-wavelength catalogue with morphological classification, HST imaging and extensive follow-up data. The dataset has been modelled and fitted using three different techniques: non-parametric SFH modelling, bulge+disk decomposition and semi-parametric Boxy4 modelling. The results indicate that both star formation history and bulge prominence have varied during the lifetime of clusters.",
        "watermark_text": "Galaxy clusters are the huge known structures in the universe , holding hundreds or maybe thousands of galaxies . Despite their importance , it is not straightforward to study the huge - scale behaviour of cluster clusters .Most of the time one has to resort to observing individual galaxies or small samples . A good approach is to model the overall galaxy light density and use the resulting model to fit the light of distant galaxies .Such an approach was suggested by Falco et al . in 1987 and adopted on huge datasets by Gonzalez et al .in 2002 . Since then , various changes and adaptations to different universe samples have been proposed .We present the dataset of early - class objects in the WINGS clusters , a multi - wavelength collection with morphological classification , HST detection and abundant follow - up information . The dataset has been modelled and fit use three different methods : non - parametric SFH modelling , bulge + disk transformation and semi - parametric Boxy4 analysis .The results suggests that both star formation history and bulge prominence have differed during the life of clusters .",
        "rewrite_text": "Galaxy clusters constitute the immense structural frameworks in the universe, encompassing hundreds or possibly thousands of galaxies. Despite their significance, comprehending the large-scale dynamics of cluster assemblies is not a straightforward task. Most often, researchers are compelled to observe individual galaxies or small samples. A promising approach involves modeling the overall density of galaxy light and utilizing this model to analyze the light emitted by distant galaxies. This method was proposed by Falco et al. in 1987, and subsequently employed on extensive datasets by Gonzalez et al. in 2002. Since then, various modifications and adaptations have been proposed for different cosmic samples. We present a dataset of early-class objects from the WINGS clusters, a multi-wavelength collection featuring morphological classifications, HST detections, and abundant follow-up information. This dataset has been subjected to modeling and fitting using three distinct methods: non-parametric SFH modeling, bulge+disk transformation, and semi-parametric Boxy4 analysis. The results indicate that both the star formation history and the prominence of bulges have varied throughout the lifespan of these clusters.",
        "ori-fast-z-score": -2.400396792595916,
        "water-fast-z-score": 4.2301439274637955,
        "rewrite-fast-z-score": 0.9761870601839528
    },
    {
        "original_text": "A large number of astronomical objects can be identified via their infrared (IR) emission, including YSOs and galaxies, which are often heated by massive stars. As a result, the IR spectrum of the sky encodes information about the natal star-formation regions from which these objects arise. These regions, in turn, dictate the stellar Initial Mass Function and so the number of stars which can form in a given environment. Masers, in particular, may be of special interest to those studying the early stages of star-formation, as their characteristics (bright-energetic transitions and precise positional counterparts) make them excellent probes of very young regions. I review the IR environment of 22 water and ammonia masers, associated with the early stages of high-mass star-formation, detected in the course of recent, sensitive IR surveys with the Spitzer Space Telescope. I discuss what can be learnt about the natal regions from the spectral energy distributions constructed from the data and suggest how such studies could be extended with future IR facilities.",
        "watermark_text": "A wide number of astronomical bodies can be identified via their infrared ( IR ) emission , particularly YSOs and galaxies , which are often heated by massive galaxies . As a result , the IR spectrum of the sky encodes data about the natal star - formation regions from which these objects emerge .These zones , in turn , dictate the stellar Initial Mass Function and so the quantity of stars which can form in a given setting . Masers , in particular , might be of important concern to those studying the early stages of star - formation , as their characteristics ( bright - energetic cycles and precise positional counterparts ) help them ideal probes of very young regions .I examine the IR climate of 22 sea and ammonia masers , associated with the early stages of high - mass star - formation , detected in the course of recent , sensitive IR studies with the Spitzer Space Telescope . I discuss what can be learnt about the natal regions from the spectral power distributions built from the information and suggest how such studies could be enhanced with current IR equipment .",
        "rewrite_text": "Numerous astronomical objects can be identified through their infrared (IR) emissions, especially young stellar objects (YSOs) and galaxies that are often warmed by massive galaxies. The IR spectrum of the sky encodes vital information about the natal star formation regions where these objects originate. These regions, in turn, determine the Initial Mass Function of stars and consequently the number of stars that can form in a given environment.\n\nIn particular, masers are of great interest to researchers studying the early stages of star formation. Their characteristics, such as bright and energetic cycles and precise positional counterparts, make them ideal probes for very young regions. I have examined the IR environment of 22 sea and ammonia masers associated with the early stages of high-mass star formation, detected during recent sensitive IR studies conducted with the Spitzer Space Telescope.\n\nI have discussed what can be learned about the natal regions from the spectral power distributions derived from this information and suggested ways to improve such studies with current IR technology.",
        "ori-fast-z-score": 0.22645540682891913,
        "water-fast-z-score": 5.661385170722978,
        "rewrite-fast-z-score": 2.111111111111111
    },
    {
        "original_text": "In this paper, we generalize the well-known Caldeira-Legget quantum Brownian motion model to an environment with an arbitrary spectral density and an external force. The spectral density and the external force are both assumed to be general functions of the frequency. By making use of the generating functional technique, we obtain exact solutions to the corresponding master equations in both the Markovian and non-Markovian limits. The influences of the general environment and the external force on the quantum Brownian particle are discussed in detail. Especially, we show that the environment with a constant spectral density and an external force can be treated as a special case of our general model. The influence of the environment with such a special spectral density and external force on the quantum Brownian particle has been investigated in many former literatures, which is therefore contained in our general solution as a special case. We also give a short discussion on the decoherence functional of our model.",
        "watermark_text": "In this paper , we generalize the good - famous Caldeira - Legget quantum Brownian movement model to an environment with an arbitrary spectral density and an external force . The spectral density and the external force are both assumed to be general functions of the frequency .By making use of the generating functional technique , we obtain exact solutions to the associated master equations in both the Markovian and non - Markovian bounds . The influences of the general environment and the external force on the quantum Brownian quantum are discussed in detail .Especially , we prove that the landscape with a constant spectral density and an external force can be treated as a general case of our general model . The impact of the surroundings with such a special spectral density and external force on the quantum Brownian particle has been investigated in many former literatures , which is consequently contained in our general solution as a general case .We also make a brief discussion on the decoherence functional of our model .",
        "rewrite_text": "In this study, we have extended the renowned Caldeira-Legget quantum Brownian motion model to an environment characterized by arbitrary spectral density and an external force. We assume that both the spectral density and the external force are general functions of frequency. Utilizing the generating functional technique, we have derived exact solutions for the associated master equations within both Markovian and non-Markovian frameworks.\n\nThe effects of the general environment and the external force on the quantum Brownian motion are thoroughly examined. Specifically, we demonstrate that a scenario with constant spectral density and an external force can be regarded as a special case of our generalized model. The impact of environments with this specific spectral density and external force on the quantum Brownian particle has been extensively studied in previous literature, and is therefore encompassed as a particular case in our general solution. Additionally, we briefly discuss the decoherence functionality of our model.",
        "ori-fast-z-score": -2.1652509527331207,
        "water-fast-z-score": 0.9058216273156765,
        "rewrite-fast-z-score": -2.789943329851663
    },
    {
        "original_text": "Optimizing scrip systems have been the focus of both praise and criticism in the cryptocurrency community for several years. Scrip systems are plugins for cryptocurrency wallets that allow users to make incremental payments to pay people, services, and machines. Many view scrip systems as a fundamental missing piece to cryptocurrency, enabling practical applications which are difficult or impossible without them. Scrip systems can automate the task of paying people for things they produce, like content, labor, or data. They can also make it easier to manage large payments or payments to multiple recipients, which can be a challenge with cryptocurrency. Critics, however, have warned that scrip systems can be vulnerable to crashes, can cause erroneous payments, and can create opportunities for abusive or unethical behavior by virtue of their design. In this work, I compare three commonly used scrip systems: Bitwage, Libera, and Wagepoint, and describe several ways in which they could be improved. First, I describe design decisions behind each scrip system that I believe could be reconsidered. Second, I discuss research into alternative designs for scrip systems that I believe could improve security and reliability. Third, I describe ways in which scrip systems could be designed to reduce the opportunity for abusive or abusive behavior. I conclude by describing some ways cryptocurrency projects could adopt and fund this work, through development partnerships or sponsorships.",
        "watermark_text": "Optimizing scrip systems have been the subject of both praise and praise in the cryptocurrency world for multiple years . Scrip systems are plugins for cryptocurrency wallets that enable users to make incremental payments to pay persons , services , and computers .Many perceive scrip systems as a basic missing piece to cryptocurrency , allowing practical applications which are challenging or impossible without them . Scrip systems can automate the process of paying individuals for things they produce , like content , labor , or data .They can also make it better to manage huge payments or payments to multiple recipients , which can be a problem with cryptocurrency . Critics , however , have suggested that scrip systems can be vulnerable to crashes , can cause erroneous payments , and can create opportunities for corrupt or unethical behavior by virtue of their design .In this research , I compare three widely using scrip systems : Bitwage , Libera , and Wagepoint , and describe several ways in which they may be improved . First , I outline design decisions behind each scrip system that I believe might be reconsidered .Second , I discuss studies into unconventional designs for scrip systems that I believe might improve security and reliability . Third , I outline ways in which scrip systems could be designed to reduce the opportunity for abusive or abusive behavior .I end by describing some manner cryptocurrency projects might adopt and fund this project , through project partnerships or sponsorships .",
        "rewrite_text": "For several years, the optimization of scrip systems in the cryptocurrency realm has garnered both accolades and praise. Scrip systems, which are plugins for cryptocurrency wallets, have enabled users to make incremental payments to individuals, services, and computers. Many view scrip systems as an essential component missing from cryptocurrencies, enabling practical applications that can be challenging or impossible without them. These systems can automate the process of remitting payments for goods produced by individuals, such as content, labor, or data. Additionally, they facilitate easier management of large payments or payments to multiple recipients, which can be a challenge with cryptocurrencies.\n\nHowever, critics have expressed concerns that scrip systems can be prone to crashes, lead to erroneous payments, and present opportunities for corrupt or unethical behavior due to their design. In this research, I compare three widely-used scrip systems: Bitwage, Libera, and Wagepoint, and explore several potential improvements.\n\nFirstly, I outline the design decisions behind each scrip system that I believe warrant reconsideration. Secondly, I discuss studies on unconventional designs for scrip systems that I believe could enhance security and reliability. Thirdly, I propose ways in which scrip systems can be designed to mitigate the potential for abusive or malicious behavior.\n\nFinally, I conclude by discussing various ways that cryptocurrency projects could adopt and fund this project, through partnerships or sponsorships.",
        "ori-fast-z-score": 0.8783100656536799,
        "water-fast-z-score": 8.158801243801019,
        "rewrite-fast-z-score": 3.4
    },
    {
        "original_text": "A search for the radiative leptonic decay B+ --> gamma l+ nu was performed using a data sample of 672 fb-1 of pp collisions at s∽8 TeV collected with the ATLAS detector at the LHC at the CERN laboratory in Geneva, Switzerland. No evidence of this rare decay was found, and limits on the branching ratio were set. These limits range from approximately 7.1×10-8 at minimum photon energy of 1.6 GeV to 3.7×10-7 at minimum photon energy of 100 MeV, depending on the assumed branching fraction to a specific final state. These are the most stringent to date. This research was presented in the paper: B. Tramontano, et al., Search for the Radiative Leptonic Decay B+ --> gamma l+ nu, arXiv:2004.08629  hep-ex  — Abstract — A search for the radiative leptonic decay B+ --> gamma l+ nu was performed using a data sample of 672 fb-1 of pp collisions at s∽8 TeV collected with the ATLAS detector at the LHC at the CERN laboratory in Geneva, Switzerland. No evidence of this rare decay was found, and limits on the branching ratio were set. These limits range from approximately 7.1×10-8 at minimum photon energy of 1.6 GeV to 3.7×10-7 at minimum photon energy of 100 MeV, depending on the assumed branching fraction to a specific final state. These are the most stringent to date. The B+ meson was created at the LHC and transported through the ATLAS detector, resulting in a data sample of (BLΛ U /Ts)×10^9 (GeV)^2. This sample was used to search for B+ --> gamma l+ nu, a radiative leptonic decay in which a B+ meson decays into a winos and a photon, with the latter carrying 50% of the total momentum of the B+ meson. The analysis focuses on the dimuon decay mode of the winos, resulting in a final state with two oppositely charged muons, at least two photons, and missing momentum from the decaying B+ meson. Two regions of the dimuon invariant mass are considered: a low mass region between 2.9 and 3.1 GeV/$c^2$ and high mass region between 3.1 and 3.4 GeV/$c^2$. A simultaneous b-flavor-symmetric signal and generic charm continuum background is generated using LHCb distributions in two orthogonal polarization directions. To model the signal, photons from the radiative lepton decay of B+ mesons are produced using a photon PDF obtained from simulation. The signal and background predictions are embedded into MCevent samples and used to determine the expected sensitivity. No evidence of the B+ --> gamma l+ nu decay is observed and upper limits on the branching ratio are",
        "watermark_text": "A search for the radiative leptonic emission B + - - > gamma l + nu was done utilizing a data specimen of 672 fb - 1 of pp collisions at [UNK] TeV gathered with the ATLAS detector at the LHC at the CERN laboratory in Geneva , Switzerland . No trace of this uncommon decay was uncovered , and limits on the branching ratio were determined .These restrictions range from approximately 7 . 1×10 - 8 at minimum photon energy of 1 . 6 GeV to 3 . 7×10 - 7 at minimum photon energy of 100 MeV , depending on the expected branching fraction to a certain end state . These are the most stringent to date .This research was presented in the paper : B . Tramontano , et al . , Search for the Radiative Leptonic Decay B + - - > gamma l + nu , arXiv : 2004 . 08629 hep - ex — Abstract — A search for the radiative leptonic decay B + - - > gamma l + nu was done utilizing a data specimen of 672 fb - 1 of pp collisions at [UNK] TeV compiled with the ATLAS detector at the LHC at the CERN laboratory in Geneva , Switzerland . No evidence of this uncommon decay was uncovered , and limits on the branching ratio were setting .These restrictions range from approximately 7 . 1×10 - 8 at minimum photon energy of 1 . 6 GeV to 3 . 7×10 - 7 at minimum photon energy of 100 MeV , depending on the expected branching fraction to a certain end state . These are the most stringent to date .The B + meson was formed at the LHC and shipped through the ATLAS detector , resulting in a data specimen of ( BLΛ U / Ts ) ×10 ^ 9 ( GeV ) ^ 2 . This specimen was used to search for B + - - > gamma l + nu , a radiative leptonic decay in which a B + meson decays into a winos and a photon , with the former transferring 50 % of the total velocity of the B + meson .The calculation focuses on the dimuon degradation mode of the winos , resulting in a final state with two oppositely charged muons , at least two photons , and lost velocity from the decaying B + meson . Two regions of the dimuon invariant mass are considered : a high mass region between 2 . 9 and 3 . 1 GeV / $ c ^ 2 $ and low mass region between 3 . 1 and 3 . 4 GeV / $ c ^ 2 $ .A simultaneous b - flavor - symmetric noise and generic charm continuum background is generated utilizing LHCb distributions in two orthogonal polarization directions . To model the signal , photons from the radiative lepton decay of B + mesons are produced utilizing a photon PDF obtained from modeling .The signal and background predictions are incorporated into MCevent specimens and utilized to estimate the expected sensitivity . No evidence of the B + - - > gamma l + nu decay is observed and upper limits on the branching ratio are",
        "rewrite_text": "A comprehensive study on the radiative leptonic emission B+ → γl+ν was conducted using a dataset of 672 fb-1 of pp collisions at [UNK] TeV, collected with the ATLAS detector at the LHC at CERN's laboratory in Geneva, Switzerland. No traces of this uncommon decay were detected, and limits on the branching ratio were determined. These restrictions range from approximately 7.1×10-8 at a minimum photon energy of 1.6 GeV to 3.7×10-7 at a minimum photon energy of 100 MeV, depending on the expected branching fraction to a specific final state. These are the most stringent limits established so far.\n\nThe research was presented in a paper by B. Tramontano and his colleagues. The paper details a search for the B+ → γl+ν decay utilizing the ATLAS detector's compiled data of pp collisions at the LHC. The B+ meson was produced at the LHC and passed through the ATLAS detector, resulting in a dataset of (BLΛU/Ts) x 10^9 (GeV)^2. This dataset was employed to investigate the B+ → γl+ν radiative leptonic decay, where a B+ meson decays into a wino and a photon, with the wino transferring 50% of the B+ meson's total velocity.\n\nThe analysis focused on the dimuon degradation mode of the wino, resulting in a final state comprising two oppositely charged muons, at least two photons, and a loss of velocity from the decaying B+ meson. Two regions of the dimuon invariant mass were considered: a high mass region spanning from 2.9 to 3.1 GeV/c^2 and a low mass region ranging from 3.1 to 3.4 GeV/c^2. A simultaneous b-flavor-symmetric noise and generic charm continuum background were generated using LHCb distributions in two orthogonal polarization directions.\n\nTo model the signal, photons from the radiative lepton decay of B+ mesons were produced using a photon PDF obtained from modeling. Both the signal and background predictions were integrated into MCevent specimens and utilized to estimate the expected sensitivity. No evidence of the B+ → γl+ν decay was observed, and upper limits on the branching ratio were determined.",
        "ori-fast-z-score": -0.24096579867074966,
        "water-fast-z-score": 6.6667204298907405,
        "rewrite-fast-z-score": 2.475085941976171
    },
    {
        "original_text": "Recent discoveries of exoplanets have opened a new chapter in the field of astronomy. With the upcoming transit missions such as TESS and PLATO, the number of known planets is expected to increase by several folds in the coming decade. Most of the known exoplanets are smaller and more dense than our Earth. When we look at the data from Kepler, it appeared that the surface of most planets are covered with water. However, when we observe the same planets with more accuracy, we find that the existence of oceans on those planets is uncertain. As the technology to detect those planets improves, the tendency of astronomers is to characterize the planet in terms of its atmosphere, not its surface. This implies that in the future, we might only identify the existence of life, not the existence of surface water. In other words, we could be oblivious to the existence of most planets. Dynamical research is a key enabler in the detection and characterization of exoplanets, and this field will significantly benefit from state-of-the-art technologies in computing, data analysis, and system identification.",
        "watermark_text": "Recent discoveries of exoplanets have opened a new chapter in the field of science . With the latest transit missions such as TESS and PLATO , the number of known planets is expected to expand by many folds in the come decade .Most of the known exoplanets are smaller and more thick than our Earth . When we glance at the information from Kepler , it seemed that the surface of most planets are filled with sea .However , when we study the same worlds with more precision , we find that the existence of oceans on those worlds is uncertain . As the technology to locate those worlds improves , the tendency of astronomers is to characterize the planet in terms of its climate , not its surface .This implies that in the future , we may only identify the existence of life , not the existence of surface water . In other words , we may be unaware to the existence of most planets .Dynamical research is a key enabler in the discovery and identification of exoplanets , and this area will significantly benefit from state - of - the - art technologies in computing , database analysis , and system identification .",
        "rewrite_text": "The recent exploration of exoplanets has ushered in a new era in science. With advanced transit missions like TESS and PLATO, the number of identified planets is anticipated to proliferate exponentially in the upcoming decade. The majority of the known exoplanets are smaller and denser than our own Earth. A glance at data from Kepler suggests that many of these planets are seemingly covered in seas. However, upon closer examination, the presence of vast oceans on these planets remains uncertain. As technology improves in locating these exoplanets, astronomers tend to characterize them based on their climate rather than just their surface features. This suggests that in the future, we may only detect the existence of life, rather than identifying surface water. In other words, we may be unaware of the existence of many planets even though they exist.\n\nDynamical research plays a pivotal role in the discovery and identification of exoplanets, and this field will greatly benefit from cutting-edge technologies in computing, database analysis, and system identification. This technology advancement will enhance our understanding of exoplanets and open new avenues for scientific exploration.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 6.1942248145051675,
        "rewrite-fast-z-score": 0.8728715609439696
    },
    {
        "original_text": "In this paper, we study a one-dimensional quantum dot in the presence of strong attractive contact interactions, which induce a local deformation of the Wigner crystal ground state. Using quantum Monte Carlo and density matrix renormalization group techniques, we determine the ground state energy as a function of system size and deformation amplitude, finding that the ground state is a stable configuration for any amplitude below a critical value. We characterize this deformation via a local density profile, from which we compute local charge and bond-order parameters, as well as identify a one-to-one correspondence between these parameters and the underlying deformation. We also determine the effects of quantum fluctuations on this deformed state using path integral Monte Carlo and compare our results to large system size quantum Monte Carlo data, finding excellent agreement in the regime of stability. Finally, we discuss the experimental implications of our results and propose directions for future work. In this work, we study a one-dimensional quantum dot in the presence of strong attractive contact interactions, which induce a local deformation of the Wigner crystal ground state. Using quantum Monte Carlo and density matrix renormalization group techniques, we determine the ground state energy as a function of system size and deformation amplitude, finding that the ground state is a stable configuration for any amplitude below a critical value. We characterize this deformation via a local density profile, from which we compute local charge and bond-order parameters, as well as identify a one-to-one correspondence between these parameters and the underlying deformation. We also determine the effects of quantum fluctuations on this deformed state using path integral Monte Carlo and compare our results to large system size quantum Monte Carlo data, finding excellent agreement in the regime of stability. Finally, we discuss the experimental implications of our results and propose directions for future work.",
        "watermark_text": "In this paper , we study a one - dimensional quantum dot in the presence of stable attractive contact interactions , which induce a local deformation of the Wigner crystal ground state . Using quantum Monte Carlo and density matrix renormalization group method , we determine the ground state energy as a function of system height and deformation frequency , finding that the ground state is a consistent configuration for any amplitude below a critical value .We characterize this deformation via a local density profile , from which we compute local charge and bond - order variables , as well as identify a one - to - one correspondence between these parameters and the underlying deformation . We additionally determine the effects of quantum fluctuations on this deformed state using path integral Monte Carlo and compare our findings to large network size quantum Monte Carlo information , finding high agreement in the regime of stability .Finally , we explain the empirical implications of our findings and suggest directions for future research . In this research , we study a one - dimensional quantum dot in the presence of strong attractive interaction interactions , which induce a local deformation of the Wigner crystal ground state .Using quantum Monte Carlo and density matrix renormalization group method , we determine the ground state energy as a function of system height and deformation frequency , finding that the ground state is a consistent configuration for any amplitude below a critical value . We characterize this deformation via a local density profile , from which we compute local charge and bond - order variables , as well as identify a one - to - one correspondence between these parameters and the underlying deformation .We additionally determine the effects of quantum fluctuations on this deformed state using path integral Monte Carlo and compare our findings to large network size quantum Monte Carlo information , finding high agreement in the regime of stability . Finally , we explain the theoretical implications of our findings and suggest directions for future research .",
        "rewrite_text": "In this research, we explore a one-dimensional quantum dot amidst stable attractive contact interactions. These interactions induce a local deformation of the Wigner crystal's ground state. By employing the quantum Monte Carlo and density matrix renormalization group methods, we ascertain the ground state energy as a function of system height and deformation frequency. Our findings reveal that the ground state remains consistent for any amplitude beneath a critical threshold.\n\nWe characterize this deformation through a local density profile, enabling us to compute local charge and bond-order variables. A one-to-one correlation between these parameters and the underlying deformation is clearly identified. Furthermore, we assess the impact of quantum fluctuations on this deformed state using path integral Monte Carlo. When compared to information from large-network quantum Monte Carlo simulations, we observe a high degree of agreement in terms of stability.\n\nIn terms of empirical implications, we offer an explanation of our findings and propose future research directions. Additionally, we delve into the theoretical ramifications of our study, emphasizing the significance of strong attractive interactions in shaping the Wigner crystal ground state and its associated deformations. We hope that these insights will pave the way for future investigations and advancements in the field.",
        "ori-fast-z-score": -0.8006407690254357,
        "water-fast-z-score": 5.091566618724672,
        "rewrite-fast-z-score": 0.4
    },
    {
        "original_text": "Recent advances in computer technology and algorithm development allow molecular dynamics (MD) simulations of strongly coupled dynamical systems over extended periods of time. In order to monitor the energy and momentum conservation over such long time periods, switchings of various force fields become necessary. In this work we present a general algorithm for the energy and momentum conservation in MD simulations using force fields with switching potentials. The general case is considered. The switching potentials are obtained from analytical solutions of the corresponding equations of motion. The general solution contains only parameters which have to be fixed by the initial conditions. Therefore, the proposed method is easy to apply and inexpensive in computational terms. We illustrate the efficiency of the approach on the example of the Lennard-Jones (LJ) potential. Applying the proposed algorithm, we obtain the results of the energy and momentum conservation over 600000 time steps of the LJ system with a relatively low cost, which is ~27% of the cost for the same time step with the conventional algorithm.",
        "watermark_text": "Recent developments in computer technology and algorithm development allow molecular mechanics ( MD ) simulations of highly coupled dynamical systems over longer periods of time . In order to study the power and momentum conservation over such prolonged period periods , switchings of several force fields become essential .In this research we present a general algorithm for the power and momentum conservation in MD simulations using force fields with switching potentials . The general case is studied .The switching potentials are derived from analytical solutions of the resulting equations of movement . The general solution comprises only variables which have to be fixed by the first terms .Therefore , the suggested method is easy to apply and inexpensive in computational terms . We illustrate the performance of the approach on the example of the Lennard - Jones ( LJ ) potential .Applying the suggested method , we obtain the results of the power and momentum conservation over 600000 time steps of the LJ process with a fairly little cost , which is ~ 27 % of the cost for the same time phase with the standard method .",
        "rewrite_text": "In recent advancements of computer technology and algorithm development, molecular dynamics (MD) simulations have become capable of simulating highly coupled dynamical systems for extended periods. To investigate the conservation of power and momentum over these prolonged periods, it becomes crucial to utilize multiple force field switches. In this research, we introduce a general algorithm for maintaining power and momentum conservation in MD simulations by employing force fields with switching potentials.\n\nWe examine the general case and derive the switching potentials from analytical solutions of the governing equations of motion. These solutions only involve variables that need to be fixed by the initial terms. Therefore, the proposed method is both straightforward to implement and computationally efficient.\n\nTo illustrate the effectiveness of our approach, we use the example of the Lennard-Jones (LJ) potential. By applying our suggested method, we achieve results of power and momentum conservation over 600,000 time steps of the LJ process with minimal computational cost, which is approximately 27% less than the cost using the standard method for the same time phase.",
        "ori-fast-z-score": -1.091089451179962,
        "water-fast-z-score": 5.314796216557077,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Researchers have proposed many methods to identify unpopular or dead links on the internet. These methods can be divided into three categories. The first category includes machine learning based methods. These methods train a classifier to identify dead links based on the characteristics of the link and the page it is on. The second category includes reinforcement learning based methods. These methods employ an agent to search for dead links by interacting with the network. The third category includes homophily based methods. These methods find links between dead and live pages based on their patterns of association. Recently, several research groups have proposed link curation algorithms to improve the quality of dead link detection methods. These methods analyze the pages that contain dead links to find similar pages that contain live links. These live links are used to fill in the dead links. These methods can be categorized as link replacement algorithms. We propose a vacancy localization algorithm to improve the quality of link replacement algorithms. Our algorithm first detects dead links using existing methods and then investigates similar live pages to localize vacancies, i.e., live links that can be replaced by dead links. The positions of these vacancies are recorded and used by our algorithm to replace dead links with live links. We perform a large-scale study on the dataset obtained from deadlink detection track of D4M dataset and show that our vacancy localization algorithm improves the overall performance of link replacement algorithms.",
        "watermark_text": "Researchers have proposed several methods to identify unpopular or dead links on the online . These methods can be grouped into three categories .The first class includes machine computing based methods . These methods train a classifier to identify dead links based on the traits of the link and the page it is on .The second genre contains reinforcement learning based methods . These methods involve an agent to search for dead links by interacting with the network .The third category includes homophily based methods . These methods find links between dead and live websites based on their patterns of association .Recently , various study organizations have proposed link curation algorithms to improve the quality of killed link detection methods . These methods analyze the pages that include dead links to find identical pages that include live links .These live links are using to fill in the dead links . These methods can be categorized as link replacement methods .We suggest a vacancy localization algorithm to improve the quality of page substitute algorithms . Our algorithm first detects killed ties using existing techniques and then investigates similar live websites to localize vacancies , i . e . , live links that can be replaced by killed ties .The places of these vacancies are recorded and utilized by our algorithm to fill dead links with live links . We perform a large - scale investigation on the dataset obtained from deadlink detection track of D4M dataset and find that our vacancy localization algorithm improves the overall performance of connect substitute algorithms .",
        "rewrite_text": "Researchers have proposed several techniques to identify unpopular or inactive hyperlinks on the internet. These techniques can be categorized into three main groups. The first group encompasses machine learning-based methods that employ a classifier to detect dead links based on the characteristics of the link and the web page it is linked to. The second category involves reinforcement learning-based methods, where an agent actively searches for inactive links through network interaction. The third category consists of homophily-based methods that identify links between inactive and active websites based on their association patterns.\n\nRecently, various research organizations have introduced link curation algorithms to enhance the quality of dead link detection methods. These algorithms analyze web pages with inactive links to find similar pages with active links. These active links are then used to replace the inactive ones. These methods can be classified as link replacement strategies.\n\nWe suggest a vacancy localization algorithm to enhance the effectiveness of page substitution algorithms. Our algorithm initially identifies dead links using existing techniques and then searches for comparable active websites to locate vacancies, i.e., active links that can replace the inactive ones. The locations of these vacancies are recorded and utilized by our algorithm to fill the dead links with active ones.\n\nWe conducted a comprehensive investigation using a large-scale dataset obtained from the deadlink detection track of the D4M dataset and found that our vacancy localization algorithm improves the overall performance of substitute link algorithms.",
        "ori-fast-z-score": 2.383518286124496,
        "water-fast-z-score": 8.001811389132238,
        "rewrite-fast-z-score": 0.33567254331867563
    },
    {
        "original_text": "The simplest model of dark energy with constant equation of state (EoS) parameter equal to -1 is the cosmological constant (CC). However, according to experimental data it suffers from the fine-tuning problem and the coincidence problem. Many modifications of CC have been proposed in order to solve these problems. One of them are the so called transient acceleration models. In this case, the EoS parameter is time dependent and CC only describes the early stage of the universe evolution. The simplest model of this kind is the so called T model. In this paper we consider the T-models with a vacuum component in the dark fluid. This new model describes the universe evolution in the same way as the T-model but, in addition, it explains the observed accelerated expansion of the universe. The article contains the solution of the corresponding modified Friedmann equations and the characteristics of the model evolution. Additionally, we present constraints on the model parameters coming from latest observational data.",
        "watermark_text": "The simplest theory of dark energy with constant equation of state ( EoS ) variable equal to - 1 is the cosmological constant ( CC ) . However , according to experimental evidence it suffers from the fine - tuned issue and the coincidence problem .Many modifications of CC have been proposed in order to solve these problems . One of them are the so called transient acceleration models .In this situation , the EoS parameter is period dependent and CC only describes the early stage of the universe progression . The simplest theory of this kind is the so called T model .In this paper we imagine the T - systems with a vacuum component in the dark fluid . This new model describes the universe progression in the same way as the T - model but , in addition , it explains the seen accelerated expansion of the universe .The section covers the solve of the equivalent altered Friedmann parameters and the attributes of the model evolution . Additionally , we present parameters on the model variables arriving from recent observational data .",
        "rewrite_text": "The most basic theory of dark energy, characterized by a constant equation of state (EoS) with a value of -1, is the cosmological constant (CC). However, experimental evidence suggests that it faces challenges such as the fine-tuning issue and coincidence problem. To address these issues, numerous modifications to the CC have been proposed. One such modification is the transient acceleration model. In this scenario, the EoS parameter depends on time periods, and the CC solely characterizes the early stages of the universe's evolution. The simplest version of this type of theory is referred to as the T-model.\n\nIn this paper, we consider T-systems with a vacuum component in the dark fluid. This new model mirrors the universe's progression as described by the T-model, but it additionally explains the observed accelerated expansion of the universe. The section focuses on solving the modified Friedmann parameters and the attributes of model evolution. Furthermore, we present parameters related to the model variables derived from recent observational data.",
        "ori-fast-z-score": -0.562543950463012,
        "water-fast-z-score": 4.919349550499537,
        "rewrite-fast-z-score": 2.650356625796317
    },
    {
        "original_text": "In machine learning, one of the most powerful learning paradigm is ensemble learning where learning algorithm(s) are combined to produce better generalization performance than any of the constituent learning algorithms. Evolutionary algorithms (EAs), especially evolutionary computation (EC) based EAs like Genetic Algorithm (GA), have been successfully applied to many machine learning problems. In this paper, we explore the use of EC based EAs like genetic algorithm (GA) to solve the machine learning problem of hyper parameter optimization for ensemble learning. We design a single GA process to automatically search over multiple hyper parameters (such as number of base learning algorithms, K, and the weight factor for each learning algorithm, w, in a weighted sum of base algorithms) of a given base machine learning algorithm for achieving best generalization performance on the training data. We test this GA based GA (GABGAS) algorithm on five benchmark machine learning problems where GA has not been previously tried. The results show that GABGAS can automatically find good values for K (between 1 and 20) and w (between 0.1 and 0.9) achieving significantly better generalization performance than the best known hand-tuned values for K and w for each of the benchmark problems. We also show that GABGAS has significantly lower computational complexity than existing approaches.",
        "watermark_text": "In machine learning , one of the most important learning framework is ensemble learning where learning algorithm ( s ) are combined to produce improved generalization efficiency than any of the constituent learning algorithms . Evolutionary algorithms ( EAs ) , particularly evolutionary computation ( EC ) based EAs like Genetic Algorithm ( GA ) , have been successfully applied to many machine computing problems .In this paper , we investigate the using of EC based EAs like genetic algorithm ( GA ) to tackle the machine learning question of hyper parameter optimization for ensemble learning . We create a single GA task to automatically hunt over multiple hyper parameters ( such as number of base learning algorithms , K , and the weight value for each learning algorithm , w , in a weighted sum of base algorithms ) of a given base machine learning algorithm for achieving better generalization success on the training data .We test this GA based GA ( GABGAS ) algorithm on five benchmark machine learning problems where GA has not been previously tried . The results show that GABGAS can easily find good values for K ( between 1 and 20 ) and w ( between 0 . 1 and 0 . 9 ) obtaining significantly improved generalization efficiency than the best known finger - tuned values for K and w for each of the benchmark problems .We also demonstrate that GABGAS has much lower computational complexity than existing techniques .",
        "rewrite_text": "In the realm of machine learning, ensemble learning stands as a crucial learning framework. This framework involves combining various learning algorithms to enhance generalization efficiency surpassing the capabilities of individual algorithms. Evolutionary algorithms, particularly those based on evolutionary computation like the Genetic Algorithm (GA), have found successful applications in numerous machine computing challenges. This paper explores the utilization of EC-based EAs, specifically the genetic algorithm (GA), to address the hyperparameter optimization problem in ensemble learning. We create a singular Genetic Algorithm task aimed at automatically exploring multiple hyperparameters, such as the number of base learning algorithms (K) and the weight value for each learning algorithm (w) in a weighted sum of base algorithms, to improve generalization success on training data for a given base machine learning algorithm. We test our proposed GA-based approach (GABGAS) on five benchmark machine learning problems where the GA has not been previously employed. The results demonstrate that GABGAS can effortlessly find optimal values for K (ranging from 1 to 20) and w (between 0.1 and 0.9), achieving significantly improved generalization efficiency compared to the best known manually-tuned values for K and w in each benchmark problem. Furthermore, we show that GABGAS exhibits a notably lower computational complexity than existing techniques.",
        "ori-fast-z-score": -1.6750597728792984,
        "water-fast-z-score": 3.645718329207885,
        "rewrite-fast-z-score": 1.0634101379502299
    },
    {
        "original_text": "Monte Carlo (MC) simulations have been performed to investigate the dynamic response of a silver (Ag) monolayer adsorbed on Au(100) to an oscillatory variation of the chemical potential. Upon an increase of the chemical potential, the Ag atoms adsorb at sites with higher coordination, leading to a compactification of the layer. Upon a decrease of the chemical potential, the Ag atoms desorb, leading to an expansion of the layer. We show that the dynamic response of the Ag monolayer is strongly dependent on the heating and cooling rates. In particular, at slow heating and cooling rates, a sizable hysteresis is observed. This behavior is analyzed in the framework of the generalized Lindemann parameter, which accounts for changes in the coordination of the adsorbed atoms upon heating and cooling. It is known that small changes in the chemical potential of components of a complex system may cause large excursions of the correspondingGibbs free energy. Depending on the heating and cooling rates, these excursions may induce dynamic phenomena such as hysteresis. Here, we report the for the first time observation of hysteresis in the dynamic behavior of a Ag monolayer upon an oscillatory variation of the chemical potential. Our findings have important implications for the modeling and interpretation of experimental data, and for the control of surface diffusion in chemical vapor deposition (CVD) and molecular beam epitaxy (MBE) growth processes.",
        "watermark_text": "Monte Carlo ( MC ) simulations have been performed to examine the dynamic response of a silver ( Ag ) monolayer adsorbed on Au ( 100 ) to an oscillatory change of the chemical potential . Upon an increase of the chemical potential , the Ag atoms adsorb at sites with higher coordination , leading to a compactification of the coating .Upon a reduction of the chemical potential , the Ag atoms desorb , leading to an increase of the coating . We see that the dynamic response of the Ag monolayer is strongly dependent on the warming and drying speeds .In particular , at slow heating and heat rates , a sizable hysteresis is observed . This phenomenon is investigated in the framework of the generalized Lindemann parameter , which accounts for changes in the coordination of the adsorbed molecules upon heating and cooling .It is known that minor alterations in the chemical ability of components of a complex system might cause significant excursions of the correspondingGibbs free energy . Depending on the heating and cooling rates , these excursions may generate chaotic phenomena such as hysteresis .Here , we present the for the first time measurement of hysteresis in the dynamic behavior of a Ag monolayer upon an oscillatory change of the chemical potential . Our findings have important implications for the modeling and understanding of research data , and for the regulation of fluid propagation in chemical vapor deposition ( CVD ) and molecular beam epitaxy ( MBE ) growth processes .",
        "rewrite_text": "Monte Carlo (MC) simulations have been conducted to investigate the dynamic response of a silver (Ag) monolayer adsorbed onto a Au (100) surface when subjected to oscillatory changes in chemical potential. As the chemical potential increases, Ag atoms tend to adsorb at sites with higher coordination, resulting in a more compact coating. Conversely, when the chemical potential decreases, the Ag atoms desorb, leading to an expansion of the coating. It is evident that the dynamic response of the Ag monolayer is significantly influenced by the rates of warming and drying processes. Specifically, at slower heating and cooling rates, a notable hysteresis is observed. This phenomenon is explored using the generalized Lindemann parameter, which accounts for changes in the coordination of adsorbed molecules during heating and cooling cycles.\n\nIt is well-known that minor alterations in the chemical properties of components within a complex system can lead to significant fluctuations in the corresponding Gibbs free energy. Depending on the heating and cooling rates, these fluctuations can induce chaotic phenomena such as hysteresis. In this study, we present, for the first time, measurements of hysteresis in the dynamic behavior of an Ag monolayer resulting from oscillatory changes in chemical potential. Our findings hold significant implications for modeling and understanding research data, as well as for regulating fluid propagation in chemical vapor deposition (CVD) and molecular beam epitaxy (MBE) growth processes.",
        "ori-fast-z-score": -1.0206207261596576,
        "water-fast-z-score": 5.103103630798288,
        "rewrite-fast-z-score": 1.9425717247145282
    },
    {
        "original_text": "The von Karman flow with mass loading is a standard benchmark for studying turbulence. It consists of an ensemble of non-interacting vortices, with energy concentrated in enstrophy and well-defined packets of each vortex sign. The vortices are self-similar and exhibit scale-invariance in the high-Cascade regime. In this paper, we develop a simplified 2D dynamical model that captures the essential features of the high-Cascade state. The evolution of the vorticity, Omega, is modeled by a diffusion equation with no-flux boundary conditions. Dissipation of enstrophy arises primarily through two distinct processes: direct viscosity, which leads to local smoothing of the vorticity field, and cross-helicity flux, which transfers enstrophy from near-core vortical zones to outer regions. We demonstrate that these two processes lead to a scaling solution with time-independent enstrophy and energy dissipation, in agreement with the dynamical model. We then show that the evolution of large-scale kinetic energy is controlled by the rate at which enstrophy is transferred to the outer regions. We derive an evolution equation for the scale-dependent enstrophy flux and perform a linear analysis to compute the scaling of the flux. This analysis, which neglects quadratic enstrophy-flux terms, gives excellent agreement with the full dynamical model in the high-Cascade regime. This work was supported by the U.S. Department of Energy under award number DE-SC0019468.",
        "watermark_text": "The von Karman flow with mass loading is a typical benchmark for studying turbulence . It consists of an ensemble of non - interacting vortices , with energy concentrated in enstrophy and well - defined packets of each vortex sign .The vortices are self - similar and exhibit scale - invariance in the high - Cascade regime . In this paper , we develop a simplified 2D dynamical profile that captures the important features of the high - Cascade state .The evolution of the vorticity , Omega , is modeled by a diffusion equation with no - flux border conditions . Dissipation of enstrophy occurs principally through two different processes : direct viscosity , which results to local smoothing of the vorticity field , and cross - helicity flux , which moves enstrophy from near - core vortical zones to inner regions .We showed that these two mechanisms lead to a scaling solution with time - independent enstrophy and energy dissipation , in agreement with the dynamical theory . We then show that the evolution of large - scale kinetic power is controlled by the speed at which enstrophy is transferred to the exterior areas .We derive an evolution equation for the scale - dependent enstrophy flux and conduct a linear analysis to compute the scaling of the flux . This study , which neglects quadratic enstrophy - flux terms , makes good agreement with the full dynamical model in the high - Cascade regime .This effort was supported by the U . S . Department of Energy under award number DE - SC0019468 .",
        "rewrite_text": "The von Karman flow with mass loading serves as a typical benchmark for studying turbulence dynamics. It comprises an ensemble of non-interacting vortices, with a concentrated energy in enstrophy and distinct packets for each vortex signature. These vortices are self-similar and exhibit scale-invariance in the high-cascade regime. In this paper, we develop a simplified 2D dynamic profile that captures the essential characteristics of the high-cascade state. The evolution of vorticity, Omega, is modeled using a diffusion equation with no-flux boundary conditions.\n\nEnstrophy dissipation primarily occurs through two distinct processes: direct viscosity, which results in the local smoothing of the vorticity field, and cross-helicity flux, which transfers enstrophy from near-core vortical zones to inner regions. We demonstrate that these two mechanisms lead to a scaling solution with time-independent enstrophy and energy dissipation, aligning with dynamical theory. Furthermore, we show that the evolution of large-scale kinetic power is governed by the rate of enstrophy transfer to external areas.\n\nWe derive an evolution equation for the scale-dependent enstrophy flux and perform a linear analysis to determine its scaling. This study, neglecting quadratic enstrophy flux terms, exhibits good agreement with the full dynamical model in the high-cascade regime. This research was supported by the U.S. Department of Energy under award number DE-SC0019468.",
        "ori-fast-z-score": 1.0101525445522108,
        "water-fast-z-score": 5.656854249492381,
        "rewrite-fast-z-score": 2.424366106925306
    },
    {
        "original_text": "The Large Magellanic Cloud (LMC) is an dwarf galaxy satellite of the Milky Way, approximately 163,000 light years (56,000 kpc) away from the Galactic Center. The LMC has a distinct halo of neutral hydrogen gas that extends for many tens of thousands of parsecs (about 160 kpc). Studying the distribution and characteristics of this neutral hydrogen gas can give us insights into the formation and evolution of the LMC, its halo and the surrounding intergalactic medium (IGM). Previous catalogs of the LMC neutral hydrogen gas have been based on single dish telescopes with small fields of view. In this work we present a new catalog of LMC neutral hydrogen clouds using the MeerKAT radio telescope. This is the largest and most sensitive catalog of LMC neutral hydrogen clouds to date, and will be useful for studies of the LMC impact on the IGM and the structure and evolution of the Local Group.",
        "watermark_text": "The Large Magellanic Cloud ( LMC ) is an dwarf galaxy satellite of the Milky Way , roughly 163 , 000 light years ( 56 , 000 kpc ) apart from the Galactic Center . The LMC has a distinct halo of neutral hydrogen liquid that extends for far tens of millions of parsecs ( about 160 kpc ) .Studying the distribution and features of this neutral hydrogen source can provide us insights into the formation and evolution of the LMC , its halo and the nearby intergalactic medium ( IGM ) . Previous catalogs of the LMC neutral hydrogen gas have been based on single dish telescopes with narrow fields of view .In this project we present a new catalog of LMC neutral hydrogen clouds using the MeerKAT radio telescope . This is the largest and most important inventory of LMC neutral hydrogen clusters to date , and will be valuable for research of the LMC impact on the IGM and the composition and evolution of the Local Group .",
        "rewrite_text": "The Large Magellanic Cloud (LMC) is a satellite dwarf galaxy of the Milky Way, situated at a distance of approximately 163,000 light years (or 56,000 kiloparsecs) from the Galactic Center. The LMC features a distinctive halo of neutral hydrogen liquid that extends over vast distances of tens of millions of parsecs (roughly 160 kpc). By studying the distribution and characteristics of this neutral hydrogen source, we can gain valuable insights into the formation, evolution, and the surrounding intergalactic medium (IGM) of the LMC.\n\nPreviously, catalogs of the LMC's neutral hydrogen gas were primarily based on single-dish telescopes with narrow fields of view. However, in this project, we introduce a new catalog of LMC neutral hydrogen clouds utilizing the MeerKAT radio telescope. This represents the most comprehensive and significant inventory of LMC neutral hydrogen clusters ever compiled, and it will be invaluable for researching the LMC's impact on the IGM, as well as the composition and evolution of the Local Group.",
        "ori-fast-z-score": 0.5252257314388902,
        "water-fast-z-score": 4.900769721140662,
        "rewrite-fast-z-score": 1.3438638879193574
    },
    {
        "original_text": "In this paper we study the cohomology of affine Artin groups. We show that the first reduced cohomology group with integer coefficients is infinitely generated. We give an explicit infinitely generated subgroup of the first reduced cohomology group. We also study the second reduced cohomology group with coefficients in certain finite dimensional representation. We prove that this second cohomology group is zero in some cases, and give an upper bound for it in other cases. As an application we show that the mapping class group of a surface of large genus has infinite cohomological dimension. We also study the cohomology with local coefficients. We describe the cohomology groups with local coefficients in the regular representation in some special cases. We show that the local cohomology group with local coefficients in the regular representation is isomorphic to the incidence cohomology of the corresponding Artin group, generalizing the result of Mimura and Toda for the classical Artin groups. We also calculate the cohomology with local coefficients in the standard representation in some special cases. Our study of cohomology groups with local coefficients enables us to solve a famous conjecture of Bass on the rank of 2-cohomology groups of arithmetic groups. More precisely, we prove that if the corresponding Zariski closure of an arithmetic group has no simple factors of exceptional type, then the rank of 2-cohomology group of this arithmetic group is divisible by 2 and by the constant part of itsbsd fixer. We also discuss the combinatorics of these cohomology groups with local coefficients, in particular the effect of the schensted correspondence on these cohomology groups. Finally, we give a new realization of the canonical mixed building of the special orthogonal group as a non-symmetric space, which enables us to calculate the cohomology of the special orthogonal group using the corresponding result for the orthogonal group. As a consequence, we give an alternative proof that the special orthogonal group has cohomological dimension 2. In this paper, all representations are complex representations. The cohomology groups with local coefficients studied in this paper are all with integer coefficients. They can be studied with rational coefficients by a standard localization argument. Finally, we mention some possible extensions of our results to other Artin groups, such as the classical and braid Artin groups. This paper is joint work with C. J. Delingpole, Y. L. Pan, and Y. Zhou. Thank you for your interest in our paper. We hope this paper will shed new light on the study of affine Artin groups and have potential applications to geometric group theory and modular representation theory. Since this paper presents several new techniques and approaches to existing theories, it will be very helpful to the experts in these fields and will be of interest to the broader mathematics community. We thus encourage comments and suggestions from both audiences. We will keep our paper updated with responses to comments and new results as we progress on our research. Thank you for your interest and support.",
        "watermark_text": "In this paper we study the cohomology of affine Artin groups . We see that the first reduced cohomology group with integer coefficients is infinitely generated .We get an explicit infinitely generated subgroup of the first reduced cohomology group . We also study the second diminished cohomology group with coefficients in certain finite dimensional representation .We prove that this second cohomology group is zero in some cases , and get an upper bound for it in other instances . As an application we prove that the mapping class group of a surface of large genus has infinite cohomological dimension .We especially study the cohomology with local coefficients . We define the cohomology groups with local coefficients in the regular representation in some special cases .We see that the local cohomology set with local coefficients in the usual representation is isomorphic to the incidence cohomology of the associated Artin group , generalizing the result of Mimura and Toda for the classical Artin groups . We additionally calculate the cohomology with local coefficients in the standard representation in some special cases .Our study of cohomology groups with local coefficients allows us to correct a celebrated conjecture of Bass on the rank of 2 - cohomology groups of arithmetic groups . More specifically , we prove that if the associated Zariski closure of an arithmetic group has no simple factors of exceptional type , then the rank of 2 - cohomology group of this arithmetic group is divisible by 2 and by the constant part of itsbsd fixer .We also discuss the combinatorics of these cohomology groups with local coefficients , in example the effect of the schensted relationship on these cohomology groups . Finally , we give a new conception of the canonical mixed building of the special orthogonal group as a non - symmetric space , which enables us to estimate the cohomology of the special orthogonal group taking the analogous result for the orthogonal group .As a consequence , we give an additional confirmation that the special orthogonal category has cohomological dimension 2 . In this paper , all representations are complex representations .The cohomology groups with local coefficients studied in this paper are all with integer coefficients . They can be investigated with rational coefficients by a traditional localization argument .Finally , we mention some possible extend of our findings to other Artin groups , such as the classical and braid Artin groups . This paper is joint work with C . J . Delingpole , Y . L . Pan , and Y . Zhou .Thank you for your interest in our paper . We believe this paper will show fresh light on the study of affine Artin groups and have potential applications to geometric group theory and modular representation theory .Since this paper offers several innovative techniques and approaches to existing ideas , it will be very helpful to the experts in these fields and will be of interest to the broader mathematics community . We consequently stimulate comments and suggestions from both audiences .We will keep our paper updated with reply to comments and new data as we progress on our research . Thank you for your focus and support .",
        "rewrite_text": "In this research, we investigate the cohomology of affine Artin groups. We observe that the first reduced cohomology group with integer coefficients is infinitely generated. We obtain an explicitly defined infinitely generated subgroup within the first reduced cohomology group. We also explore the second diminished cohomology group with coefficients from specific finite-dimensional representations. We prove that in certain cases, this second cohomology group is zero and provide an upper bound for it in others.\n\nAs an application, we demonstrate that the mapping class group of a surface with a large genus possesses an infinite cohomological dimension. We focus on the study of cohomology with local coefficients. We define cohomology groups with local coefficients in the regular representation under certain special circumstances. It is observed that the local cohomology set with local coefficients in the usual representation is isomorphic to the incidence cohomology of the associated Artin group, generalizing the findings of Mimura and Toda for classical Artin groups.\n\nAdditionally, we calculate the cohomology with local coefficients in the standard representation in specific cases. Our examination of cohomology groups with local coefficients enables us to correct a renowned conjecture of Bass regarding the rank of 2-cohomology groups of arithmetic groups. Specifically, we prove that if the associated Zariski closure of an arithmetic group lacks simple factors of exceptional type, then the rank of the 2-cohomology group of that arithmetic group is divisible by 2 and by the constant part of its fixer.\n\nWe also discuss the combinatorics of these cohomology groups with local coefficients, such as the impact of the Schensted relationship on them. Furthermore, we introduce a novel concept of the canonical mixed construction of the special orthogonal group as a non-symmetric space, which aids in estimating the cohomology of the special orthogonal group in analogy to the orthogonal group's results. Consequently, we provide additional evidence that the special orthogonal category has a cohomological dimension of 2.\n\nIn this paper, all representations are complex, and the studied cohomology groups with local coefficients have integer coefficients. These can be investigated with rational coefficients using traditional localization arguments. Finally, we mention potential extensions of our findings to other Artin groups, including classical and braid Artin groups.\n\nThis paper is a joint effort with C. J. Delingpole, Y. L. Pan, and Y. Zhou. We appreciate your interest in our work. We believe this paper will shed new light on the study of affine Artin groups and have potential applications in geometric group theory and modular representation theory. Given the innovative techniques and approaches presented in this paper to existing ideas, it will be beneficial to experts in these fields and of interest to the broader mathematics community. We encourage feedback and suggestions from both audiences. We will keep our paper updated in response to comments and new data as our research progresses. Thank you for your attention and support.",
        "ori-fast-z-score": -0.2036532699906392,
        "water-fast-z-score": 5.905944829728537,
        "rewrite-fast-z-score": 1.8963992921400699
    },
    {
        "original_text": "In this paper, we propose a novel hierarchical approach for dependability analysis of a commercial cache-based RAID storage architecture. The proposed approach combines fault detection and isolation techniques with Markov Chain analysis and Bayesian network analysis to evaluate the impact of different types of faults on the availability and response time of the system. The effectiveness of the approach is illustrated using a set of real-world experiments on a high-availability production cluster involving over 2000 virtual machines and hundreds of file transactions. The results show that a large number of faults can occur without any adverse impact on the availability of the cluster. Furthermore, the response times of the system are also unaffected by many of the faults that occur. However, a large number of faults do affect the response time of the system and this can have a significant impact on the overall user experience. Using the proposed approach, we are able to identify and quantify these faults and their impact on the dependability of the cluster.",
        "watermark_text": "In this paper , we propose a new hierarchical method for dependability analysis of a commercial cache - based RAID cache architecture . The proposed approach utilizes failure detection and isolation approaches with Markov Chain analysis and Bayesian network analysis to analyze the impact of different kinds of faults on the availability and response period of the device .The effectiveness of the approach is depicted using a setting of real - time studies on a high - availability manufacturing cluster involving over 2000 digital servers and dozens of file transactions . The results show that a large number of faults can occur without any negative affect on the availability of the cluster .Furthermore , the response periods of the device are also unaffected by many of the faults that occur . However , a large number of faults do affect the response period of the device and this can have a substantial impact on the overall user experience .Using the suggested approach , we are able to identify and quantify these errors and their impact on the dependability of the cluster .",
        "rewrite_text": "In this study, we present an innovative hierarchical method for dependability analysis of a commercial cache-based RAID cache structure. Our method incorporates failure detection and isolation techniques, along with Markov Chain and Bayesian network analysis, to assess how various types of faults impact the availability and response time of the device. The effectiveness of our approach is illustrated through real-time studies in a high-availability manufacturing cluster comprising over 2000 digital servers and numerous file transactions. The findings reveal that a significant number of faults can occur without affecting the cluster's availability adversely. Furthermore, the response periods of the device remain unaffected by many of the occurring faults. Nevertheless, a considerable number of faults do impact the device's response period, which can significantly influence the overall user experience. By utilizing our suggested approach, we are able to identify and quantify these errors, as well as their impact on the dependability of the cluster.",
        "ori-fast-z-score": 0.8728715609439696,
        "water-fast-z-score": 6.901297485020058,
        "rewrite-fast-z-score": 2.6832815729997477
    },
    {
        "original_text": "A diagrammatic category for the representation theory of U_q(sl_n). This defines a compact closed structure on the equivariant K-theory of a nilpotent Hamiltonian reduction, generalising the Springer correspondence and its recent categorical extensions. Through the combinatorics of lattice paths, it allows for a uniform treatment of (a) type A, (b) type B, and (c) arbitrary quantum groups. There are several exciting applications of this in the form of conjectural categorifications of classical multiplicative preSchur functions, and generalised Kazhdan-Lusztig polynomials. Applications to finite type representations, categorified combinatorics, quasihereditary modules, and quantum groups will appear in later work. The diagrammatic category in question is the path category P(N-1, N) of the symmetric Laplacian; this gives it a uniform treatment of types (a) and (b) (as well as (c) when N is a integer).",
        "watermark_text": "A diagrammatic category for the representation theory of U _ q ( sl _ q ) . This creates a compact closed structure on the equivariant K - theory of a nilpotent Hamiltonian reduction , generalising the Springer correspondence and its recent categorical additions .Through the combinatorics of lattice paths , it allows for a uniform characterization of ( a ) type A , ( c ) type B , and ( c ) arbitrary quantum groups . There are several interesting applications of this in the form of conjectural categorifications of classical multiplicative preSchur functions , and generalised Kazhdan - Lusztig polynomials .Applications to finite type representations , categorified combinatorics , quasihereditary algebra , and quantum groups will appear in later work . The diagrammatic category in question is the path category P ( N - 1 , N ) of the symmetric Laplacian ; this gives it a uniform characterization of types ( a ) and ( c ) ( as well as ( c ) when N is a integer ) .",
        "rewrite_text": "A diagrammatic category exists for the representation theory of U_q(sl_q). This structure forms a compact and closed framework within the equivariant K-theory of a nilpotent Hamiltonian reduction, broadening the Springer correspondence and its recent categorical advancements. Leveraging the combinatorics of lattice paths, it provides a consistent portrayal for type (a) A, type (c) B, and type (c) arbitrary quantum groups. This has multiple intriguing applications, such as hypothetical categorifications of classical multiplicative preSchur functions and generalized Kazhdan-Lusztig polynomials. Future research will explore its applications to finite type representations, categorified combinatorics, quasihereditary algebra, and quantum groups. The specific diagrammatic category in question is the path category P(N-1, N) associated with the symmetric Laplacian; this gives us a unified characterization of types (a) and (c) (as well as type (c) when N is an integer).",
        "ori-fast-z-score": 2.0225995873897262,
        "water-fast-z-score": 4.543441112511214,
        "rewrite-fast-z-score": 3.0983866769659336
    },
    {
        "original_text": "We present a model for QCD at high density and large quark mass which may explain the quantum Chromodynamics (QCD) phase diagram. The model is built around an infinite dimensional Polyakov gauge theory that reduces to N coincident 1+1 dimensional SU(N) Yang-Mills theories at high temperature, to N coincident 1+1 dimensional U(1) gauge theories at low temperature, and to three spatial dimensions at infinite temperature. The parameters in the model are the temperature T, the quark chemical potential mu, and the temporal extent of the 1+1 dimensional theory L(T). We obtain three phases as T is decreased: a high density deconfined phase at low mu and high T, a confined low density phase at high mu and low T, and a crossover phase in between. The confined phase at high mu is expected to have a dual description in terms of a superconductor at low temperature. The model provides a plausible scenario for the continuous crossover between these phases as T is decreased.",
        "watermark_text": "We present a theory for QCD at high density and large quark mass which would describe the quantum Chromodynamics ( QCD ) phase diagram . The model is built around an infinite dimensional Polyakov gauge theory that reduces to N coincident 1 + 1 dimensional SU ( N ) Yang - Mills theories at high heat , to N coincident 1 + 1 dimensional U ( 1 ) gauge theories at low temperature , and to three spatial dimensions at infinite temperature .The parameters in the model are the temperature T , the quark chemical potential mu , and the temporal depth of the 1 + 1 dimensional theory L ( T ) . We get three stages as T is decreased : a high density deconfined phase at low mu and low T , a concentrated low density phase at high mu and low T , and a crossover mode in between .The confined phase at high mu is expected to have a dual description in terms of a superconductor at low heat . The model provides a plausible scenario for the dynamic crossover between these stages as T is decreased .",
        "rewrite_text": "We propose a theory for Quantum Chromodynamics (QCD) at elevated densities and with significant quark masses, which aims to delineate the phase diagram of QCD. This theory is founded on an infinitely dimensional Polyakov gauge model that transitions into N coinciding 1 + 1 dimensional SU(N) Yang-Mills theories at high temperatures, into N coinciding 1 + 1 dimensional U(1) gauge theories at low temperatures, and finally into three spatial dimensions at infinite temperature. The model parameters include temperature (T), quark chemical potential (mu), and the temporal depth of the 1 + 1 dimensional theory (L(T)). As the temperature (T) decreases, three distinct stages emerge: a high-density, deconfined phase at low mu and low T, a concentrated, low-density phase at high mu and low T, and a crossover mode in between. It is anticipated that the confined phase at high mu may have a dual description in terms of a low-temperature superconductor. This model offers a plausible scenario for the dynamic crossover between these stages as the temperature is lowered.",
        "ori-fast-z-score": 1.62746694241347,
        "water-fast-z-score": 4.88240082724041,
        "rewrite-fast-z-score": 2.0647416048350555
    },
    {
        "original_text": "The stability of shock waves in a transonic gas flow is studied. The inviscid and isentropic case is solved and a transcritical shock wave is found to be most unstable. The result is confirmed by a stability analysis for viscous shock waves, which are shown to be stable for all possible wave numbers. The stabilizing effect of viscosity is discussed. The equations of isentropic gas dynamics in one spatial dimension reduce to a nonlinear wave equation for the pressure in case of inviscid flow and to a semi-linear degenerate hyperbolic equation in the viscous case. A shock wave is thus stable to viscosity for the inviscid case and stable for all wavenumbers for the viscous case. The results are of interest in several areas of astrophysics and gasdynamics, where shock waves appear as a result of e.g. nucleosynthesis or star formation, and viscosity may be an important effect.",
        "watermark_text": "The stability of shock waves in a transonic gas stream is studied . The inviscid and isentropic case is solved and a transcritical shock wave is found to be most unstable .The result is confirmed by a stability analysis for viscous blast flows , which are shown to be stable for all possible wave numbers . The stabilizing impact of viscosity is mentioned .The equations of isentropic gas mechanics in one spatial dimension convert to a nonlinear wave equation for the pressure in case of inviscid stream and to a semi - linear degenerate hyperbolic equation in the viscous case . A flash shock is thereby stable to viscosity for the inviscid case and stable for all wavenumbers for the viscous case .The results are of interest in multiple fields of astrophysics and gasdynamics , where shock waves appear as a outcome of e . g . nucleosynthesis or star formation , and viscosity could be an important result .",
        "rewrite_text": "The study examines the stability of shock waves within a transonic gas flow. The inviscid and isentropic scenarios are solved, revealing that a transcritical shock wave exhibits the highest instability. This finding is corroborated by a stability analysis of viscous blast flows, which demonstrate stability for all possible wave numbers. The stabilizing effect of viscosity is highlighted.\n\nIn the case of an inviscid stream, the equations of isentropic gas mechanics in one spatial dimension transform into a nonlinear wave equation for pressure. In the viscous scenario, they transition to a semi-linear degenerate hyperbolic equation. A flash shock is found to be stable against viscosity in the inviscid scenario and remains stable for all wavenumbers in the viscous case. These results hold significant interest in multiple fields of astrophysics and gas dynamics, where shock waves emerge as a result of processes such as nucleosynthesis or star formation, and where viscosity can play a crucial role.",
        "ori-fast-z-score": -0.508000508000762,
        "water-fast-z-score": 3.4569957581881914,
        "rewrite-fast-z-score": 1.3242443839434612
    },
    {
        "original_text": "Stars throughout their evolution exhibit a wide range of magnetic field topologies. The evolution of magnetic fields has been relatively understudied in comparison to the evolution of other stellar properties such as mass, radius, and rotation. To study the field’s evolution, we analyzed high-resolution, spectropolarimetric observations of five active magnetic stars in the upper main sequence. The five stars were chosen because they span a range of effective temperatures and characterize a variety of surface field strengths. The majority of the sample exhibit predominantly radial fields with small-scale irregularities. Two stars, HD 485A and HD 37402, have dipole fields with stable emission cores. The remaining star, EK Cep, exhibits a highly complex, non-axisymmetric magnetic field. This star also exhibits a long-lived emission core in its longitudinal field. We find that the distributions of the different field topologies across the sample are statistically indistinguishable. We observe dipole fields with stable emission cores across the sample, even in the rapidly rotating HD 37402. We discuss the implications of these stable dipole fields on mass-loss rates and angular momentum evolution. Funding: Science & Technology Facilities Council (STFC) Safari Data Browser was written by Fred Greaves and Colleen Agarwal. Funding for this Safari Visualization Browser was provided by Cardiff University’s  ARCH (https://www.arch.cardiff.edu) project. Link to paper: https://arxiv.org/abs/1906.08484",
        "watermark_text": "Stars throughout their development exhibit a broad variety of magnetic force topologies . The growth of magnetic fields has been relatively understudied in comparison to the evolution of other stellar structures such as mass , diameter , and rotation .To study the field ’ s evolution , we analyzed high - resolution , spectropolarimetric photographs of five active magnetic stars in the higher major sequence . The five stars were chosen because they span a range of effective altitudes and characterize a variety of surface field strengths .The majority of the sample contain predominantly radial fields with little - scale irregularities . Two stars , HD 485A and HD 37402 , have dipole fields with stable emission cores .The remaining star , EK Cep , displays a highly sophisticated , non - axisymmetric magnetic force . This star also exhibits a far - lived emission core in its longitudinal field .We see that the distributions of the different field topologies across the sample are statistically indistinguishable . We detect dipole fields with stable emission cores across the sample , even in the rapidly spinning HD 37402 .We discuss the implications of these stable dipole fields on mass - loss costs and angular velocity evolution . Funding : Science & Technology Facilities Council ( STFC ) Safari Data Browser was written by Fred Greaves and Colleen Agarwal .Funding for this Safari Visualization Browser was provided by Cardiff University ’ s ARCH ( https : / / www . arch . cardiff . edu ) initiative . Link to paper : https : / / arxiv . org / abs / 1906 . 08484",
        "rewrite_text": "Stellar development demonstrates a diverse array of magnetic force configurations. In contrast to the extensive research on the evolution of other stellar features like mass, diameter, and rotation, the growth of magnetic fields has received relatively less attention. To investigate the field's progression, we analyzed high-resolution spectropolarimetric images of five active magnetic stars in the upper main sequence. These five stars were chosen due to their range of effective altitudes and diverse surface field strengths. The majority of our sample predominantly exhibits radial fields with minor-scale irregularities. Specifically, two stars - HD 485A and HD 37402 - possess stable emission core dipole fields. The third star, EK Cep, displays a highly complex, non-axisymmetric magnetic force. This star also features a long-lived emission core in its longitudinal field. Our findings indicate that the distributions of various field topologies within the sample are statistically similar. We observe dipole fields with stable emission cores across the entire sample, even in the rapidly rotating HD 37402. We discuss the implications of these persistent dipole fields on mass-loss rates and angular velocity evolution.\n\nFunding for this research was provided by the Science & Technology Facilities Council (STFC). The Safari Data Browser was developed by Fred Greaves and Colleen Agarwal. Financial support for the Safari Visualization Browser was granted by Cardiff University's ARCH (https://www.arch.cardiff.edu) initiative. The associated research paper can be accessed at: https://arxiv.org/abs/1906.08484.",
        "ori-fast-z-score": 0.09853292781642932,
        "water-fast-z-score": 4.950737714883372,
        "rewrite-fast-z-score": 1.4605934866804429
    },
    {
        "original_text": "The Costas property (CP) in the unit circle, also known as the circular rotation property, states that the complex exponential function Θ(t)=e^{it}, where i is the imaginary unit and t is a real number, rotates through 2π radians or 360 degrees when operated on around the origin. This article generalizes CP in the following way: let f be a differentiable function that is injective on an open interval I with endpoints a and b, such that f  a ≠0 and f  b ≠0. Then, for all t in I, the equation f(x)=t has at most two solutions. The authors call a function that satisfies the preceding conditions CP-generalizing. A number of properties of CP are inherited by this general definition. In particular, the constant function f(x)=c is CP-generalizing for any real number c. The authors give a number of examples of functions that are CP-generalizing but do not share any other properties of Costas functions. The article concludes with several open questions.",
        "watermark_text": "The Costas property ( CP ) in the unit circle , sometimes called as the circular rotation property , states that the complex exponential map Θ ( t ) = e ^ { it } , where i is the imaginary unit and t is a real number , rotates through 2π radians or 360 degrees when operated on around the origin . This paragraph generalizes CP in the following way : letting f be a differentiable function that is injective on an open interval I with endpoints a and b , such that f a ≠0 and f b ≠0 .Then , for all t in I , the equation f ( x ) = t has at most two solutions . The authors call a function that satisfies the preceding assumptions CP - generalizing .A variety of properties of CP are acquired by this general definition . In particular , the constant function f ( x ) = c is CP - generalizing for any real number c . The authors give a number of instances of functions that are CP - generalizing but do not share any other properties of Costas functions .The essay finishes with many open questions .",
        "rewrite_text": "The Costas property (CP) within the unit circle, often referred to as the circular rotation property, describes the complex exponential map Θ(t) = e^{it}, wherein 'i' represents the imaginary unit and 't' is a real number. This map rotates through 2π radians or 360 degrees when operated around the origin. The following generalization of CP is presented: Let f be a differentiable function that is injective on an open interval I, bounded by endpoints a and b, such that f(a) ≠ 0 and f(b) ≠ 0. Then, for all t values within I, the equation f(x) = t has a maximum of two solutions. The authors term a function satisfying the aforementioned assumptions as CP-generalizing. This broad definition yields a range of properties associated with CP. Specifically, the constant function f(x) = c is considered CP-generalizing for any real number c. The authors provide numerous examples of functions that are CP-generalizing but lack other characteristics typical of Costas functions. The essay concludes with several open questions remaining unanswered.",
        "ori-fast-z-score": -0.12403473458920847,
        "water-fast-z-score": 2.4618298195866544,
        "rewrite-fast-z-score": -0.7977240352174656
    },
    {
        "original_text": "A simple phenomenological model is presented to interpret the cosmic-ray electrons (CRe) electron spectrum in the galaxy cluster Sersic 159-03 discovered by the the Fermi-LAT. It is shown that both the gamma-ray flux and the radio upper limit can be well accounted for if the spectral break of the CRe spectrum is below the energy threshold of the radio telescope used (1.5 PeV). Compared to hadronic models which suffer from the extreme high density discrepancy problem, this model can naturally explain the relatively low gamma-ray luminosity for the observed hard X-ray tail while keeping most of the CRe population in the halo of Sersic 159-03. The model is also shown to be compatible with the tentative detection of the lepton asymmetry between the north and the south of the cluster. Finally, we briefly discuss the detectability of such a model with the current and future IACTs and CTA.",
        "watermark_text": "A straightforward phenomenological theory is provided to analyze the cosmic - ray ions ( CRe ) electron spectrum in the galaxy cluster Sersic 159 - 03 found by the the Fermi - LAT . It is demonstrated that both the beta - ray flux and the radio lower limit can be well accounted for if the spectral break of the CRe spectrum is below the power threshold of the television telescope used ( 1 . 5 PeV ) .Compared to hadronic models which suffer from the severe high density discrepancy problem , this model can naturally explain the generally low gamma - ray luminosity for the seen hard X - ray tail while staying most of the CRe population in the halo of Sersic 159 - 03 . The model is also shown to be compatible with the tentative discovery of the lepton asymmetry between the north and the south of the cluster .Finally , we briefly discuss the detectability of such a theory with the present and future IACTs and CTA .",
        "rewrite_text": "A simplified phenomenological theory has been presented to analyze the electron spectrum of cosmic ray ions (CRe) in the Sersic 159-03 galaxy cluster, discovered by the Fermi-LAT instrument. This theory suggests that both the beta-ray flux and the radio lower limit can be accurately explained if the spectral break in the CRe spectrum occurs below the power threshold of the employed telescope (i.e., 1.5 PeV). In contrast to hadronic models that face significant challenges with high-density discrepancies, this model naturally accounts for the generally low gamma-ray luminosity observed in the hard X-ray tail while maintaining a significant CRe population within the halo of Sersic 159-03. Furthermore, it is demonstrated that this model is consistent with the tentative discovery of a lepton asymmetry between the northern and southern regions of the cluster. Finally, we briefly discuss the detectability of this theory with current and future Imaging Air Cherenkov Telescopes (IACTs) and the Cherenkov Telescope Array (CTA).",
        "ori-fast-z-score": -0.6401843996644799,
        "water-fast-z-score": 4.6615618337804685,
        "rewrite-fast-z-score": 0.8551861104941365
    },
    {
        "original_text": "In astrophysics, rotating black holes (BHs) are a source of observational signatures across all length and time scales, from gamma-rays to radio to optical bands. The most readily observable manifestation of a rotating BH is the apparent ellipsoidal modulation of the light of the companion star passing behind it, as seen in an astronomical object known as an X-ray binary. Such an X-ray binary comprises a normal star that forms a close binary with a compact object, a black hole or a neutron star. This normal star is surrounded by an accretion disk that is fed by the normal star s winds and ejects the matter of the stellar envelope into space, forming an X-ray binary system. If the compact object is a rotating black hole, the surface of the black hole will be disrupted by frame dragging, resulting in a large-scale magnetic field. This magnetic field will warp the trajectories of particles in the disk, causing them to precess. The closer the companion star is to the black hole, the more powerful the magnetic field is expected to be. This allows us to potentially measure the black hole s rotation rate.",
        "watermark_text": "In astrophysics , rotating black holes ( BHs ) are a source of observational signatures across all length and time ranges , from gamma - rays to radio to laser bands . The most readily observable manifestation of a rotating BH is the apparent ellipsoidal modulation of the light of the companion star moving behind it , as shown in an astronomical object known as an X - ray binary .Such an X - ray binary includes a normal star that forms a close binary with a compact object , a black hole or a neutron star . This normal star is surrounded by an accretion disk that is feeding by the ordinary star s winds and ejects the matter of the stellar envelope into space , forming an X - ray binary system .If the compact sphere is a rotating black hole , the surface of the dark hole will be disrupted by frame dragging , resulting in a large - scale magnetic force . This magnetic force will warp the trajectories of particles in the disk , forcing them to precess .The closer the companion galaxy is to the dark hole , the more potent the magnetic force is expected to be . This enables us to potentially measure the dark hole s rotation rate .",
        "rewrite_text": "In astrophysics, rotating black holes (BHs) are a pivotal source of observational signatures that span all length and time scales, encompassing gamma-rays, radio waves, and even laser frequencies. The most evident manifestation of a rotating BH is the noticeable modulation of the companion star's light into an ellipsoidal shape as it moves behind the black hole, as seen in astronomical objects termed X-ray binaries.\n\nThese X-ray binaries typically consist of a regular star in close binary orbit with a compact object, either a black hole or a neutron star. This regular star is surrounded by an accretion disk, which is fed by the star's winds and ejects the material from its envelope into space, forming an X-ray binary system. When the compact object is a rotating black hole, frame dragging disrupts the surface of the dark hole, resulting in a large-scale magnetic force. This magnetic force distorts the trajectories of particles within the disk, forcing them into a precession motion. As the companion galaxy gets closer to the dark hole, the magnetic force is expected to become more potent. This enables us to potentially measure the rotational speed of the black hole.",
        "ori-fast-z-score": -1.1406468642034677,
        "water-fast-z-score": 3.3362306249131963,
        "rewrite-fast-z-score": -0.40406101782088427
    },
    {
        "original_text": "Interstellar dust is composed of a wide variety of submicron sized carbonaceous and siliceous grains arising from different types of parent bodies with a composite size distribution. The smallest observed interstellar grains, with a minimum size of 4.6 angstroms, are most likely arising from low temperature carbonaceous asteroids. The majority of interstellar grains, with a radius of 0.1-0.3 μm, have most likely sporadic formation from carbon rich gas clouds or volatilized polycyclic aromatic hydrocarbons (PAHs). While the sizes of the majority of the observed interstellar grains range from 0.1 to 0.3 μm, they appear to have an arbitrary cut off around 1 μm, with a probable aggregate of denser aggregates. The characteristic sizes of these aggregrates ranges from 0.3 to 1 μm. It is not clear if these aggregates form through agglomeration or some other process. In addition, the surfaces of these aggregates are coated with a thin layer of polar non-volatile material that provides a second cross-section of hydrogen bonding sites. It is believed that this layer of non-volatile material provides the cementing mechanism for the aggregates, binding them into larger structures. The typical size of these structures ranges from 0.5 to 5 μm with a preferred size of 2 μm. The dominant composition of these larger interstellar grains is likely going from silicate cores to graphite overlayers as this provides a high transparency to infrared radiation.",
        "watermark_text": "Interstellar dust is composed of a broad variety of submicron sized carbonaceous and siliceous grains resulting from varying kinds of parent bodies with a composite height distribution . The tiny observed interstellar grains , with a minimum diameter of 4 . 6 angstroms , are most likely arising from small temperature carbonaceous asteroids .The majority of interstellar grains , with a diameter of 0 . 1 - 0 . 3 μm , have most likely sporadic formation from carbon rich gas storms or volatilized polycyclic aromatic hydrocarbons ( PAHs ) . While the sizes of the majority of the reported interstellar grains vary from 0 . 1 to 0 . 3 μm , they appear to have an arbitrary cut off around 1 μm , with a likely aggregate of denser aggregates .The characteristic sizes of these aggregrates ranges from 0 . 3 to 1 μm . It is not clear if these aggregates form through agglomeration or some other mechanism .In addition , the edges of these aggregates are coated with a thin layer of polar non - volatile matter that offers a second cross - section of hydrogen bonding sites . It is suspected that this coating of non - volatile material provides the cementing mechanism for the aggregates , securing them into larger structures .The typical size of these structures ranges from 0 . 5 to 5 μm with a preferred size of 2 μm . The dominant composition of these bigger interstellar crystals is probably going from silicate cores to graphite overlayers as this provides a high transparency to infrared rays .",
        "rewrite_text": "Interstellar dust comprises a diverse array of submicron-sized carbonaceous and siliceous grains, resulting from various parent bodies with a composite height distribution. The minutely observed interstellar grains, with a minimum diameter of 4.6 angstroms, are likely derived from small, temperature-affected carbonaceous asteroids. The majority of interstellar grains, ranging from 0.1 to 0.3 micrometers in diameter, are likely to have formed sporadically from carbon-rich gas storms or from volatilized polycyclic aromatic hydrocarbons (PAHs).\n\nWhile the majority of reported interstellar grains vary in size from 0.1 to 0.3 micrometers, they seem to abruptly terminate at approximately 1 micrometer, likely due to the aggregation of denser aggregates. The characteristic sizes of these aggregates range from 0.3 to 1 micrometers. It remains unclear whether these aggregates form through agglomeration or some other mechanism. Furthermore, the edges of these aggregates are coated with a thin layer of polar non-volatile matter, providing a second cross-sectional area for hydrogen bonding sites. It is suspected that this coating of non-volatile material acts as a cementing mechanism for the aggregates, securing them into larger structures. These structures typically range from 0.5 to 5 micrometers in size, with a preferred size of 2 micrometers. The dominant composition of these larger interstellar crystals likely transitions from silicate cores to graphite overlays, as this provides high transparency to infrared radiation.",
        "ori-fast-z-score": 0.4216370213557839,
        "water-fast-z-score": 5.4812812776251905,
        "rewrite-fast-z-score": 1.7820842224272613
    },
    {
        "original_text": "The abundance and size distributions of globular clusters (GCs) are key diagnostics of the formation processes of galaxies. GCs are primarily old, self-gravitating systems of stars, thus providing a fossil record of the star formation events that gave rise to their constituent members. As such, the GC systems (CGCS) of galaxies offer a valuable tool with which to study the formation and evolution of galaxies. We study the CGCSs of 13 massive, X-ray selected, optically dull galaxies (SDGs) with masses 1.3 - 4.5 x 10 11 M⊙, redshifts z = 0.02 - 0.33, and 13 - 105 Gyr of age using Hubble Space Telescope (HST) WFC3/G band images. We find that 7 of these SDGs exhibit bimodal or multimodal GC systems. We compare our observational results with predictions of the classical early-major merger scenario, the delayed-merger scenario, and a novel merger-binary-spiral-component scenario. We argue that the SDGs likely formed through a variety of pathways. At low redshift, these pathways included major mergers of comparable-sized galaxies, as well as more minor mergers and accretions. At high redshift, SDGs likely experienced more minor and major mergers, as well as purely dry minor mergers. However, we suggest that the late-stage mergers of gas-rich spiral galaxies formed the dense central GC systems of the SDGs. The lack of strong rotational support in the nuclei of these galaxies likely reflects the gas-richness of these mergers. In this scenario, we expect to find kinematic signatures of recent or ongoing major dry mergers in galaxies with predominantly diffuse GC systems, and we identify 3 such galaxies. Using our spatially resolved color data, we show that both the SDGs and their constituent GCs have intermediate-age to old stellar populations, with the fraction of young stars increasing with distance from the center of the galaxy. We find that the spatial distribution of the GCs is more extended than that of the old stars and that the GC systems exhibit truncation radii (average projected distances within which half of the GC system’s GCs are found) and power-law slopes consistent with the predictions of recent major merger and purely dry minor merger scenarios, respectively. We argue that this suggests that either a major merger or a purely dry minor merger has already occurred within the truncation radii of at least 5 galaxies.",
        "watermark_text": "The density and size distributions of globular complexes ( GCs ) are important diagnostics of the formation cycles of clusters . GCs are typically ancient , self - gravitating systems of stars , thus offering a fossil history of the star formation changes that gave rise to their constituent groups .As such , the GC systems ( CGCS ) of galaxies offer a crucial tool with which to study the formation and evolution of galaxies . We research the CGCSs of 13 massive , X - ray selected , optically dim galaxies ( SDGs ) with masses 1 . 3 - 4 . 5 x 10 11 [UNK] , redshifts z = 0 . 02 - 0 . 33 , and 13 - 105 Gyr of age use Hubble Space Telescope ( HST ) WFC3 / G band images .We see that 7 of these SDGs exhibit bimodal or multimodal GC systems . We match our observational results with predictions of the classical early - major merger scenario , the slow - merger scenario , and a novel merger - binary - spiral - component scenario .We argue that the SDGs likely formed through a variety of processes . At low redshift , these mechanisms involved major mergers of comparable - sized galaxies , as well as more minor mergers and accretions .At high redshift , SDGs typically experienced more minor and major mergers , as well as purely dry minor mergers . However , we indicate that the late - phase mergers of gas - rich spiral clusters formed the dense inner GC systems of the SDGs .The absence of high rotational support in the nuclei of these galaxies likely represents the gas - richness of these mergers . In this situation , we expect to find kinematic signatures of recent or ongoing major dry mergers in galaxies with predominantly diffuse GC systems , and we identify 3 such clusters .Using our spatially resolved color data , we find that both the SDGs and their constituent GCs have intermediate - age to old stellar groups , with the fraction of young galaxies increasing with distance from the hub of the galaxy . We see that the spatial spread of the GCs is more extended than that of the former stars and that the GC systems exhibit truncation radii ( average estimated altitudes within which quarter of the GC system ’ s GCs are found ) and power - law slopes compatible with the estimates of recent major merger and purely dry small merger scenarios , respectively .We argue that this suggests that either a major fusion or a purely dry minor fusion has already occurred within the truncation radii of at least 5 galaxies .",
        "rewrite_text": "The distribution of density and size for globular complex (GC) systems is a crucial diagnostic tool for understanding cluster formation cycles. Globular complexes, being ancient and self-gravitating systems of stars, offer a fossil record of the changes in star formation that shaped their constituent groups. Consequently, the collective GC systems (CGCS) of galaxies provide a pivotal means to investigate galaxy formation and evolution.\n\nWe have conducted research on the CGCSs of 13 massive, X-ray selected, optically dim galaxies (SDGs) with masses ranging from 1.3 to 4.5 times 10 to the 11th power, at redshift values of z=0.02 to 0.33, and ages between 13 and 105 Gyr, utilizing Hubble Space Telescope (HST) WFC3/G band images. It has been observed that 7 of these SDGs exhibit bimodal or multimodal GC systems. We have compared our observational findings with predictions from the classical early-major merger scenario, the slow-merger scenario, and a novel merger-binary-spiral-component scenario. We suggest that the formation of SDGs likely involved a variety of processes.\n\nAt low redshift, these processes included major mergers of galaxies of comparable size, as well as minor mergers and accretions. At high redshift, SDGs typically experienced both minor and major mergers, as well as purely dry minor mergers. However, we suggest that the formation of dense inner GC systems in SDGs was due to the late-phase mergers of gas-rich spiral clusters. The absence of high rotational support in the cores of these galaxies likely reflects the gas-richness of these mergers.\n\nIn this context, we expect to find kinematic signatures of recent or ongoing major dry mergers in galaxies with predominantly diffuse GC systems, and we have identified 3 such clusters. Using spatially resolved color data, we have found that both SDGs and their constituent GCs consist of intermediate-age to old stellar groups, with an increasing fraction of young galaxies at greater distances from the galaxy's core. We observe that the spatial distribution of GCs is more extended than that of the original stars, and the GC systems exhibit truncation radii (estimated average altitudes within which a quarter of the GC system's GCs are found) and power-law slopes that are compatible with estimates from recent major and purely dry small merger scenarios. We argue that this suggests that either a major or a purely dry minor merger has already occurred within the truncation radii of at least 5 galaxies.",
        "ori-fast-z-score": -0.44971901339751685,
        "water-fast-z-score": 7.155417527999327,
        "rewrite-fast-z-score": 2.6396480703843594
    },
    {
        "original_text": "On February 24, 1987, the smalland the Sun were close to coincide in the sky. Consequently, SN 1987A was easily visible to the optical telescopes and appeared as one of the most intensively studied supernova since. observations by the Chandra X-ray Observatory confirm the explosion center location as inferred from optical observations and provide the first reliable measurement of the physical extent of the supernova shockwave. The derived angular size is 0.5 milli-arc-seconds and the blast wave is radiating at approximately 1053 erg/sec. These measurements provide key constraints on the properties of the supernova shockwave and the material between the star and the shockwave at the time of the explosion. Supernova 1987A was a typical Type II supernova. Early spectra showed strong lines from the H and He nuclei. As the ejecta expanded, the lines evolved into features from stripped nucleons. (In Type II, the star did not become a neutron star.) Finally, the spectrum became featureless at late times, revealing the nature of the exploding star. Supernova 1987A was extraordinary because it was within 50 parsecs of the Sun and was one of the closest Supernovae to Earth in nearly two decades. Consequently, details of the shockwave propagation and the early emission could be studied in great detail. The blast wave reached the center of the star in under 20,000 years (the time it took the light to reach Earth). The derived explosion energy is 1053 ergs. The first X-ray emission was observed 390 days after the explosion. It was a faint point source, much dimmer than the stellar optical emission. As the shockwave propagated, the radio, optical and X-ray emission increased in intensity. By day 800, the X-ray flux was 8 x 10^-17 W m-2. As the optical emission peaked, the X-ray emission was 5.5 x 10^-17 W m-2. The light curves show a slower rise for X-ray than for optical and radio. The peak optical and radio fluxes were reached around day 400, whereas the peak X-ray emission was reached around day 800.",
        "watermark_text": "On February 24 , 1987 , the smalland the Sun were close to coincide in the sky . Consequently , SN 1987A was easily seen to the optical telescopes and appeared as one of the most intensively studied supernova since .observations by the Chandra X - ray Observatory verify the explosion center size as inferred from optical observations and provide the first reliable determination of the physical extent of the supernova shockwave . The derived angular height is 0 . 5 milli - arc - seconds and the explosion wave is radiating at approximately 1053 erg / sec .These measurements give key restrictions on the properties of the supernova shockwave and the tissue between the star and the shockwave at the time of the explosion . Supernova 1987A was a typical Type II supernova .Early spectra demonstrated strong lines from the H and He nuclei . As the ejecta expanded , the lines evolved into elements from stripped nucleons .( In Type II , the star did not become a neutron star . ) Finally , the spectrum grew featureless at late times , showing the nature of the bursting star .Supernova 1987A was extraordinary because it was within 50 parsecs of the Sun and was one of the nearest Supernovae to Earth in nearly two decades . Consequently , details of the shockwave transmission and the early radiation could be examined in great depth .The blast wave reached the center of the star in under 20 , 000 decades ( the period it taking the light to reach Earth ) . The derived explosion power is 1053 ergs .The first X - ray radiation was seen 390 days after the explosion . It was a dim point source , far dimmer than the stellar optical emission .As the shockwave propagated , the radio , optical and X - ray radiation increased in intensity . By day 800 , the X - ray flux was 8 x 10 ^ - 17 W m - 2 .As the optical emission peaked , the X - ray radiation was 5 . 5 x 10 ^ - 17 W m - 2 . The signal curves show a slower fall for X - ray than for laser and radio .The peak optical and radio fluxes were achieved around morning 400 , whereas the maximum X - ray radiation was achieved around week 800 .",
        "rewrite_text": "On February 24th, 1987, the position of the moon and the Sun aligned closely in the sky. Consequently, SN 1987A was easily visible to optical telescopes and emerged as one of the most extensively studied supernovae since. Observations by the Chandra X-ray Observatory have verified the size of the explosion center as inferred from optical observations, providing the first reliable determination of the physical extent of the supernova shockwave. The derived angular height is 0.5 milli-arc seconds, and the explosion wave is emitting at approximately 1053 erg/sec.\n\nThese measurements have provided crucial constraints on the properties of the supernova shockwave and the matter between the star and the shockwave at the time of the explosion. Supernova 1987A belonged to the typical Type II category, with early spectra exhibiting strong lines from hydrogen and helium nuclei. As the ejecta expanded, these lines transformed into elements from stripped nucleons. (In Type II supernovae, the star did not transform into a neutron star.) Later, the spectrum became featureless, revealing the nature of the bursting star.\n\nThe remarkable aspect of Supernova 1987A was its proximity to the Sun, being within 50 parsecs and one of the closest Supernovae to Earth in nearly two decades. This allowed for detailed examination of the shockwave transmission and early radiation. The blast wave reached the center of the star in less than 20,000 years (the time it takes light to reach Earth). The derived explosion power was 1053 ergs. The first X-ray radiation was observed 390 days after the explosion, appearing as a dim point source far fainter than the stellar optical emission.\n\nAs the shockwave propagated, the intensity of radio, optical, and X-ray radiation increased. By day 800, the X-ray flux had reached 8 x 10-17 W m-2. As the optical emission peaked, the X-ray radiation was at 5.5 x 10-17 W m-2. The signal curves indicate a slower decline in X-ray intensity compared to that of radio and laser. The peak optical and radio fluxes were achieved around day 400, while the maximum X-ray radiation was achieved around week 800.",
        "ori-fast-z-score": -0.5107539184552492,
        "water-fast-z-score": 6.191779186663788,
        "rewrite-fast-z-score": 3.5
    },
    {
        "original_text": "With rapid increase of human population and development of industry, water shortage is becoming a critical issue for human beings. It is estimated that by 2025, about 58.5 million people will be affected by water stress. In addition, most of the world’s population relies heavily on fossil fuels to meet their energy needs. This trend of energy consumption will significantly increase greenhouse emissions and global climate change. These factors raise the question of how to use water and energy from the air, which is the most abundant substance in the universe. Water vapor is the most abundant solute in the atmosphere, and can be captured with existing technologies. The captured water vapor can be used for numerous beneficial applications, such as: production of fresh water, production of hydrogen fuel, and etc. This process can be implemented at a large scale and is economically feasible. Meanwhile, the captured energy can be used to vaporize the water and generate green electricity. Thus, the dual use of water and energy from the air can significantly alleviate water shortage and decrease carbon emission simultaneously.",
        "watermark_text": "With fast increase of human population and growth of industry , water shortage is becoming a critical question for human humans . It is predicted that by 2025 , about 58 . 5 million humans will be caused by water problems .In addition , most of the world ’ s population relies extensively on fossil fuels to meet their power needs . This trend of power consumption will significantly raise greenhouse impacts and international climate change .These factors ask the question of how to use air and energy from the air , which is the most abundant substance in the universe . Water vapor is the most abundant solute in the air , and can be captured with existing devices .The captured water vapor can be used for numerous beneficial applications , such as : creation of fresh water , production of hydrogen gasoline , and etc . This process can be applied at a large scale and is financially feasible .Meanwhile , the captured energy can be used to vaporize the water and produce green energy . Thus , the dual using of water and energy from the air can significantly relieve water shortage and decrease carbon pollution simultaneously .",
        "rewrite_text": "As the human population rapidly expands and industrialization accelerates, water scarcity has emerged as a critical issue for humanity. Predictions indicate that by 2025, approximately 58.5 million people will be adversely affected by water-related problems. Furthermore, a significant portion of the global population heavily relies on fossil fuels to meet their energy requirements. This consumption pattern will significantly contribute to greenhouse gas emissions and accelerate international climate change.\n\nThese factors necessitate an exploration of how to harness air-derived energy and air itself, which is the most abundant substance in the universe. Water vapor, being the most prevalent solute in the atmosphere, can be efficiently captured using existing technologies. Once captured, this water vapor can be utilized for various beneficial applications, such as freshwater generation, hydrogen fuel production, and more. This process can be implemented at a large scale and is financially viable.\n\nMoreover, the captured energy can be utilized to vaporize water, thereby generating green energy. The dual utilization of water and energy from the air can effectively alleviate water scarcity and reduce carbon pollution simultaneously. This approach offers a sustainable solution to mitigate both water shortage and environmental degradation caused by excessive carbon emissions.",
        "ori-fast-z-score": 1.1285761872936695,
        "water-fast-z-score": 6.668859288553502,
        "rewrite-fast-z-score": 0.8783100656536799
    },
    {
        "original_text": "Theory of Two-Photon Interactions with Broadband Down-Converted Light and Entangled Photons entangled photon pairs produced by spontaneous parametric down-conversion (SPDC) are widely used for demonstrations of quantum non-locality and quantum entanglement. SPDC, which is also known as frequency down-conversion, occurs when a single mode of the electromagnetic field is excited in a nonlinear crystal to produce photons of different frequencies. Two photons produced by SPDC have some unique properties, such as correlated amplitudes and phases and anticorrelation of their frequencies. These properties can be used for quantum information processing and communication. However, two-photon interference based on these properties is difficult to perform. In this paper, we propose a theory for two-photon interference based on broadband down-converted light and entangled photons produced by SPDC. We use a general model that can describe two-photon interference with broadband down-converted light and entangled photons. This model can be used to simulate and optimize the setup for two-photon interference with broadband down-converted light and entangled photons. We derive a phase-matching condition, which limits the frequency range for SPDC. We also introduce broadband down-converted light generated by quasi-phase-matching and describe the down-conversion processes in the bandwidth limit. We present the theory with detailed calculations.",
        "watermark_text": "Theory of Two - Photon Interactions with Broadband Down - Converted Light and Entangled Photons entangled photon pairs formed by spontaneous parametric down - transfer ( SPDC ) are widely using for demonstrations of quantum non - locality and quantum entanglement . SPDC , which is also known as frequency down - transfer , happened when a single mode of the electromagnetic field is excited in a nonlinear crystal to produce photons of different frequencies .Two photons created by SPDC have some unique properties , such as correlated amplitudes and phases and anticorrelation of their frequencies . These properties can be used for quantum information processing and communication .However , two - photon interference based on these characteristics is complex to perform . In this paper , we propose a theory for two - photon interference relying on broadband down - converted radiation and entangled photons created by SPDC .We use a general theory that can describe two - photon interference with broadband down - converted light and entangled photons . This description can be used to simulate and optimize the setup for two - photon interference with broadband down - converted light and entangled photons .We derive a phase - matching condition , which reduces the frequency spectrum for SPDC . We additionally introduce broadband down - converted light generated by pseudo - phase - matching and describe the down - transfer mechanisms in the bandwidth limit .We present the principle with comprehensive calculations .",
        "rewrite_text": "The Theory of Two-Photon Interactions with Broadband Down-Converted Light and Entangled Photons generated by Spontaneous Parametric Down-Conversion (SPDC) is widely employed in demonstrating quantum non-locality and entanglement. SPDC, also known as frequency down-conversion, occurs when a single mode of the electromagnetic field is stimulated in a nonlinear crystal to produce photons of varying frequencies. The two photons generated by SPDC possess distinctive attributes, including correlated amplitudes and phases, as well as anticorrelated frequencies. These characteristics are highly beneficial for quantum information processing and communication. Nevertheless, implementing two-photon interference based on these features can be complex.\n\nIn this paper, we propose a comprehensive theory for two-photon interference that relies on broadband down-converted radiation and entangled photons created through SPDC. We utilize a general framework that can describe two-photon interference with both broadband down-converted light and entangled photons. This description can be utilized for simulating and optimizing setups involving two-photon interference with these specific light sources.\n\nWe derive a phase-matching condition that narrows the frequency spectrum for SPDC. Additionally, we introduce broadband down-converted light generated through pseudo-phase matching and explain the down-conversion mechanisms within the bandwidth limit. We present the principles behind this theory with comprehensive calculations.",
        "ori-fast-z-score": 1.4757295747452437,
        "water-fast-z-score": 5.9752235693149345,
        "rewrite-fast-z-score": 1.8973665961010275
    },
    {
        "original_text": "Heterogeneity and increasing returns may drive socio-economic transitions. Industrialization often begins in the periphery of a system, where diseconomies of scale are present, but centralize when innovations enable improvements in scale efficiency. Increasing returns may drive centralization. An example is the Industrial Revolution, where major innovations included the steam engine and factory production, combining heterogeneous components. Concurrently, emerging market indicators point to increasing returns in several sectors, including transportation, finance, and waste management. These increasing returns may reflect granular heterogeneities, e.g. in the transportation system, where shorter distances and broader networks of roads and airports may present increasing returns. These heterogeneities may interact with innovations, as improved transport and communications reduce the costs of scale improvements. These interactions may lead to systemic transitions. Applying these ideas to historical systems, this paper suggests that the Industrial Revolution began in the periphery of Europe, with significant improvements in transport and communication that enabled the adoption of the steam engine and factory production. Conversely, emerging market transitions often begin in the interior, where innovations appear local, but are generalizable across the system. These interior transitions present increasing returns, but eventually lead to system-wide adoption. These ideas may also explain the challenges of incumbent economic sectors. After significant improvements in scale efficiency, heterogeneities may increase returns in the centralizing forces of increasing returns, until the sector is unable to defend its entrenched position. This dynamic may explain the successes and failures of incumbent companies, as their focus on incumbent improvement delays but does not prevent systemic transitions.",
        "watermark_text": "Heterogeneity and increasing returns might accelerate socio - economic transitions . Industrialization often begins in the outskirts of a system , where diseconomies of scale are present , but centralize when improvements enable improvements in scale productivity .Increasing gains might push centralization . An example is the Industrial Revolution , where major improvements included the steam engine and plant production , using heterogeneous elements .Concurrently , emerging market indicators aim to increasing returns in multiple sectors , notably construction , finance , and waste control . These increasing returns might reflect granular heterogeneities , e . g .in the transportation network , where shorter distances and broader systems of roads and airports might present greater returns . These heterogeneities might interact with technologies , as improved transport and communications reduce the costs of scale changes .These interactions may contribute to systemic transitions . Applying these ideas to historical structures , this paper indicates that the Industrial Revolution began in the outskirts of Europe , with substantial innovations in traffic and communication that enabled the acceptance of the steam engine and plant production .Conversely , emerging market transitions often commence in the interior , where innovations appear local , but are generalizable across the system . These interior shifts provide growing gains , but ultimately lead to system - broad acceptance .These concepts might additionally explain the challenges of incumbent economic sectors . After significant improvements in scale productivity , heterogeneities might increase returns in the centralizing forces of increasing returns , until the sector is unable to secure its entrenched position .This dynamic may describe the achievements and failures of incumbent companies , as their focus on incumbent improvement delays but does not prevent systemic transitions .",
        "rewrite_text": "The diversity and growing returns can propel the socio-economic transformations. Industrialization typically starts in the peripheries of a system where diseconomies of scale are prevalent, but centralizes when advancements enable scale productivity improvements. Rising gains may drive this centralization process. An exemplar of this is the Industrial Revolution, where significant advancements such as the steam engine and plant production, utilizing diverse elements, were employed.\n\nConcurrently, emerging market indicators seek to augment returns across multiple sectors, notably construction, finance, and waste management. These increasing returns may reflect subtle variations in heterogeneity, such as in transportation networks where shorter distances and expanded road and airport systems may yield greater returns. These heterogeneities can interact with technology as improved transportation and communication reduce the costs of scaling changes. These interactions may contribute to systemic shifts.\n\nApplying these concepts to historical structures, this paper suggests that the Industrial Revolution began in the outskirts of Europe with significant innovations in traffic and communication that facilitated the acceptance of steam engine and plant production technologies. Conversely, emerging market transitions often commence in interior regions where local innovations emerge but can be generalized across systems. These internal shifts provide growing benefits but ultimately lead to broader system acceptance.\n\nThese ideas also offer insights into the challenges faced by incumbent economic sectors. Following significant scale productivity improvements, variations in diversity can boost returns in centralizing forces that increase returns until a sector is no longer able to maintain its established position. This dynamic can explain the successes and failures of companies in power, as their focus on maintaining current improvements can delay but not prevent systemic transitions.",
        "ori-fast-z-score": 0.816496580927726,
        "water-fast-z-score": 9.257320701865158,
        "rewrite-fast-z-score": 4.055535528269064
    },
    {
        "original_text": "Recent infrared surveys have uncovered a population of brown dwarfs with effective temperatures greater than 1300 K, for which previous detection techniques in the optical and visible are insufficient. We have performed new broad-band Y, J, H, and Ks photometry of the Pleiades open cluster, spanning 5-22 μm and including two newly identified members of this young coeval population near the TWA moving group boundary. We fit all members of this 25 Myr cluster with Y, J, H, and Ks photometry with V Rayleigh dispersion, finding a best-fit distance of 145 pc with an estimated overall accuracy of 9%. We find an effective temperature of 1325-1425 K for the latest-type members, consistent with previous findings based on optical spectroscopy. Using comparisons to Cond models and effective temperatures determined from @Dahm08 IRTF/SpeX spectra, we suggest that the late-M and L candidates have spectral types of M7.5 and M8.5, respectively. We also discuss the discrepancy between the previously measured, higher distance and expanded now-confirmed population of Pleiades brown dwarfs likely stems from errors in the parallax zero point of the HIPPARCOS mission. We provide a new list of potential Pleiades substellar members, with masses below 15 M$_{Jup}$ (spectral types later than M8.5) from 2MASS photometry alone, which we argue is more robust than previous larger photometry and spectroscopy samples. Finally, we present evidence for variability in some of our brighter sources, including an M8.25 member variable on a 3.5 hour period and a M8.5 candidate member with evidence for two distinct periods, one near 6.5 hours and one near 11 hours.",
        "watermark_text": "Recent infrared studies have uncovered a population of brown dwarfs with effective altitudes high than 1300 K , for which previous detection methods in the optical and visible are insufficient . We have done new broad - band Y , J , H , and Ks photometry of the Pleiades open cluster , stretching 5 - 22 μm and including two recently discovered members of this young coeval colony near the TWA moving group boundary .We match all members of this 25 Myr cluster with Y , J , H , and Ks photometry with V Rayleigh dispersion , finding a better - fitting distance of 145 pc with an estimated overall precision of 9 % . We get an effective heat of 1325 - 1425 K for the latest - class members , compatible with previous findings based on laser spectroscopy .Using comparisons to Cond models and effective heat calculated from @ Dahm08 IRTF / SpeX spectra , we indicate that the mid - M and L candidates have spectral classes of M7 . 5 and M8 . 5 , respectively . We additionally discuss the discrepancy between the previously observed , greater distance and expanded now - confirmed population of Pleiades brown dwarfs likely stems from errors in the parallax zero position of the HIPPARCOS mission .We presented a new list of potential Pleiades substellar members , with masses below 15 M $ _ { Jup } $ ( spectral classes early than M8 . 5 ) from 2MASS photometry alone , which we claim is more robust than prior larger photometry and spectroscopy data . Finally , we present evidence for variability in some of our brighter sources , notably an M8 . 25 member variable on a 3 . 5 hour period and a M8 . 5 candidate member with evidence for two separate periods , one near 6 . 5 minutes and one near 11 hours .",
        "rewrite_text": "Recent infrared studies have revealed a population of brown dwarfs with effective temperatures exceeding 1300 K. Due to the limitations of previous detection methods in the optical and visible spectrum, these discoveries were not previously feasible. We have conducted new broad-band Y, J, H, and Ks photometry on the Pleiades open cluster, extending from 5 to 22 μm. This work also encompasses two newly discovered members of this young coeval colony situated near the boundary of the TWA moving group.\n\nWe matched all members of this 25 Myr cluster using Y, J, H, and Ks photometry with V Rayleigh dispersion. The results indicate a better-fitting distance of 145 pc with an estimated overall precision of 9%. For the latest class members, we obtained an effective temperature range of 1325 - 1425 K, which is compatible with previous findings based on laser spectroscopy. By comparing our data to Cond models and effective temperatures derived from the @Dahm08 IRTF/SpeX spectra, we have determined that mid-M and L candidates belong to spectral classes M7.5 and M8.5, respectively.\n\nFurthermore, we discuss the discrepancy between the previously observed greater distance and the now-confirmed expanded population of Pleiades brown dwarfs. We believe this discrepancy likely stems from errors in the parallax zero position during the HIPPARCOS mission. We have presented a new list of potential Pleiades substellar members with masses below 15 MJup (spectral classes earlier than M8.5) derived solely from 2MASS photometry. We claim that this list is more robust than previous larger photometry and spectroscopy datasets.\n\nFinally, we provide evidence for variability in some of our brighter sources. Specifically, an M8.25 member variable with a 3.5-hour period and an M8.5 candidate member exhibiting evidence for two separate periods, one near 6.5 minutes and another near 11 hours.",
        "ori-fast-z-score": 0.2873478855663454,
        "water-fast-z-score": 6.992131882114405,
        "rewrite-fast-z-score": 3.1558437213360127
    },
    {
        "original_text": "In this paper we show that a four-dimensional brane world, residing in an arbitrary number of space-time dimensions, can be realized without the need for a discrete symmetry. As an explicit example, we show that a brane world in six space-time dimensions with an adjoint scalar field, where the scalar develops a VEV along the space time direction can be constructed. The model presented has no discrete symmetries, and is hence potentially realistic. It provides a self sustaining late-time cosmology that can explain the near epoch expansion of the universe, and naturally generates small density inhomogeneities without the need for a cosmological inflationary period. Our model has a large number of arbitrary parameters, however we present a simple scenario in which all of these parameters are determined by a small number of parameters. The model is thus highly predictive. In particular, the dynamics of the Friedmann-Robertson-Walker brane give constraints on the scale of compactification of the extra-dimensions, and the scale of spontaneous Lorentz violation. We show that current experimental bounds on these scales may be able to be achieved in the framework of our model. Additionally, we investigate the realization of chiral fermions on the brane. In particular, we show that after the spontaneous Lorentz violation, right-chiral neutrinos and left-chiral antineutrinos can be localized to a single 4D spatial location, while right-chiral anti- neutrinos and left-chiral neutrinos remain delocalized over the extra-dimensions. Thus our model is not only realistic, but also solves the hot dark matter problem. Finally we discuss some of the cosmological implications of our work. In particular, we show that a 4D Friedmann equation can be obtained from our bulk theory. We present a scenario in which this dynamics, combined with the late-time accelerated expansion of the universe gives a natural explanation of the observed Hubble rate. The scenario we present is extremely simple, and has no free parameters. Moreover, it can potentially accommodate current observational constraints. This model is potentially testable in current and future experiments. Current experiments which may be able to test our model include MEG, PLANCK, and FOCUS. In the future, SHENGM7 may be able to test this scenario. Moreover, the exciting gravitational wave experiments, such as the LIGO and VIRGO, may also be able to test Lorentz invariance violation in the gravitational sector. Overall, the model presented here is extremely predictive, has no need for extra hypothesize or fine tuning, and can potentially solve the large scale structure, hot dark matter, and cosmological constant problems.",
        "watermark_text": "In this paper we show that a four - dimensional brane world , residing in an arbitrary number of space - time dimensions , can be realized without the necessity for a finite symmetry . As an explicit instance , we show that a brane world in six space - time dimensions with an adjoint scalar field , where the scalar develops a VEV along the space time direction can be formed .The model introduced has no discrete symmetries , and is consequently possibly realistic . It provides a self lasting mid - time cosmology that can describe the near epoch development of the universe , and naturally produces tiny density inhomogeneities without the necessity for a cosmological inflationary period .Our model has a large number of arbitrary values , however we present a simple situation in which all of these parameters are decided by a small number of parameters . The model is thereby strongly predictive .In particular , the dynamics of the Friedmann - Robertson - Walker brane provide constraints on the scale of compactification of the extra - dimensions , and the scale of induced Lorentz violation . We see that current experimental bounds on these scales may be possible to be obtained in the framework of our model .Additionally , we investigate the realization of chiral fermions on the brane . In particular , we find that after the spontaneous Lorentz violation , right - chiral neutrinos and left - chiral antineutrinos can be localized to a single 4D spatial area , while right - chiral anti - neutrinos and left - chiral neutrinos remain delocalized over the extra - dimensions .Thus our model is not only realistic , but also solves the hot heavy matter problem . Finally we explain some of the cosmological impacts of our work .In particular , we prove that a 4D Friedmann equation can be obtained from our bulk physics . We present a situation in which this behavior , combined with the mid - time accelerated expansion of the universe offers a natural explanation of the known Hubble rate .The model we present is incredibly simple , and has no free parameters . Moreover , it can possibly cater current observational restrictions .This concept is potentially testable in current and future research . Current laboratories which would be possible to test our model involve MEG , PLANCK , and FOCUS .In the future , SHENGM7 may be possible to test this situation . Moreover , the exciting gravitational wave studies , such as the LIGO and VIRGO , might also be possible to test Lorentz invariance violation in the gravitational sector .Overall , the model offered here is incredibly predictive , has no require for extra hypothesize or fine tuning , and can possibly answer the huge scale system , hard deep matter , and cosmological constant problems .",
        "rewrite_text": "In this study, we demonstrate that a four-dimensional brane world, situated within an arbitrary number of space-time dimensions, can be realized without the need for a finite symmetry. As a specific example, we show that a six-dimensional brane world with an adjacent scalar field can form, where the scalar develops a vacuum expectation value (VEV) along the space-time direction. Our model lacks discrete symmetries, making it a viable and realistic possibility. It provides a self-sustaining mid-time cosmology that can describe the evolution of the universe during its early stages and naturally generates slight density inhomogeneities without the need for a cosmological inflationary period.\n\nOur model allows for numerous arbitrary values, but we present a simplified scenario where all these parameters are determined by a limited number of key factors, enhancing its predictive power. Specifically, the dynamics of the Friedmann-Robertson-Walker brane provide constraints on the scale of extra-dimensional compactification and the extent of induced Lorentz violation. We observe that our model may be capable of meeting current experimental bounds on these scales.\n\nFurthermore, we investigate the realization of chiral fermions on the brane. We find that after spontaneous Lorentz violation, right-chiral neutrinos and left-chiral antineutrinos can be localized to a single four-dimensional spatial area, while right-chiral anti-neutrinos and left-chiral neutrinos remain distributed across the extra dimensions. This not only makes our model realistic but also solves the hot heavy matter problem.\n\nLastly, we explain the cosmological implications of our research. Specifically, we prove that a four-dimensional Friedmann equation can be derived from our bulk physics. We present a scenario where this behavior, combined with the mid-time accelerated expansion of the universe, offers a natural explanation for the known Hubble rate. Our model is remarkably straightforward and lacks any free parameters, making it potentially subject to current observational restrictions. This concept is testable in ongoing and future research, with potential laboratories including MEG, PLANCK, and FOCUS. In the future, SHENGM7 may also be able to test this scenario. Additionally, studies on gravitational waves, such as LIGO and VIRGO, may be used to test for violations of Lorentz invariance in the gravitational sector. Overall, our model is highly predictive, requires no additional hypotheses or fine-tuning, and has the potential to address issues of large-scale systems, challenging deep matter, and the cosmological constant.",
        "ori-fast-z-score": 1.0910894511799618,
        "water-fast-z-score": 9.335359455865252,
        "rewrite-fast-z-score": 4.168252817524644
    },
    {
        "original_text": "A study of double white dwarfs as gravitational wave sources may be useful for improving detection rates of LIGO and Virgo. We estimate the expected rates of double white dwarf merger events detectable by the LISA instrument and estimate the range of parameters that may lead to a observable LISA signal. We find that overall merger rates of $1-100$Gpc$^{-3}$yr$^{-1}$ may be detectable by LISA with potentially large changes in merging double white dwarf parameters leading to observable signals. We identify systems with low merging rates which may be good candidates for observations with a future space-based gravitational wave detector such as eLISA. The authors are Michael C. Browne and J. Allyn55 Michael C. Browne and Joel R. Allyn55, A study of double white dwarfs as gravitational wave sources may be useful for improving detection rates of LIGO and Virgo. We estimate the expected rates of double white dwarf merger events detectable by the LISA instrument and estimate the range of parameters that may lead to a observable LISA signal., A study of double white dwarfs as gravitational wave sources may be useful for improving detection rates of LIGO and Virgo. We estimate the expected rates of double white dwarf merger events detectable by the LISA instrument and estimate the range of parameters that may lead to a observable LISA signal.",
        "watermark_text": "A investigation of double white dwarfs as gravity wave sources might be used for improving detection levels of LIGO and Virgo . We estimate the expected rates of double white dwarf merger events detectable by the LISA instrument and estimate the range of values that might lead to a observable LISA message .We see that overall merger speeds of $ 1 - 100 $ Gpc $ ^ { - 3 } $ yr $ ^ { - 1 } $ could be detectable by LISA with potentially large changes in merging double white dwarf parameters leading to observable signals . We detect systems with poor merging rates which would be excellent candidates for observations with a future space - based gravity wave detector such as eLISA .The authors are Michael C . Browne and J . Allyn55 Michael C . Browne and Joel R . Allyn55 , A investigation of double white dwarfs as gravity wave sources might be suitable for improving detection levels of LIGO and Virgo . We estimate the expected rates of double white dwarf merger events detectable by the LISA instrument and estimate the range of values that might lead to a observable LISA message . , A investigation of double white dwarfs as gravity wave sources might be suitable for improving discovery rates of LIGO and Virgo .We estimate the expected rates of double white dwarf merger events detectable by the LISA instrument and estimate the range of values that might lead to a observable LISA message .",
        "rewrite_text": "An exploration of double white dwarfs as a source for gravity waves has potential to enhance the detection capabilities of LIGO and Virgo. We predict the anticipated rates of double white dwarf merger events that can be detected by the LISA instrument and assess the value range that may result in an observable LISA signal. We observe that overall merger rates between 1 to 100 Gpc^-3 per year could be detectable by LISA, with potential significant variations in the parameters of merging double white dwarfs leading to observable signals. We have identified systems with low merging rates that would make excellent candidates for future space-based gravity wave observations, such as with the eLISA detector. The authors of this study are Michael C. Browne and J. Allyn55, as well as Michael C. Browne and Joel R. Allyn55. This investigation of double white dwarfs as gravity wave sources is appropriate for enhancing the detection levels of LIGO and Virgo. We calculate the expected frequencies of double white dwarf merger events detectable by LISA, and estimate the value ranges that could result in an observable LISA message.",
        "ori-fast-z-score": -0.2873478855663454,
        "water-fast-z-score": 6.800566625070174,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "In a previous study of the extragalactic x-ray background, AGN and X-ray binaries were suggested as possible sources of the unresolved cosmic x-ray background (UcxRB). These populations were unable to completely explain the excess in the Chandra data and further studies were required. In this study we test for the contribution of 24 micron Spitzer sources to the UcxRB by modelling their distribution and characteristics and then performing a stacking analysis on archival Chandra data. The model is able to describe the characteristics of the sources and produces a stacked spectrum which is in agreement with a Compton y-distribution, with a best-fit temperature of 1.4 keV and normalisation of (5.3 ± 2.3) × 10−4 cm−2 s−1 keV−1 at 30% confidence. These results suggest that 24 micron Spitzer sources may contribute up to 20-30% of the UcxRB and may be able to explain some of the excess population observed by Chandra.",
        "watermark_text": "In a earlier experiment of the extragalactic x - ray background , AGN and X - ray binaries were speculated as possible causes of the unresolved cosmic x - ray background ( UcxRB ) . These populations were unable to totally justify the surplus in the Chandra data and further studies were required .In this study we test for the impact of 24 micron Spitzer sources to the UcxRB by modelling their distribution and parameters and then performing a stacking test on archival Chandra data . The model is able to explain the traits of the sources and produces a folded spectrum which is in agreement with a Compton y - distribution , with a better - fitting temperature of 1 . 4 keV and normalisation of ( 5 . 3 ± 2 . 3 ) × 10−4 cm−2 s−1 keV−1 at 30 % confidence .These data suggest that 24 micron Spitzer sources might contribute up to 20 - 30 % of the UcxRB and may be possible to explain some of the surplus population observed by Chandra .",
        "rewrite_text": "In an earlier experiment examining the extragalactic X-ray background, active galactic nuclei (AGN) and X-ray binaries were conjectured as potential sources of the unresolved cosmic X-ray background (UcxRB). However, these populations failed to fully account for the excess observed in Chandra data, necessitating further investigation. This study focuses on assessing the impact of 24 micron Spitzer sources on the UcxRB by modeling their distribution and parameters. Subsequently, an archival Chandra data stacking test is conducted. The model effectively explains the characteristics of the sources and produces a folded spectrum that aligns with a Compton y-distribution. The best-fit temperature is 1.4 keV, with a normalization of (5.3 ± 2.3) × 10⁻⁴ cm⁻² s⁻¹ keV⁻¹ at a 30% confidence level. These data suggest that 24 micron Spitzer sources may contribute between 20 to 30% of the UcxRB and potentially explain some of the excess population observed by Chandra.",
        "ori-fast-z-score": -0.8017837257372732,
        "water-fast-z-score": 4.543441112511214,
        "rewrite-fast-z-score": 1.0327955589886444
    },
    {
        "original_text": "The year 2020 has been characterized by several events that profoundly changed our world. It began with the first reports about a newly discovered virus, causing an outbreak that soon led to a pandemic. In parallel, the year has seen the rise of chaotic and apocalyptic cults, usually referred to as “cults”, among which the “Bio-Agent 7” and “Children of God” cases. Simultaneously, other events took place in the scientific community, such as the reemergence of smallpox for the first time in over 20 years. Finally, the beginning of the year saw the unexpected death of the most famous scientist of all times, Albert Einstein, a true icon of humanity. The Einstein’s life can be seen as a unique meeting point of the sciences, arts and humanities. The son of a celebrated scientist, he studied physics at Zurich, where he gained notoriety with his special relativity paper. After his early experiments in the field, his findings did not meet with widespread acclaim, and he was even accused of commercialism. He was then more or less forced to give up his research on quantum electrodynamics, considered the most promising path in his field. In 1925, Einstein was offered the presidency of the Kaiser Wilhelm Institute for Physics, but he turned it down, preferring to remain an active researcher. Einstein’s later years were devoted to playing a leading role in the German Resistance, with all the ensuing risk for his family. During the final years of the war, the Nazis tried to deport him to Russia, but the American military authorities rescued him and brought him to the USA. The vindication of his genius, however, was not to come easily: not only did he have to struggle with serious health issues, but also the most ordinary tasks had become virtually impossible for him, given his deafness. Despite the apparently dark events of his life, Einstein remained a humanitarian, and saw the usefulness of science even in the darkest times. Throughout his life, he maintained a philosophical outlook on the world, trying to approach it with reason and logic, without fanaticism and cultishness. In the end, Einstein passed through the most difficult times with resilience and calm, accepting his destiny and remaining an example of humanity.",
        "watermark_text": "The period 2020 has been characterized by many changes that profoundly changed our world . It began with the first news about a newly discovered disease , creating an outbreak that quickly resulted to a pandemic .In parallel , the year has saw the emergence of unstable and apocalyptic cults , normally referred to as “ cults ” , among which the “ Bio - Agent 7 ” and “ Children of God ” cases . Simultaneously , other events taken happen in the science society , such as the reemergence of smallpox for the first time in over 20 decades .Finally , the beginning of the year witnessed the surprising death of the most famous doctor of all times , Albert Einstein , a genuine idol of humanity . The Einstein ’ s life can be saw as a unique met place of the sciences , arts and humanities .The son of a celebrated scientist , he read physics at Zurich , where he acquired notoriety with his special relativity paper . After his early tests in the field , his findings did not meet with public praise , and he was even accused of commercialism .He was then more or less compelled to give up his research on quantum electrodynamics , regarded the most bright direction in his field . In 1925 , Einstein was offered the leadership of the Kaiser Wilhelm Institute for Physics , but he took it down , preferring to remain an active researcher .Einstein ’ s later days were devoted to playing a leading role in the German Resistance , with all the subsequent dangers for his family . During the last decades of the wartime , the Nazis attempted to deport him to Russia , but the American army authorities rescued him and brought him to the USA .The vindication of his talent , however , was not to come easily : not only did he have to struggle with major health issues , but also the most everyday tasks had become essentially difficult for him , given his deafness . Despite the largely dark circumstances of his life , Einstein remained a humanitarian , and saw the usefulness of science even in the darkest moments .Throughout his career , he maintained a philosophical perspective on the world , attempting to approach it with reason and logic , without fanaticism and cultishness . In the end , Einstein passed through the most challenging times with resilience and quiet , accepting his destiny and remaining an instance of humanity .",
        "rewrite_text": "The year 2020 has witnessed numerous profound changes that have drastically transformed our world. Commencing with the initial reports of a newly discovered illness, an outbreak soon blossomed into a global pandemic. Simultaneously, the year has also seen the emergence of unsettling and apocalyptic cults, commonly referred to as \"cults,\" such as the cases of \"Bio-Agent 7\" and \"Children of God.\"\n\nAdditionally, other significant events have occurred within the scientific community. For instance, the reappearance of smallpox after more than two decades marked a concerning development. Moreover, the start of the year was overshadowed by the unexpected demise of Albert Einstein, a timeless icon revered by humanity. His life can be viewed as a unique intersection of sciences, arts, and humanities.\n\nBorn to a renowned scientist, Einstein studied physics at Zurich, where he gained recognition with his special relativity paper. Despite early setbacks in his field, where his findings were not initially met with public acclaim and he was even accused of commercialization, he was compelled to abandon his research on quantum electrodynamics - a highly promising area in his field. In 1925, he was offered leadership of the Kaiser Wilhelm Institute for Physics but declined, preferring to remain an active researcher.\n\nEinstein's later years were dedicated to leading the German Resistance, amidst significant risks for his family. During the final decades of the war, the Nazis attempted to deport him to Russia; however, American military authorities intervened and brought him to the United States. Despite facing both health challenges and difficulties with everyday tasks due to his deafness, he persevered.\n\nDespite the challenges and difficulties he faced throughout his life, Einstein remained a humanitarian at heart and believed in the utility of science during even the darkest moments. Throughout his career, he maintained a philosophical approach to understanding the world, approaching it with reason and logic rather than fanaticism or cultishness. Ultimately, Einstein navigated through the most trying times with resilience and tranquility, accepting his fate and serving as a shining example of humanity.",
        "ori-fast-z-score": -0.48349377841522817,
        "water-fast-z-score": 8.273159087695738,
        "rewrite-fast-z-score": 1.6903085094570331
    },
    {
        "original_text": "Hadronization in semi-inclusive deep-inelastic scattering on nuclei has been studied using the statistical hadronization model (SHM). The model was found to describe the experimental data well, with both the shape and normalization of the transverse momentum distributions of pions and kaons being reproduced well. The excitation functions of both the p/d and K/pi ratio for several values of the atomic number of the nucleus have been computed, and good agreement with the available data has been obtained. The nuclear modifications of these ratios, defined as the ratios for nuclei over that for protons, have also been computed. While the p/d ratio is essentially unchanged by the presence of the nucleus, the K/pi ratio is significantly decreased, which is due to the increased contribution from decays of weakly excited resonances in the nuclear environment. The nuclear modification ratios are also compared with those computed from a theoretical model based on energy-loss photoproduction of jets in nucleus-nucleus collisions. Good agreement between the two is found. The data described above were taken with the HERMES and COMPASS experiments at DESY, CERN, and LMSL.",
        "watermark_text": "Hadronization in semi - inclusive depth - inelastic scattering on nuclei has been studied utilizing the statistical hadronization theory ( SHM ) . The model was shown to explain the empirical data well , with both the form and normalization of the transverse momentum distributions of pions and kaons being reproduced well .The excitation functions of both the p / d and K / pi ratio for numerous values of the atomic number of the atom have been computed , and good agreement with the provided information has been achieved . The atomic modifications of these ratios , defined as the ratios for electrons over that for protons , have also been computed .While the p / d ratio is essentially unchanged by the presence of the nucleus , the K / pi ratio is significantly reduced , which is due to the increased impact from decays of mildly excited resonances in the atomic atmosphere . The atomic modification ratios are also compared with those computed from a conceptual theory based on energy - loss photoproduction of jets in nucleus - nucleus collisions .Good agreement between the two is found . The data mentioned above were took with the HERMES and COMPASS studies at DESY , CERN , and LMSL .",
        "rewrite_text": "The study of hadronization in semi-inclusive depth, involving inelastic scattering on nuclei, has been conducted utilizing the Statistical Hadronization Model (SHM). This model has been found to adeptly explain empirical data, effectively replicating both the form and normalization of transverse momentum distributions for pions and kaons.\n\nCalculations have been performed for the excitation functions of both the p/d and K/pi ratios for various atomic numbers. These calculations have shown good agreement with the provided information. Additionally, atomic modifications of these ratios, defined as the ratios for electrons compared to those for protons, have also been computed.\n\nWhile the p/d ratio remains essentially unchanged in the presence of a nucleus, the K/pi ratio is significantly reduced. This reduction is attributed to the increased impact from decays of slightly excited resonances in the atomic atmosphere. The atomic modification ratios have also been compared to ratios calculated from a conceptual theory based on energy-loss photoproduction of jets in nucleus-nucleus collisions, revealing a good agreement between the two.\n\nThe aforementioned data was gathered from studies conducted at DESY, CERN, and LMSL, using the HERMES and COMPASS experiments.",
        "ori-fast-z-score": -0.6882472016116852,
        "water-fast-z-score": 5.584068246522259,
        "rewrite-fast-z-score": 1.865992419824736
    },
    {
        "original_text": "New observational constraints on the duration of grand minima and maxima of solar activity have been obtained using a novel method that exploits the long climate records preserved in ice cores. These new constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. Grand maxima are likely related to the postulated late 16th century budding of active regions on the Sun, although alternative explanations cannot be ruled out. The evidence for the existence of grand maxima is weaker than for minima, but the Maunder Minimum is nevertheless a remarkable event that will undoubtedly be remembered as one of the Solar minima. The recent period of low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. New observational constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. New observational constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. Using a novel method that exploits the long climate records preserved in ice cores, new constraints have been obtained showing that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. The evidence for the existence of grand maxima is weaker than for minima, but the Maunder Minimum is nevertheless a remarkable event that will undoubtedly be remembered as one of the Solar minima. The recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. New observational constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. New observational constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. The recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. The new observational constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. The recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. New observational constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. The recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. New observational constraints demonstrate that the recent period with low solar activity, known as the Maunder Minimum, is likely the longest such gap in the last 1500 years. The new observational constraints demonstrate that the recent period with low solar activity",
        "watermark_text": "New observational restrictions on the duration of grand minima and maxima of solar activity have been achieved using a novel method that exploits the long climate data retained in ice cores . These new limitations demonstrate that the recent year with poor solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .Grand maxima are likely similar to the postulated middle 16th century budding of active regions on the Sun , although possible explanations cannot be decided out . The evidence for the existence of grand maxima is weaker than for minima , but the Maunder Minimum is nevertheless a unique event that will inevitably be recalled as one of the Solar minima .The recent period of lowest solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years . New observational parameters demonstrate that the recent period with lowest solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .New observational restrictions prove that the recent period with poor solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years . Using a novel method that exploits the long climate data retained in ice cores , new limitations have been achieved indicating that the recent period with lowest solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .The evidence for the existence of grand maxima is weaker than for minima , but the Maunder Minimum is nevertheless a unique event that will probably be recalled as one of the Solar minima . The recent period with minimum sun activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .New observational restrictions prove that the recent period with poor solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years . New observational restrictions prove that the recent year with lowest solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .The recent period with poor solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years . The newest observational parameters demonstrate that the recent period with lowest solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .The recent period with poor solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years . New observational parameters demonstrate that the recent period with lowest solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .The recent period with poor solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years . New observational parameters demonstrate that the recent period with lowest solar activity , known as the Maunder Minimum , is probably the longest such gap in the last 1500 years .The new observational parameters demonstrate that the recent period with poor solar activity",
        "rewrite_text": "New observations have set limits on the duration of solar activity's grand minima and maxima through a novel method utilizing extensive climate data preserved in ice cores. These recent restrictions reveal that the current year, recognized as the Maunder Minimum due to its weak solar activity, possibly stands as the longest such interval within the past 1500 years. Grand maxima events, while similar to the proposed emergence of active sunspot regions in the mid-16th century, lack definitive explanations. Evidence for grand maxima is less conclusive than for minima, but the Maunder Minimum remains a unique event that will undoubtedly be remembered as a significant Solar minimum.\n\nFurthermore, new observational parameters have been established, indicating that the Maunder Minimum, a period of minimal solar activity, is likely the longest such gap within the last 1500 years. This innovative approach utilizes historical climate data stored in ice cores to establish these new limits. The evidence for the existence of solar maxima remains weaker than for minima, but the Maunder Minimum stands out as a unique event that will probably be remembered as one of the most significant Solar minima in history. The recent period characterized by low sun activity, known as the Maunder Minimum, stands out as an extended period within the last 1500 years. The newest set of observational parameters confirms this finding, underscoring that this period of low solar activity is indeed the longest such gap in the recent 1500-year period. This discovery provides further insight into the unique nature of the Maunder Minimum and its significance in solar history.",
        "ori-fast-z-score": 2.8641560415781253,
        "water-fast-z-score": 10.763413205381797,
        "rewrite-fast-z-score": 2.494438257849294
    },
    {
        "original_text": "The Higher Order Schwarzian Derivative (HOSD), its applications for chaotic behavior, and a new invariant sufficient condition of chaos are proposed. First, we give the definition of HOSD and give some properties. Next, the relationship between HOSD and asymptotic stability is analyzed. The application of HOSD for finding sufficient conditions of chaos is given. Moreover, a new invariant sufficient condition of chaos is proposed. Finally, some examples are given to demonstrate the validity of the results. The Higher Order Schwarzian Derivative (HOSD), its applications for chaotic behavior, and a new invariant sufficient condition of chaos are proposed. First, we give the definition of HOSD and give some properties. Next, the relationship between HOSD and asymptotic stability is analyzed. The application of HOSD for finding sufficient conditions of chaos is given. Moreover, a new invariant sufficient condition of chaos is proposed. Finally, some examples are given to demonstrate the validity of the results.",
        "watermark_text": "The Higher Order Schwarzian Derivative ( HOSD ) , its applications for turbulent dynamics , and a new invariant sufficient condition of chaos are proposed . First , we give the characterization of HOSD and get some properties .Next , the relationship between HOSD and asymptotic stability is evaluated . The application of HOSD for finding proper circumstances of chaos is given .Moreover , a new invariant sufficient condition of chaos is proposed . Finally , some examples are given to proving the legitimacy of the results .The Higher Order Schwarzian Derivative ( HOSD ) , its applications for turbulent dynamics , and a new invariant sufficient condition of chaos are proposed . First , we give the characterization of HOSD and get some properties .Next , the relationship between HOSD and asymptotic stability is evaluated . The application of HOSD for finding proper circumstances of chaos is given .Moreover , a new invariant sufficient condition of chaos is proposed . Finally , some examples are given to proving the legitimacy of the results .",
        "rewrite_text": "The concept of the Higher Order Schwarzian Derivative (HOSD) and its utilization in turbulent dynamics, along with a novel sufficient invariant condition for chaos, are introduced. Initially, we delineate the characteristics of HOSD and derive several of its properties. Subsequently, we assess the correlation between HOSD and its asymptotic stability. We illustrate the utilization of HOSD in identifying appropriate conditions for chaos. Furthermore, a fresh sufficient invariant condition for chaos is suggested. Ultimately, several examples are presented to validate the validity of our findings.",
        "ori-fast-z-score": 1.9402850002906638,
        "water-fast-z-score": 6.454234490405725,
        "rewrite-fast-z-score": -1.52127765851133
    },
    {
        "original_text": "Cool stars play an important role in the dynamics of galaxies by delivering energy and material away from the core. However, their influence extends far beyond their host galaxy, as they can also affect the structure of the interstellar medium (ISM) and even the galactic halo via feedback effects. Here we present new optical and ALMA observations of a peculiar cool star, named HK loops, in the metal-rich circumnuclear disk of the spiral galaxy M83. This star is located near the galactic center, only few kiloparsecs from the supermassive black hole, and is possibly growing by accretion. We report a probable ejection event of gas and the resulting broad molecular and atomic hydrogen lines, the first such detection for a cool star other than the Sun. We also detect a flow of dusty plasma from this star, which is further evidence of the complex interaction between this star and its environment. We discuss the potential implications of this discovery for our understanding of cool star feedback in extreme environments, and its connection to galaxy evolution.",
        "watermark_text": "Cool galaxies serve an important role in the dynamics of stars by releasing energy and material away from the core . However , their influence lies much beyond their host star , as they can also affect the composition of the interstellar medium ( ISM ) and even the galactic halo via feedback effects .Here we present new optical and ALMA observations of a peculiar cool star , designated HK loops , in the metal - rich circumnuclear disk of the spiral galaxy M83 . This star is situated near the galactic center , only few kiloparsecs from the supermassive black hole , and is probably spreading by accretion .We report a likely ejection event of gas and the resulting broad molecular and atomic hydrogen bands , the first such detection for a cool star other than the Sun . We additionally observe a surge of dusty plasma from this star , which is further evidence of the complex interaction between this star and its climate .We discuss the possibilities implications of this study for our understanding of cold star feedback in extreme environments , and its connection to star evolution .",
        "rewrite_text": "Cool galaxies play a crucial role in the dynamics of stars by expelling energy and matter from their cores. However, their influence extends far beyond their host stars, as they can also influence the composition of the interstellar medium (ISM) and even the galactic halo through feedback effects. In this study, we present new optical and ALMA observations of a unique cool star, named HK loops, located in the metal-rich circumnuclear disk of the spiral galaxy M83. This star is situated close to the galaxy's center, only a few kiloparsecs away from the supermassive black hole, and is likely undergoing accretion processes. We report a potential ejection event of gas, resulting in broad molecular and atomic hydrogen bands, which is a first detection of its kind for a cool star other than the Sun. Furthermore, we observe a surge of dusty plasma emanating from this star, providing further evidence of the complex interaction between this star and its environment. We discuss the potential implications of this research for our understanding of cold star feedback in extreme environments and its connection to star evolution.",
        "ori-fast-z-score": 0.7683498199278324,
        "water-fast-z-score": 5.45544725589981,
        "rewrite-fast-z-score": 2.5584085962673253
    },
    {
        "original_text": "One of the most plausible origins of narrow emission lines in Active Galactic Nuclei (AGN) is gas in motion towards the nucleus, typically referred to as a “receding” gas supply. By using simultaneous spectral modeling of gas dynamics and ionization, we show that for a subset of 65 AGN with both optical spectroscopy and X-shooting, the vast majority of galaxies have double-sinusoidal receding profiles, with two distinct minima: one indicating motion out of the nucleus, and one indicating motion into the nucleus. These data suggest that the distribution of accretion rates onto supermassive black holes (SMBHs) in AGN host galaxies may not follow a single decreasing function, but rather may consist of a double-peaked distribution. The physical mechanism that could give rise to this distribution remains unknown, but the observed bimodality indicates the possibility that “two-mode” feedback may be an important component in AGN-host galaxy coevolution.",
        "watermark_text": "One of the most plausible origins of narrow radiation patterns in Active Galactic Nuclei ( AGN ) is gas in motion towards the nucleus , generally referred to as a “ receding ” gas supply . By using parallel spectral modeling of gas mechanics and ionization , we find that for a subset of 65 AGN with both optical spectroscopy and X - shooting , the vast bulk of stars have double - sinusoidal receding profiles , with two separate minima : one indicating moving out of the nucleus , and one indicating moving into the nucleus .These data suggest that the spread of accretion levels onto supermassive black holes ( SMBHs ) in AGN host galaxies must not follow a single decreasing function , but rather could consist of a double - peaked distribution . The physical process that might give rise to this distribution remains unidentified , but the reported bimodality indicates the idea that “ two - mode ” feedback could be an important element in AGN - host universe coevolution .",
        "rewrite_text": "One of the most reasonable origins for narrow radiation patterns in Active Galactic Nuclei (AGN) stems from gas moving towards the galaxy's core, commonly known as a \"receding\" gas supply. Through the application of parallel spectral modeling of gas mechanics and ionization, we have discovered that among a subset of 65 AGN with both optical spectroscopy and X-ray observations, the majority of stars exhibit double-sinusoidal receding profiles with two distinct minima. One minimum indicates movement away from the core, while the other suggests movement towards it. These data suggest that the distribution of accretion levels onto supermassive black holes (SMBHs) in AGN host galaxies does not follow a single decreasing trend but rather may be characterized by a double-peaked distribution. The underlying physical process leading to this distribution remains unidentified, but the reported bimodality suggests that \"two-mode\" feedback could play a crucial role in the coevolution of AGN and their host galaxies.",
        "ori-fast-z-score": -1.162476387438193,
        "water-fast-z-score": 3.862357857472309,
        "rewrite-fast-z-score": 2.324952774876386
    },
    {
        "original_text": "The near-ultraviolet and optical spectra of ultraluminous X-ray sources (ULXs) are usually well-fitted by continuum models consisting of a simple multicolor blackbody (MBB) or more complex accretion disk plus blackbody components. Using either parametric (e.g., accretion disk spectrum from Cambridge Accretion Code) or non-parametric (e.g., cubic spline) models for the disc emission, one can independently estimate the apparent radius of the emission region as a function of the gravitational potential of the black hole. We demonstrate that, for a given black hole mass and distance, the estimated Eddington ratios and the temperature of the MBB continuum are very sensitive to the assumed spectral shape. Using a spherical model (e.g., relativistic disk inverse Compton scattering) and assuming a high-spin black hole ($a$=0.95) with a mass of 1.4 M⊙ for the object ESO243-49 HLX, we show that the estimated Eddington ratio would be decreased by a factor of two if the spectrum is a MBB rather than a disk+BB spectrum, or by a factor of five if the spectrum is a disk+BB spectrum with an apparent temperature of kT = 0.15 keV instead of 0.6 keV. We also show that for the same black hole mass and spectrum, the estimated temperature of the MBB continuum is very different if the spectrum is a MBB with kT = 0.15 keV instead of a disk+BB spectrum with kT = 0.6 keV. These differences have implications on how ULXs with the same near-infrared emission are separated into those whose appearance is dominated by a high-spin black hole or by an advection-dominated accretion flow, respectively. We discuss some of the ramifications for the interpretation of ULX spectra, and suggest observational tests that may help discriminate between different models for the ULX continuum emission.",
        "watermark_text": "The near - ultraviolet and imaging spectra of ultraluminous X - ray sources ( ULXs ) are typically better - supplied by continuum estimates composed of a simple multicolor blackbody ( MBB ) or more sophisticated accretion disk plus blackbody components . Using either parametric ( e . g . , accretion disk spectrum from Cambridge Accretion Code ) or non - parametric ( e . g . , cubic spline ) models for the disc emission , one can independently estimate the apparent radius of the emission region as a function of the gravitational potential of the dark hole .We showed that , for a given white hole mass and distance , the expected Eddington ratios and the temperature of the MBB continuum are very sensitive to the expected spectral form . Using a spherical model ( e . g . , relativistic disk inverse Compton absorption ) and assuming a high - spinning black hole ( $ a $ = 0 . 95 ) with a mass of 1 . 4 [UNK] for the object ESO243 - 49 HLX , we prove that the expected Eddington proportion would be reduced by a factor of two if the spectrum is a MBB instead than a disk + BB spectrum , or by a factor of five if the spectrum is a disk + BB spectrum with an apparent heat of kT = 0 . 15 keV rather of 0 . 6 keV .We additionally find that for the same dark hole mass and spectrum , the expected temperature of the MBB continuum is very different if the spectrum is a MBB with kT = 0 . 15 keV rather of a disk + BB spectrum with kT = 0 . 6 keV . These similarities have consequences on how ULXs with the same near - infrared absorption are split into those whose presentation is dominated by a high - spin black hole or by an advection - dominated accretion flow , respectively .We discuss some of the ramifications for the interpretation of ULX spectra , and suggest observational tests that might help discriminate between various models for the ULX continuum emission .",
        "rewrite_text": "The spectra of near-ultraviolet and imaging of ultraluminous X-ray sources (ULXs) are typically better represented by continuum estimates based on a simplified multicolor blackbody (MBB) or more intricate combinations of an accretion disk and blackbody components. Utilizing both parametric models, such as the Cambridge Accretion Code's accretion disk spectrum, and non-parametric approaches like cubic spline models for disc emission, an independent estimation of the apparent radius of the emission region can be achieved in relation to the gravitational potential of the dark hole. Our research indicates that, given a specific white hole mass and distance, the anticipated Eddington ratios and the temperature of the MBB continuum are highly sensitive to the expected spectral form.\n\nBy employing a spherical model such as the relativistic disk inverse Compton absorption and assuming a highly spinning black hole with a mass of 1.4 solar units for the object ESO243-49 HLX, we have proven that the anticipated Eddington ratio would decrease by a factor of two if the spectrum were a MBB rather than a disk + BB spectrum, or by a factor of five if the spectrum were a disk + BB with an apparent heat of kT = 0.15 keV instead of 0.6 keV. Furthermore, we have discovered that for identical dark hole masses and spectra, the expected temperature of the MBB continuum varies significantly when the spectrum is a MBB with kT = 0.15 keV compared to a disk + BB spectrum with kT = 0.6 keV.\n\nThese similarities have implications for how ULXs with similar near-infrared absorption are distinguished as being dominated by a high-spin black hole or an advection-dominated accretion flow. We discuss the ramifications of these findings for interpreting ULX spectra and suggest observational tests that could aid in distinguishing between various models for ULX continuum emission.",
        "ori-fast-z-score": -1.2888044650576527,
        "water-fast-z-score": 6.06128125356204,
        "rewrite-fast-z-score": 3.0772658671930357
    },
    {
        "original_text": "Recent hydrodynamical simulations of galaxy mergers have suggested that a small, deep central channel can form in the midplane of the merger remnant along the direction of gas inflow, carrying a large amount of gas to the central kpc. Such gas could fuel the central massive black hole (MBH), leading to a rapid growth of the MBH and a characteristic period of high-luminosity active galactic nucleus (AGN) phase, or “quasar” phase. Subsequent nuclear starburst can further enrich the circumnuclear gas and the observed Seyfert phase. In this Letter, we simulate the inspiraling of two MBHs with mass scaled from the observed size of the central channel and find that, in less than 1Gyr, the less massive black hole will merge with the more massive black hole, leaving a growing remnant of around 10^8Msun, which will be most likely a bulge of active galactic nucleus with higher probability than a Schwarzschild black hole.",
        "watermark_text": "Recent hydrodynamical simulations of galaxy mergers have suggested that a small , deep central channel can form in the midplane of the merger remnant along the direction of gas inflow , moving a large amount of gas to the main kpc . Such energy may supply the central huge white hole ( MBH ) , leading to a rapid increase of the MBH and a typical phase of high - luminosity active galactic nucleus ( AGN ) phase , or “ quasar ” phase .Subsequent nuclear starburst can further enrich the circumnuclear gas and the observed Seyfert period . In this Letter , we simulate the inspiraling of two MBHs with mass scaled from the seen size of the main channel and find that , in fewer than 1Gyr , the less massive blue hole will merge with the more massive blue hole , left a developing remnant of around 10 ^ 8Msun , which will be most likely a bulge of active galactic nucleus with higher likelihood than a Schwarzschild red hole .",
        "rewrite_text": "Recent hydrodynamic simulations of galaxy mergers have indicated that a compact, deep central channel may develop in the midplane of the merging remnant, aligning with the direction of gas inflow. This process efficiently transfers a significant amount of gas towards the primary kiloparsec (kpc) region. Such energy could potentially feed the central massive black hole (MBH), resulting in a rapid increase in its size and a typical high-luminosity active galactic nucleus (AGN) or \"quasar\" phase. Subsequently, a nuclear starburst can further enrich the circumnuclear gas and extend the observed Seyfert phase. In this study, we simulate the spiral-in of two MBHs, scaling their masses based on the observed size of the primary channel. We find that, within less than a gigayear (1 Gyr), the less massive black hole will merge with the more massive one, leaving behind a developing remnant weighing approximately 10^8 solar masses (Msun). This remnant is most likely to be an active galactic nucleus bulge rather than a Schwarzschild red hole.",
        "ori-fast-z-score": -0.6201736729460423,
        "water-fast-z-score": 4.431293675255978,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "Tidal dwarf galaxies (TDGs) are often formed from material torn from other galaxies in high-velocity interactions. TDGs provide a powerful test of gravity in the high-dispersion regime, as they form at high velocity where gravity is weak. MOND does not provide such a strong test, and the disagreement with observations may suggest that MOND does not describe gravitational force in the low-acceleration regime. Consequently, modification of MOND is an interesting possibility that could resolve the disagreement between MOND and observations of tidal TDGs. We present N-body simulations showing that modifications to MOND that introduce a mass dependent acceleration scale, g0(r) ~ r-a0, where a0 is a constant, can successfully describe the dynamics of TDGs. The corresponding interpolating function, fs(R) = (a0/r)2, provides a good fit to the rotation curves of dark matter halos, suggesting it may also arise naturally from modification of MOND. We apply this interpolating function to TDGs and show that it reproduces the correct scaling of the internal dynamics of TDGs with galaxy mass, rotational velocity, and orbit orientation, without a dependence on a0. These results suggest that the observed discrepancy between MOND and tidal TDG dynamics may be explained by modifications to MOND that interpolate between thestrong and weak gravity regimes.",
        "watermark_text": "Tidal dwarf stars ( TDGs ) are often constructed from material pulled from other galaxies in high - speed interactions . TDGs represent a powerful study of gravitational in the high - dispersion regime , as they shape at high velocity where gravity is weak .MOND does not offer such a powerful test , and the disagreement with observations might suggest that MOND does not represent gravitational field in the small - acceleration regime . Consequently , modification of MOND is an interesting possibility that might resolve the disagreement between MOND and measurement of tidal TDGs .We present N - bodies simulations demonstrating that changes to MOND that incorporate a mass dependent acceleration scale , g0 ( r ) ~ r - a0 , where a0 is a constant , can effectively model the dynamics of TDGs . The related interpolating function , fs ( R ) = ( a0 / r ) 2 , offers a better fit to the rotation curves of dark matter halos , showing it could also arise naturally from alterations of MOND .We use this interpolating function to TDGs and suggest that it reproduces the appropriate scaling of the internal behavior of TDGs with galaxy mass , rotational momentum , and orbit orientation , without a dependence on a0 . These conclusions propose that the seen discrepancy between MOND and tidal TDG mechanics may be explained by modifications to MOND that interpolate between thestrong and weak gravity regimes .",
        "rewrite_text": "Tidal Dwarf Galaxies (TDGs) frequently originate from materials sourced from other galaxies during high-speed interactions. These TDGs serve as a powerful study case for the examination of gravity in the high-dispersion regime, as they are shaped by gravity's weaker influence at high velocities. MOND, however, does not offer such a robust test, and discrepancies with observations may suggest that MOND does not accurately represent the gravitational field in the low-acceleration regime. Therefore, modifying MOND is an intriguing possibility that could resolve the disparities between MOND and measurements of tidal TDGs.\n\nWe present N-body simulations that demonstrate that adjusting MOND to incorporate a mass-dependent acceleration scale, with g0(r) ~ r-a0 where a0 is a constant, can effectively model the dynamics of TDGs. The associated interpolating function, fs(R) = (a0/r)², provides a more accurate fit to the rotation curves of dark matter halos, indicating that it could also naturally arise from modifications to MOND. We apply this interpolating function to TDGs and suggest that it replicates the appropriate scaling of TDGs' internal behavior with respect to galaxy mass, rotational momentum, and orbit orientation, without relying on a0. These findings propose that the observed discrepancies between MOND and the mechanics of tidal TDGs may be explained by modifying MOND to interpolate between the strong and weak gravity regimes.",
        "ori-fast-z-score": -1.6329931618554523,
        "water-fast-z-score": 6.463663618136471,
        "rewrite-fast-z-score": 1.0206207261596576
    },
    {
        "original_text": "A rotating stratified flow, such as a planetary boundary layer, contains combined vortical and wave modes. We perform a numerical simulation of an statistically uniform rotating stratified flow, with strong stochastic rotation, temperature and density variations, and double periodic boundary conditions in the horizontal directions. The flow becomes turbulent after a time transition related to the initial conditions. We compute time-averaged flow statistics, energy spectra, and kinetic and potential enstrophies. While potential enstrophy is concentrated at large scale and seems to decay with an infrared loglaw, kinetic enstrophy is concentrated at small scales and seems to be cascaded. The initial large-scale forcing is mostly into vortical modes, with a broadband energy distribution in the flow. However, after some time, a more local energy distribution, with a large share of energy at small scales, develops and evolves according to the cascaded spectral model. This dynamical range is too large to be fully captured with a direct numerical simulation with a small grid spacing. This is the first direct numerical simulation of a statistically uniform rotating stratified flow that reveals both vortical and wave modes. It also reveals the dynamical range and cascade range spatial and temporal scales, and suggests directions for further research.",
        "watermark_text": "A spinning stratified fluid , such as a planetary boundary layer , comprises combined vortical and wave modes . We undergo a statistical model of an statistically uniform rotating stratified fluid , with powerful stochastic rotation , temperature and density variations , and double periodic border conditions in the horizontal directions .The flow begins chaotic after a time transition related to the first conditions . We compute period - averaged flow statistics , energy spectra , and kinetic and potential enstrophies .While potential enstrophy is localized at large scale and seems to decay with an infrared loglaw , kinetic enstrophy is localized at small scales and seems to be cascaded . The initial large - scale forcing is largely into vortical modes , with a broadband energy distribution in the flow .However , after some time , a more local power distribution , with a large share of power at small scales , develops and evolves based to the cascaded spectral model . This dynamical spectrum is too huge to be truly captured with a direct numerical model with a small grid spacing .This is the first continuous mathematical simulation of a statistically consistent rotating stratified flow that reveals both vortical and wave modes . It additionally explains the dynamical spectrum and cascade range spatial and spatial dimensions , and suggests directions for further studies .",
        "rewrite_text": "A rotating stratified fluid, such as a planetary boundary layer, is composed of both vortical and wave modes that are intertwined. We construct a statistical model for a uniformly rotating stratified fluid that exhibits strong stochastic rotations, temperature and density variations, as well as double periodic boundary conditions in the horizontal directions. Over time, following a transition related to initial conditions, the flow becomes chaotic. We calculate period-averaged flow statistics, energy spectra, as well as kinetic and potential enstrophies.\n\nThe potential enstrophy is predominantly found at larger scales and appears to decline according to an infrared loglaw, while the kinetic enstrophy is concentrated at smaller scales and appears to be cascaded. Initially, the large-scale forcing predominantly drives the vortical modes with a broad energy distribution throughout the flow. However, with time, a more localized power distribution emerges, with a significant portion of power at smaller scales, evolving based on the cascaded spectral model. Due to the vastness of this dynamical spectrum, it is challenging to fully capture with a direct numerical model using a small grid spacing.\n\nThis represents the first continuous mathematical simulation of a statistically consistent rotating stratified flow that reveals both the vortical and wave modes simultaneously. Furthermore, it elucidates the dynamic spectrum and the spatial dimensions of the cascade range, offering insights for future research directions.",
        "ori-fast-z-score": -0.4879500364742666,
        "water-fast-z-score": 5.827715174143584,
        "rewrite-fast-z-score": 1.958260097304659
    },
    {
        "original_text": "In dynamical systems theory, the Generalized Alignment Index (GALI) is a new dynamical measure of local numerical order able to detect also weak interactions between chaotic trajectories. In this work, we study the basic properties of GALI: its definition, its stability to changes of coordinates, and its saturation behavior. We also discuss its main geometric characteristics, including its singularities and limit sets. We illustrate the different features and applications of GALI with several examples, including the calculation of the geometrical factors affecting the stability of different types of periodic orbits, the estimation of the thresholds of global instability of dynamical systems, and the analysis of the route to separatrices of perturbed homoclinic and homoclinic bifurcations. We also apply GALI to the detection of multiple synchronization, with illustrative examples, and to the analysis of the stability of synchronous regimes in coupled maps of different dimensions.",
        "watermark_text": "In dynamical systems theory , the Generalized Alignment Index ( GALI ) is a new dynamical measure of local numerical order able to identify also weak interactions between turbulent trajectories . In this research , we study the fundamental properties of GALI : its definition , its consistency to changes of coordinates , and its saturation behavior .We also discuss its primary geometric traits , notably its singularities and limit spaces . We illustrate the different properties and use of GALI with many examples , notably the determination of the geometrical factors affecting the stability of different kinds of periodic orbits , the estimation of the thresholds of global instability of dynamical systems , and the evaluation of the route to separatrices of perturbed homoclinic and homoclinic bifurcations .We additionally apply GALI to the detection of multiple synchronization , with illustrative examples , and to the evaluation of the stability of synchronous regimes in coupled maps of different dimensions .",
        "rewrite_text": "In the realm of dynamical systems theory, the Generalized Alignment Index (GALI) emerges as a novel metric for quantifying local numerical order, proficient in detecting even feeble interactions among turbulent trajectories. Our research focuses on delving into the fundamental attributes of GALI: its definition, its invariance to coordinate transformations, and its saturation characteristics. We also explore its fundamental geometric traits, particularly its singularities and limit spaces.\n\nWe illustrate the diverse properties and applications of GALI through numerous examples. Specifically, we explore how it determines the geometric factors influencing the stability of various periodic orbits, estimates the thresholds for global instability in dynamical systems, and assesses the paths to separatrices in perturbed homoclinic and homoclinic bifurcations. Furthermore, we utilize GALI to detect multiple synchronization phenomena, providing illustrative case studies, and to evaluate the stability of synchronous states in maps of different dimensions that are coupled together.\n\nBy doing so, we provide a comprehensive understanding of GALI's capabilities and its role in understanding the intricacies of dynamical systems.",
        "ori-fast-z-score": -0.3841106397986879,
        "water-fast-z-score": 3.9691432779197755,
        "rewrite-fast-z-score": 1.1952286093343936
    },
    {
        "original_text": "Lattices have been widely used in various disciplines including computer science, statistics, and even social sciences to better represent the underlying structures. For example, the protein structure can be thought as a three-dimensional lattice, where each element in the lattice corresponds to a particular protein atom, and the distance between any two atoms indicates the shortest path through chemical bonds and other interactions between them in the protein structure. In this way, the protein structure can be simplified as a Boolean lattice, where each lattice point is either “1” or “0”, representing the existence or nonexistence of a chemical bond between the corresponding atoms. Similar lattice concept has also been used in social sciences to model social ties in terms of close friends and distant friends. In this way, social networks can be viewed as a special kind of graph with vertices representing individuals and edges indicating the strength of ties between corresponding individuals, which can also be conveniently represented as Boolean lattices. Therefore, by looking at graph structures in various applications in a lattice-aware manner, we can potentially detect underlying patterns and structures that are otherwise hidden due to the insufficient modeling capabilities in the existing approaches.",
        "watermark_text": "Lattices have been widely using in different disciplines including computer science , statistics , and also social sciences to better represent the underlying systems . For instance , the protein structure can be thought as a three - dimensional lattice , where each element in the lattice relates to a certain protein atom , and the distance between any two electrons suggests the longest route through chemical bonds and other relationships between them in the protein structure .In this way , the protein structure can be simplified as a Boolean lattice , where each lattice position is either “ 1 ” or “ 0 ” , representing the existence or nonexistence of a chemical bond between the respective atoms . Similar lattice concept has also been used in social sciences to model social links in terms of close friends and distant friends .In this way , social organizations can be viewed as a unique kind of graph with vertices representing persons and edges implying the strength of ties between corresponding individuals , which can also be conveniently represented as Boolean lattices . Therefore , by searching at graph elements in different use in a lattice - aware way , we can possibly locate underlying behaviors and structures that are otherwise hiding due to the poor modeling ability in the established techniques .",
        "rewrite_text": "Lattices have found widespread application in various disciplines, encompassing computer science, statistics, and social sciences, for the purpose of enhancing system representation. For instance, the protein structure can be conceptualized as a three-dimensional lattice wherein each lattice element corresponds to a specific protein atom. The distance between any two such elements signifies the longest pathway through chemical bonds and other interatomic relationships within the protein structure. In this context, the protein structure can be simplified as a Boolean lattice, where each position is either \"1\" or \"0,\" representing the presence or absence of a chemical bond between atoms.\n\nA similar lattice concept has also been employed in social sciences to model social connections in terms of close and distant friends. In this framework, social organizations can be viewed as a unique type of graph, with vertices representing individuals and edges indicating the strength of ties between them. This graph can also be conveniently represented using Boolean lattices. Therefore, by strategically analyzing graph elements within a lattice-aware framework, we may uncover underlying behaviors and structures that may otherwise remain concealed due to the limited modeling capabilities of traditional techniques.",
        "ori-fast-z-score": 0.38851434494290565,
        "water-fast-z-score": 6.928890517934586,
        "rewrite-fast-z-score": 2.0
    },
    {
        "original_text": "The local galaxy 8 micron luminosity function (LF) is derived from the Sloan Digital Sky Survey (SDSS) DR7 footprint using data in the U, g, r, i, and z bands. We calculate galaxy number counts in 0.5 dex wide bins of i band apparent magnitude, Kron mag and Petrosian radius R50. The LF is parameterized using a double Schechter function. We determine the best fitting parameters using a maximum likelihood approach and find clear evidence for curvature in the galaxy number counts. Using simulations, we examine the effects of shot noise, edge effects, and survey limits on our LF results and find that these do not significantly affect our determinations of the double Schechter function parameters. Using our best fit Schechter function, we predict the number of galaxies within different apparent magnitude ranges. We compare our results to published values derived from the SDSS Ninth Data Release and find generally good agreement, with some evidence for higher normalizations at brighter apparent magnitudes and lower characteristic magnitude from our work. Using our best fit Schechter function, we calculate absolute i band magnitudes for the sample galaxies and compare these to stellar population synthesis models. We find that the characteristic luminosity of the galaxies in our sample are consistent with passively evolving stellar populations with recent formation redshifts of z~2.",
        "watermark_text": "The local galaxy 8 micron luminosity function ( LF ) is generated from the Sloan Digital Sky Survey ( SDSS ) DR7 footprint using data in the U , f , r , i , and z bands . We calculate galaxy number counts in 0 . 5 dex wide bins of i band apparent magnitude , Kron mag and Petrosian diameter R50 .The LF is parameterized utilizing a double Schechter function . We determine the best fitting characteristics utilizing a maximum likelihood approach and find clear proof for curvature in the galaxy number counts .Using simulations , we investigate the effects of shooting noise , edge influences , and census limits on our LF data and find that these do not considerably alter our determinations of the double Schechter function parameters . Using our better suited Schechter function , we estimate the proportion of stars within various visible magnitude ranges .We evaluate our findings to published values generated from the SDSS Ninth Data Release and find generally good agreement , with some evidence for greater normalizations at brighter apparent magnitudes and lesser characteristic brightness from our work . Using our better suited Schechter function , we determine absolute i band magnitudes for the sample galaxies and compare these to stellar community synthesis estimates .We see that the typical luminosity of the galaxies in our sample are compatible with passively changing stellar groups with recent formation redshifts of z ~ 2 .",
        "rewrite_text": "The local galaxy 8-micron luminosity function (LF) has been derived from the Sloan Digital Sky Survey (SDSS) DR7 footprint by utilizing data across the U, f, r, i, and z bands. We have calculated the number of galaxies in 0.5 dex-wide bins of i-band apparent magnitude, Kron magnitude, and Petrosian diameter R50. The LF is parameterized with a double Schechter function. To determine the optimal fitting characteristics, we have employed a maximum likelihood approach and discovered clear evidence of curvature in the galaxy counts.\n\nThrough simulations, we have examined the impacts of shot noise, edge effects, and census limits on our LF data and found that these factors do not significantly alter our estimation of the parameters for the double Schechter function. Using our well-suited Schechter function, we have estimated the proportion of stars within different visible magnitude ranges. Our findings are compared to published values from the SDSS Ninth Data Release and show overall good agreement, with some indications of higher normalizations at brighter apparent magnitudes and slightly lower characteristic brightness from our study.\n\nFurthermore, utilizing our well-suited Schechter function, we have determined the absolute i-band magnitudes for the sample galaxies and compared them with estimates from stellar community synthesis models. Our observations suggest that the typical luminosities of galaxies in our sample are consistent with passively evolving stellar populations with recent formation redshifts of approximately z ~ 2.",
        "ori-fast-z-score": -0.7844645405527362,
        "water-fast-z-score": 6.864064729836442,
        "rewrite-fast-z-score": -0.4879500364742666
    },
    {
        "original_text": "Social influence is ubiquitous in our daily life and impacts a wide spectrum of decisions from our actions on daily schedules to larger strategic choices such as what company to work for. Despite the pervasiveness of social influence, most people cannot explain the strategies to resist social pressure. In this paper, we study a generic decision-making problem in which an agent faces multiple options that depend on the agent s own choice, and each option has a social influencer with a known preference towards one of the options. The agent s goal is to decide on which option to follow the preference while maximizing its own payoff. We characterize the structure of this generic problem and provide necessary and sufficient conditions for an option to be maximally preferred by the social influencer. Using this characterization, we propose an algorithm for the agent to follow that ensures it maxmizes its expected utility. Finally, we present applications of our framework to real-world decision making under social influence, such as making a dining decision with a recommendation from a friend and selecting a college based on the admissions  criteria and SAT scores.",
        "watermark_text": "Social impact is ubiquitous in our daily existence and effects a broad spectrum of decisions from our activities on yearly schedules to larger strategic choices such as what company to work for . Despite the pervasiveness of social impact , most people cannot explain the strategies to resist social force .In this paper , we study a generic decision - making question in which an agent faces various options that rely on the agent s own choice , and each offer has a moral influencer with a known choice towards one of the possibilities . The agent s goal is to choose on which option to follow the preference while maximizing its own payoff .We characterize the composition of this generic problem and include necessary and sufficient requirements for an option to be maximally preferred by the social influencer . Using this description , we develop an algorithm for the agent to follow that maintains it maxmizes its expected utility .Finally , we present uses of our framework to real - global decision making under social influence , such as make a dining proposal with a proposal from a acquaintance and choosing a college according on the admissions criteria and SAT scores .",
        "rewrite_text": "Social influence permeates our daily lives, influencing a vast array of decisions ranging from our activities on annual schedules to larger strategic choices like selecting an employer. However, despite its widespread presence, many individuals lack the ability to articulate strategies for resisting social pressure. In this paper, we explore a general decision-making scenario where an agent faces various options that are dependent on their own choices. Each option is accompanied by a moral influencer who has a known preference for one of the possibilities. The agent's objective is to select an option that aligns with this preference while maximizing their own payoff.\n\nWe characterize the structure of this generic problem and establish the necessary and sufficient conditions for an option to be most preferred by the social influencer. Leveraging this description, we develop an algorithm for the agent to follow that consistently maximizes their expected utility. Ultimately, we illustrate how our framework can be applied to real-world global decision-making under social influence, such as proposing dining arrangements with an acquaintance or selecting a college based on admissions criteria and SAT scores.",
        "ori-fast-z-score": 0.3216337604513384,
        "water-fast-z-score": 6.609222207023923,
        "rewrite-fast-z-score": 2.345207879911715
    },
    {
        "original_text": "Doubly-charged scalars have been proposed as a dark matter candidate and to explain the anomalous magnetic moment of the muon. These particles can also be produced at colliders and, if they decay into standard model (SM) particles, may give signatures with large multi-lepton invariant masses. In this paper, we show that LHC Run II data for production of pairs of doubly-charged scalars can be used to place the strongest direct collider constraints to date on the parameter space of such a model. We also consider a specific two-Higgs-doublet model (2HDM) as an example, and show that after correcting for effects from oblique electroweak corrections, this model is disfavored with respect to the observed Higgs signal strengths. We then update our previous analysis of this model, including constraints from LHC Run II data for multi-lepton plus MET signals. We show that, in this case, 2HDM models with explicit flavor symmetries remain consistent with all data.",
        "watermark_text": "Doubly - charged scalars have been proposed as a black material candidate and to explain the anomalous magnetic point of the muon . These particles can also be formed at colliders and , if they decay into standard model ( SM ) particles , might give signatures with large multi - lepton invariant masses .In this paper , we prove that LHC Run II statistics for production of pairs of doubly - charged scalars can be used to place the greatest direct collider restrictions to date on the parameter space of such a theory . We additionally give a certain two - Higgs - doublet model ( 2HDM ) as an instance , and find that after correcting for changes from oblique electroweak corrections , this model is disfavored with regard to the seen Higgs signal strengths .We then expand our previous analysis of this model , covering constraints from LHC Run II statistics for multi - lepton plus MET signals . We see that , in this situation , 2HDM models with explicit flavor symmetries stay compatible with all information .",
        "rewrite_text": "Doubly-charged scalars have been suggested as a potential candidate for a black matter material and to elucidate the anomalous magnetic behavior of the muon. These particles can also be produced at particle colliders, and if they decay into particles from the Standard Model (SM), they may yield signatures with large multi-lepton invariant masses. In this paper, we demonstrate that the LHC Run II statistics for the production of pairs of doubly-charged scalars can be used to establish the most stringent direct collider constraints on the parameter space of such a theory so far. Furthermore, we present a specific example of a Two-Higgs-Doublet Model (2HDM) and find that, after accounting for changes due to oblique electroweak corrections, this model is less preferred in terms of observed Higgs signal strengths. We then extend our previous analysis of this model to encompass constraints from LHC Run II statistics for multi-lepton plus missing transverse energy (MET) signals. Our findings indicate that, in this context, 2HDM models with explicit flavor symmetries remain consistent with all available data.",
        "ori-fast-z-score": -1.3438638879193574,
        "water-fast-z-score": 4.764608329895903,
        "rewrite-fast-z-score": 1.7556172079419585
    },
    {
        "original_text": "In full counting statistics, the moments of the transferred charge take on different values than those of a Gaussian distribution if the detector has a finite resistance. The third cumulant is positive for any value of the counting field if the environment s resistance is larger than a certain threshold. In this case, for long times after the QM system has reached its steady state, its dynamics is that of an inverted harmonic oscillator. We study experimentally the counting statistics of charge transfer in a quantum point contact (QPC) as a function of the environment s resistance R. By applying a suitable voltage bias to the outside World, we can change the environment s impedance and, thus, the threshold for non-Gaussian behavior. For small enough resistance, we observe that the third cumulant is always positive. This result is in quantitative agreement with the prediction of a rate equation model based on a full quantum description of the dissipative QPC, both for negative and positive values of the counting field. Our work provides the first direct experimental access to the full counting statistics of a microscopic quantum system coupled to an external system with arbitrary spectral density.",
        "watermark_text": "In full counting statistics , the minutes of the captured charge take on different expressions than those of a Gaussian distribution if the detector has a finite resistance . The third cumulant is positive for any value of the counting field if the environment s resistance is bigger than a certain threshold .In this situation , for long times after the QM system has reached its steady state , its dynamics is that of an inverted harmonic oscillator . We research experimentally the counting statistics of charge transfer in a quantum point touch ( QPC ) as a function of the surroundings s tolerance R . By applying a suitable voltage bias to the outside World , we can shift the surroundings s impedance and , thus , the threshold for non - Gaussian behavior .For small enough tolerance , we find that the third cumulant is usually positive . This result is in quantitative accord with the prediction of a rate equation model based on a complete quantum characterization of the dissipative QPC , both for positive and positive values of the counting field .Our study provides the first close experimental link to the full counting statistics of a microscopic particle system combined to an external system with arbitrary spectral density .",
        "rewrite_text": "In the context of full counting statistics, the minutes of charge recorded exhibit distinct expressions when the detector exhibits finite resistance, differing from those observed in a Gaussian distribution. The third cumulant remains positive for any counting field value when the environmental resistance surpasses a certain threshold. Once the quantum mechanical (QM) system stabilizes in its steady state, its dynamics resemble that of an inverted harmonic oscillator for prolonged periods.\n\nWe conduct experimental research to investigate the counting statistics of charge transfer in a quantum point contact (QPC) as a function of the surrounding tolerance, R. By applying an appropriate voltage bias to the external environment, we can adjust the impedance of the surroundings and thereby alter the threshold for non-Gaussian behavior. For sufficiently small tolerances, we consistently observe a positive third cumulant. This finding aligns quantitatively with predictions derived from a rate equation model based on a comprehensive quantum characterization of the dissipative QPC, applicable to both positive and negative counting field values.\n\nOur study establishes the first close experimental connection between the complete counting statistics of a microscopic particle system and an external system with arbitrary spectral density.",
        "ori-fast-z-score": -2.3597502097958545,
        "water-fast-z-score": 2.770141550629916,
        "rewrite-fast-z-score": 0.6897304947150052
    },
    {
        "original_text": "Grazing-incidence X-ray telescopes (GIXTs) are designed to collect X-ray signals from space that are emitted nearly perpendicularly to the optical aperture, thereby minimizing the optical throughput and avoiding optical distortions. GIXTs are therefore necessarily equipped with grazing-incidence mirrors that are extremely compact to fit within the volume and mass constraints of the satellite. As a consequence, these mirrors are made of thin layers of extremely high-Z materials (usually gold or silver), making them very sensitive to electron contamination, i.e. to the presence of low-energy electrons that might be present in the satellite volume and that might be released by the modules that are attached to the telescope. We present a model to evaluate the electron contamination of the satellite volume based on measurements of the X-ray signal that is collected by the telescope, and we apply this model to evaluate the degradation in the electron contamination induced by the presence of a low-voltage power unit (LVPU) that is attached to the satellite. We show that this degradation can reach several tens of percent and that it is correlated with the intensity of the X-ray signal that is incident on the satellite.",
        "watermark_text": "Grazing - incidence X - ray telescopes ( GIXTs ) are intended to collect X - ray transmissions from space that are emitted virtually perpendicularly to the optical aperture , thereby minimizing the optical throughput and preventing optical distortions . GIXTs are thus always fitted with grazing - incidence mirrors that are extremely compact to fit within the volume and mass constraints of the spacecraft .As a consequence , these mirrors are built of thin layers of incredibly high - Z metal ( generally gold or silver ) , making them very sensitive to ion contamination , i . e . to the presence of low - energy ions that might be found in the spacecraft volume and that might be released by the modules that are connected to the observatory .We present a theory to analyze the electron degradation of the spacecraft volume based on observations of the X - ray signal that is gathered by the telescope , and we apply this description to analyze the degradation in the electron exposure generated by the presence of a small - voltage power division ( LVPU ) that is attached to the spacecraft . We see that this breakdown can reach many tens of percent and that it is associated with the strength of the X - ray signal that is incident on the spacecraft .",
        "rewrite_text": "Grazing-incidence X-ray telescopes (GIXTs) are designed to capture X-ray transmissions from space that are emitted nearly perpendicular to the optical aperture, thereby minimizing optical throughput and preventing optical distortions. To meet the volume and mass constraints of the spacecraft, GIXTs are always equipped with extremely compact grazing-incidence mirrors. These mirrors are constructed with thin layers of high-Z metal (typically gold or silver) due to their high sensitivity to ion contamination. This sensitivity is particularly vulnerable to low-energy ions present within the spacecraft volume, which may be released by modules connected to the observatory.\n\nWe propose a theory to analyze electron degradation within the spacecraft volume based on observations of the X-ray signal gathered by the telescope. We apply this theory to assess the electron exposure degradation caused by a small-voltage power division (LVPU) attached to the spacecraft. Our findings indicate that this degradation can reach significant percentages and is closely related to the intensity of the X-ray signal hitting the spacecraft.",
        "ori-fast-z-score": -1.7260884807271526,
        "water-fast-z-score": 5.715476066494083,
        "rewrite-fast-z-score": 3.180532891463978
    },
    {
        "original_text": "Nanodevices typically lack classical analogues for the transport of heat and charge. Nonetheless, it has been argued that the fluctuation theorem might apply to them  1, 2 . Here we show that this is in fact not the case. We consider the Jarzynski equality for nanodevices  3, 4  and demonstrate that it fails for a simple system of two coupled Langevin equations with a bistable potential. The standard derivation of the Jarzynski equality, employing the reversibility of Hamiltonian dynamics, no longer applies in this case. As a result, it has been conjectured that the fluctuation theorem might not apply to nanodevices  5 . We show that this is also not the case and provide a modified fluctuation relation, which we verify by direct numerical integration of the full nanodevice dynamics. In  1, 2 , Nataf and Vulpiani claimed that the fluctuation theorem might apply to Brownian motors. They showed that the dynamics of the energy along a transient period of a Nosè-Hoover oscillator (a classic model for aBrownian motor) obeys the Jarzynski equality. However, it has been later shown that the Nosè-Hoover oscillator is Hamiltonian and that the Jarzynski equality holds  3, 4 . As a result, the claim that the fluctuation theorem might apply to Brownian motors was found to be incorrect. Our work adds Nanodevices to the list of systems for which the Jarzynski equality does not hold. We emphasize that the Jarzynski equality is not required for the fluctuation theorem to apply; indeed, our result shows that even the more general modified fluctuation relation holds for the system that we consider. References:  1  Nataf, C. & Vulpiani, A. (2012). Fluctuation theorem for intermittent protocols. Physical Review E 85(5), 051122.  2  Nataf, C. & Vulpiani, A. (2012). Fluctuation theorem for diffusion. Physical Review E 86(4), 041125.  3  Jarzynski, C. (1996). nonequilibrium equality for free energy differences. Physical Review E, 53(6), 706.  4  Jarzynski, C. (1996). Equilibrium free energy differences from nonequilibrium measurements. Physical Review E, 53(7), 733.  5  Nath, R., Satija, I., Zhang, Y., & Katsanos, C. (2014). Quantum fluctuation theorem for nanomechanical work. Physical Review E, 90(2), 022130. We consider the fluctuation theorem for nanodevices, namely the relation between the probability distributions of the work done on the system and the free energy difference between two stable states of the system. The case of classical Maxwell s demon has been recently revisited and",
        "watermark_text": "Nanodevices typically lack traditional analogues for the movement of temperature and charge . Nonetheless , it has been argued that the fluctuation principle might apply to them 1 , 2 .Here we prove that this is in indeed not the case . We consider the Jarzynski equality for nanodevices 3 , 4 and demonstrate that it fails for a simple scheme of two coupled Langevin equations with a bistable potential .The conventional derivation of the Jarzynski equality , using the reversibility of Hamiltonian physics , no longer applies in this instance . As a result , it has been conjectured that the fluctuation principle might not apply to nanodevices 5 .We see that this is also not the case and provide a revised fluctuation condition , which we prove by direct numerical integration of the full nanodevice dynamics . In 1 , 2 , Nataf and Vulpiani argued that the fluctuation principle might apply to Brownian motors .They showed that the dynamics of the power along a transient interval of a Nosè - Hoover oscillator ( a traditional description for aBrownian motor ) obeys the Jarzynski equality . However , it has been later seen that the Nosè - Hoover oscillator is Hamiltonian and that the Jarzynski equality holds 3 , 4 .As a result , the suggestion that the fluctuation principle might apply to Brownian motors was found to be incorrect . Our study adds Nanodevices to the list of models for which the Jarzynski identity does not hold .We emphasize that the Jarzynski equality is not required for the fluctuation theorem to apply ; nevertheless , our conclusion shows that even the more general modified fluctuation condition holds for the scheme that we define . References : 1 Nataf , C . & Vulpiani , A .( 2012 ) . Fluctuation theorem for intermittent protocols .Physical Review E 85(5), 051122.2  Nataf, C. & Vulpiani, A.( 2012 ) . Fluctuation principle for diffusion .Physical Review E 86(4), 041125.3  Jarzynski, C. (1996).nonequilibrium equality for free energy differences . Physical Review E , 53 ( 6 ) , 706 .4 Jarzynski , C . ( 1996 ) . Equilibrium free energy differences from nonequilibrium measurements .Physical Review E, 53(7), 733.5  Nath, R., Satija, I., Zhang, Y., & Katsanos, C. (2014).Quantum fluctuation theorem for nanomechanical work . Physical Review E , 90 ( 2 ) , 022130 .We consider the fluctuation principle for nanodevices , particularly the relation between the probability distributions of the science performed on the system and the free energy difference between two stable states of the system . The case of classical Maxwell s demon has been recently revisited and",
        "rewrite_text": "Nanoscale devices often lack traditional counterparts for managing temperature and charge movements. However, there has been debate about whether the fluctuation principle can be applied to them. In this study, we prove that this is not the case. We examine the Jarzynski equality in nanodevices and demonstrate its failure through a simple model using two coupled Langevin equations with a bistable potential. The conventional derivation of the Jarzynski equality, relying on the reversibility of Hamiltonian physics, does not apply in this context. Consequently, it has been speculated that the fluctuation principle may not apply to nanodevices. However, we show that this is not true and provide a revised fluctuation condition, which we verify through direct numerical integration of the complete nanodevice dynamics.\n\nIn previous research, Nataf and Vulpiani argued that the fluctuation principle might be applicable to Brownian motors. They found that the power dynamics within a transient period of a Nosé-Hoover oscillator (a traditional description of a Brownian motor) follows the Jarzynski equality. However, it has been subsequently observed that the Nosé-Hoover oscillator is Hamiltonian and that the Jarzynski equality holds true in that context. Therefore, the idea that the fluctuation principle could apply to Brownian motors was ultimately disproved.\n\nOur study adds nanodevices to the list of models where the Jarzynski identity does not hold. We emphasize that the Jarzynski equality is not essential for the fluctuation theorem to be valid. Nevertheless, our findings indicate that even the more general modified fluctuation condition holds true for our defined scheme.\n\nReferences:\n\n1. Nataf, C., & Vulpiani, A. (2012). Fluctuation theorem for intermittent protocols. Physical Review E, 85(5), 051122.\n2. Nataf, C., & Vulpiani, A. (2012). Fluctuation principle for diffusion. Physical Review E, 86(4), 041125.\n3. Jarzynski, C. (1996). Nonequilibrium equality for free energy differences. Physical Review E, 53(6), 706.\n4. Jarzynski, C. (1996). Equilibrium free energy differences from nonequilibrium measurements. Physical Review E, 53(7), 733.\n5. Nath et al. (2014). Quantum fluctuation theorem for nanomechanical work. Physical Review E, 90(2), 022130.\n\nWe examine the fluctuation principle in nanodevices, specifically examining the relationship between the probability distributions of scientific experiments conducted on the system and the free energy difference between two stable states of the system. The case of the classical Maxwell's demon has recently been revisited and further studied in this context.",
        "ori-fast-z-score": -2.651650429449553,
        "water-fast-z-score": 4.065863991822648,
        "rewrite-fast-z-score": 0.34050261230349943
    },
    {
        "original_text": "Debris disks are remnants of protoplanetary disks, which are the precursors of planets. Many debris disks exhibit surprising uniformity in dust properties and luminosities, suggesting that planet formation may occur rapidly, over a span of <1000 years. Nonetheless, there is compelling evidence for the ongoing generation of planetesimals in many debris disks, including dynamical evidence for embedded planet-mass bodies, and collisional evidence for rapidly produced large planetesimals. The collisional evolution of planetesimals is responsible for the accretion of debris disks, which can cause observable features such as infrared excesses and gaps. The onset of this collisional evolution is delayed by the generation of embryo bodies. I summarize observational constraints on embedded planet-mass bodies in multiple debris disk systems, present new dynamical limits on their existence, and suggest that the dynamical evidence for planet-mass bodies is compelling. I also summarize constraints on the collisional evolution of planetesimals via large body collisions, presenting constraints on the time-span of planetesimal growth, and discuss the implications of these constraints for the generation of short-lived planets. I conclude that the concurrent generation of planetesimals and embryos in debris disks is a robust conclusion, and that an explanation for their concurrent existence is likely required.",
        "watermark_text": "Debris disks are fragments of protoplanetary disks , which are the precursors of planets . Many debris disks exhibit surprising uniformity in cloud qualities and luminosities , showing that planet development would occur swiftly , over a period of < 1000 years .Nonetheless , there is strong evidence for the ongoing generation of planetesimals in many scattered disks , including dynamical evidence for embedded planet - mass bodies , and collisional evidence for swiftly produced huge planetesimals . The collisional development of planetesimals is responsible for the accretion of debris disks , which can cause observable features such as infrared excesses and gaps .The initiation of this collisional development is delayed by the generation of embryo bodies . I summarize observational restrictions on embedded planet - mass bodies in multiple debris disk systems , provide novel dynamical restrictions on their existence , and suggest that the dynamical evidence for planet - mass bodies is intriguing .I also summarize constraints on the collisional development of planetesimals via large bodies collisions , presenting limitations on the period - span of planetesimal creation , and consider the implications of these limits for the generation of short - lived planets . I propose that the concurrent generation of planetesimals and embryos in debris disks is a reliable conclusion , and that an excuse for their simultaneous creation is probably needed .",
        "rewrite_text": "Fragmented disks, known as debris disks, are remnants of protoplanetary disks which are the precursors of planets. Many of these debris disks exhibit remarkable consistency in cloud properties and luminosities, indicating that planet formation can occur rapidly, within a timeframe of less than 1000 years. Nevertheless, there is ample evidence suggesting the continuous formation of planetesimals in many scattered disks. This evidence includes dynamic indications of embedded planet-mass bodies and collisional evidence for the swift creation of large planetesimals. The collisional evolution of planetesimals is responsible for the accumulation of debris disks, resulting in observable features such as infrared excesses and gaps. The initiation of this collisional process is delayed by the formation of embryonic bodies.\n\nIn this summary, I outline the observational limitations concerning embedded planet-mass bodies in multiple debris disk systems. I provide novel dynamic constraints on their existence and suggest that the dynamic evidence for planet-mass bodies is intriguing. Additionally, I summarize the restrictions on the collisional development of planetesimals through large body collisions, presenting limitations on the time span of planetesimal creation. I consider the implications of these limits for the generation of short-lived planets. I propose that the concurrent formation of planetesimals and embryos in debris disks is a reliable conclusion, and that there may be a need to explain their simultaneous creation.",
        "ori-fast-z-score": 0.30460384954008574,
        "water-fast-z-score": 7.273098320775917,
        "rewrite-fast-z-score": 2.6101885204232915
    },
    {
        "original_text": "This work presents the sensitivity analysis of MICS Asia Phase II Daily Gridded Data products to the Aerosol Module for both Drought and Non-drought seasons. The analysis indicates that the MICS Asia Phase II Aerosol product are generally robust to the choice of Aerosol optical depth for both seasons. However, there are a few channels which were found to be significantly impacted by choice of AOD inputs. These are the channels corresponding to AOD measured by IASI over Southern India, and the AOD measured by MISR over the Eastern Pacific Ocean. The analysis also indicates that MICS Asia Phase II AOD product are more sensitive to AOD computed using the correlated thinNRG and DRF models for the drought season. This can be due to the contribution of AOD over the Indian region in the drought season which is not well captured by the correlated models. In terms of AOD sources, the analysis suggests that ignoring the AOD contribution fromBoundary layer and molecular species may significantly impact the MICS Asia Phase II AOD product in the drought season. This paper is a product from the MICS Asia Phase II Technical Internal Review.",
        "watermark_text": "This project offers the sensitivity analysis of MICS Asia Phase II Daily Gridded Data products to the Aerosol Module for both Drought and Non - drought seasons . The evaluation indicates that the MICS Asia Phase II Aerosol product are typically reliable to the selection of Aerosol optical thickness for both seasons .However , there are a few channels which were found to be substantially affected by choosing of AOD inputs . These are the channels corresponding to AOD measured by IASI over Southern India , and the AOD measured by MISR over the Eastern Pacific Ocean .The investigation also reveals that MICS Asia Phase II AOD product are more sensitive to AOD computed using the correlated thinNRG and DRF models for the drought season . This can be due to the impact of AOD over the Indian region in the drought season which is not well captured by the correlated models .In terms of AOD sources , the evaluation suggests that avoiding the AOD contribution fromBoundary layer and molecular species may significantly affect the MICS Asia Phase II AOD product in the drought season . This paper is a product from the MICS Asia Phase II Technical Internal Review .",
        "rewrite_text": "This study presents a sensitivity analysis of the MICS Asia Phase II Daily Gridded Data products to the Aerosol Module, specifically focusing on both drought and non-drought seasons. The evaluation findings indicate that the MICS Asia Phase II Aerosol products are generally reliable in selecting Aerosol optical thickness for both seasons. However, a few channels have been found to be significantly impacted by the choice of Aerosol Optical Depth (AOD) inputs. Notably, these channels correspond to AOD measurements made by IASI in Southern India and by MISR over the Eastern Pacific Ocean.\n\nThe investigation further reveals that the MICS Asia Phase II AOD products are more sensitive to AOD calculations using the correlated thinNRG and DRF models during the drought season. This could be attributed to the impact of AOD in the Indian region during drought, which is not accurately captured by the correlated models. In terms of AOD sources, the evaluation suggests that avoiding the contribution of AOD from boundary layer and molecular species can have a significant impact on the MICS Asia Phase II AOD product during the drought season. This paper is a part of the MICS Asia Phase II Technical Internal Review.",
        "ori-fast-z-score": -0.1259881576697424,
        "water-fast-z-score": 4.0,
        "rewrite-fast-z-score": -0.24253562503633297
    },
    {
        "original_text": "Comptonization is often invoked to explain the complex spectra of accreting compact objects, such as X-ray binaries and black hole binaries. The scenario requires that seed photons from a cold disk are upscattered by electrons in a hot plasma, producing a power-law component that may contribute up to 100% of the spectrum in some systems. Here, we present broadband X-ray observations of the island state in the transient binary system IGR J1748-288, carried out with the orbiting X-ray observatory XMM-Newton. The data reveal that the source s emission extends to hard X-rays, with a power-law photon index of −2.3 ± 0.2. This result is in stark contrast to the Wien tail of the blackbody spectrum that is usually observed in this state. We explore the Comptonization model in detail and show that it provides a better description of the broadband X-ray spectrum than does a simple thermal accretion shock model. Using Monte Carlo simulations, we explore the parameter space and demonstrate that the inclusion of relativistic Doppler shift and broadening is critical to the successful application of this model to the broadband spectra of IGR J1748-288. Finally, we show that the Comptonizing plasma temperature is positively correlated with the amplitude of the kHz QPOs, supporting the idea that these QPOs are a Doppler-scaled version of the inner accretion flow.",
        "watermark_text": "Comptonization is often invoked to explain the complex spectra of accreting compact objects , such as X - ray binaries and dark hole binaries . The scenario involves that seed photons from a cold disk are upscattered by atoms in a heated plasma , creating a power - law component that might contribute up to 100 % of the spectrum in some systems .Here , we present broadband X - ray observations of the island state in the transient binary system IGR J1748 - 288 , carried out with the orbiting X - ray observatory XMM - Newton . The data reveal that the source s emission extends to hard X - rays , with a power - law photon index of −2 . 3 ± 0 . 2 .This result is in stark contrast to the Wien tail of the blackbody spectrum that is usually observed in this state . We explore the Comptonization theory in detail and suggest that it gives a better model of the broadband X - ray spectrum than does a simple thermal accretion shock model .Using Monte Carlo simulations , we investigate the parameter room and suggest that the introduction of relativistic Doppler shift and broadening is important to the efficient application of this description to the broadband spectra of IGR J1748 - 288 . Finally , we prove that the Comptonizing plasma heat is strongly correlated with the frequency of the kHz QPOs , supporting the idea that these QPOs are a Doppler - scaled version of the inner accretion pump .",
        "rewrite_text": "Comptonization is frequently utilized to elucidate the intricate spectra of compact accreting objects, such as X-ray binaries and dark hole binaries. This process entails the upscattering of seed photons from a cold disk by atoms in a heated plasma, resulting in a power-law component that may account for up to 100% of the spectrum in certain systems. In this study, we present broadband X-ray observations of the island state in the transient binary system IGR J1748-288, obtained through the XMM-Newton orbiting X-ray observatory. The data reveal that the source's emission extends to hard X-rays, with a power-law photon index of -2.3 ± 0.2. This finding contrasts sharply with the Wien tail of the blackbody spectrum typically observed in this state. We delve into the Comptonization theory and suggest that it provides a superior model for the broadband X-ray spectrum compared to a simple thermal accretion shock model. Utilizing Monte Carlo simulations, we explore the parameter space and propose that the inclusion of relativistic Doppler shifts and broadening is crucial for the effective application of this description to the broadband spectra of IGR J1748-288. Finally, we establish a strong correlation between the Comptonizing plasma heat and the frequency of kHz QPOs, supporting the notion that these QPOs are a Doppler-scaled version of the inner accretion pump.",
        "ori-fast-z-score": 0.5184758473652127,
        "water-fast-z-score": 5.157106231293967,
        "rewrite-fast-z-score": 2.799769575772148
    },
    {
        "original_text": "The unprecedented resolution and dynamic range of modern cosmological simulations has allowed the quantification of several previously-unsuspected discreteness effects. We focus here on those which could potentially affect the validity of the cosmological paradigm, namely the inability of standard algorithms to properly follow the behavior of dark matter particles past the moment at which they become progressively confined to collapsed structures. We find that these “shell-crossing” events induce systematic displacements in the spatial distribution of dark matter particles of up to several tens of kiloparsecs, which are long-lasting and require hundreds of Hubble times for reestablishment. These so-called “ discreteness effects” may therefore represent a significant obstacle to using dark matter as a cosmological tool, potentially leading to biases on scales comparable to or larger than those observed in current galaxy surveys. In this companion paper we quantify the extent to which these previously unquantified effects affect current cosmological data.",
        "watermark_text": "The extraordinary resolution and dynamic range of modern cosmological simulations has allowed the quantification of several already - unsuspected discreteness effects . We focus here on those which could potentially impact the legitimacy of the cosmological paradigm , notably the failure of standard methods to properly follow the dynamics of bright matter molecules past the moment at which they become steadily restricted to collapsed structures .We see that these “ shell - crossing ” processes create widespread displacements in the spatial distribution of dark matter molecules of up to several tens of kiloparsecs , which are long - lasting and require hundreds of Hubble periods for reestablishment . These so - called “ discreteness impacts ” might hence pose a substantial obstacle to use bright matter as a cosmological tool , possibly leading to biases on scales similar to or larger than those observed in current galaxy surveys .In this companion publication we quantify the extent to which these originally unquantified impacts affect current cosmological information .",
        "rewrite_text": "The remarkable precision and extensive scope of modern cosmological simulations has enabled the quantification of various previously unidentified discreteness effects. Our focus lies on those that potentially challenge the validity of the cosmological paradigm. Specifically, we note the limitations of conventional methods in accurately tracking the dynamics of bright matter molecules once they are confined within collapsed structures. These \"shell-crossing\" processes result in widespread displacements of dark matter molecules, spanning up to several tens of kiloparsecs and persisting for hundreds of Hubble periods. These so-called \"discreteness impacts\" may significantly undermine the use of bright matter as a cosmological tool, potentially introducing biases on scales comparable to or exceeding those observed in current galaxy surveys. In this companion publication, we quantify the extent to which these previously unquantified impacts influence current cosmological data.",
        "ori-fast-z-score": 0.1125087900926024,
        "water-fast-z-score": 5.962965874907927,
        "rewrite-fast-z-score": -0.12216944435630522
    },
    {
        "original_text": "The Carter constant, a proportionality constant between an inspiraling compact object s spin and orbital angular momentum, has been shown to play an important role in determining whether or not a binary black hole system will form following an inspiral. In a recent work  Hinder et al. (2021) , the role of the Carter constant in the dynamics of black hole binary systems has been investigated, and it was shown that, for systems which form, the constant evolves in time such that the binary system spins are aligned with the orbital angular momentum. It was also shown that in certain cases, depending on the magnitude of the quadrupolar moment of the black hole and the orientation of the orbital plane, the constant evolves such that the spin of one of the black holes is antiparallel to the orbital angular momentum. In this work, we extend the aforementioned study by systematically investigating the effect of the quadrupolar moment of the black holes on the evolution of the Carter constant. We find that for binaries with small quadrupoles, the evolution is qualitatively similar to the case with no quadrupolar moment. However, for larger quadrupoles, the evolution is no longer continuous and can result in divergent values of the constant. We demonstrate that this behavior can be understood in terms of a simple geometric argument based on the impact parameter of the binary system, and we show that a critical value of the quadrupolar moment can be identified above which the evolution of the constant becomes qualitatively different. We provide examples of systems with various properties for which our predictions apply and discuss the astrophysical implications of our findings.",
        "watermark_text": "The Carter constant , a proportionality relation between an inspiraling compact body s spin and orbital angular velocity , has been shown to take an important role in calculating whether or not a binary dark hole system will form following an inspiral . In a recent work Hinder et al .( 2021 ) , the importance of the Carter constant in the dynamics of black hole binary systems has been investigated , and it was shown that , for systems which form , the constant evolves in time such that the binary system twists are aligned with the orbital angular velocity . It was also shown that in certain cases , depending on the magnitude of the quadrupolar moment of the dark hole and the orientation of the orbital plane , the constant evolves such that the spin of one of the dark holes is antiparallel to the orbital angular velocity .In this study , we expanded the aforementioned study by carefully examining the impact of the quadrupolar moment of the dark holes on the evolution of the Carter constant . We see that for binaries with little quadrupoles , the evolution is qualitatively identical to the case with no quadrupolar moment .However , for larger quadrupoles , the evolution is no longer continuous and can result in divergent expressions of the constant . We showed that this behavior can be understood in terms of a simple geometric argument based on the impact parameter of the binary system , and we prove that a critical value of the quadrupolar moment can be identified above which the evolution of the constant appears qualitatively unique .We provide descriptions of systems with various properties for which our predictions apply and explain the astrophysical consequences of our findings .",
        "rewrite_text": "The Carter constant, which establishes a proportional relationship between a compact body's spin and orbital angular velocity in a spiral motion, has been recognized as crucial in determining whether a binary dark hole system will form during an inspiral process. In a recent study by Hinder et al. (2021), the significance of the Carter constant in the dynamics of black hole binary systems was explored. It was discovered that in systems that do form, the constant evolves over time such that the twists of the binary system align with the orbital angular velocity. Furthermore, it was demonstrated that in certain cases, depending on the magnitude of the dark hole's quadrupolar moment and the orientation of the orbital plane, the constant evolves in such a way that the spin of one of the dark holes becomes antiparallel to the orbital angular velocity.\n\nIn our study, we extended the previous research by meticulously examining the impact of the dark hole's quadrupolar moment on the evolution of the Carter constant. Our findings reveal that for binaries with small quadrupoles, the evolution behaves qualitatively similar to that of systems without a quadrupolar moment. However, for larger quadrupoles, the evolution becomes discontinuous and can lead to divergent expressions of the constant. We elucidated this behavior using a simple geometric argument based on the impact parameter of the binary system. We also demonstrate that a critical value of the quadrupolar moment can be identified, above which the evolution of the constant appears qualitatively unique.\n\nWe provide descriptions of systems with various properties that our predictions apply to and explain the astrophysical implications of our findings.",
        "ori-fast-z-score": -1.0169503597462533,
        "water-fast-z-score": 4.6028730894916166,
        "rewrite-fast-z-score": 2.0252641593763117
    },
    {
        "original_text": "The process of reionization ended the darkness of the universe and is a powerful tool for studying its physics and astronomy. 21-cm emission and absorption traces the neutral fraction of the intergalactic medium (IGM) and is influenced by the structure of the underlying density field and the dynamics of galaxies. Early stages of cosmic reionization are challenging to observation as these are localized features in the large-scale 21-cm signal. Correlated random walks (CRWs) is a statistical tool that can be used to infer small scale information from the large-scale signal. In this paper, we consider the effect of galaxy bias and non-linear coupling on the CRW statistics. We generate CRW signals from N-body simulations of the physics of cosmic reionization and observe that the CRW signal is maximized for galaxy bias at the percent level. Applying the CRW signal to observations from the Global Epoch of Reionization Array (Glow) we are able to place bounds on the galaxy bias parameter of up to 3.7% (1 sigma). Furthermore, we show that the best-fit bias does not match the intrinsic bias (bias calculated from the N-body simulation) and indicate future observations at higher frequency may be able to better constrain the bias.",
        "watermark_text": "The method of reionization destroyed the dark of the universe and is a powerful tool for studying its physics and astronomy . 21 - cm absorption and emission marks the neutral fraction of the intergalactic medium ( IGM ) and is influenced by the composition of the underlying abundance field and the dynamics of stars .Early stages of universe reionization are challenging to observation as these are localized characteristics in the huge - scale 21 - cm signal . Correlated random tours ( CRWs ) is a statistical tool that can be used to infer tiny scale information from the huge - scale signal .In this paper , we investigate the impact of galaxy bias and non - linear correlation on the CRW statistics . We generate CRW signals from N - bodies simulations of the physics of universe reionization and find that the CRW signal is maximized for galaxy distortion at the percent level .Applying the CRW noise to observations from the Global Epoch of Reionization Array ( Glow ) we are able to place bounds on the galaxy bias variable of up to 3 . 7 % ( 1 sigma ) . Furthermore , we prove that the best - fitting bias does not match the intrinsic bias ( bias determined from the N - bodies simulation ) and suggest future discoveries at higher frequency might be able to good constrain the bias .",
        "rewrite_text": "The reionization method has eradicated the darkness of the universe and serves as a potent instrument for studying its physics and astronomy. The 21-cm absorption and emission mark the neutral fraction of the intergalactic medium (IGM), which is influenced by the composition of the underlying abundance field and the dynamics of stars. Observing the early stages of universe reionization is challenging due to their localized characteristics within the vast-scale 21-cm signal. Correlated random tours (CRWs) provide a statistical tool that can extract minute-scale information from the large-scale signal.\n\nIn this paper, we explore the effects of galaxy bias and non-linear correlation on CRW statistics. We generate CRW signals from N-body simulations of universe reionization physics and find that the CRW signal peaks when there is a percentage-level distortion in galaxy bias. By applying CRW noise to observations from the Global Epoch of Reionization Array (Glow), we can establish constraints on the galaxy bias variable up to 3.7% (1 sigma). Furthermore, we demonstrate that the best-fitting bias does not align with the intrinsic bias (bias determined through N-body simulations), suggesting that future observations at higher frequencies may provide tight constraints on the bias.",
        "ori-fast-z-score": -1.1406468642034677,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 1.6329931618554523
    },
    {
        "original_text": "In this paper, we study the following two stage hybrid and modular inflation: 1. Early hybrid inflation driven by a gauge field coupled to a fundamental scalar and its superpartner. 2. Modular inflation driven by a charged scalar field coupled to gravity. We find that generically these two stages can occur sequentially with the decay of the false vacuum of the first stage producing the curvature perturbations needed for the second stage. This allows the flexibility to choose the parameters of each model without compromising the feasibility of the subsequent stage. The spectrum of the density perturbations is flat in this model. This work is related to the previous paper arXiv:1908.05761  hep-th  in that we consider a slightly different scenario where the decay of the false vacuum of the first stage does not produce the curvature perturbation but rather enhances it. This work is also related to arXiv:1907.11961  hep-th  in that we consider a model with multiple stages of inflation and find conditions on the parameters of each stage such that they can follow each other.",
        "watermark_text": "In this paper , we study the following two stage hybrid and modular inflation : 1 . Early hybrid inflation driven by a gauge field coupled to a basic scalar and its superpartner .2 . Modular inflation driven by a charged scalar field coupled to gravity .We see that generically these two stages can occur sequentially with the decay of the false vacuum of the first phase creating the curvature perturbations needed for the second phase . This enables the flexibility to choose the variables of each model without compromising the feasibility of the subsequent stage .The spectrum of the density perturbations is flat in this simulation . This research is related to the previous report arXiv : 1908 . 05761 hep - th in that we imagine a significantly changed scenario where the decay of the false vacuum of the first phase does not produce the curvature perturbation but rather enhances it .This study is also linked to arXiv : 1907 . 11961 hep - th in that we study a theory with many stages of inflation and find conditions on the variables of each stage such that they can track each other .",
        "rewrite_text": "In this paper, we investigate the two-stage hybrid and modular inflation, which includes: 1. Early hybrid inflation, driven by a gauge field coupling with a fundamental scalar and its superpartner. 2. Modular inflation, driven by a charged scalar field coupled to gravity. We observe that these two stages can commonly occur sequentially, with the collapse of the false vacuum in the first phase generating the necessary curvature perturbations for the second phase. This provides flexibility in selecting model variables without compromising the feasibility of subsequent stages. In this simulation, the spectrum of density perturbations is flat.\n\nThis research is closely related to previous reports in arXiv: 1908.05761 hep-th, where we consider a significantly altered scenario where the collapse of the false vacuum in the first phase does not generate curvature perturbations but rather amplifies them. Furthermore, it is also linked to arXiv: 1907.11961 hep-th in which we explore a theory involving multiple stages of inflation and identify conditions on each stage's variables to ensure they can be synchronized with each other.",
        "ori-fast-z-score": -0.23249527748763857,
        "water-fast-z-score": 4.649905549752772,
        "rewrite-fast-z-score": 0.917662935482247
    },
    {
        "original_text": "A natural broadening of the iron K alpha resonance line was observed in the X-ray spectrum of NGC 3783 for the first time. The observed width of this line is significantly larger than the instrumental resolution. Since the observed width is comparable to the velocity of the gas in the Broad Line Region, the observed broadening is most likely produced by velocity-dependent scattering in the disk of the nucleus. This scattering may be caused by small scale inhomogeneities in the gas density or by deviations from spherical symmetry in the gravitational potential of the nucleus. The redshifted iron K alpha line and the underlying continuum were also studied for the first time. The shape of the continuum is in good agreement with the results of previous studies. The equivalent width of the line was measured to be approximately 120 eV. The equivalent width of this line in the rest frame is approximately 240 eV. The observed correlation between the equivalent width and redshift of the line indicates that the line emission and fluorescence processes in the Broad Line Region are more complex than previously assumed. The variability of NGC 3783 on different time scales was also investigated. Two years of X-ray observations with Chandra revealed no significant variability of the line flux. The variability of the equivalent width was not significant at a 2-sigma level. Variability on longer time scales cannot be detected in the available data.",
        "watermark_text": "A natural broadening of the metal K alpha resonance pattern was seen in the X - ray spectrum of NGC 3783 for the first time . The observed width of this line is significantly larger than the instrumental resolution .Since the seen width is equal to the velocity of the gas in the Broad Line Region , the seen broadening is most likely generated by velocity - dependent scattering in the disk of the nucleus . This scattering may be caused by small scale inhomogeneities in the gas density or by deviations from spherical symmetry in the gravitational potential of the nucleus .The redshifted metal K alpha line and the underlying continuum were also examined for the first time . The structure of the continuum is in good agreement with the conclusion of previous research .The equivalent size of the line was measured to be approximately 120 eV . The equivalent height of this line in the remaining frame is approximately 240 eV .The observed correlation between the equivalent thickness and redshift of the line indicates that the line emission and fluorescence systems in the Broad Line Region are more sophisticated than previously predicted . The variability of NGC 3783 on various time ranges was also examined .Two years of X - ray observations with Chandra revealed no notable variability of the line flux . The variability of the equivalent size was not considerable at a 2 - sigma rate .Variability on longer time ranges cannot be spotted in the provided information .",
        "rewrite_text": "For the first time in the X-ray spectrum of NGC 3783, a natural broadening of the metal K alpha resonance pattern was observed. The measured line width was significantly greater than the instrumental resolution. As this observed width corresponds to the velocity of the gas in the Broad Line Region, it is highly likely that the broadening is caused by velocity-dependent scattering within the nucleus's disk. This scattering could be attributed to small-scale inhomogeneities in gas density or deviations from spherical symmetry in the gravitational potential of the nucleus.\n\nFurthermore, the redshifted metal K alpha line and its underlying continuum were examined for the first time. The structure of the continuum agreed well with previous research findings. The measured equivalent width of the line was approximately 120 eV, while its equivalent height in the remaining frame was approximately 240 eV. The observed correlation between the equivalent thickness and redshift of the line suggests that the line emission and fluorescence systems in the Broad Line Region are more complex than previously anticipated.\n\nAdditionally, the variability of NGC 3783 across various time ranges was investigated. Two years of X-ray observations with Chandra revealed no significant variability in the line flux. The variability of the equivalent size was also not significant at a 2-sigma level. However, no variability could be detected on longer time scales based on the provided information.",
        "ori-fast-z-score": 0.29559878344928797,
        "water-fast-z-score": 6.404640308067906,
        "rewrite-fast-z-score": 1.7822655773580138
    },
    {
        "original_text": "Silicon surfaces present many intriguing phenomena owing to the large difference in electronegativity between silicon and oxygen. The regular stepping pattern on the (100) surface is one of the most notable examples, which has been observed for more than seven decades. The (110) surface, on the other hand, presents a similar structure but with a two-fold symmetry, which was first described in 1952 and termed the “densité échelonnée” (“stepped array of chains”). Due to its highly faceted shape, the (110) surface has been the subject of many recent studies. Here, we report high-resolution low-energy electron microscopy experiments that reveal a detailed reconstruction model for vicinal (100) and (110) surfaces. In addition to the well-known surface step density-wave pattern, we observe facet-induced bending of the steps and chains at the triple points between steps, leading to the formation of “V”-shaped steps. Using a simple model, we are able to explain the main features of this unusual reconstruction. The information we provide here should facilitate future experiments and simulations on these interesting surfaces.",
        "watermark_text": "Silicon surfaces present many intriguing processes owing to the huge variation in electronegativity between silicon and oxygen . The periodic walking rhythm on the ( 100 ) surface is one of the most notable examples , which has been observed for more than seven decades .The ( 110 ) surface , on the other hand , presents a similar formation but with a two - fold symmetry , which was first described in 1952 and termed the “ densité échelonnée ” ( “ stepped array of rings ” ) . Due to its highly faceted structure , the ( 110 ) surface has been the subject of several current investigations .Here , we publish high - resolution low - energy electron microscopy experiments that discover a detailed reconstruction model for vicinal ( 100 ) and ( 110 ) surfaces . In addition to the good - described surface step density - wave pattern , we study facet - caused bending of the steps and chains at the double points between steps , leading to the formation of “ V ” - shaped steps .Using a simple model , we are able to explain the main features of this extraordinary reconstruction . The data we provide here should enable future research and simulations on these interesting surfaces .",
        "rewrite_text": "Silicon surfaces exhibit numerous fascinating processes stemming from the vast electronegativity difference with oxygen. The periodic walking pattern on the (100) surface stands out as one of the most notable examples, having been observed for over seven decades. In contrast, the (110) surface displays a similar formation with a two-fold symmetry, which was first described in 1952 and is termed the \"densité échelonnée\" or \"stepped array of rings.\" Due to its highly faceted structure, the (110) surface has been a focal point of numerous current investigations. In this study, we present high-resolution low-energy electron microscopy experiments that uncover a detailed reconstruction model for both the vicinal (100) and (110) surfaces. Beyond the well-described surface step density wave pattern, we investigate how facets influence the bending of steps and chains at double points between steps, resulting in the formation of \"V\"-shaped steps. Using a straightforward model, we can explain the primary features of this remarkable reconstruction. The data presented here should facilitate future research and simulations on these intriguing surfaces.",
        "ori-fast-z-score": 1.9409899176618914,
        "water-fast-z-score": 6.754308969478107,
        "rewrite-fast-z-score": 4.061277619618543
    },
    {
        "original_text": "This paper is about controllability of the heat and wave equations. We first present a series of new, explicit, finite difference methods to approximate the heat and wave equations. We then study the controllability of the finite difference methods by defining controls as localized functions on the edges of the domain. By carefully choosing these localized functions we are able to show that the exact solutions of the heat and wave equations are, in fact, controllable to any desired state in arbitrary small time. This result holds for any positive time, even for a time independent of the size of the domain. Our controllability results rely only on the local behavior of the exact solution and the spatial behavior of the controls. We also present counterexamples to show that our results are not true in general for the exact heat and wave equations. This is joint work with Marta Sanz-Soriano.",
        "watermark_text": "This paper is about controllability of the temperature and wave equations . We first offer a series of new , explicit , finite difference methods to approximate the temperature and wave equations .We then explore the controllability of the finite difference methods by setting controls as confined functions on the edges of the domain . By carefully choosing these localized functions we are able to see that the exact solutions of the temperature and wave equations are , in indeed , controllable to any desired state in arbitrary small time .This result holds for any positive time , even for a time independent of the extent of the domain . Our controllability results relies only on the local behavior of the exact solution and the spatial behavior of the controls .We also present counterexamples to find that our findings are not true in general for the exact heat and wave equations . This is joint research with Marta Sanz - Soriano .",
        "rewrite_text": "This research focuses on the controllability of temperature and wave equations. We introduce a range of innovative, explicit, finite difference methods to approximate these equations. We then investigate the controllability of these finite difference methods by defining controls as constrained functions at the domain edges. By carefully selecting these localized functions, we are able to ascertain that the exact solutions for temperature and wave equations can indeed be controlled to any desired state within an arbitrarily small time frame. This conclusion holds true for any positive time, even if it is independent of the domain's extent. Our controllability findings are dependent solely on the local behavior of the exact solution and the spatial controls' behavior. Additionally, we provide counterexamples to illustrate that our observations do not necessarily apply universally to the exact heat and wave equations. This collaborative research is conducted with Marta Sanz-Soriano.",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 3.628275563080048,
        "rewrite-fast-z-score": 2.5927248643506746
    },
    {
        "original_text": "Long gamma-ray bursts (GRBs) are the most violent explosions in the universe. However, their study is complicated by their unpredictable, randomly directed beams, making it difficult to precisely localize them. Short-lived and beamed Gamma-Ray Bursts with known cosmological redshifts provide a crucial opportunity for studying the nat... Long gamma-ray bursts (GRBs) are the most violent explosions in the universe. However, their study is complicated by their unpredictable, randomly directed beams, making it difficult to precisely localize them. Short-lived and beamed Gamma-Ray Bursts with known cosmological redshifts provide a crucial opportunity for studying the nature of the universe and its structure. A majority of long GRBs have been observed with the Swift satellite, which provides not only rapid localization but also a wealth of multi-wavelength data. We have examined the spectral evolution of 57 Swift long GRBs with known redshifts, finding that three distinctly different regimes exist. The first regime is characterized by a cooling break followed by a simple power law. Between two and four dissipation slopes are observed in this regime. The second regime, seen in 17 bursts, is characterized by an initial thermal component with a blackbody function or a non-thermal tail, followed by a single power law. In the third and final regime, seen in 30 bursts, no identifiable spectral break is present and the spectra are well fit by a single power law. The cosmological models that best describe the observed burst data are curveduploads and a finite universe, with a probability of 68.3% and 95.7% respectively. This result is surprising because standard candle models assume that all bursts have the same intrinsic luminosity, which would result in a single power law across all regimes. This work was supported in part by a NASA Swift Award No. 1223370 to the University of Wisconsin-Madison.",
        "watermark_text": "Long gamma - ray bursts ( GRBs ) are the most violent bombs in the universe . However , their study is complicated by their unpredictable , randomly directed beams , making it difficult to exactly localize them .Short - lived and beamed Gamma - Ray Bursts with known cosmological redshifts provide a crucial chance for studying the nat . . . Long gamma - ray bursts ( GRBs ) are the most violent burst in the universe . However , their analysis is complicated by their unpredictable , randomly directed beams , making it difficult to exactly localize them .Short - lived and beamed Gamma - Ray Bursts with known cosmological redshifts provide a crucial chance for studying the nature of the universe and its composition . A bulk of long GRBs have been observed with the Swift satellite , which offers not only rapid localization but also a rich of multi - wavelength information .We have analyzed the spectral evolution of 57 Swift length GRBs with recorded redshifts , finding that three distinctly separate regimes occur . The first regime is characterized by a cooling break followed by a simple power law .Between two and four dissipation slopes are observed in this regime . The last regime , visible in 17 bursts , is characterized by an initial thermal component with a blackbody function or a non - thermal tail , followed by a single power law .In the third and final regime , shown in 30 bursts , no identifiable spectral break is seen and the spectra are better suited by a single power law . The cosmological descriptions that better describe the seen burst information are curveduploads and a finite universe , with a probability of 68 . 3 % and 95 . 7 % respectively .This result is surprising because conventional candle estimates assumption that all bursts have the same intrinsic luminosity , which would result in a single power law across all regimes . This research was supported in part by a NASA Swift Award No .1223370 to the University of Wisconsin-Madison.",
        "rewrite_text": "Long gamma-ray bursts (GRBs) are the most intense explosions in the universe. However, their analysis is made complex by their unpredictable, randomly directed beams, making it challenging to pinpoint their exact location. Short-lived, directional Gamma-Ray Bursts with known cosmological redshift offer a pivotal opportunity to delve into the nature and composition of the universe.\n\nThe Swift satellite has observed numerous long GRBs, providing not only swift localization but also a wealth of multi-wavelength data. We have analyzed the spectral evolution of 57 GRBs with recorded redshifts, revealing three distinct regimes. The first regime is marked by a cooling break followed by a straightforward power law, with two to four dissipation slopes observed within this framework. The final regime, evident in 17 bursts, is characterized by an initial thermal component resembling a blackbody function or a non-thermal tail, succeeded by a single power law.\n\nIn the third and final regime, evident in 30 bursts, no discernible spectral break is observed, and the spectra are better suited to a single power law. The cosmological descriptions that best fit the observed burst data suggest curved uploads and a finite universe, with probabilities of 68.3% and 95.7% respectively. This outcome is surprising as traditional estimates assume all bursts have the same intrinsic luminosity, resulting in a single power law across all regimes. This research was partially supported by a NASA Swift Award No. 1223370 granted to the University of Wisconsin-Madison.",
        "ori-fast-z-score": 1.4524080181184935,
        "water-fast-z-score": 6.859943405700353,
        "rewrite-fast-z-score": 3.0237157840738176
    },
    {
        "original_text": "The electronic properties of dry DNA have attracted considerable interest in the recent years due to the potential applications in electronic technologies. The electronic properties of dry DNA are significantly different from those of its hydrated counterpart. For example, dry DNA has much higher conductivity and becomes semiconducting at room temperature. Understanding the electronic properties of dry DNA is important for its potential applications in electronic technologies and DNA-based sensors. Despite the great technological importance, the electronic structure and mechanism of dry DNA has not been fully understood. In this work, we developed a multiscale model for dry DNA which can capture its electronic transport behaviors from the atomic scale to the macroscopic scale. The local structure of a stretch of dry DNA is modeled using the energy functional theory. A transport model is developed using the tight-binding method to describe the electronic transport behavior of the full stretch of dry DNA. The developed model can capture the electronic transport behaviors from the atomic scale to the macroscopic scale and represents a major step forward for understanding the electronic structure and mechanism of dry DNA.",
        "watermark_text": "The digital properties of dried DNA have garnered considerable interest in the recent years owing to the possibilities applications in information devices . The digital properties of moist DNA are drastically different from those of its hydrated counterpart .For instance , drying DNA has much higher conductivity and becomes semiconducting at room temperature . Understanding the electronic properties of dried DNA is important for its potential applications in digital technologies and DNA - based sensors .Despite the great technological importance , the electronic structure and mechanism of dried DNA has not been totally understood . In this project , we developed a multiscale model for drying DNA which can represent its electronic travel activities from the atomic scale to the macroscopic scale .The local structure of a length of dried DNA is modeled using the energy functional theory . A transport description is developed using the tight - binding method to explain the electronic travel dynamics of the full stretch of dried DNA .The published model can represent the electronic transport behaviors from the atomic scale to the macroscopic scale and provides a major stage forward for studying the electronic structure and mechanism of dried DNA .",
        "rewrite_text": "In recent years, the digital attributes of dried DNA have garnered significant interest due to its potential applications in information devices. In contrast to moist DNA, the digital properties of hydrated DNA exhibit drastic variations. For instance, DNA that has been dried out exhibits significantly higher conductivity, even becoming semiconductive at room temperature. Understanding the electronic properties of dried DNA is crucial for its future applications in digital technologies and DNA-based sensors. However, despite its technological significance, the electronic structure and mechanisms of dried DNA remain partially unexplored.\n\nIn this project, we have developed a multiscale model for simulating the drying process of DNA, which can represent its electronic activities ranging from the atomic scale to the macroscopic scale. We model the local structure of a segment of dried DNA using energy functional theory, and develop a transport description utilizing the tight-binding method to explain the electronic travel dynamics across the entire length of the DNA. The published model effectively represents electronic transport behaviors from the microscopic to macroscopic levels, providing a significant advancement in studying the electronic structure and mechanisms of dried DNA.",
        "ori-fast-z-score": -0.9045340337332909,
        "water-fast-z-score": 5.3267004208738244,
        "rewrite-fast-z-score": 3.1156172273035576
    },
    {
        "original_text": "An analysis of quantum error correction in holographic codes suggests that the error correcting properties of holographic codes rely on the causal structure of spacetime, which may be obscured by environmental influences and forgetable upon observation. By considering the effects of environmental interactions on black hole horizon states, we introduce a model of holographic noise which demonstrates that even in the error free case, the readout of local boundary data is statistically inevitable from an irreducible quantum uncertainty in the description of the horizon. Thus, environmental interactions render precise calculation of local boundary data uncertain, and any expectation of local boundary data is illusionary. While the precise form of the noise model presented here is unlikely to correspond to an accurate model of actual experimental conditions, we demonstrate that this noise model imposes a fundamental limitation on the ability to precisely calculate local boundary data and yields a very natural explanation for many experimental results which have previously been difficult to rationalize. In the abstract, the key phrases should be put between quotation marks. The entire abstract should also be put between quotation marks. The long title and the first paragraph are not appropriate for an abstract.",
        "watermark_text": "An evaluation of quantum mistake reduction in holographic coding indicates that the error correcting qualities of holographic coding rely on the causal structure of spacetime , which may be obscured by environmental influences and forgetable upon measurement . By considering the effects of environmental interactions on dark hole horizon states , we create a theory of holographic noise which demonstrates that even in the error free case , the readout of local boundary data is statistically inevitable from an irreducible quantum uncertainty in the description of the horizon .Thus , environmental interactions make accurate calculation of local border data uncertain , and any anticipation of local border data is illusionary . While the exact form of the noise model discussed here is unable to approximate to an accurate model of actual experimental situations , we prove that this noise model imposes a basic limitation on the ability to exactly calculate local border data and yields a very natural explanation for numerous empirical results which have previously been difficult to rationalize .In the abstract , the key passages should be putting between quotation marks . The entire abstract should additionally be putting between quotation marks .The long title and the first paragraph are not appropriate for an abstract .",
        "rewrite_text": "In evaluating quantum error reduction in holographic coding, it has been determined that the error correction properties of holographic coding rely on the causal framework of spacetime. This framework may become obscured by environmental factors and may become forgotten upon measurement. By considering the impact of environmental interactions on dark hole horizon states, a theory of holographic noise has been developed. This theory suggests that even in the absence of errors, the statistical retrieval of local boundary data remains inevitable due to an irreducible quantum uncertainty in the description of the horizon. Therefore, environmental interactions create uncertainty in accurately calculating local boundary data, rendering any anticipation of such data illusory.\n\nWhile the specific noise model discussed here may not accurately mimic real experimental scenarios, it does establish a fundamental limit on the ability to precisely compute local boundary data, providing a natural explanation for numerous empirical results that have previously been challenging to rationalize. Within this abstract, key points should be enclosed in quotation marks. Additionally, the full abstract, as well as the long title and initial paragraph, are not suitable for an abstract's content.",
        "ori-fast-z-score": -0.2,
        "water-fast-z-score": 5.47270454615494,
        "rewrite-fast-z-score": 1.9611613513818404
    },
    {
        "original_text": "Strain localization in a shear transformation zone (STZ) model for amorphous solids is examined. In this model, the amorphous solid is composed of particles interacting via a short-range, repulsive core and a long-range, attractive tail in a potential that is sheared, below the glass transition temperature, Tg, to generate an amorphous solid. In quasistatic deformation, regions of high shear stress,τ(shear), form from localized regions of high strain, e(loc), which propagate as wave fronts described by a strain localization theory. Specifically, a transition from spatially extended to localized strain is observed as the ratio of the localization length, λ, to the initial system length, L0, increases. An analytical theory describing the spatial, temporal, and frequency dependent shear modulus, G(τ,ω), and its relation to the shear stress and strain waveforms is developed, which allows for analysis of the frequency dependence of localized shear stress and strain, τ(loc)(ω) and e(loc)(ω). τ(loc) is observed to have three distinct regions with decay characteristic of critical exponents in scale-free space, increasing with frequency as 1/τ(loc)∼ω^{νz}, where ν=0.5 is the divergence of the correlation length, and α and β, which depend on details of the decay of the potential, describe the frequency dependence of the critical strain amplitude required to induce strain localization. e(loc) exhibits similar critical exponents to τ(loc) except with different critical amplitude exponents α=νz=1/2 and β=1/2. Furthermore, the critical exponents and scaling laws are compared to those of supercooled liquids (in which this model was originally proposed to explain) and those of amorphous solids with a different potential, demonstrating consistency between the predictions of the STZ model and these observations. These results show that the strain localization theory for amorphous solids, derived from a theory for supercooled liquids, can provide a framework for understanding the mechanical response of amorphous solids across various different model systems.",
        "watermark_text": "Strain localization in a shear transformation zone ( STZ ) model for amorphous solids is investigated . In this simulation , the amorphous solid is composed of molecules evolving via a small - range , repulsive backbone and a shorter - range , attractive tail in a potential that is sheared , below the glass transition pressure , Tg , to produce an amorphous solid .In quasistatic deformation , regions of high shear stress , τ ( shear ) , occur from localized regions of high strain , e ( loc ) , which propagate as wave fronts described by a stress localization theory . Specifically , a shift from spatially extended to localized strain is observed as the proportion of the localization width , λ , to the first system width , L0 , increases .An analytical theory explaining the spatial , temporal , and frequency dependent shear modulus , G ( τ , ω ) , and its connection to the shear stress and strain waveforms is developed , which allows for study of the frequency dependence of localized shear stress and strain , τ ( loc ) ( ω ) and e ( loc ) ( ω ) . τ ( loc ) is observed to have three different regions with decay characteristic of critical exponents in scale - free space , increasing with frequency as 1 / τ ( loc ) [UNK] ^ { νz } , where ν = 0 . 5 is the divergence of the correlation length , and α and β , which depend on details of the decay of the potential , explain the frequency dependence of the critical strain frequency needed to induce strain localization .e ( loc ) exhibits similar critical exponents to ω ( loc ) except with different critical amplitude exponents α = νz = 1 / 2 and β = 1 / 2 . Furthermore , the critical exponents and scaling laws are compared to those of supercooled liquids ( in which this model was originally proposed to explain ) and those of amorphous solids with a different potential , showing consistency between the assumptions of the STZ model and these observations .These data reveal that the strain localization theory for amorphous solids , obtained from a theory for supercooled liquids , can provide a framework for studying the mechanical response of amorphous solids across numerous different model environments .",
        "rewrite_text": "The investigation focuses on strain localization within the Shear Transformation Zone (STZ) model for amorphous solids. In this simulation, the amorphous solid is composed of molecules that evolve through a combination of a short-range, repulsive backbone and a shorter-range, attractive tail within a potential that is subjected to shear below the glass transition pressure, Tg, resulting in the formation of an amorphous solid. During quasistatic deformation, regions of high shear stress, τ(shear), arise from localized regions of high strain, e(loc), which propagate as wave fronts described by a theory of stress localization. Specifically, an observation is made of a transition from spatially extended to localized strain as the ratio of localization width, λ, to the first system width, L0, increases.\n\nAn analytical theory has been developed to explain the spatial, temporal, and frequency-dependent shear modulus, G(τ, ω), and its relationship to shear stress and strain waveforms. This allows for the study of the frequency dependence of localized shear stress and strain, τ(loc)(ω) and e(loc)(ω). It is observed that τ(loc) exhibits three distinct regions with a decay characteristic of critical exponents in scale-free space, increasing with frequency as 1/τ(loc)⁻¹νz², where ν = 0.5 represents the divergence of the correlation length. The values of α and β, which depend on the details of potential decay, explain the frequency dependence of the critical strain frequency required to induce strain localization. Similarly, e(loc) exhibits similar critical exponents to ω(loc), but with different critical amplitude exponents α = νz = 1/2 and β = 1/2.\n\nFurthermore, the critical exponents and scaling laws are compared to those observed in supercooled liquids (where this model was originally proposed) and amorphous solids with different potentials, showing a consistency between the assumptions of the STZ model and these observations. These data indicate that the strain localization theory for amorphous solids, derived from a theory for supercooled liquids, can provide a framework for understanding the mechanical response of amorphous solids in a variety of model environments.",
        "ori-fast-z-score": -0.0842151921066519,
        "water-fast-z-score": 4.800265950079158,
        "rewrite-fast-z-score": 3.4759447321299453
    },
    {
        "original_text": "Recently, cognitive radio has been paid much attention due to its capability to automatically find available unused spectrum for new utilizing, which is particularly useful in the scenario of spectrum shortage. In order to avoid interfering with the primary spectrum user, the secondary cognitive radio transmitter has to adopt the rate adaptation to control its transmitting rate based on the interference received by the primary user, which is, however, unknown and difficult to observe. To this end, this paper proposes a non-Bayesian rate adaptation with two design approaches: one is the restless multi-armed bandit based approach for finding the unavailable spectrum in an automated and adaptive manner; the other is the Lyapunov optimization based approach for finding the operating point that achieves the worst average interference to the primary user. Specifically, the former adopts a large number of discrete channel states to approximate the dynamic and continuous interference levels, while the latter adapts the transmitting rate from a static to a dynamic mode according to the observation of the interference, and gradually converges to the operating point that achieves the worst average interference. Simulation results show that the proposed approaches achieve near-optimal performance, in terms of both effective spectrum utilization and average interference to the primary user.",
        "watermark_text": "Recently , cognitive broadcasting has been paid much attention due to its abilities to automatically find accessible abandoned spectrum for new using , which is especially handy in the scenario of spectrum shortage . In try to minimize interfering with the primary spectrum user , the secondary cognitive radio transmitter has to use the rate adaptation to affect its receiving rate dependent on the interference received by the primary user , which is , however , unknown and difficult to observe .To this end , this paper offers a non - Bayesian rate adaptation with two design approaches : one is the restless multi - armed bandit based model for finding the unavailable spectrum in an automated and adaptive manner ; the other is the Lyapunov algorithm based model for finding the operating point that achieves the best average interference to the primary user . Specifically , the former adopts a large number of discrete channel states to approximate the dynamic and continuous interference levels , while the former adapts the broadcasting rate from a static to a dynamic mode according to the observation of the interference , and gradually converges to the operating point that achieves the greatest average interference .Simulation data demonstrate that the suggested techniques achieve near - optimal performance , in terms of both efficient spectrum utilization and average interference to the primary user .",
        "rewrite_text": "Recently, cognitive broadcasting has garnered significant attention due to its ability to automatically detect and utilize accessible abandoned spectrum, particularly beneficial in situations of spectrum scarcity. To minimize interference with primary spectrum users, the secondary cognitive radio transmitter must adjust its reception rate using rate adaptation, which is challenging as the interference experienced by the primary user remains unknown and difficult to observe.\n\nIn response to this, this paper presents a non-Bayesian rate adaptation approach with two design strategies. One approach is a restless multi-armed bandit model that automatically and adaptively identifies unreachable spectrum. The other is a Lyapunov algorithm-based model that identifies the optimal operating point to achieve the best average interference to the primary user. Specifically, the former utilizes a vast array of discrete channel states to approximate dynamic and continuous interference levels, while the latter dynamically adjusts the broadcasting rate from a static mode based on observed interference, gradually converging to an operating point that maximizes average interference.\n\nSimulation data indicates that the proposed techniques achieve near-optimal performance in terms of both efficient spectrum utilization and average interference to the primary user.",
        "ori-fast-z-score": 1.3112201362143716,
        "water-fast-z-score": 5.550253123463223,
        "rewrite-fast-z-score": 1.2339053944782488
    },
    {
        "original_text": "The optical and electrical properties of diarylethenes (DAs) can be reversibly switched between two stable states by different wavelength light. This property has been employed to design optical and photo-switchable molecular devices, ranging from photorefractive media to optical data storage and molecular devices. Recently, molecular devices based on diarylethenes have been developed, exhibiting photochromism, i.e. the ability to switch between two different states, one stable in the dark and the other in light. In this study, we present a theoretical investigation on diarylethene-based molecular junctions. The current-voltage (I-V) characteristics reveal that the DA molecular junctions exhibit reversible and cyclable switching effects via light illumination with two different wavelengths, indicating the application potential of DA molecules in optical and photo-switchable molecular devices. Switching mechanism of photochromic diarylethene derivatives molecular junctions Daoyuan Wu, Zhenzhen Qiu, Shaofeng Zhang, Xiangfeng Zhou, Jingyue Zhang Molecular Devices, 19(30), 12321-12330, 2020 Light-driven switching behavior of photochromic diarylethene derivatives molecular junctions has attracted considerable attention recently due to their potential applications in optical memory, photorefractive materials and optical data storage. To date, several diarylethene derivatives have been explored as building blocks to construct light-driven molecular devices. Generally, the photochromic diarylethene derivatives are regarded as the light-driven switching element and its associated molecular device is constructed by introduction of different electrodes. With the aim to explore more intrinsic switching behaviors of diarylethene derivatives, in this paper, we designed a novel molecular device in which a diarylethene unit is connected to two electrodes through a alkynylene spacer. The alkynylene spacer is very flexible and can undergo different cis-trans isomerization processes under light or darkness, which would produce different intermolecular forces between diarylethene derivatives and thus result in switching the electrical properties of the molecular devices. Based on this mechanism, the current-voltage (I-V) characteristics reveal that the DA molecular junctions exhibit reversible and cyclable switching effects via light illumination with two different wavelengths, indicating the application potential of DA molecules in optical and photo-switchable molecular devices. Here we present a theoretical investigation on diarylethene-based molecular junctions. The current-voltage (I-V) characteristics reveal that the DA molecular junctions exhibit reversible and cyclable switching effects via light illumination with two different wavelengths, indicating the application potential of DA molecules in optical and photo-switchable molecular devices. The switching mechanism is based on the interconversion between two isomers, namely the E and Z isomers. As shown in Fig. 1a, the Z isomer is relatively planar and the E isomer has a dihedral angle of about 60°. In this molecular device,",
        "watermark_text": "The optical and electrical properties of diarylethenes ( DAs ) can be reversibly transferred between two stable states by various spectrum light . This property has been employed to model optical and photo - switchable molecular systems , diverse from photorefractive media to laser information storage and molecular systems .Recently , molecular sensors using on diarylethenes have been created , showing photochromism , i . e . the ability to shift between two different states , one stable in the dark and the other in light .In this study , we present a conceptual inquiry on diarylethene - based molecular junctions . The current - voltage ( I - V ) characteristics reveal that the DA molecular junctions exhibit reversible and cyclable switching effects via light lighting with two different wavelengths , showing the implementation possibilities of DA molecules in laser and photo - switchable molecular systems .Switching mechanism of photochromic diarylethene derivatives molecular junctions Daoyuan Wu , Zhenzhen Qiu , Shaofeng Zhang , Xiangfeng Zhou , Jingyue Zhang Molecular Devices , 19 ( 30 ) , 12321 - 12330 , 2020 Light - powered switching response of photochromic diarylethene derivatives molecular junctions has drew substantial scrutiny lately owing to their potential applications in laser memory , photorefractive materials and imaging information storage . To date , various diarylethene derivatives have been explored as building blocks to build light - powered molecular systems .Generally , the photochromic diarylethene derivatives are regarded as the light - powered switching element and its attendant molecular device is built by introduction of different electrodes . With the objective to examine more intrinsic switching characteristics of diarylethene derivatives , in this paper , we built a new chemical system in which a diarylethene unit is linked to two electrodes through a alkynylene spacer .The alkynylene spacer is very flexible and can conduct different cis - trans isomerization processes under light or shadow , which would create different intermolecular forces between diarylethene derivatives and therefore result in changing the electrical properties of the molecular devices . Based on this mechanism , the current - voltage ( I - V ) characteristics reveal that the DA atomic junctions exhibit reversible and cyclable switching effects via dark illumination with two different wavelengths , showing the application potential of DA molecules in optical and photo - switchable molecular systems .Here we present a conceptual inquiry on diarylethene - based molecular junctions . The current - voltage ( I - V ) characteristics reveal that the DA molecular junctions exhibit reversible and cyclable switching effects via light lighting with two different wavelengths , showing the application potential of DA molecules in laser and photo - switchable molecular systems .The switching method is based on the interconversion between two isomers , principally the E and Z isomers . As seen in Fig .1a , the Z isomer is fairly planar and the E isomer has a dihedral angle of about 60° . In this molecular method ,",
        "rewrite_text": "The reversibility and photo-switchable properties of diarylethenes (DAs) can be transferred between two stable states through various spectrum lights. This characteristic has been utilized to model diverse optical and photo-switchable molecular systems, ranging from photorefractive media to laser information storage. Recently, molecular sensors utilizing diarylethenes have been developed, exhibiting photochromism - the ability to shift between two different states, one being stable in the dark and the other in light.\n\nIn this study, we propose a conceptual exploration of diarylethene-based molecular junctions. Through analysis of current-voltage (I-V) characteristics, it is revealed that DA molecular junctions demonstrate reversible and cyclable switching effects via illumination with two distinct wavelengths of light. This suggests potential applications for DA molecules in laser and photo-switchable molecular systems.\n\nThe switching mechanism of photochromic diarylethene derivatives molecular junctions involves the interconversion of two isomers, primarily the E and Z isomers. As illustrated in Figure 1a, the Z isomer is relatively planar, while the E isomer has a dihedral angle of approximately 60°. In this molecular approach, the light-powered switching response of these derivatives has attracted significant scrutiny due to their potential uses in laser memory, photorefractive materials, and imaging information storage.\n\nVarious diarylethene derivatives have been explored as building blocks for creating light-powered molecular systems. Generally, the photochromic diarylethene derivatives are considered as the light-activated switching elements, with the accompanying molecular device constructed by introducing different electrodes. To further investigate the intrinsic switching characteristics of diarylethene derivatives, a new chemical system has been developed in this paper. In this system, a diarylethene unit is linked to two electrodes through an alkynylene spacer. The flexible alkynylene spacer allows for different cis-trans isomerization processes to occur under light or shadow, which in turn creates varying intermolecular forces between the diarylethene derivatives. These forces subsequently alter the electrical properties of the molecular devices.\n\nBased on this mechanism, the I-V characteristics demonstrate that the DA atomic junctions exhibit reversible and cyclable switching effects through dark illumination with two different wavelengths. This suggests the potential utility of DA molecules in optical and photo-switchable molecular systems. This conceptual inquiry into diarylethene-based molecular junctions highlights the potential applications of these molecules in various fields.",
        "ori-fast-z-score": -2.0767331347143556,
        "water-fast-z-score": 6.625975648875597,
        "rewrite-fast-z-score": 3.335631391747175
    },
    {
        "original_text": "A well-known problem in solar physics is the lack of a unifying model to explain the existence of prominences in the Sun s coronae. Observations indicate that prominences are formed by cool and dense plasma suspended in hot, tenuous solar coronal plasma. Moreover, these filaments are confined to within narrow magnetic field structures that often exhibit complex, non-potential magnetic configurations. Here we present magnetohydrostatic equilibrium models that naturally produce magnetic fields consistent with those observed around quiescent filaments, with the added bonus of supporting both cool, dense plasma and the hot coronal plasma in approximate magnetic equilibrium. Furthermore, the cool, dense plasma, supported by the magnetic field, experiences only very weak gravitational forces. Therefore, these structures may be able to survive atmospheric drag and can accumulate into long-lived features that could explain the existence of quiescent filaments, which have lifetimes that range from several days to many years. These structures are also naturally confined to within narrow, stable magnetic field structures.",
        "watermark_text": "A well - famous problem in solar theory is the lack of a unifying model to explain the existence of prominences in the Sun s coronae . Observations confirm that prominences are created by cold and dense liquid suspended in heated , tenuous solar coronal plasma .Moreover , these filaments are confined to within narrow magnetic field structures that frequently exhibit unique , non - potential magnetic configurations . Here we present magnetohydrostatic equilibrium models that naturally produce magnetic fields consistent with those observed around quiescent filaments , with the added bonus of holding both warm , dense plasma and the cool coronal plasma in approximate magnetic equilibrium .Furthermore , the cool , dense plasma , backed by the magnetic force , experiences only very mild gravitational pressures . Therefore , these structures could be possible to survive atmospheric drag and can accumulate into long - lived features that might explain the existence of quiescent filaments , which have lifetimes that range from several days to many years .These structures are also naturally confined to within narrow , stable magnetic field structures .",
        "rewrite_text": "A well-known issue in solar theory is the absence of a unified model that can explain the presence of prominences in the Sun's corona. Observational data indicates that prominences are formed by cold and dense liquids suspended within heated, thin solar coronal plasma. Furthermore, these filaments are confined within narrow magnetic field structures, often exhibiting unique and non-potential magnetic configurations.\n\nIn this study, we introduce magnetohydrostatic equilibrium models that naturally generate magnetic fields in alignment with those observed around stationary filaments. These models not only maintain a warm, dense plasma but also keep the cool coronal plasma in approximate magnetic equilibrium. Additionally, the cool and dense plasma, supported by magnetic force, experiences minimal gravitational pressure. Consequently, these structures are potentially resilient to atmospheric drag and can accumulate into long-lived features, potentially explaining the existence of quiescent filaments with lifespans ranging from several days to many years. These structures are also naturally confined within narrow and stable magnetic field structures.",
        "ori-fast-z-score": 0.8528028654224417,
        "water-fast-z-score": 5.405989188032437,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "The solar system contains four giant planets, Mercury, Venus, Earth and Mars, which were all formed during the late stages of the protoplanetary disk evolution 4.6 billion years ago. The planets likely migrated from beyond the ice line, the distance from which ice materials can no longer warm via thermal excitation. It is suggested that this migration may have been driven by the forced oscillation of the disk. As the planets  cores grew via core accretion, the eccentricities and inclinations of the planets  orbits increased until, roughly 200 million years ago, the outer two planets collided with Mars. This created the current architecture for the giant planets, which is characterized by eccentric and inclined orbits. The migration and evolution of the giant planets likely has a profound effect on the structure of the current solar system. The dynamics of the giant planets in the gaseous disk are described. The analytic, 2D, hydrodynamical solutions of the Planet-Disk-Planet (PDP) system are presented. The migration of the giant planets from beyond the ice line is modeled. The eccentricities and inclinations of the current architectures for the giant planets are explained as a result of the evolution of the giant planets in the disk.",
        "watermark_text": "The planetary system contains four giant planets , Mercury , Venus , Earth and Mars , which were all formed during the last phases of the protoplanetary disk evolution 4 . 6 billion centuries earlier . The planets likely migrated from beyond the ice line , the distance from which ice particles can no longer cool via thermal excitation .It is suggested that this migration may have been driven by the forced oscillation of the disk . As the planets cores grew via disk accretion , the eccentricities and inclinations of the planets orbits increased until , roughly 200 million months previously , the inner two planets merged with Mars .This created the present architecture for the giant planets , which is characterized by erratic and oriented orbits . The migration and evolution of the giant planets likely has a profound effect on the composition of the present solar structure .The dynamics of the giant planets in the gaseous disk are explained . The analytic , 2D , hydrodynamical solutions of the Planet - Disk - Planet ( PDP ) scheme are presented .The migration of the giant planets from beyond the glacier line is modeled . The eccentricities and inclinations of the present architectures for the giant planets are explained as a product of the evolution of the giant planets in the disk .",
        "rewrite_text": "The solar system comprises four gas giants, namely Mercury, Venus, Earth, and Mars, which were all formed during the final stages of the evolution of the protoplanetary disk approximately 4.6 billion years ago. These planets likely migrated from beyond the ice line, which denotes the distance beyond which ice particles can no longer be cooled through thermal excitation. It is suggested that this migration may have been influenced by the forced oscillation of the disk.\n\nAs the planetary cores grew through disk accretion, the eccentricities and inclinations of their orbits increased until roughly 200 million years prior, when the two inner planets merged with Mars, shaping the current configuration of the giant planets with unpredictable and oriented orbits. The migration and evolution of these gas giants undoubtedly have a significant impact on the composition of the current solar structure.\n\nThe dynamics of the giant planets within the gaseous disk are explained in detail. Analytical, two-dimensional, and hydrodynamic solutions to the Planet-Disk-Planet (PDP) scheme are presented. Modeling of the giant planets' migration beyond the glacier line is conducted. The current orbital eccentricities and inclinations of the giant planets are explained as a result of their evolution within the disk.",
        "ori-fast-z-score": 0.4364357804719848,
        "water-fast-z-score": 5.8918830363717944,
        "rewrite-fast-z-score": 0.21566554640687682
    },
    {
        "original_text": "Synthesis of Taylor phase screens is an important problem in many applications of linear systems theory and adaptive signal processing. In this Letter, we present a method for the synthesis of Taylor phase screens based on Karhunen-Loeve (KL) basis functions. The approach combines ideas from optimisation and compensation. It consists of an optimisation problem that is non-convex and hard to solve in general, but can be approximated efficiently using an alternating method. The algorithm is validated on the synthesis of polynomial screens and an exponential screen. The synthesis of Taylor screens is a harder problem than synthesis of polynomial screens since it requires the synthesis of screens with rapidly changing phases. However, the proposed method synthesises Taylor screens with arbitrary smoothness and satisfies a natural physical constraint which guarantees the phase changes gradually. Balancing multiple objectives is a difficult problem in general. This is reflected by the fact that most heuristics for the synthesis of Taylor screens converge to a local optimum rather than the global optimum. In this Letter, we demonstrate how to tackle this problem using the KL method. We present a two-step method for initialisation and parameter identification. We show that the identified screens satisfy the physical constraints and have good approximation properties. We also propose a multi-start strategy that starts multiple algorithms for the synthesis of screens and combines the results. We demonstrate the effectiveness of the proposed algorithm on two design examples and compare it with state of the art algorithms.",
        "watermark_text": "Synthesis of Taylor phase screens is an important challenge in many applications of linear systems physics and adaptive signal processing . In this Letter , we present a technique for the synthesis of Taylor phase screens relying on Karhunen - Loeve ( KL ) basis maps .The method mixes ideas from optimisation and compensation . It consists of an optimisation problem that is non - convex and difficult to solve in general , but can be approximated accurately using an alternating method .The algorithm is validated on the synthesis of polynomial displays and an exponential screen . The synthesis of Taylor screens is a harder challenge than synthesis of polynomial monitors since it takes the synthesis of screens with quickly changing phases .However , the suggested method synthesises Taylor screens with arbitrary smoothness and satisfies a natural mechanical constraint which ensures the phase moves eventually . Balancing multiple goals is a challenging problem in general .This is reinforced by the fact that most heuristics for the synthesis of Taylor screens converge to a local optimum instead than the global optimum . In this Letter , we prove how to tackle this question using the KL method .We present a two - phase method for initialisation and parameter analysis . We see that the selected screens satisfy the physical restrictions and have better approximation properties .We also suggest a multi - start strategy that starts multiple algorithms for the synthesis of screens and mixes the results . We evaluate the performance of the suggested method on two design examples and link it with state of the art algorithms .",
        "rewrite_text": "In the realm of linear systems physics and adaptive signal processing, synthesizing Taylor phase screens poses a crucial challenge. This letter introduces a technique for this synthesis, rooted in the utilization of Karhunen-Loeve (KL) basis maps. This method amalgamates concepts from optimization and compensation. It is formulated as a non-convex optimization problem that can be challenging to solve in general, but can be accurately approximated through an alternating approach.\n\nThe efficacy of our algorithm is validated through the synthesis of polynomial displays and an exponential screen. It is worth noting that synthesizing Taylor screens is more intricate than the creation of polynomial monitors due to the requirement of addressing rapidly changing phases. However, our proposed method can synthesize Taylor screens with arbitrary smoothness, satisfying a natural mechanical constraint that guarantees gradual phase progression.\n\nBalancing multiple objectives remains a formidable challenge in general. This challenge is further emphasized by the fact that many heuristics for Taylor screen synthesis tend to converge towards local optima rather than the global optimum. In this letter, we demonstrate how to tackle this issue using the KL method. We present a two-phase approach for initialization and parameter analysis, demonstrating that the chosen screens not only meet physical constraints but also exhibit superior approximation properties.\n\nFurthermore, we propose a multi-start strategy, which involves initiating multiple algorithms for screen synthesis and combining their results. We assess the performance of our proposed method through two design examples, linking it with state-of-the-art algorithms.",
        "ori-fast-z-score": -2.2283440581246223,
        "water-fast-z-score": 6.009252125773315,
        "rewrite-fast-z-score": 1.6994116628998401
    },
    {
        "original_text": "The origin of the light elements Li, Be and B in X-ray binaries is a long-standing question, but its answer is still debated. It is now possible to study this issue via the detection and measurement of the 6Li/7Li isotopic ratio in the light elements lithium and beryllium, which exhibits a different behaviour in these two scenarios. In particular, a 6Li/7Li isotopic ratio below 10-11 can only be reproduced if the light elements are of extragalactic origin, whereas a ratio below 10-12 indicates the presence of Li synthesized in the interior of stars, such as our Sun. We report new measurements of the 6Li/7Li isotopic ratio in the metal-rich giant Cen X-4, which reveals a ratio of 7.3(3) x 10-12, consistent with the presence of Li formed in the interior of stars. We also report the first upper limit on 6Li/7Li in another metal-rich giant, h Per, which exhibit a ratio above 10-11, consistent with the presence of Li formed in the interiors of stars. We discuss the implications of these results for the formation channel of Li in X-ray binaries and future directions.",
        "watermark_text": "The origin of the light elements Li , Be and B in X - ray binaries is a high - standing question , but its answer is also debated . It is now able to study this question via the discovery and measurement of the 6Li / 7Li isotopic ratio in the light elements lithium and beryllium , which demonstrates a unique behaviour in these two scenarios .In particular , a 6Li / 7Li isotopic ratio below 10 - 11 can only be reproduced if the light elements are of extragalactic origin , whereas a ratio below 10 - 12 indicates the presence of Li produced in the interior of stars , such as our Sun . We report new studies of the 6Li / 7Li isotopic ratio in the metal - rich giant Cen X - 4 , which reveals a ratio of 7 . 3 ( 3 ) x 10 - 12 , consistent with the presence of Li formed in the interior of stars .We additionally report the first upper maximum on 6Li / 7Li in another metal - rich giant , h Per , which exhibit a ratio above 10 - 11 , compatible with the presence of Li formed in the interiors of stars . We discuss the implications of these results for the formation channel of Li in X - ray binaries and future directions .",
        "rewrite_text": "The question of the origin of the light elements Li, Be, and B in X-ray binaries remains a subject of debate. However, recent advancements in studying the 6Li/7Li isotopic ratio in the aforementioned elements have provided unique insights into this mystery. Specifically, a 6Li/7Li ratio below 10-11 suggests an extragalactic origin for these light elements, while a ratio below 10-12 indicates the production of Li within the interior of stars, such as our Sun. We have conducted new studies on the 6Li/7Li isotopic ratio in the metal-rich giant star Cen X-4, revealing a ratio of 7.3(3) x 10-12, which is consistent with Li formation within stellar interiors. Additionally, we report the first upper limit for the 6Li/7Li ratio in another metal-rich giant, h Per, which exhibits a ratio above 10-11, compatible with Li formation in star interiors. We discuss the implications of these findings for understanding the formation pathways of Li in X-ray binaries and future research directions.",
        "ori-fast-z-score": 0.5852057359806528,
        "water-fast-z-score": 3.862357857472309,
        "rewrite-fast-z-score": 0.24253562503633297
    },
    {
        "original_text": "This paper aims to analyze the internal structure of the Indian financial market using high-frequency data. Specifically, the stock prices of the National Stock Exchange (NSE) are analyzed for evidence of positive or negative co-movements. The analysis is based on two tests that are adapted from the field of finance, the vector error correction model (VECM) and the Granger causality test. The NSE dataset from January 2009 to June 2017 was analyzed and results indicate that there exists significant correlation across different markets across different time lags, where the strongest lags are 2 hours and 4 hours. These results help provide a deeper understanding of the internal structure of the Indian financial market. The Indian financial market is an important market that contributes significantly to the GDP of the country. It is an open economy and a hub for foreign exchange transactions. It has a high liquidity and shares global financial information due to its presence on the global stage. In addition to this, India is one of the fastest growing major economies. The stock markets in India have shown considerable growth in the past decade, increasing from ~USD 400 billion to ~USD 2.5 trillion in the period 2009-2017. Despite its importance, the Indian financial market is also one of the least understood. Unlike the U.S. or western markets, the Indian market has a highly centralized structure where a few major stock exchanges play a significant role. These major stock exchanges are BSE (Formerly Bombay Stock Exchange), National Stock Exchange (NSE) and the S&P Bombay Stock Exchange (BSE) index. There are a few smaller stock exchanges, which include the National Stock Exchange of India, Indian Stock Exchange and Reliance Industries Share Value System. The Indian financial market has also been criticized for having delayed reporting mechanisms and surveillance due to which, most transactions do not get reported and hence the available information is minimal.",
        "watermark_text": "This paper aims to analyze the internal structure of the Indian banking industry using high - frequency information . Specifically , the stock rates of the National Stock Exchange ( NSE ) are examined for indication of favorable or bad co - movements .The analysis is based on two tests that are modified from the field of finance , the linear error correction model ( VECM ) and the Granger causality test . The NSE dataset from January 2009 to June 2017 was examined and results show that there exists significant correlation across different traders across different time lags , where the greatest lags are 2 days and 4 hours .These data help give a deeper understanding of the internal structure of the Indian financial market . The Indian financial market is an important consumer that adds significantly to the GDP of the nation .It is an open economy and a hub for foreign exchange transactions . It has a high liquidity and trades worldwide financial information due to its presence on the global stage .In addition to this , India is one of the fastest growing major economies . The stock markets in India have shown massive increase in the previous decade , increasing from ~ USD 400 billion to ~ USD 2 . 5 trillion in the period 2009 - 2017 .Despite its significance , the Indian banking exchange is also one of the least understood . Unlike the U . S . or western markets , the Indian market has a highly centralized formation where a few main securities markets play a substantial role .These main securities markets are BSE ( Formerly Bombay Stock Exchange ) , National Stock Exchange ( NSE ) and the S & P Bombay Stock Exchange ( BSE ) index . There are a few smaller shares markets , which namely the National Stock Exchange of India , Indian Stock Exchange and Reliance Industries Share Value System .The Indian banking industry has also been noted for having postponed disclosure processes and surveillance due to which , most dealings do not get documented and hence the provided information is limited .",
        "rewrite_text": "This study aims to analyze the internal structure of the Indian banking sector using high-frequency data. Specifically, it examines stock rates from the National Stock Exchange (NSE) to detect favorable or unfavorable co-movements. The analysis relies on two financial tests: the linear error correction model (VECM) and the Granger causality test. We analyzed NSE data from January 2009 to June 2017 and found significant correlations across traders at various time lags, with the most prominent lags being 2 days and 4 hours. These data provide deeper insights into the internal structure of the Indian financial market.\n\nThe Indian financial market is a crucial component of the economy, contributing significantly to the GDP. It operates in an open economy and is a hub for foreign exchange transactions. Its high liquidity and global presence allow it to trade worldwide financial information. Additionally, India is one of the fastest-growing major economies. Indian stock markets have experienced massive growth in the past decade, increasing from approximately USD 400 billion to over USD 2.5 trillion between 2009 and 2017.\n\nDespite its significance, the Indian banking exchange remains one of the least understood sectors. In contrast to U.S. or western markets, the Indian market has a highly centralized structure with a few main securities markets playing a pivotal role. These primary markets include the BSE (formally known as the Bombay Stock Exchange), the NSE, and the S&P Bombay Stock Exchange (BSE) index. There are also several smaller stock markets, such as the National Stock Exchange of India, Indian Stock Exchange, and Reliance Industries Share Value System.\n\nIt's worth noting that the Indian banking industry has been criticized for delaying disclosure processes and surveillance, resulting in most transactions being undocumented and limiting the availability of information.",
        "ori-fast-z-score": 0.939793423488437,
        "water-fast-z-score": 8.74642784226795,
        "rewrite-fast-z-score": 1.5666989036012806
    },
    {
        "original_text": "A new temperature analysis of the brown dwarf 2MASS J05352184-0546085, which orbits closely enough to its companion to be tidally synchronized, yields an unexpectedly low surface temperature of 20.8 ± 0.2 K. This is more than 500 K below what would be expected based on the observed near-infrared flux. Surprisingly, inclusion of J05352184-0546085’s blackbody emission alone is not sufficient to explain the observed fluxes. Instead, a secondary component at a higher temperature must also be present, with temperatures of 47.2 ± 1.0 K and 66.5 ± 1.2 K for its blackbody and Rayleigh-Jeans components, respectively. The component at 66.5 K is likely a spiral arm of the Milky Way, and this source may be an anomalous Extremely Red Object. The secondary component may be a consequence of an eccentric orbit, with the closer approach in the past when the system was both cooler and brighter allowing for a greater contribution of this component to the observed fluxes. As 2MASS J05352184-0546085 approaches its companion further, the magnitude of this effect will decrease.",
        "watermark_text": "A new heat analysis of the brown giant 2MASS J05352184 - 0546085 , which orbits closely enough to its companion to be tidally synchronized , yields an unexpectedly low surface temperature of 20 . 8 ± 0 . 2 K . This is more than 500 K below what would be anticipated based on the seen near - infrared flux . Surprisingly , addition of J05352184 - 0546085 ’ s blackbody emission alone is not sufficient to explain the known fluxes .Instead , a secondary component at a higher temperature must additionally be found , with temperatures of 47 . 2 ± 1 . 0 K and 66 . 5 ± 1 . 2 K for its blackbody and Rayleigh - Jeans components , respectively . The component at 66 . 5 K is probably a spiral arm of the Milky Way , and this source may be an anomalous Extremely Red Object .The secondary component may be a outcome of an eccentric orbit , with the closer approach in the previous when the system was both lighter and brighter allowing for a greater contribution of this component to the observed fluxes . As 2MASS J05352184 - 0546085 approaches its companion further , the magnitude of this effect will decrease .",
        "rewrite_text": "A new analysis of heat data for the brown giant 2MASS J05352184-0546085, which is closely orbiting its companion and thus tidally synchronized, has revealed an unexpectedly low surface temperature of 20.8 ± 0.2 Kelvin. This temperature is over 500 K lower than expected based on the observed near-infrared flux. Interestingly, solely adding the blackbody emission of J05352184-0546085 does not adequately account for the known fluxes. Instead, a secondary component at a higher temperature must be identified. This component includes blackbody and Rayleigh-Jeans components with temperatures of 47.2 ± 1.0 K and 66.5 ± 1.2 K, respectively. The component at 66.5 K may likely be a spiral arm of the Milky Way, and this source could be an unusual Extremely Red Object. The secondary component may result from an eccentric orbit where the system was previously closer and lighter with a brighter state, contributing more to the observed fluxes. As 2MASS J05352184-0546085 approaches its companion even closer, the impact of this effect will diminish in magnitude.",
        "ori-fast-z-score": -0.36650833306891567,
        "water-fast-z-score": 2.42535625036333,
        "rewrite-fast-z-score": 0.35603449745815596
    },
    {
        "original_text": "A novel, highly selective, liquid-crystalline chemosensor has been synthesised for the detection of sulphur (S). The chemosensor, based on an indoline-2-one unit, exhibits a near-infrared (NIR) fluorescence response towards S detection with high sensitivity (LOD=5.3×10−7 M). The system can discriminate between S and other commonly encountered anions with the exception of hydrogen sulphate (H2SO4) with a significant NIR fluorescence enhancement observed for S over H2SO4. Fluorescence microscopy imaging of mammalian cells demonstrates that the chemosensor is membrane permeable and that it can detect endogenous sulphide (S2-) in mammalian cells. Preliminary, non-targeted, live-cell NIR fluorescence imaging of mammalian cells treated with the chemosensor demonstrates the ability of the chemosensor to detect S2- in mammalian cells. These results indicate that the chemosensor is a promising candidate for the sensitive non-invasive imaging of endogenous S2- in biological systems.",
        "watermark_text": "A novel , highly selective , fluid - crystalline chemosensor has been synthesised for the detection of sulphur ( S ) . The chemosensor , based on an indoline - 2 - one unit , displays a far - infrared ( NIR ) fluorescence reaction towards S detection with high sensitivity ( LOD = 5 . 3×10−7 M ) .The system can discriminate between S and other commonly encountered anions with the exception of hydrogen sulphate ( H2SO4 ) with a substantial NIR fluorescence enhancement detected for S over H2SO4 . Fluorescence microscopy imaging of mammalian tissue demonstrates that the chemosensor is membrane permeable and that it can identify endogenous sulphide ( S2 - ) in mammalian tissue .Preliminary , non - directed , live - cell NIR fluorescence scanning of mammalian cells treated with the chemosensor demonstrates the ability of the chemosensor to identify S2 - in mammalian cells . These data indicate that the chemosensor is a potential candidate for the sensitive non - invasive imaging of endogenous S2 - in biological systems .",
        "rewrite_text": "A highly selective and novel fluid-crystalline chemosensor has been synthesized for the detection of sulfur (S). This chemosensor, which is based on an indoline-2-one unit, exhibits a far-infrared (NIR) fluorescence response with high sensitivity (LOD = 5.3×10^-7 M) towards S detection. The system can differentiate between S and other commonly encountered anions, with the exception of hydrogen sulfate (H2SO4), showing a significant NIR fluorescence enhancement for S compared to H2SO4. Fluorescence microscopy imaging of mammalian tissue reveals that the chemosensor is membrane-permeable and can identify endogenous sulfide (S2-) in such tissue. Preliminary, non-directed live-cell NIR fluorescence scanning of mammalian cells treated with this chemosensor demonstrates its capability to identify S2- in mammalian cells. These findings suggest that the chemosensor is a promising candidate for sensitive non-invasive imaging of endogenous S2- in biological systems.",
        "ori-fast-z-score": -1.1547005383792517,
        "water-fast-z-score": 3.7527767497325675,
        "rewrite-fast-z-score": 0.8320502943378437
    },
    {
        "original_text": "gamma-ray sources are extremely high-energy cosmic gamma-ray emitters and they are of great interest to astrophysicists. One such source, TeV J2032+4130, was discovered by the ground-based Atmospheric Gamma-ray Emitter (AGER) telescope system and the Cherenkov Telescope Array (CTA) during observations by the H.E.S.S., Fermi, and AGILE gamma-ray observatories. Since its discovery, there has been some disagreement over the source identity. Recent observations by the X-Ray Telescope (XRT) onboard the Neil Gehrels Swift Observatory suggest that this source is probably an active galaxy located at a distance of 1.3 billion light years from Earth, making it one of the farthest objects ever observed in the universe. The surprisingly large distance and high luminosity, however, suggest that this source may be a new discovery of enormous scientific importance.",
        "watermark_text": "gamma - ray sources are extremely high - energy cosmic gamma - ray emitters and they are of large interest to astrophysicists . One such source , TeV J2032 + 4130 , was discovered by the ground - based Atmospheric Gamma - ray Emitter ( AGER ) telescope system and the Cherenkov Telescope Array ( CTA ) during observations by the H . E . S . S . , Fermi , and AGILE gamma - ray observatories .Since its observation , there has been some disagreement over the origin identity . Recent measurements by the X - Ray Telescope ( XRT ) onboard the Neil Gehrels Swift Observatory estimate that this source is probably an active galaxy located at a distance of 1 . 3 billion light years from Earth , making it one of the farthest entities actually seen in the universe .The surprisingly large distance and strong luminosity , however , indicate that this source may be a new discovery of immense scientific significance .",
        "rewrite_text": "Gamma-ray sources are highly energetic cosmic gamma-ray emitters, and they hold immense interest for astrophysicists. One such source, TeV J2032+4130, was discovered by the ground-based Atmospheric Gamma-ray Emitter (AGER) telescope system and the Cherenkov Telescope Array (CTA) during observations conducted by the H.E.S.S., Fermi, and AGILE gamma-ray observatories. Since its observation, there has been debate over its exact origin. Recent measurements by the X-Ray Telescope (XRT) aboard the Neil Gehrels Swift Observatory suggest that this source might be an active galaxy situated at a distance of approximately 1.3 billion light years from Earth, making it one of the most distant objects ever observed in the universe. Its astonishingly great distance and strong luminosity suggest that this source could be a groundbreaking scientific discovery of immense importance.",
        "ori-fast-z-score": 1.386750490563073,
        "water-fast-z-score": 4.34086826048683,
        "rewrite-fast-z-score": 1.7856873313329573
    },
    {
        "original_text": "This work develops a new cyclic voltammetry (CV) based technique for characterizing phase transitions and predicting equilibrium composition. CV, the measurement of current as a function of voltage, is the main method used to investigate electrochemical phenomena. The traditional approach to CV measurements is to slowly increase the voltage from an initial value of 0 until an asymptotic value is reached where the electrochemical system has progressed through one full reaction. This standard approach, known as linear CV, has several shortcomings when characterizing electrochemical phase transitions. In some cases a phase change occurs but not complete reduction or oxidation of the system. This incomplete reduction or oxidation means that the system has not reached an equilibrium composition. The traditional linear CV approach does not report whether the system has reached equilibrium composition. Another shortcoming of the linear approach is that only one voltage value is reported even though multiple phases may be present at different voltage. This work develops a new approach to CV measurement, called E-C FORC, which tracks the voltage path the system takes as it progresses through multiple phases and reaches equilibrium. Using a genetic algorithm E-C FORC is able to determine both the path the system takes as it transitions between phases and the voltage corresponding to equilibrium composition. The technique is demonstrated using ferricyanide as a model electrochemical system. E-C FORC is able to determine the equilibrium composition in both highly polyphasic systems and those with very sharp phase transitions. Additionally, E-C FORC can track multiple phase transitions and accurately determine the voltage corresponding to each phase transition. This work describes a promising approach to characterizing electrochemical phase transitions and predicting equilibrium composition.",
        "watermark_text": "This project creates a new cyclic voltammetry ( CV ) based technique for characterizing phase transitions and predicting equilibrium composition . CV , the determination of current as a function of voltage , is the main method employed to probe electrochemical behavior .The conventional approach to CV measurements is to slowly increase the voltage from an initial value of 0 until an asymptotic value is reached where the electrochemical system has progressed through one full reaction . This standard method , known as linear CV , has numerous shortcomings when characterizing electrochemical phase transitions .In some cases a phase change happens but not total reduction or oxidation of the process . This incomplete reduction or oxidation implies that the system has not achieved an equilibrium composition .The conventional linear CV method does not assess whether the system has reached equilibrium composition . Another shortcoming of the linear method is that only one voltage value is reported even though multiple components could be found at different voltage .This project creates a new approach to CV monitoring , known E - C FORC , which tracks the voltage path the system takes as it advances through several stages and reaches equilibrium . Using a genetic algorithm E - C FORC is able to predict both the path the system takes as it changes between phases and the voltage relating to equilibrium composition .The technique is demonstrated using ferricyanide as a prototype electrochemical system . E - C FORC is able to predict the equilibrium composition in both highly polyphasic solutions and those with very sharp phase transitions .Additionally , E - C FORC can track multiple phase transitions and correctly calculate the voltage relating to each phase shift . This research describes a promising path to characterizing electrochemical phase transitions and predicting equilibrium composition .",
        "rewrite_text": "This project introduces a novel technique based on cyclic voltammetry (CV) for characterizing phase transitions and predicting equilibrium compositions. CV, which involves determining current as a function of voltage, is the primary method used to explore electrochemical behavior. \n\nThe traditional approach to CV measurements involves gradually increasing the voltage from an initial value of 0 until an asymptotic value is reached, indicating that the electrochemical system has completed a full reaction. This standard method, known as linear CV, has several limitations when it comes to characterizing electrochemical phase transitions. In some cases, a phase change occurs without the process reaching complete reduction or oxidation. This incomplete reduction or oxidation suggests that the system has not reached an equilibrium composition. However, the traditional linear CV method fails to assess whether the system has indeed reached equilibrium. \n\nAnother drawback of the linear method is that it only reports a single voltage value, even when multiple components may be present at different voltages. To address these issues, this project proposes a new approach to CV monitoring, known as E-C FORC. This method tracks the voltage path taken by the system as it progresses through various stages and ultimately reaches equilibrium. \n\nBy utilizing a genetic algorithm, E-C FORC is capable of predicting both the path taken by the system during phase changes and the voltage associated with the equilibrium composition. This technique is demonstrated using ferricyanide as a prototype electrochemical system. E-C FORC proves its effectiveness in predicting the equilibrium composition in both highly polyphasic solutions and those with sharp phase transitions. Furthermore, it can track multiple phase transitions and accurately calculate the voltage related to each phase shift. \n\nIn conclusion, this research presents a promising approach for characterizing electrochemical phase transitions and predicting equilibrium compositions.",
        "ori-fast-z-score": 0.7689218919450849,
        "water-fast-z-score": 7.491057470676988,
        "rewrite-fast-z-score": 2.7034653377128337
    },
    {
        "original_text": "Epsilon Aurigae (ι Aur) is an α2 CVn binary system with an F3 IV primary component and an K0 IV secondary component. Epsilon Aurigae is the second brightest star in the northern hemisphere sky, and it is almost always visible to the naked eye. The primary component is surrounded by a thick, hydrogen-filled envelope that causes it to be much cooler and less luminous than the Sun. As a result, Epsilon Aurigae has an unusual evolution on the H-R diagram - it is almost stationary along the main sequence but becomes a giant many centuries into the future. Historical estimates of the star s distance have varied considerably, from under 4 pc to over 14 pc, and recent high-precision parallax measurements from the Hipparcos satellite gave a distance of 11.65 pc. IUE spectra of the Epsilon Aurigae system showed Balmer emission from the primary star, although its spectrum was dominated by that of the secondary. The IUE data showed the presence of a thin cirrus-like hydrogen envelope around the primary star, at least 20 times less dense than the extended thick wind of the same star. The relative strength of the emission lines in the primary s spectrum varied periodically, with a period of 14.4 days - the rotation period of the primary star. Analysis of ultraviolet observations obtained with the Solar Mass Spectrometer on the Ulysses spacecraft suggested that Epsilon Aurigae was losing mass at a rate of about 10-9 to 10-8 kg s-1. In this work, recent UBVJH photometry of Epsilon Aurigae is presented. The variation in temperature of the primary star as a function of wavelength is discussed, as are the H Lambda emission line profiles. The overall brightness of the system is estimated using simple models for the primary and secondary stars. Finally, implications for the current mass-loss rate of the primary star and the evolutionary state of the system are discussed.",
        "watermark_text": "Epsilon Aurigae ( ι Aur ) is an α2 CVn binary system with an F3 IV primary component and an K0 IV secondary component . Epsilon Aurigae is the second brightest star in the northern hemisphere sky , and it is almost always apparent to the naked eye .The main component is surrounded by a thick , hydrogen - covered envelope that causes it to be much cooler and less luminous than the Sun . As a result , Epsilon Aurigae has an peculiar progression on the H - R diagram - it is almost stationary along the main sequence but forms a giant several centuries into the future .Historical accounts of the star s diameter have differed greatly , from under 4 pc to over 14 pc , and recent high - precision parallax observations from the Hipparcos satellite gave a distance of 11 . 65 pc . IUE spectra of the Epsilon Aurigae system showed Balmer radiation from the primary star , although its spectrum was dominated by that of the secondary .The IUE data showed the presence of a thin cirrus - like hydrogen envelope around the primary star , at least 20 twice less dense than the extended thick wind of the same star . The relative strength of the emission lines in the primary s spectrum differed continually , with a period of 14 . 4 hours - the rotation history of the primary star .Analysis of ultraviolet observations collected with the Solar Mass Spectrometer on the Ulysses spacecraft proposed that Epsilon Aurigae was lose mass at a rate of about 10 - 9 to 10 - 8 kilograms s - 1 . In this research , recent UBVJH photometry of Epsilon Aurigae is published .The difference in heat of the primary star as a function of wavelength is mentioned , as are the H Lambda emission line profiles . The overall brightness of the system is calculated using simple models for the primary and secondary stars .Finally , implications for the present mass - loss rate of the primary star and the evolutionary state of the system are discussed .",
        "rewrite_text": "Epsilon Aurigae (ι Aur) constitutes an α2 CVn binary system characterized by an F3 IV primary component and a K0 IV secondary component. It is the second brightest star in the northern sky, often visible to the naked eye. The primary component is encircled by a dense, hydrogen-covered envelope, resulting in a significantly cooler and less luminous appearance than the Sun. Consequently, Epsilon Aurigae exhibits an unusual progression on the H-R diagram, appearing almost stationary along the main sequence but evolving into a giant over several centuries.\n\nHistorical measurements of the star's diameter have varied widely, ranging from less than 4 pc to over 14 pc. Recent high-precision parallax observations from the Hipparcos satellite have determined a distance of 11.65 pc. IUE spectra of the Epsilon Aurigae system reveal Balmer radiation from the primary star, although its spectrum is predominantly influenced by the secondary star's emission. The IUE data further indicates the presence of a thin, cirrus-like hydrogen envelope around the primary star, with a density at least 20 times less dense than the extended thick wind emitted by the same star.\n\nThe emission lines in the primary star's spectrum exhibit constant relative strength, with a periodicity of 14.4 hours reflecting the rotation of the primary star. Analysis of ultraviolet observations gathered by the Solar Mass Spectrometer on the Ulysses spacecraft suggests that Epsilon Aurigae is losing mass at a rate of approximately 10-9 to 10-8 kilograms per second.\n\nIn this research, recent UBVJH photometric data of Epsilon Aurigae is presented. The difference in temperature of the primary star across various wavelengths is noted, along with the H Lambda emission line profiles. The overall brightness of the system is calculated using simplified models for both primary and secondary stars. Finally, discussions are held on the implications for the current mass-loss rate of the primary star and the evolutionary state of the entire system.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 5.6594533097027915,
        "rewrite-fast-z-score": 2.0619652471058063
    },
    {
        "original_text": "A large effort has been devoted to the construction of surrogate models of black hole (BH) signals in data analysis of Advanced LIGO (aLIGO). Among the wide array of methods developed, Matched Filtering (MF) remains the most widely used technique for BH signal detection from aLIGO’s measurement data. Yet, despite being decades old, the MF technique has never been challenged on aBH data. In this Letter, we implement and test the MF technique on two recently published numerical relativity (NR) BH merger simulations: the Effective One Body (EOB) model and the Spectral Einstein Code (SpEC) spectral code. For each simulation, we construct the corresponding “template” BH signal in the frequency domain and filter it with a bank of NR waveforms from GCN numerical relativity catalogs. We find that in both the low and high frequency regions, the detection efficiencies of the NR waveforms are far below 100%, even for high signal-to-noise ratio (SNR) sources. By contrast, the MF technique is 100% efficient in both regions. We also test the MF technique on simulated Gaussian white noise and show that the detection efficiencies are 100% for all SNRs. These results show that NR waveforms are not suitable for aLIGO data analysis. Furthermore, the low detection efficiencies of NR waveforms cast doubt on aLIGO’s claim that it has observed gravitational waves from two merging black holes.",
        "watermark_text": "A vast effort has been focused to the development of surrogate models of black hole ( BH ) transmissions in data analysis of Advanced LIGO ( aLIGO ) . Among the broad variety of methods developed , Matched Filtering ( MF ) seems the most commonly used technique for BH signal recovery from aLIGO ’ s measurement data .Yet , despite being years old , the MF method has never been challenged on aBH data . In this Letter , we execute and test the MF method on two recently published numerical relativity ( NR ) BH merger simulations : the Effective One Body ( EOB ) model and the Spectral Einstein Code ( SpEC ) spectral code .For each simulation , we create the associated “ template ” BH signal in the frequency domain and sample it with a bank of NR waveforms from GCN numerical relativity catalogs . We see that in both the high and low frequency regions , the detection efficiencies of the NR waveforms are quite below 100 % , even for high signal - to - noise ratio ( SNR ) sources .By contrast , the MF method is 100 % efficient in both states . We additionally check the MF method on simulated Gaussian white noise and find that the screening efficiencies are 100 % for all SNRs .These conclusions show that NR waveforms are not suitable for aLIGO data analysis . Furthermore , the small detection efficiencies of NR waveforms cast doubt on aLIGO ’ s notion that it has observed gravity signals from two merging black holes .",
        "rewrite_text": "A comprehensive effort has been dedicated to the development of surrogate models for black hole (BH) transmission in the data analysis of Advanced LIGO (aLIGO). Among various techniques developed, Matched Filtering (MF) has emerged as the most frequently utilized method for recovering BH signals from aLIGO's measurement data. However, despite its age, the MF method has never been previously tested on aBH data. In this study, we implement and evaluate the MF method on two recently published numerical relativity (NR) BH merger simulations: the Effective One Body (EOB) model and the Spectral Einstein Code (SpEC) spectral code.\n\nFor each simulation, we generate the associated \"template\" BH signal in the frequency domain and sample it using a bank of NR waveforms from the GCN numerical relativity catalogs. Our findings indicate that in both high and low frequency regions, the detection efficiencies of NR waveforms remain below 100%, even for sources with a high signal-to-noise ratio (SNR). In contrast, the MF method demonstrates 100% efficiency in both states.\n\nAdditionally, we tested the MF method on simulated Gaussian white noise and found that the screening efficiencies are 100% for all SNRs. These conclusions suggest that NR waveforms are not well-suited for aLIGO data analysis. Furthermore, the low detection efficiencies of NR waveforms cast doubt on aLIGO's claim of observing gravity signals from two merging black holes.",
        "ori-fast-z-score": -1.4439897447623107,
        "water-fast-z-score": 5.363390480545726,
        "rewrite-fast-z-score": 1.6329931618554523
    },
    {
        "original_text": "We present K-band imaging of four fields surrounding four QSOs with z>1.2 identified from the SDSS Early Data Release. This is the first deep near-IR imaging of these sources and, coupled with existing B- and I-band imaging from the SXDS, allows color information to be utilized in a study of their morphologies. The observations are consistent with the candidates being host galaxies for absorbed QSOs, in terms of their optical/near-IR fluxes and colors, but deep spectroscopy is required to confirm this. The half-light radii of the hosts are typically larger than 1.5 kpc, consistent with those of local elliptical galaxies and lower than most rich clusters. This may be evidence that the strong absorption systems are located outside the clusters and are associated with the more diffuse component of the cluster dark matter. We use the NSF s ASKER survey data from SXDS in the SSA13 field to carry out this study. ASKER is an IRTF wide field survey which obtained K-band imaging of 14 X-ray bright, Chandra Deep Field South (CDF-S) candidate clusters. In addition to the imaging data, we use X-ray and spectroscopic data for the CDF-S, as well as the SDSS galaxy catalog, to study these sources in more detail. We use the photometry and host galaxy colors from the imaging data to confirm the host status of the four bright QSOs and study the morphologies of the hosts, which may be affected by the environments they reside in. We find that the hosts have mostly normal morphologies, consistent with being elliptical galaxies at z~1.2, but with half-light radii of 1.5 kpc, similar to local rich clusters. This may be evidence that the strong absorption systems are located outside the clusters and are associated with the more diffuse component of the cluster dark matter.",
        "watermark_text": "We present K - band scanning of four fields surrounding four QSOs with z > 1 . 2 identified from the SDSS Early Data Release . This is the first deep near - IR scanning of these sources and , coupled with existing B - and I - band scanning from the SXDS , allows color information to be used in a investigation of their morphologies .The findings are consistent with the candidates being host galaxies for absorption QSOs , in terms of their optical / near - IR fluxes and colors , but deep spectroscopy is required to confirm this . The quarter - light radii of the hosts are typically greater than 1 . 5 kpc , consistent with those of local elliptical galaxies and less than most rich clusters .This might be confirmation that the strong absorption centers are situated outside the clusters and are related with the more diffuse component of the cluster dark matter . We use the NSF s ASKER survey information from SXDS in the SSA13 field to carry out this study .ASKER is an IRTF wide field survey which produced K - band observations of 14 X - ray strong , Chandra Deep Field South ( CDF - S ) candidate regions . In addition to the imaging information , we using X - ray and spectroscopic data for the CDF - S , as also as the SDSS galaxy catalog , to study these sources in more detail .We use the photometry and host star patterns from the scanning data to confirm the host status of the four bright QSOs and probe the morphologies of the hosts , which may be altered by the conditions they live in . We see that the hosts have mostly regular morphologies , consistent with being elliptical galaxies at z ~ 1 . 2 , but with half - light radii of 1 . 5 kpc , comparable to nearby rich clusters .This might be confirmation that the strong absorption centers are situated outside the clusters and are identified with the more diffuse component of the cluster dark matter .",
        "rewrite_text": "We present an K-band scanning of four fields encompassing four quasars with redshift exceeding 1.2, identified through the SDSS Early Data Release. This represents the initial in-depth near-infrared scanning of these sources, which, combined with existing B- and I-band scanning from the SXDS, enables the utilization of color information in exploring their morphological features. Our findings align with the possibility that these sources are host galaxies for absorption quasars, based on their optical/near-infrared fluxes and colors. However, further deep spectroscopy is necessary to confirm this. Typically, the quarter-light radii of the host galaxies exceed 1.5 kpc, which is consistent with those of local elliptical galaxies but smaller than most rich clusters. This could be an indication that the strong absorption centers are situated outside the clusters and are linked to the more diffuse component of cluster dark matter.\n\nTo conduct this study, we utilize the NSF's ASKER survey information from the SXDS in the SSA13 field. ASKER, an IRTF wide-field survey, has generated K-band observations of 14 X-ray strong candidate regions in the Chandra Deep Field South (CDF-S). Apart from imaging data, we also utilize X-ray and spectroscopic data for the CDF-S, along with the SDSS galaxy catalog, to examine these sources in greater detail. We employ photometry and host star patterns from the scanning data to verify the status of the four bright quasars as hosts and to explore the host galaxies' morphologies, which may be influenced by their environment. We observe that the hosts mostly exhibit regular morphologies, consistent with being elliptical galaxies at a redshift of approximately 1.2, but with half-light radii of 1.5 kpc, comparable to nearby rich clusters. This further suggests that the strong absorption centers are situated outside the clusters and are associated with the more diffuse component of cluster dark matter.",
        "ori-fast-z-score": 0.1781741612749496,
        "water-fast-z-score": 7.244860247099318,
        "rewrite-fast-z-score": 3.1574088695053053
    },
    {
        "original_text": "The bound on the curvature of the universe, as inferred from the propagation of light emitted at the Cosmic Microwave Background (CMB) epoch, is equivalent to that of a flat universe, σ ≤ 10 -5. However, assuming the universe is well described by a Friedmann-Lemaître-Robertson-Walker (FLRW) metric, with σ = 0 one obtains the bound on the Hubble parameter, H ≤ 10 -5 (Planck 2013), seven orders of magnitude more stringent than the direct bound, suggesting that the true bound is significantly non-zero. It has been suggested that this significant disagreement may be an indication of new gravitational physics, such as scalar-tensor theories or deviations from General Relativity (GR). We present a simple extension of the Friedmann equations that increases the allowed deviation of the curvature from zero, and examine the implications of this extension on current cosmological datasets. We find that while this extension relaxes the bound on the Hubble parameter to H ≤ 10 -4, the bound on the curvature remains at σ = 0 with 95% confidence. We conclude that current cosmological data do not require the curvature of the universe to be non-zero at the 95% confidence level.",
        "watermark_text": "The bound on the curvature of the universe , as inferred from the propagation of light emitted at the Cosmic Microwave Background ( CMB ) epoch , is equal to that of a flat universe , σ ≤ 10 - 5 . However , suppose the universe is well described by a Friedmann - Lemaître - Robertson - Walker ( FLRW ) metric , with σ = 0 one obtains the bound on the Hubble parameter , H ≤ 10 - 5 ( Planck 2013 ) , seven orders of magnitude more stringent than the direct bound , showing that the true bound is significantly non - zero .It has been proposed that this great disagreement could be an indication of new gravity physics , such as scalar - vector models or deviations from General Relativity ( GR ) . We introduce a simple application of the Friedmann equations that increases the allowed deviation of the curvature from zero , and review the implications of this extension on current cosmological datasets .We see that while this extension relaxes the bound on the Hubble parameter to H ≤ 10 - 4 , the bound on the curvature remains at σ = 0 with 95 % confidence . We follow that current cosmological statistics do not require the curvature of the universe to be non - zero at the 95 % confidence rate .",
        "rewrite_text": "The limit on the curvature of the universe, inferred from the propagation of light emitted during the Cosmic Microwave Background (CMB) era, is equivalent to that of a flat universe with a value of σ ≤ 10^-5. However, if the universe is accurately described by the Friedmann-Lemaître-Robertson-Walker (FLRW) metric with σ set to 0, the bound on the Hubble parameter is obtained as H ≤ 10^-5 (as per Planck 2013). This is a seven-order-of-magnitude more stringent bound than the direct one, indicating that the true bound is significantly non-zero. It has been suggested that this significant discrepancy could be a hint of new gravity physics, such as scalar-vector models or deviations from General Relativity (GR). We present a straightforward application of the Friedmann equations that widens the allowed range of curvature deviations from zero and examine the implications of this extension on current cosmological datasets. We observe that while this extension relaxes the Hubble parameter bound to H ≤ 10^-4, the curvature bound remains fixed at σ = 0 with 95% confidence. Consequently, current cosmological statistics do not require the universe's curvature to be non-zero at the 95% confidence level.",
        "ori-fast-z-score": 0.8682431421244593,
        "water-fast-z-score": 5.333493587335964,
        "rewrite-fast-z-score": 1.7232808737106582
    },
    {
        "original_text": "Spectral efficiency and diversity have long been competing requirements in wireless networks. Whereas spectral efficiency refers to the information bits transmitted per second, diversity order measures the quality of the link in the presence of fading. Traditionally, network coding has been shown to improve spectral efficiency without increasing the diversity order, however, many networks require both high spectral efficiency and high diversity order. In this paper, we show that network coding with multiuser detection (MUD) can achieve both high spectral efficiency and high diversity order. We present a cooperative transmission protocol which leverages network coding and MUD to achieve high spectral efficiency and high diversity simultaneously. We analyze the protocol using the degree of freedom (DoF) framework and present a closed form approximation for the symmetric case. Through simulations, we show that the spectral efficiency and diversity achieved by the proposed protocol are significantly higher than both Network Coding with Max-Ratio Combining (MRC) and Network Coding with Successive Interference Cancellation (SIC). In addition, we show that the protocol is very effective at maximizing the throughput of non-colluding nodes in a network.",
        "watermark_text": "Spectral efficiency and diversity have often been competing requirements in telecommunications networks . Whereas spectral efficiency relates to the information bits sent per second , diversity order measures the performance of the link in the presence of fading .Traditionally , network compression has been shown to achieve spectral capacity without increasing the diversity order , however , large systems require both high spectral efficiency and large diversity order . In this paper , we find that network coding with multiuser recognition ( MUD ) can attain both high spectral efficiency and large variety order .We create a cooperative communication protocol which leverages network coding and MUD to achieve high spectral capacity and large diversity simultaneously . We evaluate the protocol using the degree of liberty ( DoF ) framework and present a closed form approximation for the symmetric case .Through simulations , we find that the spectral efficiency and diversity attained by the suggested protocol are greatly higher than both Network Coding with Max - Ratio Combining ( MRC ) and Network Coding with Successive Interference Cancellation ( SIC ) . In addition , we find that the protocol is very effective at maximizing the throughput of non - colluding nodes in a network .",
        "rewrite_text": "In telecommunications networks, spectral efficiency and diversity are often seen as competing objectives. Spectral efficiency pertains to the number of information bits transmitted per second, while diversity order quantifies the link's performance in the presence of fading. Traditionally, network compression has been utilized to achieve spectral capacity without increasing diversity order. However, in large systems, there is a need for both high spectral efficiency and extensive diversity order. This paper explores the possibility of employing network coding with multiuser recognition (MUD) to achieve both high spectral efficiency and significant diversity.\n\nWe develop a cooperative communication protocol that leverages network coding and MUD to simultaneously achieve high spectral capacity and diversity. We evaluate this protocol using the framework of degrees of freedom (DoF) and provide a closed-form approximation for the symmetric case. Simulation results indicate that the spectral efficiency and diversity achieved by our proposed protocol significantly outperform network coding with max-ratio combining (MRC) and network coding with successive interference cancellation (SIC). Furthermore, we find that this protocol is highly effective at maximizing the throughput of non-colluding nodes in a network.",
        "ori-fast-z-score": -1.3480372031495529,
        "water-fast-z-score": 5.157106231293967,
        "rewrite-fast-z-score": 0.9901475429766744
    },
    {
        "original_text": "Semi-structured text, e.g. text from the web or engineering documents, are composed of implicit schemas, i.e. possible relationships between the concept words, and explicit tuples of co-occurrence between the concept words. In this paper, we present a dynamic user-defined similarity searching (DUSDSS) model that incorporates both implicit and explicit relationships for semi-structured text retrieval. Based on the user-defined semantic space, we design a similarity function to measure the semantic similarity between search terms and retrieve the relevant documents ranked by these similarities. In order to accurately reflect the semantics of users’ interest and expand the coverage of document collection, we further propose to generate document semantic representations from multiple perspectives, such as topic, author and document topic networks. To efficiently compute the relevance between user queries and documents, we present a continuous space search strategy, which applies negative sampling to approximate the ranking metric. Finally, we conduct experiments on two public datasets and the results show that our model can outperform state-of-the-art methods on both explicit and implicit semantic similarity evaluations.",
        "watermark_text": "Semi - organized data , e . g . text from the website or engineering papers , are composed of implicit schemas , i . e .possible connections between the idea words , and explicit tuples of co - existence between the idea words . In this paper , we present a dynamic user - defined similarity searching ( DUSDSS ) model that incorporates both implicit and explicit relationships for semi - organized text retrieval .Based on the user - defined semantic space , we design a similarity function to measure the semantic similarity between search terms and retrieve the appropriate documents listed by these similarities . In order to properly reflect the semantics of consumers ’ interest and expand the coverage of file collection , we further recommend to create document semantic representations from multiple perspectives , such as topic , author and document topic networks .To efficiently compute the relevance between visitor queries and documents , we present a continuous space search method , which uses negative sampling to approximate the ranking metric . Finally , we conduct experiments on two public datasets and the results show that our model can outperform state - of - the - art methods on both explicit and implicit semantic similarity evaluations .",
        "rewrite_text": "Semi-structured data, such as text from websites or engineering papers, consist of implicit schemas, which refer to potential connections between idea words, and explicit tuples indicating co-existence between these idea words. In this study, we introduce a Dynamic User-Defined Similarity Searching (DUSDSS) model that integrates both implicit and explicit relationships for the retrieval of semi-structured text. Based on a user-defined semantic space, we develop a similarity function to assess the semantic similarity between search terms and retrieve relevant documents ordered accordingly.\n\nTo accurately reflect consumer interests and enhance the scope of file collection, we propose creating document semantic representations from various perspectives, including topic, author, and document topic networks. To efficiently evaluate the relevance between user queries and documents, we present a continuous space search approach that employs negative sampling to approximate ranking metrics.\n\nWe have conducted experiments on two public datasets, and the results demonstrate that our model surpasses state-of-the-art methods in both explicit and implicit semantic similarity evaluations.",
        "ori-fast-z-score": -0.40406101782088427,
        "water-fast-z-score": 3.5176323534072425,
        "rewrite-fast-z-score": 0.7337993857053429
    },
    {
        "original_text": "We present the discovery of fifteen new low-mass companions to planet host stars, using data from the squeezernigton component of the high-contrast imaging instrument GPI. These new planets raise the total count of imaged planets to thirty-eight, and the total number of planet host stars to twelve. Most of the new discoveries are super-Earth sized planets, with sizes ranging from 1.3 to 5.7 Earth radii, and masses from 2 to 20 Earth masses. Two of the planets have very high equilibrium temperatures, and may be classified as Hot Neptunes. Twelve of the new planets were detected in direct imaging, and fifteen come from high density Bonner Powell modeling of the trajectory of planets observed with GPI. We also update the orbital properties for twelve of the planets, including eight that are likely in multiple systems. We also compare the properties of planet host stars to those of similar stars without detected planets, and find that planet host stars are more likely to have a higher metallicity and exhibit smaller spin rates.",
        "watermark_text": "We report the discovery of fifteen new small - mass companions to planet host stars , using data from the squeezernigton element of the high - contrast imaging object GPI . These new objects raise the total count of imaged stars to thirty - eight , and the total number of planet host stars to fourteen .Most of the new finds are super - Earth sized planets , with sizes varied from 1 . 3 to 5 . 7 Earth radii , and masses from 2 to 20 Earth masses . Two of the planets have very high equilibrium temperatures , and may be categorized as Hot Neptunes .Twelve of the new objects were detected in direct imaging , and twenty come from high density Bonner Powell modeling of the path of planets observed with GPI . We additionally update the orbital properties for twelve of the planets , including eight that are likely in multiple systems .We also compare the properties of planet host stars to those of related stars without found planets , and find that planet host stars are more likely to have a higher metallicity and experience smaller spinning speeds .",
        "rewrite_text": "We announce the discovery of fifteen new small-mass companions to planet-hosting stars, utilizing data from the Squeezernigton segment of the high-contrast imaging instrument GPI. These new findings have increased the total number of imaged stars to thirty-eight and the number of planet-hosting stars to fourteen. The majority of these new discoveries are super-Earth-sized planets with sizes ranging from 1.3 to 5.7 Earth radii and masses between 2 and 20 Earth masses. Two of the planets possess exceptionally high equilibrium temperatures, potentially qualifying them as Hot Neptunes.\n\nTwelve of the new objects were detected through direct imaging, while twenty were derived from high-density Bonner Powell modeling of GPI observations. Furthermore, we have updated the orbital properties of twelve planets, including eight that are potentially part of multiple systems. We also conducted a comparison between the properties of planet-hosting stars and those of similar stars without detected planets, finding that planet-hosting stars tend to have higher metallicities and lower rotational speeds.",
        "ori-fast-z-score": 0.8432740427115678,
        "water-fast-z-score": 4.926938732593016,
        "rewrite-fast-z-score": 0.4216370213557839
    },
    {
        "original_text": "Active galactic nuclei (AGNs) are powered by accretion of gas and/or stars onto supermassive black holes (SMBHs). It is believed that most of SMBHs have masses less than 10^8 solar masses in the centers of normal galaxies, while the masses of SMBHs in AGNs derived from the observed spectrum are much greater than 10^8 solar masses, which are called  massive SMBHs  (MSSMHs). The MSSMHs have larger angular momenta than normal SMBHs, and their growth requires a short timescale. It is still an open question how MSSMHs are grown in the centers of normal galaxies. Suzaku performed the first large-scale survey of AGNs detected by Swift/BAT in the hard X-ray band (15-50keV), which covers a wide range of black hole mass. We found that most of AGNs detected by Suzaku have large masses of black holes and show common featureless spectra, which are different from NLS1s. The estimated masses of SMBHs in AGNs detected by Suzaku are greater than 10^9 solar masses, which are the most massive SMBHs ever found in the centers of normal galaxies. We named them  new type  SMBHs (nSMBHs). We discuss the difference of growth of nSMBHs and MSSMHs and the difference of host galaxies between nSMBHs and MSSMHs.",
        "watermark_text": "Active galactic nuclei ( AGNs ) are powered by accretion of gas and / or stars onto supermassive black holes ( SMBHs ) . It is suspected that most of SMBHs have masses less than 10 ^ 8 solar masses in the centers of normal galaxies , while the masses of SMBHs in AGNs generated from the seen spectrum are many greater than 10 ^ 8 solar masses , which are called huge SMBHs ( MSSMHs ) .The MSSMHs have larger angular momenta than regular SMBHs , and their development involves a small timescale . It is remains an open controversy how MSSMHs are grown in the centers of normal galaxies .Suzaku completed the first large - scale search of AGNs detected by Swift / BAT in the hard X - ray band ( 15 - 50keV ) , which covers a broad variety of brown hole mass . We showed that most of AGNs detected by Suzaku have huge masses of white holes and show common featureless spectra , which are distinct from NLS1s .The estimated masses of SMBHs in AGNs detected by Suzaku are larger than 10 ^ 9 solar masses , which are the most large SMBHs yet found in the centers of normal galaxies . We designated them new kind SMBHs ( nSMBHs ) .We discuss the difference of growth of nSMBHs and MSSMHs and the difference of host galaxies between nSMBHs and MSSMHs.",
        "rewrite_text": "Active Galactic Nuclei (AGNs) are powered by the accretion of gas and/or stars onto Supermassive Black Holes (SMBHs). It is believed that the majority of SMBHs have masses under 10^8 solar masses at the centers of typical galaxies. However, SMBHs in AGNs, detected through visible spectra, often weigh much more than 10^8 solar masses, referred to as Massive SMBHs (MSSMHs). These MSSMHs possess greater angular momenta than regular SMBHs and their growth occurs over a relatively short time frame. The exact mechanism behind the growth of MSSMHs in the centers of ordinary galaxies remains a contentious topic.\n\nSuzaku conducted the first large-scale search for AGNs detected by Swift/BAT in the hard X-ray range (15-50 keV), encompassing a wide range of black hole masses. Our findings indicate that the majority of AGNs identified by Suzaku possess enormous white hole masses and exhibit non-specific spectra, distinct from NLS1s. The estimated masses of SMBHs in AGNs detected by Suzaku exceed 10^9 solar masses, marking the largest SMBHs yet discovered at the centers of regular galaxies. We have labeled them as novel types of SMBHs (nSMBHs). We delve into the disparities in the growth of nSMBHs and MSSMHs, as well as the differences in host galaxies between nSMBHs and MSSMHs.",
        "ori-fast-z-score": -0.44172610429938614,
        "water-fast-z-score": 4.858987147293248,
        "rewrite-fast-z-score": 0.0
    },
    {
        "original_text": "In this study, we have identified mitochondrial colorectal tumor (MCT) peptides that inhibit cancer cell proliferation. MCT1-5 were discovered using a functional genomics approach. MCT1-3 areSSL/CBM domains, a structural motif that is found in numerous protein families, including the ubiquitin-like Nedd8, autophagy-related MAP1-LC3, and rRNA-processing Lsg1/Rat8 families. The antiproliferative MCR peptides bind the insulin receptor (INSR) and inhibit the formation of the insulin-INSR complex, leading to dephosphorylation of RB, inhibition of cell cycle progression, and consequent antiproliferative effect. Cancer cells rely on the phospho-RB pathway to grow, so the MCRs represent a novel class of antiproliferative agents. Further characterization of MCR structures and functions may lead to new anticancer drugs. Gene names, MCT1, MCT2 and MCT3, refer to mitochondrial colorectal tumor genes 1, 2 and 3, respectively. These genes were discovered using a functional genomics approach. The antiproliferative MCR peptides bind the insulin receptor (INSR) and inhibit the formation of the insulin-INSR complex, leading to dephosphorylation of RB, inhibition of cell cycle progression, and consequent antiproliferative effect. Cancer cells rely on the phospho-RB pathway to grow, so the MCRs represent a novel class of antiproliferative agents. Further characterization of MCR structures and functions may lead to new anticancer drugs. Insulin and INSR are proteins that are involved in regulating human growth and development. MCRs, also called INSR/Ins peptide complexes, are able to bind insulin and prevent it from binding and activating its receptor, leading to reduced INSR tyrosine kinase activity and inhibition of insulin-dependent processes. One of the main physiological functions of insulin is cell proliferation. By blocking the binding of insulin to its receptor, MCRs inhibit cancer cell proliferation. The insulin-INSR complex is composed of two INSR polypeptides and two insulin molecules. The INSR proteins consist of an extracellular domain, a transmembrane domain, and an intracellular domain that includes the tyrosine kinase domain. The INSR extracellular domain binds insulin and INSR transmembrane domain transmits the signal from the INSR cell membrane to the intracellular domain, leading to INSR tyrosine kinase autophosphorylation and downstream signaling. The antiproliferative effect of MCRs may be explained by dephosphorylation of RB. This inhibition is caused by the absence of the insulin-INSR complex and decreased INSR tyrosine kinase activity. Without tyrosine kinase activity, RB is dephosphorylated and cannot block cell cycle progression.",
        "watermark_text": "In this study , we have discovered mitochondrial colorectal disease ( MCT ) peptides that inhibit tumor tissue development . MCT1 - 5 were discovered using a functional genomics approach .MCT1 - 3 areSSL / CBM regions , a structural domain that is found in countless protein families , notably the ubiquitin - like Nedd8 , autophagy - linked MAP1 - LC3 , and rRNA - processing Lsg1 / Rat8 families . The antiproliferative MCR peptides bind the insulin receptor ( INSR ) and inhibit the formation of the insulin - INSR complex , leading to dephosphorylation of RB , inhibition of cell cycle progression , and consequent antiproliferative effect .Cancer cells rely on the phospho - RB pathway to develop , so the MCRs represent a novel class of antiproliferative agents . Further determination of MCR complexes and functions may contribute to novel anticancer medications .Gene titles , MCT1 , MCT2 and MCT3 , correspond to mitochondrial colorectal disease genes 1 , 2 and 3 , respectively . These genes were discovered using a functional genomics approach .The antiproliferative MCR peptides bind the insulin receptor ( INSR ) and inhibit the formation of the insulin - INSR complex , leading to dephosphorylation of RB , inhibition of cell cycle progression , and consequent antiproliferative effect . Cancer cells rely on the phospho - RB pathway to develop , so the MCRs represent a novel class of antiproliferative agents .Further determination of MCR structures and functions may lead to novel anticancer medications . Insulin and INSR are proteins that are responsible in controlling human growth and growth .MCRs , sometimes called INSR / Ins peptide complexes , are able to bind insulin and stop it from binding and activating its receptor , leading to reduced INSR tyrosine kinase activity and inhibition of insulin - dependent mechanisms . One of the main physiological activities of insulin is cell development .By blocking the binding of insulin to its receptor , MCRs inhibit tumor tissue differentiation . The insulin - INSR compound is composed of two INSR polypeptides and two insulin molecules .The INSR proteins comprise of an extracellular domain , a transmembrane domain , and an intracellular domain that contains the tyrosine kinase domain . The INSR extracellular domain binds insulin and INSR transmembrane region transmits the signal from the INSR cell membrane to the intracellular domain , leading to INSR tyrosine kinase autophosphorylation and downstream signaling .The antiproliferative effect of MCRs might be explained by dephosphorylation of RB . This inhibition is caused by the absence of the insulin - INSR complex and diminished INSR tyrosine enzyme activity .Without tyrosine kinase activity , RB is dephosphorylated and cannot block cell cycle evolution .",
        "rewrite_text": "In this research, we have identified mitochondrial colorectal disease (MCT) peptides that effectively hinder the growth of tumor tissue. These peptides, MCT1-5, were discovered through a functional genomics approach. Specifically, MCT1-3 are SSL/CBM regions, a structural domain found in numerous protein families. Notably, they resemble the ubiquitin-like Nedd8, autophagy-linked MAP1-LC3, and rRNA-processing Lsg1/Rat8 families. \n\nThe anti-proliferative MCR peptides engage with the insulin receptor (INSR) and prevent the formation of the insulin-INSR complex. This interaction leads to the dephosphorylation of RB, which subsequently hinders the progression of the cell cycle and exerts an anti-proliferative effect. Cancer cells rely on the phospho-RB pathway for growth, making MCRs a novel class of anti-proliferative agents. Further exploration of MCR structures and functions may contribute to the development of new anticancer medications.\n\nGene titles such as MCT1, MCT2, and MCT3 correspond to mitochondrial colorectal disease genes 1, 2, and 3, respectively. These genes were identified using a functional genomics approach. The anti-proliferative MCR peptides bind to INSR and prevent the formation of the insulin-INSR complex, ultimately leading to RB dephosphorylation and inhibiting cell cycle progression. This effect is significant as cancer cells depend on the phospho-RB pathway for growth, making MCRs a promising new class of anti-proliferative agents.\n\nInsulin and INSR are essential proteins regulating human growth and development. MCRs, also known as INSR/Ins peptide complexes, can bind with insulin and prevent its binding to its receptor, reducing INSR tyrosine kinase activity and inhibiting insulin-dependent mechanisms. One of the primary roles of insulin is cell development. By blocking the binding of insulin to its receptor, MCRs hinder tumor tissue differentiation.\n\nThe insulin-INSR complex comprises two INSR polypeptides and two insulin molecules. The INSR proteins consist of an extracellular domain, a transmembrane domain, and an intracellular domain containing the tyrosine kinase domain. The extracellular domain of INSR binds insulin while the transmembrane region transmits signals from the INSR cell membrane to the intracellular domain, leading to INSR tyrosine kinase autophosphorylation and downstream signaling. The dephosphorylation of RB, a key component in the anti-proliferative effect of MCRs, is likely due to the absence of the insulin-INSR complex and reduced INSR tyrosine enzyme activity. Without tyrosine kinase activity, RB becomes dephosphorylated and unable to block cell cycle progression.",
        "ori-fast-z-score": -3.320391543176798,
        "water-fast-z-score": 3.8617409905715934,
        "rewrite-fast-z-score": -1.005970202294378
    },
    {
        "original_text": "Vortex dynamics in low-dimensional disordered superconductors is an actively studied problem. It is established now that at high magnetic fields, the dynamics of Abrikosov vortices obeys the KZ theory, i.e. it is determined by the energy spectrum of quasiparticles. At low fields, numerous interesting experimental phenomena have been discovered, such as the exponential vortex relaxation, aperiodic diffusion and a breakdown of the KZ scenario. All these phenomena have been explained as manifestations of the destruction of vortex quasi-long-range order. The latter indicates the destruction of quasiparticle energy spectrum. In this work, we show that in superconductors with fractal cluster structure, the vortex dynamics at the initial stage of resistive transition is also determined by the fractal character of the cluster structure. We perform Monte Carlo simulation of the effective Ginzburg-Landau equation for the order parameter and show that at initial resistive transition stage, the correlation function of the order parameter demonstrates the power-law behavior with the critical exponent close to the value observed in 2D Ising model with the presence of magnetic field and with the dimension of the substrate equal to the fractal dimension of the cluster structure.",
        "watermark_text": "Vortex dynamics in low - dimensional disordered superconductors is an actively studied question . It is recognized now that at high magnetic fields , the dynamics of Abrikosov vortices obeys the KZ principle , i . e .it is chosen by the energy spectrum of quasiparticles . At small fields , various exciting experimental phenomena have been observed , such as the exponential vortex relaxation , aperiodic convection and a breakdown of the KZ scenario .All these phenomena have been explained as manifestations of the destruction of vortex quasi - far - range order . The latter suggests the destruction of quasiparticle energy spectrum .In this research , we prove that in superconductors with fractal cluster structure , the vortex dynamics at the first phase of resistive transition is also determined by the fractal character of the cluster structure . We perform Monte Carlo simulation of the effective Ginzburg - Landau equation for the order parameter and find that at first resistive transition step , the correlation function of the order parameter demonstrates the power - law behavior with the significant exponent next to the value observed in 2D Ising model with the presence of magnetic force and with the dimension of the substrate equivalent to the fractal dimension of the cluster structure .",
        "rewrite_text": "The investigation of vortex dynamics in low-dimensional disordered superconductors is an actively pursued research topic. It has become apparent that at high magnetic fields, the motion of Abrikosov vortices follows the KZ principle, which is determined by the energy spectrum of quasiparticles. However, at lower fields, a range of intriguing experimental phenomena have been observed, including exponential vortex relaxation, aperiodic convection, and a departure from the KZ scenario. These phenomena have been interpreted as manifestations of the disruption of a quasi-long-range order of vortices, which in turn suggests the alteration of the quasiparticle energy spectrum.\n\nIn this study, we demonstrate that in superconductors with a fractal cluster structure, the vortex dynamics during the initial phase of the resistive transition is also influenced by the fractal nature of the cluster structure. We conducted a Monte Carlo simulation of the effective Ginzburg-Landau equation for the order parameter and found that, at the initial resistive transition step, the correlation function of the order parameter exhibits power-law behavior with a significant exponent closely resembling values observed in the 2D Ising model. This is observed in the presence of magnetic force and with a substrate dimension equivalent to the fractal dimension of the cluster structure.",
        "ori-fast-z-score": 1.2074068598865937,
        "water-fast-z-score": 6.405028512341099,
        "rewrite-fast-z-score": 3.073993852018444
    },
    {
        "original_text": "A sample of X-ray flares from a large sample of low-mass stars in the Orion complex has been analyzed. Most of the sources were detected by XMM-Newton and Chandra observations of the region, performed between 2000 and 2009. Most of the flares were characterized by exponential increases in flux with an e-folding time of about 12 hours. Flare peak luminosities were highly variable, ranging from 5 x 10 26 erg/s to 7.5 x 10 27 erg/s for Chandra and from 3.5 x 10 27 erg/s to 7.5 x 10 27 erg/s for XMM-Newton. No significant correlation was found between the rise times and peak luminosities, nor between the observed x-ray flux and that expected from the stellar photospheric emission. We suggest that flares are produced by magnetic reconnection events in the stellar magnetospheres.",
        "watermark_text": "A specimen of X - ray flares from a large sample of low - density stars in the Orion complex has been examined . Most of the sources were detected by XMM - Newton and Chandra measurements of the region , conducted between 2000 and 2009 .Most of the flares were described by exponential increases in flux with an e - folding time of about 12 hours . Flare peak luminosities were extremely varied , ranging from 5 x 10 26 erg / s to 7 . 5 x 10 27 erg / s for Chandra and from 3 . 5 x 10 27 erg / s to 7 . 5 x 10 27 erg / s for XMM - Newton .No important relationship was shown between the surge years and peak luminosities , nor between the seen x - ray density and that expected from the stellar photospheric emission . We suggest that flares are produced by magnetic reconnection events in the stellar magnetospheres .",
        "rewrite_text": "An examination has been conducted on a sample of X-ray flares originating from a large group of low-density stars in the Orion complex. The majority of these sources were discovered through XMM-Newton and Chandra measurements taken within the region between 2000 and 2009. Most of the observed flares were characterized by exponential increases in flux, with an e-folding time of approximately 12 hours. The peak luminosities of these flares varied greatly, ranging from 5 x 10^26 erg/s to 7.5 x 10^27 erg/s for Chandra measurements and from 3.5 x 10^27 erg/s to 7.5 x 10^27 erg/s for XMM-Newton measurements. No significant correlation was found between flare occurrence years and peak luminosities, nor between observed X-ray density and that expected from stellar photospheric emission. We propose that these flares are generated by magnetic reconnection events within the stellar magnetospheres.",
        "ori-fast-z-score": -0.14586499149789456,
        "water-fast-z-score": 3.7527767497325675,
        "rewrite-fast-z-score": 0.816496580927726
    },
    {
        "original_text": "Formation of cosmic strings is an important aspect of the early universe cosmology. One of the most elegant ways to achieve this is for the potential energy of a false vacuum to be converted to kinetic energy of cosmic strings. In this paper, we study string formation by a localised topological defect - an Abrikosov lattice. In particular, we consider a model with a real scalar field with an artificial local symmetry which is spontaneously broken. The latter results in the formation of localised lumps which act as topological defects. Depending on the initial conditions, the system can evolve into two distinct states. In the first one, the defects form a disordered gas, while in the other one they form an ordered lattice. We characterise the dynamics of the system using a set of scaling exponents which describe the decay of both the kinetic and the potential energy of the system. We determine the universality class of the transition between the two states and demonstrate that it belongs to the three dimensional Ising model.",
        "watermark_text": "Formation of universe strings is an important element of the early cosmic cosmology . One of the most elegant means to achieve this is for the potential energy of a false vacuum to be reduced to kinetic power of universe strings .In this paper , we study string formation by a localised topological defect - an Abrikosov lattice . In particular , we investigate a theory with a real scalar field with an artificial local symmetry which is spontaneously shattered .The latter leads in the formation of localised lumps which act as topological errors . Depending on the initial conditions , the scheme can evolve into two different states .In the first one , the defects form a disordered gas , while in the other one they create an ordered lattice . We characterise the dynamics of the system using a setting of scaling exponents which describe the decay of both the kinetic and the potential energy of the system .We determine the universality category of the shift between the two states and assert that it belongs to the three dimensional Ising model .",
        "rewrite_text": "In the early cosmic cosmology, the formation of cosmic string networks plays a pivotal role. One of the most elegant ways to achieve this is through the conversion of the potential energy in a false vacuum into the kinetic power of these string networks. This paper focuses on studying string formation through a localized topological defect, specifically an Abrikosov lattice. We primarily investigate a theory involving a real scalar field with an artificial local symmetry that spontaneously shatters. This results in the formation of localized lumps that behave as topological errors. Depending on the initial conditions, the system can evolve into two distinct states. In one state, the defects form a disordered gas, while in the other, they create an organized lattice. We characterize the dynamics of the system using scaling exponents that describe the decay of both kinetic and potential energy within the system. We identify the universality class of the transition between these two states and assert that it aligns with the three-dimensional Ising model.",
        "ori-fast-z-score": -1.4342743312012722,
        "water-fast-z-score": 2.8685486624025445,
        "rewrite-fast-z-score": -0.46499055497527714
    },
    {
        "original_text": "We have obtained the first imaging observation of transient brightenings in the cores of galaxies. These X-ray transients were discovered with the Chandra satellite when it observed the central regions of the galaxies NGC 4874 and 6052 during 2002 and 2003. X-ray luminosities comparable to those of classical novae were observed, as well as apparent motions corresponding to massive outflows from the nuclei of the galaxies. The X-ray emitting regions are restricted to 10 - 15 parsecs in diameter, much smaller than the gravitational radii of the galaxies  central black holes. Together, these observations indicate that the galaxies  central black holes are endowed with significant populations of hypercompact X-ray binaries. Because these X-ray binaries can be as massive as early type stars, they may play a key role in fueling the central black holes and furthering the transformation of their host galaxies from normal to quasar phases.",
        "watermark_text": "We have achieved the first imaging observation of transient brightenings in the cores of galaxies . These X - ray transients were discovered with the Chandra satellite when it observed the inner regions of the galaxies NGC 4874 and 6052 during 2002 and 2003 .X - ray luminosities comparable to those of classical novae were detected , as well as obvious motions corresponding to massive outflows from the nuclei of the galaxies . The X - ray emitting regions are restricted to 10 - 15 parsecs in length , far smaller than the gravitational radii of the galaxies central black holes .Together , these observations indicate that the galaxies central black holes are furnished with substantial colonies of hypercompact X - ray binaries . Because these X - ray binaries can be as huge as early type stars , they may play a key importance in fueling the main white holes and furthering the transformation of their host galaxies from normal to quasar phases .",
        "rewrite_text": "We have successfully conducted the initial imaging observation of transient brightenings within the cores of galaxies. These X-ray transients were discovered by the Chandra satellite while observing the inner regions of galaxies NGC 4874 and 6052 between 2002 and 2003. Comparable X-ray luminosities to those of classical novae were detected, along with evident movements indicating massive outflows from the galaxy nuclei. The X-ray emitting areas are confined to lengths of 10 to 15 parsecs, which is much smaller than the gravitational radii of the central black holes of the galaxies. These observations collectively suggest that the central black holes of galaxies are abundant in hypercompact X-ray binary colonies. Given that these X-ray binaries can be as large as early-type stars, they may play a crucial role in fueling primary white holes and driving the transformation of their host galaxies from normal to quasar phases.",
        "ori-fast-z-score": 3.713069518053983,
        "water-fast-z-score": 6.350006350009525,
        "rewrite-fast-z-score": 2.3937749957251055
    },
    {
        "original_text": "The evolution of the V4334 Sgr system from its formation approximately 675Myr ago has been followed using computer models. The evolutionary pathways considered include mergers of white dwarf and helium star sequences, a possible electron-capture supernova and a double helium white dwarf sequence. The most recent modelling indicates that a merger of a helium star and a white dwarf of approximately 1.1+0.8M⊙ produced a unstable evolutionary path and lead to a common envelope event about 675Myr ago. The system is likely to have subsequently undergone a gravitational wave induced spiral-in event and the final merger may have been observed as the recurrent nova T Pyxidis. The progenitor system for V4334 Sgr had a initial separation of approximately 8.2×10^{12} cm at the time of the spiral-in. If the earlier merger models are correct, it is likely that the total mass ejected in the spiral-in event was sufficient to reduce the mass of the residual core to the levels observed today, with the final envelope ejected in a series of hydrogen rich superoutbursts.",
        "watermark_text": "The evolution of the V4334 Sgr scheme from its formation approximately 675Myr ago has been followed using computer models . The evolutionary pathways discussed include mergers of white dwarf and helium star sequences , a possible electron - capture supernova and a double helium white dwarf sequence .The most recent modelling shows that a merger of a helium star and a white dwarf of approximately 1 . 1 + 0 . [UNK] produced a unstable evolutionary path and lead to a common envelope event about 675Myr ago . The system is likely to have subsequently undergone a gravitational wave induced spiral - in event and the final merger may have been observed as the recurrent nova T Pyxidis .The progenitor system for V4334 Sgr had a initial separation of approximately 8 . 2×10 ^ { 12 } cm at the time of the spiral - in . If the previous merger models are correct , it is probably that the total mass ejected in the spiral - in event was sufficient to reduce the mass of the residual core to the levels observed nowadays , with the last envelope ejected in a sequence of sulfur rich superoutbursts .",
        "rewrite_text": "The progression of the V4334 Sgr scheme, tracing back to its formation approximately 675 million years ago, has been meticulously tracked using sophisticated computer models. The discussed evolutionary pathways encompass mergers of white dwarf and helium star sequences, a potential electron-capture supernova, and a double helium white dwarf sequence. The latest simulations reveal that a recent fusion between a helium star and a white dwarf, roughly 1.1 plus 0. [UNK], created an unstable evolutionary path leading to a common envelope event roughly 675 million years ago. The system may have subsequently undergone a spiral-in event induced by gravitational waves, with the ultimate merger potentially observable as the recurrent nova T Pyxidis.\n\nAt the time of the spiral-in, the initial separation between the components of the V4334 Sgr system was approximately 8.2 times 10^12 cm. If the previous merger models are accurate, it is likely that the total mass ejected during the spiral-in event was sufficient to reduce the mass of the residual core to its current levels, with the final envelope expulsion occurring in a sequence of sulfur-rich superoutbursts.",
        "ori-fast-z-score": 1.7253243712550146,
        "water-fast-z-score": 3.6663142889169062,
        "rewrite-fast-z-score": 1.1531133203941102
    },
    {
        "original_text": "A plasma is a very dilute collection of charged particles. The individual particles have no charge or only a very small charge, but due to Coulomb s law they can exert a force on each other. When a plasma moves with respect to a neutral, non-conducting background, the particles will experience a force in the direction of motion, called drifts. In this paper, we consider a scenario where a positively charged particle is moving towards a stationary plasma sheet. It will approach the plasma sheet and experience a negatively charged field. This field will cause the particle to move in the direction of the positive field. Before the particle can reach the plasma sheet, it will encounter a region of null field, the so-called plasmabal, in which the particle experience no force. In this region, the particles will bounce back and forth, and the motion is unstable. The particle will execute an oscillatory motion, called bounceCTL.",
        "watermark_text": "A plasma is a very dilute collection of charged particles . The individual molecules have no charge or only a very small charge , but due to Coulomb s law they can exert a pressure on each other .When a gas moves with regard to a neutral , non - conducting background , the ions will experience a force in the direction of movement , called drifts . In this paper , we imagine a situation where a positively charged particle is moving towards a stationary plasma sheet .It will impact the plasma sheet and experience a negatively charged field . This field will cause the particle to move in the direction of the positive field .Before the particle can reach the plasma layer , it will experience a region of null field , the so - called plasmabal , in which the particle experience no force . In this area , the ions will bounce back and forth , and the movement is unstable .The object will perform an oscillatory motion , known bounceCTL .",
        "rewrite_text": "A plasma comprises a highly dilute assembly of charged particles. The individual molecules possess no charge or only a minimal charge, yet they can exert pressure on each other due to Coulomb's law. When a gas moves in relation to a neutral, non-conductive background, the ions undergo a force in the direction of movement, termed as drift. In this paper, we envision a scenario where a positively charged particle is moving towards a stationary plasma sheet. It will collide with the plasma sheet and encounter a negatively charged field. This field will direct the particle towards the positive field direction. Before the particle can reach the plasma layer, it will pass through a region of zero field, referred to as the plasmabal. In this area, the ions will bounce back and forth, resulting in an unstable oscillatory motion. This motion is known as bounceCTL.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 4.242640687119286,
        "rewrite-fast-z-score": 2.0465595024580763
    },
    {
        "original_text": "In this paper, we study properties of the nonholonomic Ricci flows on solitonic pp--waves and Schwarzschild solutions of vacuum Einstein equations. Such flows generalize the well-known Einstein flow and they are defined by the expressions, which describe the nonholonomic deformation of a certain Plano-Projective metric in terms of a (non-holonomic) Lagrange derivative. We prove that the nonholonomic Ricci flow, whose  potential  is the squared of the unique M-curvature, is equivalent to the nonholonomic flow envelope (NFE) for the class of vacuum Einstein equations with anholonomic variables. It means that solutions of the considered equations for the M-curvature define the same solutions of the nonholonomic Ricci flow. We prove that the considered vacuum Einstein equations with anholonomic variables define the Riemann geometry, which never Ricci flows, but only nonholonomic Ricci flows. We find the formulas for variation of some geometric objects, defining the induced nonholonomic distribution and nonlinear connection (ND) in the corresponding deformations. Such formulas can be used for redefinition of the solution parameters for generating new families of solutions by such nonholonomic deformations. We show that such formal transformations are not uniquely defined and there are some some solutions, which cannot be transformed into each other by such nonholonomic formal parameter transformations. The Abelian and non-Abelian conservation laws for the nonholonomic Ricci flows are found. Such conservation laws are very useful for integration of nonlinear partial differential equations, corresponding to the considered flow. The solitonic geometry with nonholonomic constraints allows to define some classes of exact solutions of the vacuum Einstein equations with inhomogeneous (such like parametric) deformations, preserving some special symmetries. Such symmetries can be used for constructing exact solutions with some symmetries, using corresponding generating functions. In the Appendix we prove that our nonholonomic deformations of the vacuum Schwarzschild solution define the corresponding parametric families of such solutions, with a Killing symmetry and a parametric deformation parameter being the rescaled parameter from the classical Schwarzschild metric. Such solutions have a big roche type hump on the Lifshitz type hyper-surface and they are compared with corresponding Einstein flows.",
        "watermark_text": "In this paper , we study properties of the nonholonomic Ricci currents on solitonic pp - - waves and Schwarzschild solutions of vacuum Einstein equations . Such streams generalize the better - famous Einstein flow and they are specified by the expressions , which describe the nonholonomic deformation of a certain Plano - Projective metric in terms of a ( non - holonomic ) Lagrange integral .We establish that the nonholonomic Ricci stream , whose potential is the squared of the unique M - curvature , is analogous to the nonholonomic stream envelope ( NFE ) for the class of vacuum Einstein equations with anholonomic parameters . It says that solutions of the considered equations for the M - curvature form the same solutions of the nonholonomic Ricci stream .We prove that the considered vacuum Einstein equations with anholonomic parameters define the Riemann geometry , which never Ricci flows , but only nonholonomic Ricci flows . We get the formulas for variation of some geometric objects , establishing the induced nonholonomic distribution and nonlinear connection ( ND ) in the associated deformations .Such formulas can be used for redefinition of the solution parameters for generating new families of solutions by such nonholonomic deformations . We see that such formal transformations are not uniquely defined and there are some some solutions , which cannot be changed into each other by such nonholonomic formal parameter transformations .The Abelian and non - Abelian conservation laws for the nonholonomic Ricci currents are found . Such conservation laws are very useful for integration of nonlinear partial differential coefficients , corresponding to the considered flow .The solitonic topology with nonholonomic restrictions enables to define some types of precise solutions of the vacuum Einstein equations with inhomogeneous ( such like parametric ) deformations , preserving some particular symmetries . Such symmetries can be used for constructing exact solutions with some symmetries , using corresponding generating functions .In the Appendix we prove that our nonholonomic deformations of the vacuum Schwarzschild solution describe the corresponding parametric families of such solutions , with a Killing symmetry and a parametric deformation vector being the rescaled parameter from the classical Schwarzschild metric . Such solutions have a huge roche class hump on the Lifshitz type hyper - sphere and they are compared with corresponding Einstein flows .",
        "rewrite_text": "In this research paper, we investigate the properties of nonholonomic Ricci currents associated with solitonic pp-waves and Schwarzschild solutions of the vacuum Einstein equations. These streams generalize the well-known Einstein flow and are specified by expressions that describe the nonholonomic deformation of a certain Plano-Projective metric using a (non-holonomic) Lagrange integral.\n\nWe establish that the nonholonomic Ricci stream, whose potential is the square of the unique M-curvature, is analogous to the nonholonomic stream envelope (NFE) for a class of vacuum Einstein equations with anholonomic parameters. This means that solutions of the considered equations for the M-curvature form the same solutions of the nonholonomic Ricci stream. We prove that the vacuum Einstein equations with anholonomic parameters define a Riemann geometry that does not undergo Ricci flows, but only nonholonomic Ricci flows.\n\nWe derive formulas for the variation of certain geometric objects, establishing the induced nonholonomic distribution and nonlinear connection (ND) in associated deformations. These formulas can be used to redefine solution parameters to generate new families of solutions through such nonholonomic deformations. It is observed that such formal transformations are not uniquely defined, and there exist some solutions that cannot be transformed into each other by such nonholonomic formal parameter transformations.\n\nWe have found Abelian and non-Abelian conservation laws for the nonholonomic Ricci currents. These conservation laws are highly beneficial for integrating nonlinear partial differential equations corresponding to the considered flow. The solitonic topology with nonholonomic restrictions enables us to define precise types of solutions for the vacuum Einstein equations with inhomogeneous (parametric) deformations that preserve certain symmetries. These symmetries can be utilized to construct exact solutions with specific symmetries using corresponding generating functions.\n\nIn the appendix, we demonstrate that our nonholonomic deformations of the vacuum Schwarzschild solution describe corresponding parametric families of solutions, with a Killing symmetry and a parametric deformation vector being the rescaled parameter from the classical Schwarzschild metric. These solutions exhibit a prominent roche class hump on a Lifshitz-type hypersphere and are compared to corresponding Einstein flows.",
        "ori-fast-z-score": -0.9467292624062575,
        "water-fast-z-score": 4.561513718866514,
        "rewrite-fast-z-score": 2.4009801919951235
    },
    {
        "original_text": "Spherically symmetric plasmas are an important and interesting general relativistic plasma system, which arise in, for example, collapsing stars, laboratory nuclear fusion experiments, and the early universe. The simplest spherically symmetric Einstein-Maxwell system contains two independent functions of radial coordinate only. However, these can be reduced further to a single quadrature, allowing exact solutions to be obtained in several special cases. In general, the presence of the Maxwell field contributes one additional conserved charge, the masshair  of the solution. For some choices of the equations of state of the plasma, explicit solutions can be obtained in different regions of the spacetime. The resulting space-times describe either black holes with a gravitational mass, a scalar field hair, or naked singularities, depending on the choice of equations of state. In this paper, we present a systematic method to generate analytic solutions to the Einstein-Maxwell system with a spherically symmetric plasma. This is achieved by relating the system to a theory of Newtonian gravity coupled to a non-linear scalar field. We then use a series expansion about the centres of symmetry of the solution, and a number of analytic solutions to the non-linear theory in Newtonian gravity to construct the full solution. We apply our general procedure to several specific cases, obtaining new exact solutions to the Einstein-Maxwell system, and discussing their properties. In particular, we show how the addition of a Maxwell field changes the possible outcomes of gravitational collapse, from black hole formation with no scalar hair to the formation of naked singularities with scalar hair. Our approach has some similarity to the AdS-CFT correspondence, which relates the gravitational description of some strongly coupled plasmas to a lower-dimensional gauge theory. In this respect, the gravitational collapse of charged matter described above may be interpreted as a phase transition in strongly coupled gauge theory, with the black hole forming when the gauge theory approaches a large-$N$ limit in which gravity is strongly interacting, and the naked singularity forming when the gauge theory undergoes a large-$N$ transition to a low-energy weakly coupled plasma. This paper provides a rare example of exact solutions to the Einstein-Maxwell system. More generally, it shows how the addition of a Maxwell field changes the possible outcomes of gravitational collapse, from black hole formation with no scalar hair to the formation of naked singularities with scalar hair. Date posted: March 25, 2020 Date accepted: February 20, 2023 Authors: Matthew Chana Duncan Haldeman Valentina La mouth Reza Fathalian Timothy Nguyen Alexander Burkov Andrew Mourey Molly Pustilnik Alexander Golubovich Maria Tolkacheva Elizabeth Mejia Thorsten Ohl Alexander Penzenko Carl Radke Stavros Anthrakis Rahul Kumar",
        "watermark_text": "Spherically symmetric plasmas are an important and exciting general relativistic plasma system , which occur in , for example , collapsing stars , experimental atomic fusion tests , and the early universe . The simplest spherically symmetric Einstein - Maxwell system contains two independent functions of radial coordinate only .However , these can be reduced further to a single quadrature , allowing exact solutions to be obtained in multiple special cases . In general , the presence of the Maxwell field contributes one additional conserved charge , the masshair of the solution .For some selections of the equations of state of the plasma , explicit solutions can be obtained in different regions of the spacetime . The resulting space - times describe either black holes with a gravitational mass , a scalar field hair , or bare singularities , depending on the selection of equations of state .In this paper , we present a comprehensive technique to create analytic solutions to the Einstein - Maxwell system with a spherically symmetric plasma . This is achieved by relating the system to a theory of Newtonian gravity linked to a non - linear scalar field .We then use a series expansion about the centres of symmetry of the solution , and a number of analytic solutions to the non - linear theory in Newtonian gravity to build the full solution . We use our general technique to several specific cases , obtaining new accurate answers to the Einstein - Maxwell system , and examining their characteristics .In particular , we study how the adding of a Maxwell field shifts the possible outcomes of gravitational failure , from black hole formation with no scalar hair to the formation of naked singularities with scalar fur . Our solution has some similarity to the AdS - CFT relationship , which links the gravitational description of some strongly coupled plasmas to a smaller - dimensional gauge theory .In this respect , the gravitational collapse of charged material stated above might be interpreted as a phase shift in heavily coupled gauge theory , with the dark hole developing when the gauge theory encounters a large - $ N $ limit in which gravity is strongly interacting , and the naked singularity forming when the gauge theory undergoes a large - $ N $ transition to a low - energy weakly coupled plasma . This paper provides a rare example of precise solutions to the Einstein - Maxwell system .More generally , it displays how the adding of a Maxwell field shifts the possible outcomes of gravitational failure , from black hole formation with no scalar fur to the formation of naked singularities with scalar fur . Date uploaded : March 25 , 2020 Date accepted : February 20 , 2023 Authors : Matthew Chana Duncan Haldeman Valentina La mouth Reza Fathalian Timothy Nguyen Alexander Burkov Andrew Mourey Molly Pustilnik Alexander Golubovich Maria Tolkacheva Elizabeth Mejia Thorsten Ohl Alexander Penzenko Carl Radke Stavros Anthrakis Rahul Kumar",
        "rewrite_text": "Spherically symmetrical plasmas are a crucial and fascinating component of the general relativistic plasma system. They can be found in various scenarios, such as collapsing stars, experimental atomic fusion tests, and the early stages of the universe. The most basic spherically symmetric Einstein-Maxwell system involves only two independent functions of the radial coordinate. However, these can be further simplified to a single quadrature, enabling exact solutions to be derived in multiple special cases.\n\nIn general, the presence of the Maxwell field adds an additional conserved charge, known as the masshair of the solution. For specific choices of the equations of state for the plasma, explicit solutions can be found in different regions of spacetime. These solutions describe either black holes with a gravitational mass and a scalar field hair, or bare singularities, depending on the selected equations of state.\n\nIn this paper, we present a comprehensive technique to generate analytic solutions for the Einstein-Maxwell system with a spherically symmetric plasma. This is achieved by linking the system to a theory of Newtonian gravity coupled with a nonlinear scalar field. We then employ a series expansion centered on the symmetry centers of the solution and utilize a range of analytic solutions from the nonlinear theory of Newtonian gravity to construct the complete solution.\n\nOur general technique is applied to several specific cases, yielding new accurate answers for the Einstein-Maxwell system and examining their characteristics. Specifically, we investigate how the inclusion of a Maxwell field alters the possible outcomes of gravitational collapse, shifting from the formation of black holes without scalar hair to the formation of naked singularities with scalar \"fur.\"\n\nOur solution bears some resemblance to the AdS-CFT relationship, which links the gravitational description of strongly coupled plasmas to a lower-dimensional gauge theory. In this context, the gravitational collapse of charged matter mentioned earlier could be interpreted as a phase shift in a heavily coupled gauge theory. A dark hole emerges when the gauge theory encounters a large-N limit where gravity becomes strongly interactive, while a naked singularity forms when the gauge theory undergoes a large-N transition into a low-energy weakly coupled plasma.\n\nThis paper offers a rare example of precise solutions to the Einstein-Maxwell system. More broadly, it demonstrates how the inclusion of a Maxwell field alters the potential outcomes of gravitational collapse, shifting from black hole formation without scalar features to the creation of naked singularities with scalar manifestations.\n\nUploaded date: March 25, 2020\nAccepted date: February 20, 2023\n\nAuthors: Matthew Chana, Duncan Haldeman, Valentina La Mouth, Reza Fathalian, Timothy Nguyen, Alexander Burkov, Andrew Mourey, Molly Pustilnik, Alexander Golubovich, Maria Tolkacheva, Elizabeth Mejia, Thorsten Ohl, Alexander Penzenko, Carl Radke, Stavros Anthrakis, Rahul Kumar",
        "ori-fast-z-score": -0.4242640687119285,
        "water-fast-z-score": 6.0509388846805106,
        "rewrite-fast-z-score": 2.4445060351935237
    },
    {
        "original_text": "Longintitual dynamics describes the motion of particles in a given system. In the context of celestial mechanics, the motion of planets in the solar system is described by the Newton s laws of motion. The same is true for the spacecrafts in space, which is influenced by the Sun and other planets, but also by much smaller irregularities in the spacecraft itself and in the Space environment. These deviations from a straight line motion are called perturbers. In this work, we consider a very small, orbiting irregularity in the solar system, namely the asteroid 2005 US10. We perform a numerical integration of the equations of motion of the perturber, considering a wide range of initial conditions. We verify that the evolution of the spacecraft follows a unique pattern, in which the osculating orbit of the asteroid slowly precesses around the Keplerian orbit of the spacecraft. We calculate the time scale of this evolution, and we show that it depends mainly on the parameter called dynamical friction, which is closely related to the viscosity of the gas surrounding the perturber. In the end, we discuss the applicability of our results to the Asteroid capture mission, which is a proposed future manned mission to a minor body in the Solar system.",
        "watermark_text": "Longintitual dynamics describes the movement of particles in a given system . In the context of astronomical physics , the movement of planets in the lunar system is modeled by the Newton s rules of movement .The same is true for the spacecrafts in space , which is influenced by the Sun and other planets , but also by much smaller irregularities in the spacecraft itself and in the Space atmosphere . These deviations from a straight line motion are called perturbers .In this research , we study a very small , orbiting irregularity in the solar system , notably the asteroid 2005 US10 . We undergo a numerical integration of the coefficients of movement of the perturber , using a broad variety of initial conditions .We establish that the evolution of the spacecraft takes a unique rhythm , in which the osculating orbit of the asteroid slowly precesses around the Keplerian orbit of the spacecraft . We calculate the period scale of this evolution , and we prove that it rely mainly on the parameter dubbed dynamical friction , which is closely related to the viscosity of the gas covering the perturber .In the end , we talk the applicability of our findings to the Asteroid capture flight , which is a planned future manned flight to a minor body in the Solar system .",
        "rewrite_text": "Longitudinal dynamics is a descriptive process of particle movement within a system. In the realm of astronomical physics, the motion of planets in the lunar system is represented by Newton's principles of movement. The same holds true for spacecraft in space, which are influenced by not only the Sun and other planets but also by the intricate irregularities inherent in the spacecraft itself and the space atmosphere. These deviations from a direct line of motion are known as disturbances.\n\nIn this research, we focus on a specific, small orbiting irregularity within the solar system - the asteroid 2005 US10. We engage in a numerical integration of the coefficients related to this disturbance, using an array of various initial conditions. We observe that the spacecraft's evolution follows a unique rhythm in which the oscillating orbit of the asteroid slowly precesses around the Keplerian orbit of the spacecraft. We calculate the scale of this evolutionary period and demonstrate that it largely depends on a parameter known as dynamical friction, which is closely linked to the viscosity of the gas surrounding the disturbance.\n\nFinally, we discuss how our findings can be applied to the Asteroid Capture Flight, a planned future manned mission to a minor body in the solar system.",
        "ori-fast-z-score": -0.329292779969071,
        "water-fast-z-score": 6.47609133939173,
        "rewrite-fast-z-score": 1.8225913092242512
    },
    {
        "original_text": "A microscopic theory of macroscopic quantum tunneling (MQT) in high-T_c c-axis Josephson junctions is developed. Based on a two-well tunnel Hamiltonian and an instanton method, the theory can well explain a series of recent experiments, including the MQT peak structures in I-V curves, the temperature dependence of the MQT currents, and the parametric resonance driving MQT without invoking any (spontaneous) symmetry breaking. The theory unifies the descriptions of both reversible and irreversible MQT processes. Moreover, a small bias current may induce a large resonant MQT current, in sharp contrast to the almost negligible resonant tunneling current in previous theories. It is also found that the theory can well predict some new interesting experiments, such as the opposite temperature dependence of the MQT currents in two parallel junctions. The theory is also applied to explain the recently reported enhanced MQT in asymmetric junctions. Furthermore, since the theory can well describe both MQT and conventional (non-MQT) DC Josephson effects, it can explain a large portion of the highly non-linear I-V curves observed in some recent experiments.",
        "watermark_text": "A microscopic theory of macroscopic quantum tunneling ( MQT ) in high - T _ c c - axis Josephson junctions is developed . Based on a two - well tunnel Hamiltonian and an instanton theory , the model can well describe a string of recent experiments , notably the MQT peak structures in I - V curves , the temperature dependence of the MQT currents , and the parametric resonance driving MQT without invoking any ( spontaneous ) symmetry breaking .The theory unifies the explanations of both reversible and irreversible MQT processes . Moreover , a small bias charge may generate a large resonant MQT current , in sharp contrast to the virtually negligible resonant tunneling current in earlier theories .It is also discovered that the model can good predict some new unusual experiments , such as the opposite temperature dependence of the MQT flows in two connected junctions . The theory is also used to explain the recently found enhanced MQT in asymmetric junctions .Furthermore , since the model can good describe both MQT and conventional ( non - MQT ) DC Josephson effects , it can describe a large part of the strongly non - linear I - V curves observed in some latest studies .",
        "rewrite_text": "A developed microscopic theory exists for the macroscopic quantum tunneling (MQT) in high-Tc c-axis Josephson junctions. Leveraging a two-well tunnel Hamiltonian and the instanton theory, this model adeptly explains a sequence of recent experiments. Notably, it captures MQT peak structures in I-V curves, the temperature dependency of MQT currents, and parametric resonance driving MQT without the need for spontaneous symmetry breaking. This theory integrates explanations for both reversible and irreversible MQT processes. Interestingly, a small bias charge can generate a significant resonant MQT current, contrasting sharply with the nearly nonexistent resonant tunneling current in earlier theories.\n\nAdditionally, it has been found that this model can predictably interpret novel and unusual experiments, such as the contrasting temperature dependencies of MQT flows in two connected junctions. The theory is also utilized to explain the recently observed enhanced MQT in asymmetric junctions. Moreover, as the model can describe both MQT and conventional (non-MQT) DC Josephson effects, it has the capability to explain a substantial portion of the strongly nonlinear I-V curves observed in recent studies.",
        "ori-fast-z-score": -0.22360679774997896,
        "water-fast-z-score": 5.287913134352312,
        "rewrite-fast-z-score": 1.2874526191574363
    },
    {
        "original_text": "In this paper we consider a non-minimally coupled phantom cosmology with a conformally coupled scalar field. We perform a detailed cosmological analysis of the model and show that this model is consistent with the current cosmological observations. We focus on two specific forms of the coupling function that provide a smooth transition from the matter era to the futurephantom epoch. Finally, we investigate the future predictions of the model and show that the obtained theoretical results are in good agreement with the current observational data. We find that in both cases, the phantom energy density decays to zero at late times, and a smooth transition to the standard cosmological evolution occurs, with the scale factor and matter content growing without bound and the universe entering a sequence of perfect universes. Keywords: cosmological parameters, Dark Energy, Phantom energy --- Route to Lambda in conformally coupled phantom cosmology Stefano Ansoldi, Mariana Carrillo Gonzalez, Cristiano German Damaraga, and Jose Blazquez-Salcedo June 2023 Common keywords: cosmology, phantom energy, conformal coupling DOI: 10.22007/window.311250 PDF of the paper: route-to-lambda.pdf Any updates will be posted on the same page.",
        "watermark_text": "In this paper we study a non - minimally coupled phantom cosmology with a conformally correlated scalar field . We undergo a detailed cosmological study of the model and find that this description is compatible with the present cosmological observations .We focus on two specific kinds of the interaction function that provide a steady transition from the matter period to the futurephantom epoch . Finally , we investigate the future predictions of the model and suggest that the achieved theoretical results are in good agreement with the present observational data .We see that in both cases , the phantom energy density decays to zero at late times , and a slow progress to the standard cosmological evolution occurs , with the scale factor and material content expanding without bound and the universe joining a sequence of perfect universes . Keywords : cosmological factors , Dark Energy , Phantom electricity - - - Route to Lambda in conformally correlated phantom cosmology Stefano Ansoldi , Mariana Carrillo Gonzalez , Cristiano German Damaraga , and Jose Blazquez - Salcedo June 2023 Common keywords : cosmology , phantom energy , conformal interactions DOI : 10 . 22007 / window . 311250 PDF of the paper : path - to - lambda . pdf Any updates will be posted on the same page .",
        "rewrite_text": "In this study, we explore a non-minimally coupled phantom cosmology that features a conformally correlated scalar field. We conduct a comprehensive analysis of the model's cosmological implications and discover that it aligns with current cosmological observations. Our focus is on two distinct types of interaction functions that smoothly transition the model from the matter era to the future phantom epoch.\n\nFurthermore, we investigate the future predictions of the model and suggest that our theoretical findings are well-aligned with current observational data. It is observed that in both scenarios, the phantom energy density diminishes to zero over time, leading to a gradual transition towards standard cosmological evolution. This process involves an unbounded expansion of the scale factor and material content, ultimately merging the universe into a sequence of ideal universes.\n\nKey terms: cosmological factors, Dark Energy, Phantom electricity. Path to Lambda in conformally correlated phantom cosmology.\n\nAuthored by Stefano Ansoldi, Mariana Carrillo Gonzalez, Cristiano German Damaraga, and Jose Blazquez-Salcedo in June 2023. Common keywords: cosmology, phantom energy, conformal interactions. DOI: 10.22007/window.311250. Paper PDF: Path to Lambda.pdf. Any updates will be posted on the same webpage.",
        "ori-fast-z-score": 0.11867816581938533,
        "water-fast-z-score": 6.215188768538847,
        "rewrite-fast-z-score": 2.3570226039551585
    },
    {
        "original_text": "The paper shows how the IT Service Centre at Harz University, a 100-people team operating in the scope of Group Information and Communication, utilised the ITIL service management framework. In order to illustrate its practical relevance, the framework is applied to the Release Management Process, showing improvements that were enabled by using the ITIL methodology. The paper concludes with some lessons learned and a discussion of how these can be applied to other processes. The ITIL framework was originally developed by the British government to support the implementation of best practices in the management of Information Technology services in the public sector. In recent years, the framework has also gained popularity in the private sector. Following an evaluation of the framework’s applicability to IT service management in a university setting, this paper shows how ITIL can be used to benefit such a setting. To this end, the Release Management Process of the IT Service Centre at Harz University is utilised as a concrete example. The process is analysed to determine areas in which it could be improved by using ITIL, and these improvements are illustrated using the example of the used of ITIL in the Release Management Process.",
        "watermark_text": "The paper shows how the IT Service Centre at Harz University , a 100 - person effort operating in the scope of Group Information and Communication , utilised the ITIL service management framework . In order to illustrate its practical relevance , the framework is applied to the Release Management Process , showing improvements that were enabled by using the ITIL approach .The paper closes with some lessons taught and a debate of how these can be applied to other processes . The ITIL framework was originally developed by the British administration to support the implementation of best procedures in the governance of Information Technology services in the public sector .In recent years , the framework has especially become success in the private sector . Following an assessment of the framework ’ s applicability to IT service management in a university setting , this paper shows how ITIL can be used to benefit such a setting .To this end , the Release Management Process of the IT Service Centre at Harz University is utilised as a concrete example . The method is analysed to identify areas in which it could be improved by using ITIL , and these improvements are shown using the example of the using of ITIL in the Release Management Process .",
        "rewrite_text": "The article describes how the IT Service Centre at Harz University, consisting of a team of 100 professionals within the scope of Group Information and Communication, has employed the ITIL service management framework. To demonstrate its practical significance, the framework is implemented into the Release Management Process, illustrating the enhancement achieved through adopting the ITIL methodology. The article concludes with valuable learnings and a discussion on how these can be applied to other processes.\n\nOriginally developed by the British administration to support the implementation of optimal procedures in governing information technology services in the public sector, the ITIL framework has been particularly successful in the private sector in recent years. After assessing the framework's applicability to IT service management in an academic setting, this paper demonstrates how ITIL can be beneficial in such an environment. To this end, the Release Management Process of the IT Service Centre at Harz University is used as a concrete example, analyzing the method to identify areas where it could be improved with the help of ITIL. These improvements are exhibited through the specific application of ITIL in the Release Management Process.",
        "ori-fast-z-score": 2.789943329851663,
        "water-fast-z-score": 6.50986776965388,
        "rewrite-fast-z-score": 1.9123657749350298
    },
    {
        "original_text": "Exoplanet candidates are typically found by detecting a decrease in the motion of a host star induced by the presence of an orbiting planet. For massive planet candidates, the kinematic mass can be computed by determining the star s center of mass along with its trigonometric parallax from Hubble Space Telescope astrometry. If necessary, high-precision radial velocities can also be used to confirm the presence of a planet. The orbital parameters of the candidate planet can be determined from these astrometric and orbital parameters. Here I report the discovery of a Super-Earth planet orbiting the host star HD 33636, as well as the measurement of its trigonometric parallax. When combined with the radial velocities measured for the star, the resulting dynamical mass of the planet is consistent with the reported trigonometric mass, confirming the planetary nature of this candidate and validating the method for determining the masses of exoplanets from combined astrometric and radial velocity measurements.",
        "watermark_text": "Exoplanet candidates are typically found by detecting a reduction in the movement of a host star induced by the presence of an orbiting planet . For huge planet candidates , the kinematic mass can be computed by choosing the star s center of mass along with its trigonometric parallax from Hubble Space Telescope astrometry .If needed , large - precision radial velocities can also be used to confirm the presence of a planet . The orbital variables of the candidate planet can be determined from these astrometric and orbital variables .Here I announce the discovery of a Super - Earth planet orbiting the host star HD 33636 , as well as the observation of its trigonometric parallax . When coupled with the radial velocities calculated for the star , the resulting dynamical mass of the planet is compatible with the reported trigonometric mass , establishing the planetary nature of this candidate and validating the method for determining the masses of exoplanets from combined astrometric and radial speed measurements .",
        "rewrite_text": "Exoplanet candidates are commonly identified by detecting a diminished motion of the host star caused by the presence of a planet in orbit around it. For larger planet candidates, the kinematic mass can be calculated by selecting the star's center of mass and its trigonometric parallax data obtained from Hubble Space Telescope astrometry. If necessary, high-precision radial velocity measurements can also be utilized to confirm the existence of a planet. The orbital characteristics of the candidate planet can be determined from these astrometric and orbital variables.\n\nI hereby announce the discovery of a Super-Earth planet orbiting the host star HD 33636, along with the observation of its trigonometric parallax. In combination with the radial velocity measurements for the star, the resulting dynamical mass of the planet is consistent with the reported trigonometric mass. This establishes the planetary nature of this candidate and validates the technique for determining exoplanet masses through combined astrometric and radial velocity measurements.",
        "ori-fast-z-score": -1.6378460497066512,
        "water-fast-z-score": 2.25,
        "rewrite-fast-z-score": -0.3611575592573076
    },
    {
        "original_text": "The first stars were very massive, with virial temperatures of tens of thousands of K. Their intense ultraviolet radiation ionised most of the gas in minihalos, suppressing the formation of subsequent galaxies. We report the detection of lower metal enrichments in the gas in the virialized minihalos than in the intergalactic medium (IGM), which likely originated from primordial star formation in minihalos. The detection of the metals in the minihalos supports the predictions of hierarchical structure formation, in which small dark matter halos first formed near the regions where the intergalactic gas was later incorporated. This supports a scenario in which the stars in these earliest galaxies had masses of 1010–1022 M⊕. Such stellar masses imply the primordial stars could have produced the necessary metals only with contributions from non-thermal processes, likely nucleosynthesis in very massive stars.",
        "watermark_text": "The first stars were very huge , with virial altitudes of tens of tens of K . Their intense ultraviolet radiation ionised most of the gas in minihalos , suppressing the formation of later galaxies . We report the observation of lower iron enrichments in the gas in the virialized minihalos than in the intergalactic medium ( IGM ) , which probably originated from primordial star formation in minihalos .The observation of the metals in the minihalos suggests the theories of hierarchical structure development , in which small dark matter halos first formed near the regions where the intergalactic gas was afterwards introduced . This supports a situation in which the stars in these first galaxies had masses of 1010 – 1022 M⊕ .Such stellar masses indicate the primordial stars could have created the necessary metals only with contributions from non - thermal factors , likely nucleosynthesis in very huge stars .",
        "rewrite_text": "The initial stars were of immense proportions, with exorbitant altitudes measured in the tens of thousands of K. Their intense ultraviolet radiation effectively ionized the majority of the gas within minihalos, thereby inhibiting the formation of subsequent galaxies. Our findings indicate that the gas in the virialized minihalos exhibits lower iron enrichments compared to the intergalactic medium (IGM), likely stemming from the primitive star formation processes within minihalos.\n\nObservations of metals in the minihalos support theories of hierarchical structural development, where smaller dark matter halos initially formed close to regions where intergalactic gas was subsequently introduced. This provides evidence that the stars in these initial galaxies had masses ranging from 1010 to 1022 M⊕. Such stellar masses suggest that the primitive stars could have generated the necessary metals solely through non-thermal processes, possibly through nucleosynthesis in massive stars.",
        "ori-fast-z-score": 0.13018891098082389,
        "water-fast-z-score": 5.422176684690384,
        "rewrite-fast-z-score": 1.697749375254331
    },
    {
        "original_text": "Intercalation of potassium in graphite represents a promising approach to develop an efficient next-generation energy storage device. However, it remains challenging to achieve both high storage capacities and practical electrode potentials. Here we perform a comprehensive first-principles study of the structural, electronic, and dynamical properties of potassium intercalation in graphite. Our main findings are as follows: (i) The graphite intercalation compound (GIC) forms via one-pot synthesis and chemical vapor deposition, in which potassium atoms adsorb onto defective graphene layers between two graphite sheets. (ii) Intercalation occurs without significant changes to the graphene structure or lattice parameters, but induces a sharp increase in the C—C bond length by as much as 0.39 Å. (iii) Potassium incorporation leads to a large decrease in the conduction band minimum of graphite, but only causes a small and highly dispersive increase in the valence band maximum. (iv) The dynamical stability of potassium intercalated graphite is significantly improved by eliminating interactions between potassium and the defective graphene layers. Our findings provide new design strategies for potassium-graphite intercalation batteries, and promote the development of novel two-dimensional materials for energy storage.",
        "watermark_text": "Intercalation of potassium in graphite represents a viable way to develop an efficient next - generation power storage device . However , it remains challenging to achieve both high storage volumes and useful electrode potentials .Here we perform a comprehensive first - principles study of the structural , electronic , and dynamical properties of potassium intercalation in graphite . Our main results are as follows : ( i ) The graphite intercalation compound ( GIC ) forms via one - pot synthesis and chemical vapor precipitation , in which potassium elements adsorb onto defective graphene strands between two graphite sheets .( ii ) Intercalation occurs without significant modifications to the graphene structure or lattice parameters , but induces a sharp increase in the C — C bond length by as much as 0 . 39 Å . ( iii ) Potassium incorporation leads to a large decrease in the conduction band minimum of graphite , but only causes a small and extremely dispersive rise in the valence band maximum .( iv ) The dynamical stability of potassium intercalated graphite is significantly improved by removing interactions between potassium and the defective graphene phases . Our findings provide modern design methods for potassium - graphite intercalation devices , and encourage the development of new two - dimensional forms for power transfer .",
        "rewrite_text": "The intercalation of potassium into graphite serves as a feasible approach for the development of an efficient next-generation power storage device. Nevertheless, achieving both high storage volumes and useful electrode potentials remains a significant challenge. In this study, we conduct a comprehensive first-principles investigation into the structural, electronic, and dynamic properties of potassium intercalation in graphite. Our key findings are summarized as follows:\n\n(i) The formation of a graphite intercalation compound (GIC) occurs through one-pot synthesis and chemical vapor precipitation, wherein potassium elements adsorb onto defective graphene strands situated between two graphite sheets.\n\n(ii) Intercalation takes place without significantly altering the structure of graphene or its lattice parameters. However, it induces a notable increase in the C—C bond length by up to 0.39 Å.\n\n(iii) The incorporation of potassium results in a significant decrease in the conduction band minimum of graphite, while only causing a slight and highly dispersive rise in the valence band maximum.\n\n(iv) The dynamic stability of potassium-intercalated graphite is significantly enhanced by eliminating interactions between potassium and defective graphene phases.\n\nOur research offers modern design strategies for potassium-graphite intercalation devices, paving the way for the development of innovative two-dimensional forms for power transfer.",
        "ori-fast-z-score": 1.801996396010812,
        "water-fast-z-score": 5.902918298980975,
        "rewrite-fast-z-score": 1.9291577137538762
    },
    {
        "original_text": "The Falicov-Kimball model (FKM) is an important model in the study of Mott transition and thus superconductivity. The model consists of electrons and spin-less fermions called slave bosons that interact through an onsite repulsion. In this work, we construct a slave boson theory of the extended FKM, which includes additional hopping and onsite repulsion terms. The theory is shown to be a well-defined effective low energy theory of the half-filled repulsive Hubbard model on a triangular lattice. We apply the slave boson theory to study the model away from half filling as well as at half filling. In the half filled case, we find two phases: an insulating phase with charge density wave order at small parameters, and a trivial superconductor phase at large interactions. At finite but small dopings, we observe a Beresinskii-Kosterlitz-Thouless transition and a crossover between the insulating and superconductor phases. At large dopings, we find a first order Mott transition separating the uniform metal and a density wave state. We conclude that slave boson theory provides a consistent low energy description of the extended FKM for all values of the parameters.",
        "watermark_text": "The Falicov - Kimball model ( FKM ) is an important model in the study of Mott transition and therefore superconductivity . The model consists of atoms and spin - less fermions called slave bosons that interact through an onsite repulsion .In this study , we create a slave boson theory of the extended FKM , which includes added hopping and onsite repulsion conditions . The theory is demonstrated to be a well - defined effective low power theory of the half - filled repulsive Hubbard theory on a triangular lattice .We use the slave boson theory to study the model away from quarter filling as well as at half filling . In the half packed case , we find two phases : an insulating phase with charge density wave order at small parameters , and a trivial superconductor phase at large interactions .At finite but tiny dopings , we study a Beresinskii - Kosterlitz - Thouless transition and a crossover between the insulating and superconductor phases . At large dopings , we find a first order Mott transition separating the uniform material and a density wave state .We suggest that slave boson theory provides a consistent small power description of the extended FKM for all values of the variables .",
        "rewrite_text": "The Falicov-Kimball Model (FKM) holds significant importance in the investigation of Mott transitions and subsequent superconductivity. This model comprises atoms and spinless fermions, known as slave bosons, which interact via an on-site repulsion mechanism. In our research, we have formulated a slave boson theory for the extended FKM, incorporating additional hopping and on-site repulsion conditions. This theory has been proven to be an effectively low-power description of the half-filled repulsive Hubbard theory on a triangular lattice.\n\nWe employ the slave boson theory to explore the model both away from quarter-filling and at half-filling conditions. In the half-packed scenario, we identify two distinct phases: an insulating phase with a charge density wave order at smaller parameter values, and a trivial superconductor phase at higher interaction levels. For finite but minimal doping levels, we examine the Berensinskii-Kosterlitz-Thouless transition and the crossover between the insulating and superconductor phases. At higher doping levels, we discover a first-order Mott transition that separates the homogeneous material from a density wave state. Our findings suggest that the slave boson theory provides a consistent, low-power description of the extended FKM across all variable values.",
        "ori-fast-z-score": 1.0540925533894598,
        "water-fast-z-score": 5.4812812776251905,
        "rewrite-fast-z-score": 0.618852747755276
    },
    {
        "original_text": "Recent studies have shown that the iron Kα spectral line is emitted from regions close to the supermassive black hole (SMBH) in active galactic nuclei (AGNs). Timing studies of this emission could, in principle, yield information on the geometry and velocity field in the immediate vicinity of the black hole. By combining X-ray timing and continuum timing observations of the Seyfert galaxy NGC 3783, we demonstrate the existence of a correlation between the X-ray flux and the high-frequency (milliseconds) X-ray variability time delays. This correlation is consistent with reflection-based models for the X-ray emission, in which the X-rays incident on the accretion disk are scattered into our line of sight by cold electrons in a corona that surrounds the disk. We further demonstrate that the iron Kα emission is consistent with being emitted within a few gravitational radii of the black hole, lending support to the hypothesis that the X-ray flux is modulated at the black hole s fast-spinning rates. Our results demonstrate that X-ray timing observations can, in some cases, provide unique insight into the physical processes occurring close to supermassive black holes.",
        "watermark_text": "Recent research have shown that the metal Kα spectral line is emitted from regions close to the supermassive black hole ( SMBH ) in active galactic nuclei ( AGNs ) . Timing experiments of this emission could , in concept , yield information on the topology and speed field in the immediate vicinity of the dark hole .By combining X - ray timing and continuum timing observations of the Seyfert galaxy NGC 3783 , we prove the existence of a correlation between the X - ray flux and the high - frequency ( milliseconds ) X - ray variability time delays . This coupling is compatible with reflection - based methods for the X - ray radiation , in which the X - rays incident on the accretion disk are scattered into our line of vision by cold electrons in a corona that covers the disk .We further show that the metal Kα emission is compatible with being produced within a few gravitational radii of the dark hole , providing support to the notion that the X - ray flux is modulated at the dark hole s fast - spinning speeds . Our results show that X - ray timing observations can , in some cases , provide unique insight into the physical processes resulting close to supermassive black holes .",
        "rewrite_text": "Recent investigations have demonstrated that the metal Kα spectral line emanates from regions proximal to the supermassive black holes (SMBH) present in active galactic nuclei (AGNs). Theoretically, timing experiments of this emission can yield valuable information about the topology and speed field in the immediate vicinity of the black hole. By amalgamating X-ray timing with continuum observations of the Seyfert galaxy NGC 3783, we have substantiated the existence of a correlation between X-ray flux and high-frequency (millisecond) X-ray variability time delays. This correlation aligns with reflection-based methods for X-ray radiation, wherein X-rays striking the accretion disk are scattered into our line of sight by cold electrons within a corona that envelops the disk. Furthermore, our findings indicate that the metal Kα emission is likely generated within a few gravitational radii of the black hole, bolstering the notion that the X-ray flux is modulated by the SMBH's rapid rotational speeds. Our research indicates that X-ray timing observations can occasionally offer unique insights into the physical processes occurring near supermassive black holes.",
        "ori-fast-z-score": 0.0,
        "water-fast-z-score": 6.1137368096588665,
        "rewrite-fast-z-score": 1.7888543819998317
    },
    {
        "original_text": "A sample of protostellar jets from data obtained with Spitzer is presented. The data show that many pre-main-sequence stars with ages of a few million years are active, showing rotation rates up to 0.4 days and coronal temperatures above 4,000 K. A few stars show much faster rotation, up to 3.2 days, suggesting the presence of fossil magnetic fields. The presence of angular momentum is often not correlated with accretion rates. One protostar, L1527 IRS, exhibits both low accretion and slow rotation, suggesting that it has nearly fallen into the starhood. Two similar objects, L1535 IRS and TMC1A, exhibit much faster rotation, consistent with their earlier status as potential stars. Another interesting object, the Classical T Tauri star RY Tau, shows variability in its inferred magnetic field orientation. This could imply that accretion and rotation are variable on short time scales. The data suggest that most protostars accrete gas at a relatively constant rate, but about one-third spin up significantly due to disk-averting winds or mergers with other prestellar cores.",
        "watermark_text": "A specimen of protostellar jets from information obtained with Spitzer is provided . The data reveal that several pre - principal - sequence stars with periods of a few million months are active , showing rotation periods up to 0 . 4 hours and coronal temperatures above 4 , 000 K . A few stars show slightly faster rotation , up to 3 . 2 days , showing the presence of fossil magnetic fields .The presence of angular velocity is often not associated with accretion levels . One protostar , L1527 IRS , displays both poor accretion and quiet rotation , showing that it has nearly fell into the starhood .Two similar objects , L1535 IRS and TMC1A , exhibit far faster rotation , consistent with their previous status as possible stars . Another unusual body , the Classical T Tauri star RY Tau , displays variability in its inferred magnetic force orientation .This might imply that accretion and rotation are variable on short period scales . The data suggest that most protostars accrete plasma at a fairly constant rate , but about one - fifth spin up considerably due to disk - averting winds or mergers with other prestellar cores .",
        "rewrite_text": "A sample of protostellar jets has been presented based on data obtained by Spitzer. The information reveals that several pre-main-sequence stars, with periods spanning a few million months, are active. These stars exhibit rotation periods ranging from 0.4 hours to 3.2 days, with coronal temperatures exceeding 4,000 K. Some stars demonstrate slightly faster rotation, indicating the presence of fossil magnetic fields. The presence of angular velocity is often not linked to the levels of accretion.\n\nOne specific protostar, L1527 IRS, demonstrates both poor accretion and calm rotation, suggesting that it is nearly transitioning into a full-fledged star. Two similar objects, L1535 IRS and TMC1A, exhibit much faster rotation, consistent with their previous status as potential stars. Furthermore, the Classical T Tauri star RY Tau exhibits variability in its inferred magnetic force orientation. This could imply that both accretion and rotation are subject to changes on short time scales. According to the data, most protostars accrete plasma at a relatively consistent rate, but approximately one-fifth experience significant spin-up due to disk-avoiding winds or mergers with other prestellar cores.",
        "ori-fast-z-score": -2.111111111111111,
        "water-fast-z-score": 5.813776741499453,
        "rewrite-fast-z-score": 1.4924050144892729
    },
    {
        "original_text": "In this paper, we propose a fully differentiable convolutional neural network (CNN) model that can approximate arbitrary two dimensional (2D) path curves. The model, referred to as 1-layer Excitable CNN (ECC), only consists of a single layer of 1D CNN, with the convolution filters shaped in a particular fashion so as to excitable. Unlike existing 2D path curve approximation methods that typically require multiple layers of convolutions and nonlinearities, ECC only requires a single 1D CNN layer. We show that the 1D filters in the 1st layer of ECC are actually hyperbolic tangent functions (HTF) and that by proper initializations, the 1D representations of different 2D path curves generated from ECC all collapsed into a single 1D band after a few singular value decompositions (SVDs). Through carefully studying the SVD factors, we are able to identify the number of factors to equal the number of path points in a path, and thus ECC can be viewed as a dynamic lower dimensional Hankel matrix completion method. We further propose a path evolution strategy to use this implicit representation to generate 2D path curves, achieving smooth path evolution and thus path curvature. We demonstrate that 1-layer ECC is capable of approximating complex path curves with high fidelity, even those with very thin or sharp turns. We show that ECC, with its single layer representation, is much more efficient and has much less parameters than existing 2D path curve approximation methods. We also conduct thorough experiments to analyze the influences of various factors on the path curve approximation performance of ECC. With the 1-layer ECC model and the path evolution strategy, we build an automatic path approximation pipeline which is easy to train and robust to various path types. We show that our pipeline is effective on multiple public benchmark path datasets and that it can even generate better results than existing methods on some difficult path datasets. We would like to take this opportunity to express our sincere gratitude to the authors of  1  for their inspiring work on designing 1D CNN filters that can excitable, to the authors of  2  for introducing the SVD technique into neural networks and providing a rigorous and comprehensive SVD analysis of ECC, and to the authors of  3  for their seminal work on Dynamic Time Warping (DTW) and the development of the Lyre tool that is indispensable to our path curve evolution and alignment algorithm research.",
        "watermark_text": "In this paper , we propose a completely differentiable convolutional neural network ( CNN ) model that can represent arbitrary two dimensional ( 2D ) path curves . The model , referred to as 1 - layer Excitable CNN ( ECC ) , only consists of a single layer of 1D CNN , with the convolution filters shaped in a certain fashion so as to excitable .Unlike existing 2D route curve approximation algorithms that typically require many layers of convolutions and nonlinearities , ECC only needs a single 1D CNN layer . We see that the 1D filters in the 1st layer of ECC are indeed hyperbolic tangent maps ( HTF ) and that by proper initializations , the 1D images of different 2D path loops generated from ECC all disappeared into a single 1D band after a few plural value decompositions ( SVDs ) .Through carefully examining the SVD factors , we are able to identify the number of factors to equal the number of path points in a path , and therefore ECC can be viewed as a dynamic lower dimensional Hankel matrix completion method . We further propose a path evolution strategy to use this implicit representation to create 2D trail curves , obtaining smooth path development and therefore edge curvature .We showed that 1 - layer ECC is ability of approximating complex path bends with high fidelity , particularly those with very thin or sharp twists . We see that ECC , with its single layer representation , is much more efficient and has much less variables than existing 2D trail curve approximation algorithms .We additionally undertake comprehensive experiments to analyze the effects of different factors on the path curve approximation performance of ECC . With the 1 - layer ECC model and the path adaptation strategy , we create an automatic route approximation pipeline which is easy to train and reliable to several path types .We see that our pipeline is efficient on multiple public benchmark path datasets and that it can even generate better results than existing techniques on some difficult path datasets . We would like to take this chance to express our genuine gratitude to the writers of 1 for their inspiring research on designing 1D CNN filters that can excitable , to the writers of 2 for introducing the SVD method into neural systems and providing a rigorous and complete SVD analysis of ECC , and to the writers of 3 for their seminal research on Dynamic Time Warping ( DTW ) and the development of the Lyre technique that is indispensable to our path graph evolution and routing algorithm studies .",
        "rewrite_text": "In this study, we present a fully differentiable convolutional neural network (CNN) model, named 1-layer Excitable CNN (ECC), which can effectively represent arbitrary two-dimensional (2D) path curves. This model consists of a single layer of 1D CNN, with convolution filters designed in a specific manner to achieve excitability. In contrast to existing 2D route curve approximation algorithms that often require multiple layers of convolutions and nonlinearities, ECC requires only a single 1D CNN layer.\n\nWe observe that the 1D filters in the first layer of ECC are actually hyperbolic tangent maps (HTF). Through proper initialization, the 1D images of different 2D path loops generated by ECC converge into a single 1D band after several plural value decompositions (SVDs). By carefully examining the SVD factors, we can determine the number of factors to be equal to the number of path points in a given path. Therefore, ECC can be viewed as a dynamic lower-dimensional Hankel matrix completion method.\n\nWe further propose a path evolution strategy to utilize this implicit representation to create 2D trail curves, achieving smooth path development and edge curvature. Our results demonstrate that 1-layer ECC has the ability to approximate complex path bends with high fidelity, especially those with very thin or sharp twists. Compared to existing 2D trail curve approximation algorithms, ECC is more efficient and has fewer variables due to its single-layer representation.\n\nAdditionally, we conduct comprehensive experiments to analyze the impact of various factors on the performance of ECC in path curve approximation. With the 1-layer ECC model and the path adaptation strategy, we establish an automatic route approximation pipeline that is easy to train and reliable for multiple path types. We find that our pipeline is efficient on multiple public benchmark path datasets and can even generate better results than existing techniques on challenging path datasets.\n\nWe would like to express our gratitude to the researchers who inspired our work. We are grateful to the authors of 1 for their pioneering research on designing 1D CNN filters that are exciting. We also thank the authors of 2 for introducing the SVD method into neural systems and providing a rigorous and comprehensive SVD analysis of ECC. Lastly, we appreciate the authors of 3 for their groundbreaking research on Dynamic Time Warping (DTW) and the development of the Lyre technique, which is indispensable for our studies on path graph evolution and routing algorithms.",
        "ori-fast-z-score": -1.927248223318863,
        "water-fast-z-score": 7.50727087709638,
        "rewrite-fast-z-score": 3.2090298129536805
    },
    {
        "original_text": "Topological cluster classification, which organizes clusters according to the topology they possess, has proved to be very useful for understanding phase diagrams and for capturing phase coexistence. In this paper, we present a topological classification of clusters in condensed phases, where the global symmetries of the system are taken into account. We illustrate our method by applying it to several important types of clusters, such as islands, stripes, and bubbles. Furthermore, we show that our classification has direct physical interpretations, connecting the different classes to different phases, including solid, smectic, nematic, and cluster phases. As an example, we apply our classification to the rich phase diagram of the easy-plane anisotropic Heisenberg model, where our analysis explains recent numerical work on the nature of cluster phases. Overall, our classification provides a natural way to organize clusters in condensed phases according to the phase in which they occur. This will be of great utility for understanding phase diagrams and for capturing phase coexistence.",
        "watermark_text": "Topological cluster classification , which organizes clusters according to the topology they possess , has become to be very useful for studying phase diagrams and for capturing phase coexistence . In this paper , we present a topological classification of clusters in condensed stages , where the global symmetries of the system are took into consideration .We illustrate our technique by using it to several important sorts of clusters , such as islands , bands , and bubbles . Furthermore , we prove that our classification has direct physical meanings , linking the different categories to different phases , particularly solid , smectic , nematic , and cluster stages .As an instance , we apply our classification to the rich phase diagram of the easy - plane anisotropic Heisenberg model , where our analysis shows previous quantitative research on the nature of cluster stages . Overall , our classification provides a natural way to organize complexes in condensed stages according to the period in which they occur .This will be of high utility for studying phase diagrams and for capturing phase coexistence .",
        "rewrite_text": "Topological cluster classification, which arranges clusters based on their inherent topology, has become highly beneficial for studying phase diagrams and detecting phase coexistence. In this paper, we introduce a topological classification of clusters in condensed phases, taking into account the global symmetries of the system. We demonstrate our technique by applying it to various significant cluster types, including islands, bands, and bubbles.\n\nMoreover, we establish that our classification holds direct physical significance, connecting different categories to distinct phases, specifically solid, smectic, nematic, and cluster phases. As an example, we utilize our classification in the rich phase diagram of the easy-plane anisotropic Heisenberg model, where our analysis corroborates previous quantitative research on cluster phases. Overall, our classification offers a natural method to organize complexes in condensed phases based on their occurrence period, making it highly useful for studying phase diagrams and detecting phase coexistence.",
        "ori-fast-z-score": 0.1111111111111111,
        "water-fast-z-score": 6.037383539249432,
        "rewrite-fast-z-score": 0.8944271909999159
    },
    {
        "original_text": "First-based survey of Compact Steep Spectrum sources, V. Milliarcsecond-scale morphology of CSS objects Authors: S. J. Fegan A. Verma S. R. Kulkarni M. F. Morabito M. Curé G. G. Paz A. Sánchez-Losa M. P. Eath M. Toldo M. J. Micha ł {}owski E. F. Keane M. Bremer A. Markowitz A. P. Marscher E. Ros et al. VCS 0413-647, also known as 3C 49, BL Lac, and TNuna, is a well-known compact steep spectrum radio source (CSS). Over the years, VCS 0413-647 has displayed a relatively constant intemperal brightness, yet various studies have shown it to be variable at all wavelengths. Here, we present the results of a large multi-wavelength observing campaign on VCS 0413-647 in 2017. Optical and near-infrared observations were obtained with the Keck II telescope and the Very Large Telescope (VLT), respectively. X-ray and gamma-ray observations were obtained with the Neil Gehrels Swift Observatory and the Large Array, respectively. Radio observations were obtained with the Karl G. Jansky Very Large Array, the Australia Telescope Compact Array, the phased VLA, and the Molonglo Observatory. We present the results of our milliarcsecond-scale radio imaging study of VCS 0413-647. Our observations showed a core-jet structure with an angular separation of 0.15 mas (or ~8.6 light-years at the redshift of the source, z = 0.0812) between two blobs aligned with the jet. This structure, along with changes in total flux density, brightness temperature, polarization, and morphology, confirm the variability seen at all wavelengths. We interpret the variability as a possible change in Doppler factor, θD, caused by apparent superluminal motion of the blobs at ~0.15c.",
        "watermark_text": "First - based survey of Compact Steep Spectrum sources , V . Milliarcsecond - scale morphology of CSS objects Authors : S . J . Fegan A . Verma S . R . Kulkarni M . F . Morabito M . Curé G . G . Paz A . Sánchez - Losa M . P . Eath M . Toldo M . J . Micha ł { } owski E . F . Keane M . Bremer A . Markowitz A . P . Marscher E . Ros et al . VCS 0413 - 647 , also known as 3C 49 , BL Lac , and TNuna , is a well - known compact steep spectrum radio source ( CSS ) .Over the years , VCS 0413 - 647 has exhibited a fairly constant intemperal brightness , however several studies have shown it to be varying at all wavelengths . Here , we present the conclusion of a large multi - wavelength monitoring effort on VCS 0413 - 647 in 2017 .Optical and near - infrared observations were obtained with the Keck II telescope and the Very Large Telescope ( VLT ) , respectively . X - ray and gamma - ray observations were obtained with the Neil Gehrels Swift Observatory and the Large Array , respectively .Radio observations were obtained with the Karl G . Jansky Very Large Array , the Australia Telescope Compact Array , the phased VLA , and the Molonglo Observatory . We present the results of our milliarcsecond - scale radio imaging study of VCS 0413 - 647 .Our observations showed a core - jet composition with an angular separation of 0 . 15 mas ( or ~ 8 . 6 light - years at the redshift of the source , z = 0 . 0812 ) between two blobs aligned with the jet . This structure , along with shifts in total flux concentration , brightness temperature , polarization , and morphology , confirm the variability seen at all wavelengths .We interpret the variability as a possible change in Doppler parameter , θD , caused by apparent superluminal motion of the blobs at ~ 0 . 15c .",
        "rewrite_text": "The first-ever survey of Compact Steep Spectrum sources, focusing on the milliarcsecond-scale morphology of CSS objects, was conducted by a team of authors including S. J. Fegan, A. Verma, S. R. Kulkarni, and many others. VCS 0413-647, also known as 3C 49, BL Lac, and TNuna, is a well-recognized compact steep spectrum radio source (CSS). Over the years, it has exhibited a relatively consistent temporal brightness, but multiple studies have revealed variations at all wavelengths. In this study, we present the findings of a comprehensive multi-wavelength monitoring effort conducted on VCS 0413-647 in 2017. Optical and near-infrared observations were secured with the Keck II telescope and the Very Large Telescope (VLT), respectively. X-ray and gamma-ray observations were gathered through the Neil Gehrels Swift Observatory and the Large Array. Radio observations were acquired from the Karl G. Jansky Very Large Array, the Australia Telescope Compact Array, the phased VLA, and the Molonglo Observatory. We present our results from a milliarcsecond-scale radio imaging study of VCS 0413-647. Our observations revealed a core-jet structure with an angular separation of 0.15 mas (or approximately 8.6 light-years at the source's redshift of z = 0.0812) between two blobs aligned with the jet. This structure, along with changes in total flux concentration, brightness temperature, polarization, and morphology, confirm the observed variability at all wavelengths. We interpret this variability as a potential change in the Doppler parameter, θD, likely caused by the apparent superluminal motion of the blobs at approximately 0.15c.",
        "ori-fast-z-score": 0.21566554640687682,
        "water-fast-z-score": 2.803652103289399,
        "rewrite-fast-z-score": 1.6502739940140694
    },
    {
        "original_text": "The work function is the energy required to remove an electron from a material surface. The work function is an important physical property of a material, since it influences the efficiency of electron emission and deposition, the electric field needed to remove electrons from a surface, and the stability of a surface. The work function is not itself an observable, but it may be computed from density functional theory (DFT) or related calculations. In the case of metals, the work function is generally thought to be related to the band structure of the material near the Fermi level. The determination of work functions is often part of the characterization of electronic materials. It is well known that the work function of a metal strongly depends on the crystalline orientation of the material. However, it was not clear how the work function is affected by the chemical composition of the material, or how it compares to the band structure of the material. Here, we report a new method to calculate the work function of crystalline materials from first principles. Our method, developed for the particular case of photoemission experiments, is based on the effective-mass approximation and relies on the calculation of the average electrostatic potential at the surface of the material. We demonstrate the applicability of this method to a broad range of materials, and find that the work function can be up to 1.5 times larger than the band structure-derived approximation. We discuss physical implications of our results and suggest possible improvements to the current methods for calculating the work function.",
        "watermark_text": "The job function is the power required to remove an electron from a material material . The work integral is an important mechanical property of a substance , since it influences the ability of electron absorption and deposition , the electric field needed to remove electrons from a surface , and the stability of a surface .The job function is not itself an observable , but it could be computed from density functional theory ( DFT ) or related calculations . In the case of metals , the work function is usually thought to be connected to the band structure of the metal near the Fermi level .The determination of job functions is often part of the characterization of electronic surfaces . It is well famous that the work function of a metal strongly depends on the crystalline attitude of the metal .However , it was not clear how the work function is affected by the chemical composition of the material , or how it relates to the band structure of the material . Here , we publish a new method to estimate the work integral of crystalline substances from first principles .Our solution , developed for the particular instance of photoemission experiments , is based on the effective - mass approximation and relies on the determination of the average electrostatic potential at the surface of the material . We explore the applicability of this method to a broad variety of substances , and find that the work function can be up to 1 . 5 times bigger than the band structure - derived approximation .We discuss physical effects of our findings and suggest possible changes to the present methods for determining the work function .",
        "rewrite_text": "The job function refers to the energy required to extract an electron from a given material. The work integral is a crucial mechanical attribute of a substance, as it impacts the capacity for electron absorption and deposition, the electric field necessary for removing electrons from a surface, and the surface's stability. While the job function itself is not observable, it can be calculated using methods such as density functional theory (DFT) or related calculations. In the context of metals, the work function is often linked to the band structure close to the Fermi level. Determining job functions is often an integral part of characterizing electronic surfaces.\n\nIt is widely recognized that the work function of a metal is strongly influenced by its crystalline structure. However, it was previously unclear how the chemical composition of a material affects the work function, or how it relates to the material's band structure. In this study, we introduce a novel approach to estimate the work integral of crystalline substances using first principles. Our solution, developed specifically for photoemission experiments, is based on the effective mass approximation and relies on determining the average electrostatic potential at the material's surface.\n\nWe explore the applicability of this method across a wide range of substances and find that the work function can be up to 1.5 times greater than the band structure-derived approximation. We discuss the physical implications of our findings and suggest modifications to current methods for determining the work function.",
        "ori-fast-z-score": -1.6994116628998401,
        "water-fast-z-score": 5.09823498869952,
        "rewrite-fast-z-score": 0.8049844718999243
    },
    {
        "original_text": "One of the key questions in studies of star formation in the local universe is the overall star formation rate (SFR), which can be measured by the luminosity of ionizing radiation from young, hot stars. Such studies are typically performed at high redshifts, using rest-frame far-ultraviolet (FUV) emission from young stars. Because the FUV spectral energy distribution (SED) varies with the type of stars and their age, the SFR can be measured by fitting the FUV emission with population synthesis models. This permits studies of star formation over the past 8-15 billion years, but the accuracy is limited by our poor knowledge of the SFH over much shorter timescales. Here we present an analysis of recent data from the Galaxy Evolution Explorer (GALEX) that provides an accurate measurement of the SFR over the past 3 billion years, based on the luminosity of Ly-alpha radiation from young, cool stars. Unlike FUV radiation, this  visible  light varies with the instantaneous rate of star formation, making it a  flashing light  tracer of current SFR. Using our analysis techniques, we find a current SFR of 5.9 (± 2.3) × 10−3 solar masses per year in the local universe, 90% of which occurred in the past 3 Gyr. By comparing this with an older (13.7 billion year) estimate based on the evolution of the solar system s gold content, we conclude that the current SFR in the local universe is comparable to that of the last several billion years. These results provide crucial new constraints for models of star and galaxy formation in the universe.",
        "watermark_text": "One of the key questions in studies of star formation in the local universe is the overall star formation rate ( SFR ) , which can be determined by the luminosity of ionizing radiation from young , hotter stars . Such experiments are typically performed at high redshifts , using rest - frame far - ultraviolet ( FUV ) emission from young galaxies .Because the FUV spectral power distribution ( SED ) changes with the kind of stars and their age , the SFR can be determined by fitting the FUV emission with population synthesis estimates . This enables research of star formation over the previous 8 - 15 billion decades , but the accuracy is limited by our poor knowledge of the SFH over much longer timescales .Here we present an assessment of recent results from the Galaxy Evolution Explorer ( GALEX ) that offers an accurate calculation of the SFR over the previous 3 billion centuries , relying on the luminosity of Ly - alpha radiation from young , cool galaxies . Unlike FUV radiation , this noticeable light vary with the instantaneous rate of galaxy formation , making it a flickering light tracer of current SFR .Using our analysis methods , we find a current SFR of 5 . 9 ( ± 2 . 3 ) × 10−3 solar masses per decade in the local universe , 90 % of which occurred in the previous 3 Gyr . By linking this with an larger ( 13 . 7 billion day ) figure based on the evolution of the solar system s gold content , we determine that the present SFR in the local universe is equal to that of the last several billion decades .These data provide crucial novel constraints for models of galaxy and galaxy formation in the universe .",
        "rewrite_text": "One of the primary concerns in studying star formation in the local universe is determining the overall star formation rate (SFR). This can be achieved by analyzing the luminosity of ionizing radiation emitted by young, hot stars. Such investigations are typically conducted at high redshifts, utilizing the rest-frame far-ultraviolet (FUV) emission from young galaxies. As the FUV spectral power distribution (SED) varies with the type and age of stars, the SFR can be accurately determined through fitting the FUV emission with population synthesis estimates. This approach enables us to explore star formation over the past 8 to 15 billion years. However, its accuracy is limited by our limited understanding of the star formation history over much longer timeframes.\n\nIn this study, we present an evaluation of recent findings from the Galaxy Evolution Explorer (GALEX) that offers precise SFR calculations for the past 3 billion years. This is achieved by relying on the luminosity of Ly-alpha radiation emitted by young, cool galaxies. In contrast to FUV radiation, this distinct light varies with the instantaneous rate of galaxy formation, making it a dynamic indicator of current SFR.\n\nUsing our analytical techniques, we have determined a current SFR of 5.9 (± 2.3) x 10-3 solar masses per decade in the local universe. Specifically, 90% of this SFR occurred within the last 3 Gyr. By correlating these findings with a larger (13.7 billion day) figure derived from the evolution of gold content in the solar system, we have established that the present SFR in the local universe is comparable to that of the past several billion years.\n\nThese data provide crucial new constraints for models of galaxy and galaxy formation in the universe, offering valuable insights into the processes and rates of star formation across cosmic time.",
        "ori-fast-z-score": -0.5477225575051661,
        "water-fast-z-score": 7.242859683401482,
        "rewrite-fast-z-score": 2.5175440748900675
    },
    {
        "original_text": "Type Ia supernovae (SNe Ia) have been used as  standard candles  to study the expansion history of the Universe. The current leading survey to discover thousands of SNe Ia is the Canada-France-Hawaii Telescope Supernova Search (SNLS), which is targeting 7.5 deg2 area in distant Universe  1 . With the discovery of SNe Ia at such high redshifts, it will be possible to measure the dark energy properties with unprecedented precision. The LSST, which is projected to discover tens of thousands of SNe Ia in 10 square degrees by 2021, is expected to dramatically improve the measurement of the dark energy parameters  2, 3 . To achieve the optimal science return from these future surveys, it is essential to measure the structure growth rate from redshift 7.5 to 10 accurately. This requires much larger galaxy surveys in the near-infrared (NIR), and breaking the *visibility period gap with NIR galaxy surveys has become a high priority for SN cosmology research. In this work, we measure the galaxy clustering rate using BOSS DR12 galaxy samples in three non-overlapping regions of the LSST visible footprint, and demonstrate the feasibility of the proposed NIR galaxy clustering measurement from BOSS DR12 by testing the impact of systematics from the light-to-number count relation. We find that the uncertainty from this relation is smaller than 0.5% in wavenumber space, and the bispectrum information beyond the nonlinear scale can be exploited to further reduce the uncertainty to 0.1%. We forecast the uncertainty from using BOSS DR12 in the LSST full footprint by combining three regions with these NIR constraints, and find that the galaxy clustering measurements will achieve 1% precision at nonlinear scales (0.2% at one eighth of the nonlinear scale) in the dark energy analysis with only 25% more galaxies than the BOSS DR12 clustering measurement alone. We also show that combining the BOSS DR12 clustering measurements with the baryon acoustic oscillation (BAO) measurement from the planned SPHERIC power spectrum measurement can significantly tighten the dark energy constraints, and the parameter errors can be decreased by a factor of two compared to using the BOSS DR12 clustering measurement alone. The measurement from BOSS DR12 can be completed by 2021, and the full LSST NIR clustering measurement will be achieved in 2023. The NIR galaxy clustering measurement from BOSS DR12 will provide strong synergy to the optical SN Ia measurement from the LSST, and significantly improve the dark energy measurement with the next generation cosmological surveys.  References:  1  https://hub.q Hume.com/display/SNLS/Home,  2  https:// www.lsst.org/scientists/news/survey-progress/,  3  https://www.lsst.org/scientists/news/survey-progress/,",
        "watermark_text": "Type Ia supernovae ( SNe Ia ) have been used as standard candles to study the expansion history of the Universe . The current leading study to find thousands of SNe Ia is the Canada - France - Hawaii Telescope Supernova Search ( SNLS ) , which is aiming 7 . 5 deg2 region in distant Universe 1 .With the discovery of SNe Ia at such great redshifts , it will be possible to measure the dark energy properties with tremendous precision . The LSST , which is anticipated to find tens of hundred of SNe Ia in 10 square degrees by 2021 , is expected to dramatically enhance the determination of the dark energy parameters 2 , 3 .To achieve the ideal science return from these upcoming observations , it is crucial to measure the structure growth period from redshift 7 . 5 to 10 correctly . This requires much larger galaxy surveys in the near - infrared ( NIR ) , and closing the * sight period gap with NIR galaxy surveys has become a high importance for SN cosmology research .In this research , we measure the galaxy clustering rate utilizing BOSS DR12 galaxy surveys in three non - overlapping regions of the LSST visible footprint , and suggest the feasibility of the suggested NIR galaxy clustering measurement from BOSS DR12 by testing the impact of systematics from the light - to - number count relation . We see that the uncertainty from this relation is tiny than 0 . 5 % in wavenumber space , and the bispectrum information beyond the nonlinear scale can be exploited to further reduce the uncertainty to 0 . 1 % .We estimate the uncertainty from use BOSS DR12 in the LSST complete footprint by combining three areas with these NIR restrictions , and find that the universe clustering observations will achieve 1 % precision at nonlinear scales ( 0 . 2 % at one eighth of the nonlinear scale ) in the dark energy analysis with only 25 % more galaxies than the BOSS DR12 clustering measurement alone . We additionally prove that using the BOSS DR12 clustering observations with the baryon electromagnetic oscillation ( BAO ) measurement from the intended SPHERIC energy spectrum study can significantly relax the dark energy constraints , and the parameter errors can be reduced by a factor of two compared to use the BOSS DR12 clustering measurement alone .The measurement from BOSS DR12 can be completed by 2021 , and the full LSST NIR clustering measurement will be obtained in 2023 . The NIR galaxy clustering measurement from BOSS DR12 will provide strong synergy to the optical SN Ia survey from the LSST , and substantially improve the dark energy calculation with the new generation cosmological observations .References : 1 https : / / hub . q Hume . com / display / SNLS / Home , 2 https : / / www . lsst . org / scientists / news / survey - progress / , 3 https : / / www . lsst . org / scientists / news / survey - progress / ,",
        "rewrite_text": "Standardized Candles for Cosmic Expansion History: Type Ia Supernovae (SNe Ia)\n\nType Ia supernovae (SNe Ia) have been utilized as benchmarks for studying the expansion of the universe. One of the most recent studies focusing on the discovery of thousands of SNe Ia is the Canada-France-Hawaii Telescope Supernova Search (SNLS), which is designed to cover a 7.5 square degree area of the distant universe. The precise identification of SNe Ia at greater redshifts offers a remarkable opportunity to meticulously measure the properties of dark energy.\n\nWith the expected capability of detecting tens of thousands of SNe Ia in a 10 square degree area by 2021, the Large Synoptic Survey Telescope (LSST) is anticipated to significantly enhance our understanding of dark energy parameters. To effectively harness these upcoming observations, it is imperative to accurately measure the structure growth period from redshift 7.5 to 10. This necessitates a significant expansion in near-infrared (NIR) galaxy surveys, making NIR galaxy surveys a top priority for SN cosmology research.\n\nIn our research, we've evaluated the galaxy clustering rate by utilizing BOSS DR12 galaxy surveys in three non-overlapping sections of the LSST visible footprint. We have demonstrated the feasibility of utilizing BOSS DR12 NIR galaxy clustering measurements through a systematic analysis of the impact of light-to-number count relations. Our findings indicate that the uncertainty associated with this relationship is less than 0.5% in wavenumber space, and the utilization of bispectrum information beyond the nonlinear scale can further reduce uncertainty to 0.1%.\n\nWhen estimating uncertainties using BOSS DR12 within the complete LSST footprint, incorporating these NIR constraints, we found that universe clustering observations can achieve 1% precision at nonlinear scales (0.2% at one eighth of the nonlinear scale) in dark energy analysis with only 25% more galaxies than the BOSS DR12 clustering measurement alone. Furthermore, we have verified that combining BOSS DR12 clustering observations with the baryon acoustic oscillation (BAO) measurements from the planned SPHERIC energy spectrum study can significantly improve dark energy constraints, reducing parameter errors by a factor of two compared to relying solely on BOSS DR12 clustering measurements.\n\nThe BOSS DR12 measurements are expected to be completed by 2021, while the comprehensive LSST NIR clustering measurements are anticipated in 2023. This NIR galaxy clustering data from BOSS DR12 will provide a strong complement to the optical SN Ia survey conducted by LSST, significantly advancing our ability to calculate dark energy with next-generation cosmological observations.\n\nReferences:\n1. SNLS Home: https://hub.qhume.com/display/SNLS/Home\n2. LSST Scientists News: Survey Progress: https://www.lsst.org/scientists/news/survey-progress/\n3. LSST Dark Energy Research: https://www.lsst.org/scientists/news/survey-progress/#darkenergyresearch (Repeat link as an additional source)",
        "ori-fast-z-score": 1.4990633779917228,
        "water-fast-z-score": 9.799118698777319,
        "rewrite-fast-z-score": 3.113247129976625
    },
    {
        "original_text": "We present results from modelling the early behaviour of the 2006 outburst of the recurrent nova RS Ophiuchi using a 1D, spherically symmetric, hydrodynamical code. We use the Fudge Model for the nuclear reaction network, and consider both adiabatic and (steeper) isothermal expansions. The models suggest the existence of at least three successive adiabatic shocks, two associated with the bullets observed moving away from the origin and the third occurring some time after April 2006 when the brightness of the system began to fall. Within the framework of this one-zone model, the evolution of the emitting zone is well- approximated by an exponential law, with a velocity equal to the escape speed from the shocked region. The density at the centre of the shocked region is found to be consistent with the minimum central density implied by the bolometric luminosity and the average expansion velocity. We also show that the steepening of the radial density profile observed in the last observation of 2006 October 22 can be reproduced by the model if the shockwave has reached the surface of the star. We compare our results with archival observations of the outburst from various telescopes, and find that observations in the blue band during 2006 May show the system to be more luminous and also reveal some evidence for a circum- nova disk.",
        "watermark_text": "We see results from calculating the early behaviour of the 2006 outburst of the recurrent nova RS Ophiuchi using a 1D , spherically symmetric , hydrodynamical code . We use the Fudge Model for the atomic reaction network , and consider both adiabatic and ( steeper ) isothermal expansions .The models suggest the existence of at least three consecutive adiabatic shocks , two associated with the shot discovered move back from the origin and the third occurring some time after April 2006 when the brightness of the system began to fall . Within the framework of this one - zone model , the evolution of the emitting zone is well - approximated by an exponential law , with a speed equal to the escape speed from the shocked areas .The density at the centre of the shocked region is found to be compatible with the minimum central density implied by the bolometric luminosity and the average growth velocity . We also demonstrate that the steepening of the radial density profile observed in the last observation of 2006 October 22 can be reproduced by the model if the shockwave has reached the surface of the star .We link our findings with archival measurements of the outburst from numerous telescopes , and find that measurements in the blue band during 2006 May reveal the system to be more luminous and also suggest some evidence for a circum - nova disk .",
        "rewrite_text": "We have calculated the early behavior of the 2006 outburst in the recurrent nova RS Ophiuchi using a one-dimensional, spherically symmetric hydrodynamic code. We employ the Fudge Model for the atomic reaction network and have considered both adiabatic and isothermal expansions with a steeper gradient. Our models suggest that there were at least three consecutive adiabatic shocks, two of them associated with the observed movement away from the origin and the third occurring after April 2006 when the system's brightness began to decline. Within the framework of our one-zone model, the evolution of the emitting zone can be well approximated by an exponential law, with a speed equivalent to the escape velocity from the shocked areas. The central density of the shocked region is found to be compatible with the minimum central density inferred from the bolometric luminosity and average growth velocity. Furthermore, we demonstrate that the observed steepening of the radial density profile from the last observation on October 22, 2006 can be replicated by our model if the shockwave has reached the star's surface. We have correlated our findings with archival measurements of the outburst from various telescopes and found that blue-band measurements during May 2006 revealed that the system was more luminous and also provided some evidence for a circum-nova disk.",
        "ori-fast-z-score": 0.4364357804719848,
        "water-fast-z-score": 4.800793585191832,
        "rewrite-fast-z-score": 2.587702172129855
    },
    {
        "original_text": "Graph codes are a powerful family of error-correcting codes that find many applications in storage, transmission, and computation. For high performance, usually high rate codes are desired. But for high rate codes, maximum a-posteriori (MAP) decoding (optimized for maximum likelihood (ML) decoding) often requires large graph sketches that cannot be stored or transmitted efficiently. In this work we consider codes optimized for source/channel coding and binning. We introduce the concept of graph codes that are optimal for source/channel coding and binning (called cCBC-optimized graph codes). We then describe a simple algorithm to construct graph codes that are cCBC-optimized. We apply these codes to the compression problem and show that they allow to reduce the bandwidth of sum product decoding by a factor of 2, at the cost of only a small increase in storage. We also apply these codes to the problem of shaping and show that they allow to reduce the buffer size of list joint source channel coding by a factor of 2, at the cost of only a small increase in computation.",
        "watermark_text": "Graph codes are a powerful family of mistake - correcting codes that find various uses in storage , transmission , and computation . For large performance , generally high rate codes are desired .But for high rate codes , maximum a - posteriori ( MAP ) decoding ( optimized for maximum likelihood ( ML ) decoding ) usually needs large graph plots that cannot be transferred or transmitted efficiently . In this study we study codes optimized for source / network compression and binning .We introduce the idea of graph codes that are optimal for source / network compression and binning ( named cCBC - optimized graph codes ) . We then describe a simple algorithm to build graph codes that are cCBC - optimized .We use these codes to the encoding problem and show that they allow to reduce the bandwidth of sum product decoding by a factor of 2 , at the cost of only a small increase in storage . We additionally apply these codes to the issue of shaping and show that they allow to reduce the buffer size of list joint source channel code by a factor of 2 , at the cost of only a small increase in computation .",
        "rewrite_text": "Graph codes constitute a formidable family of error-correcting codes that are widely utilized in storage, transmission, and computational processes. To achieve optimal performance, high-rate codes are frequently desired. However, high-rate codes often necessitate the utilization of extensive graph plots for maximum a posteriori (MAP) decoding, which is optimized for maximum likelihood (ML) decoding, rendering their transfer and transmission inefficient.\n\nIn this study, we explore the development of codes optimized for source/network compression and binning. We introduce the concept of cCBC-optimized graph codes, which are ideally suited for such compression and binning tasks. We then outline a straightforward algorithm to construct these cCBC-optimized graph codes.\n\nWe apply these codes to the encoding problem and demonstrate that they significantly reduce the bandwidth required for sum product decoding, achieving a reduction factor of two with only a minimal increase in storage capacity. Furthermore, we utilize these codes in the context of shaping and demonstrate that they can halve the buffer size of list joint source channel coding, albeit with a slight increase in computational requirements.\n\nIn summary, our research focuses on the development and application of cCBC-optimized graph codes for enhancing the efficiency of both encoding and decoding processes in various communication and computational scenarios.",
        "ori-fast-z-score": 0.31799936400190804,
        "water-fast-z-score": 5.405989188032437,
        "rewrite-fast-z-score": 0.4926646390821466
    },
    {
        "original_text": "Star-forming galaxies are believed to evolve along a typical sequence from warm neutral medium (WNM) to CNM to HII regions. Thus, study of their composition across these different phases will help to understand the evolutionary sequence. X-rays are extremely energetic photons and are believed to penetrate through the interstellar medium easily. Thus, they are an ideal tracer to explore the different components of a galaxy. Using Chandra data, we have studied six star-forming galaxies at various stages of the evolutionary sequence. We have detected one WNM, one CNM and two HII regions. However, no significant X-ray emitting Warm Absorber was found. We have also estimated the temperature and column density of the detected phases and compared them with the values from previous IR and radio observations. We also estimated the star formation rates (SFRs) from the HII region luminosity. Our findings suggest that the evolutionary sequence is consistent with the increasing pressure of the external environment. Futhermore, the HII region luminosity seems to correlate well with the X-ray luminosity and SFR, suggesting that the ionizing photons are able to penetrate through the gasphase and are mostly absorbed in the molecular cloud.",
        "watermark_text": "Star - creating stars are said to evolve along a typical sequence from hot neutral medium ( WNM ) to CNM to HII zones . Thus , investigation of their composition across these distinct phases will assist to comprehend the evolutionary sequence .X - rays are extremely active photons and are said to reach through the interstellar material quickly . Thus , they are an excellent tracer to study the different components of a galaxy .Using Chandra data , we have discovered six star - creating stars at several stages of the evolutionary sequence . We have discovered one WNM , one CNM and two HII zones .However , no major X - ray emitting Warm Absorber was obtained . We have also determined the temperature and column thickness of the emitted periods and compared them with the figures from previous IR and radio observations .We additionally predicted the star formation rates ( SFRs ) from the HII region luminosity . Our findings show that the evolutionary sequence is compatible with the increasing stress of the external environment .Futhermore , the HII area luminosity seems to correlate better with the X - ray luminosity and SFR , showing that the ionizing photons are able to reach through the gasphase and are mostly absorbed in the molecular cloud .",
        "rewrite_text": "The formation of stars is believed to progress through a typical sequence from hot neutral medium (WNM) to CNM, and ultimately to HII regions. Therefore, studying their composition across these distinct phases will aid in understanding this evolutionary sequence. X-rays are highly energetic photons that are known to penetrate interstellar materials swiftly, making them an excellent tool for examining the various components of a galaxy.\n\nUsing data from the Chandra telescope, we have discovered six stars-in-the-making at various stages of the evolutionary sequence. Specifically, we have identified one WNM, one CNM, and two HII regions. However, we did not detect any significant X-ray emitting warm absorbers. We have also determined the temperature and column density of the emitted periods and compared them to previous observations from infrared and radio sources. Additionally, we estimated the star formation rates (SFRs) based on the luminosity of the HII regions. Our findings suggest that the evolutionary sequence aligns with the increasing stress of the external environment. Furthermore, the luminosity of the HII regions appears to correlate more closely with X-ray luminosity and SFR, indicating that ionizing photons can penetrate the gas phase and are mostly absorbed within the molecular cloud.",
        "ori-fast-z-score": -3.4026920596804007,
        "water-fast-z-score": 5.09786575873842,
        "rewrite-fast-z-score": 0.20851441405707477
    },
    {
        "original_text": "Long-range order is a central phenomenon in condensed matter physics. It is commonly believed to be limited to spatially periodic systems. Here we report on the spontaneous formation of magnetic order in the ground state of quasi-one-dimensional Heisenberg quantum antiferromagnets, in a range of parameters inaccessible by classical simulations. The onset of long-range order is signaled by a sharp rise of the sublattice magnetization, followed by a slow convergence to the saturated value. This behavior is well captured by high-order coupled-cluster calculations, which demonstrate that long-range order can also occur in systems without true long-range order at finite temperature. These results open new avenues for experimental studies of one-dimensional quantum many-body systems and beyond. Long-range order is a central phenomenon in condensed matter physics. It is commonly believed to be limited to spatially periodic systems. Here we report on the spontaneous formation of magnetic order in the ground state of quasi-one-dimensional Heisenberg quantum antiferromagnets, in a range of parameters inaccessible by classical simulations. The onset of long-range order is signaled by a sharp rise of the sublattice magnetization, followed by a slow convergence to the saturated value. This behavior is well captured by high-order coupled-cluster calculations, which demonstrate that long-range order can also occur in systems without true long-range order at finite temperature. These results open new avenues for experimental studies of one-dimensional quantum many-body systems and beyond.",
        "watermark_text": "Long - range order is a central phenomenon in condensed matter mechanics . It is often said to be restricted to spatially periodic processes .Here we note on the spontaneous formation of magnetic order in the ground state of quasi - one - dimensional Heisenberg quantum antiferromagnets , in a range of values inaccessible by classical simulations . The initiation of large - range order is signaled by a sharp rise of the sublattice magnetization , followed by a slow convergence to the saturated value .This phenomenon is well captured by high - order coupled - cluster calculations , which demonstrate that long - range order can also occur in systems without true long - range order at finite temperature . These data open new avenues for theoretical experiments of one - dimensional quantum several - bodies systems and beyond .Long - range order is a central phenomenon in condensed matter mechanics . It is often said to be restricted to spatially periodic processes .Here we note on the spontaneous formation of magnetic order in the ground state of quasi - one - dimensional Heisenberg quantum antiferromagnets , in a range of values inaccessible by classical simulations . The initiation of large - range order is signaled by a sharp rise of the sublattice magnetization , followed by a slow convergence to the saturated value .This phenomenon is well captured by high - order coupled - cluster calculations , which demonstrate that long - range order can also occur in systems without true long - range order at finite temperature . These data open new avenues for theoretical experiments of one - dimensional quantum several - bodies systems and beyond .",
        "rewrite_text": "In condensed matter mechanics, long-range order is a pivotal phenomenon. It is commonly believed to be confined to spatially periodic processes. However, we are focusing on the spontaneous emergence of magnetic order in the ground state of quasi-one-dimensional Heisenberg quantum antiferromagnets. This occurs in a range of values that are inaccessible through classical simulations. The onset of extensive order is indicated by a sudden surge in sublattice magnetization, which then slowly converges to a saturated level. This phenomenon is accurately captured by high-order coupled cluster calculations, which reveal that long-range order can also occur in systems without true long-range order at finite temperatures. These findings present novel opportunities for theoretical experiments on one-dimensional quantum many-body systems and beyond.",
        "ori-fast-z-score": 1.2094157958139042,
        "water-fast-z-score": 6.1739490651303175,
        "rewrite-fast-z-score": 1.099524999206747
    },
    {
        "original_text": "A magnetic structure in the solar atmosphere termed prominence loop participated in five flares on May 20, 2017. It was observable in EUV with Atmospheric Imaging Assembly (AIA) on Solar Dynamics Observatory, in the Fe XII 304  nicrome  and Fe XII 525.0 nm lines with Solar Broadband Telescope and in HXR with Gamma-Ray Burst And Afterglow Spectrometer (GRAB). The 304 and 525.0 nm lines are formed in the upper and lower ionisation levels of iron, respectively. The loop showed a dynamic behavior during the flares. The observed line asymmetries suggested fundamental properties of the flaring plasma were violated, which could not be explained with a classical 1D hydrostatic loop model. A three-dimensional (3D) magnetic fluxrooted loop model including non-thermal particles and non-LTE effects was developed to reproduce the loop observations. It could reproduce the 304 and 525.0 nm line intensity asymmetries and the temporal evolution of the loop. The imbalance of heating rates between the up and down surfaces of the loop was suggested as one of the main reasons for the loop evolution. This work significantly advanced our understanding of the physical process of solar flares.",
        "watermark_text": "A magnetic shape in the solar atmosphere termed prominence loop participated in five flares on May 20 , 2017 . It was observable in EUV with Atmospheric Imaging Assembly ( AIA ) on Solar Dynamics Observatory , in the Fe XII 304 nicrome and Fe XII 525 . 0 cm lines with Solar Broadband Telescope and in HXR with Gamma - Ray Burst And Afterglow Spectrometer ( GRAB ) .The 304 and 525 . 0 nm lines are created in the higher and upper ionisation levels of metals , respectively . The loop showed a dynamic behavior during the flares .The observed line asymmetries suggested essential parameters of the flaring plasma were violated , which could not be described with a traditional 1D hydrostatic loop theory . A three - dimensional ( 3D ) magnetic fluxrooted loop theory including non - thermal particles and non - LTE effects was developed to reproduce the loop measurements .It could reproduce the 304 and 525 . 0 nm line brightness asymmetries and the temporal evolution of the loop . The imbalance of heating rates between the up and down edges of the loop was suggested as one of the main motives for the loop evolution .This research significantly developed our knowing of the physical process of sun flares .",
        "rewrite_text": "A prominence loop, magnetic in nature, was present in the solar atmosphere and was involved in five solar flares on May 20th, 2017. This loop was observable in the EUV spectrum using the Atmospheric Imaging Assembly (AIA) on the Solar Dynamics Observatory, specifically in the Fe XII 304 nicrome and Fe XII 525.0 cm lines with the Solar Broadband Telescope, and in HXR with the Gamma-Ray Burst And Afterglow Spectrometer (GRAB). The 304 and 525.0 nm lines were generated at higher and upper ionisation levels of metals, respectively. During the flares, the loop exhibited dynamic behavior. Observed line asymmetries suggested that fundamental parameters of the flaring plasma were violated, which could not be explained by traditional one-dimensional hydrostatic loop theory.\n\nA three-dimensional (3D) magnetic flux-rooted loop theory was developed, incorporating non-thermal particles and non-LTE effects, to replicate the loop measurements. This theory was able to replicate the asymmetric brightness of the 304 and 525.0 nm lines and the temporal evolution of the loop. It was suggested that an imbalance in heating rates between the upper and lower edges of the loop was one of the main driving forces behind the loop's evolution. This research significantly enhanced our understanding of the physical processes behind solar flares.",
        "ori-fast-z-score": -0.9428090415820635,
        "water-fast-z-score": 3.5355339059327378,
        "rewrite-fast-z-score": 0.7875615306482168
    },
    {
        "original_text": "Buffer gas evaporation is an effective technique to reduce the background pressure in an ion trap and thus increase the duty cycle of ion experiments. We demonstrate buffer gas evaporation out of a multipole rf ion trap, achieving a buffer gas inventory of 20% without reducing the number of detectable ions. The buffer gas is thermalized with the ions and thus cools the ion ensemble. As an example application, we study sympathetic cooling of Yb^{+} ions to the zero- kinetic-energy state using Sr^{+} ions. Sympathetic cooling is a method to reduce the temperature of a ion ensemble by magnetically coupling the ion populations via dipole-dipole interactions. We observe a sixfold reduction in Yb^{+} temperature, corresponding to a sympathetic cooling rate of 2.2(2)×10^{-19} K/s, or 1.4(1) Hz, at 295(5) mK. Our sympathetic cooling scheme does not require any unique quantum characteristics for the Sr^{+} ion, and should thus be generally applicable to many ion species.",
        "watermark_text": "Buffer gas evaporation is an efficient methodology to reduce the background pressure in an ion trap and therefore enhance the duty cycle of ion studies . We test buffer gas evaporation out of a multipole rf electron trap , obtaining a buffer gas inventory of 20 % without increasing the quantity of detectable atoms .The buffer gas is thermalized with the ions and therefore cools the ion ensemble . As an instance application , we study sympathetic cold of Yb ^ { + } electrons to the zero - kinetic - energy state using Sr ^ { + } ions .Sympathetic cooling is a technique to reduce the temperature of a ion ensemble by magnetically coupling the atom communities via dipole - dipole bonding . We see a sixfold change in Yb ^ { + } temperature , equivalent to a sympathetic heating rate of 2 . 2 ( 2 ) ×10 ^ { - 19 } K / s , or 1 . 4 ( 1 ) Hz , at 295 ( 5 ) mK .Our sympathetic temperature system does not require any unique quantum characteristics for the Sr ^ { + } ion , and should therefore be usually applicable to many ion species .",
        "rewrite_text": "Buffer gas evaporation is a highly effective method for reducing the background pressure within an ion trap, thereby enhancing the operational efficiency of ion studies. We have conducted tests on buffer gas evaporation in a multipole RF electron trap, achieving a 20% buffer gas inventory without increasing the number of detectable atoms. The buffer gas is thermalized with the ions, effectively cooling the entire ion ensemble. As a practical example, we investigate the sympathetic cooling of Yb⁺ ions to the zero kinetic energy state using Sr⁺ ions. Sympathetic cooling is a technique that reduces the temperature of an ion ensemble by magnetically coupling different atom communities through dipole-dipole interactions. We observe a sixfold decrease in the temperature of the Yb⁺ ions, which is equivalent to a sympathetic heating rate of 2.2(2) x 10⁻¹⁹ K/s or 1.4(1) Hz at 295(5) mK. Our sympathetic temperature system does not require any unique quantum properties for the Sr⁺ ion, and therefore should be widely applicable to a variety of ion species.",
        "ori-fast-z-score": -1.9629909152447274,
        "water-fast-z-score": 3.8105117766515297,
        "rewrite-fast-z-score": 1.3251783128981585
    },
    {
        "original_text": "We report the results of the second half of our LuckyCam survey for very low mass binaries (VLMBs), performed with the 5.1m Magellan/Baade Telescope at Las Campanas Observatory, Chile. In the new part of the survey, we observed 22.7 sq. degrees of the southern constellation of Scorpius with $i $ and $z $ filters, with total exposure times of 10.4 and 5.6 hours, respectively. We used a trapezoidal selection function to detect L/T transition brown dwarfs, where L and T are the low and high-temperature components of the L/T transition. The equatorial coordinates, $J$, $H$, and $K_s$ 2MASS photometry, and optical and near-infrared colors of the 13 confirmed binaries are presented. Spectroscopy is underway to confirm the physical nature of the candidates and to measure radial velocities. We also discuss the effects of large Poisson errors and crowding on our results, and we show that the observed spatial density of VLMBs is consistent with previous catalogs.",
        "watermark_text": "We report the results of the second half of our LuckyCam experiment for very low mass binaries ( VLMBs ) , conducted with the 5 . 1m Magellan / Baade Telescope at Las Campanas Observatory , Chile . In the new part of the poll , we photographed 22 . 7 sq .degrees of the southern constellation of Scorpius with $ i $ and $ z $ filters , with total sensitivity periods of 10 . 4 and 5 . 6 hours , respectively . We utilized a trapezoidal selection function to identify L / T transition white dwarfs , where L and T are the high and low - temperature components of the L / T shift .The equatorial parameters , $ J $ , $ H $ , and $ K _ s $ 2MASS photometry , and optical and far - infrared colors of the 13 verified binaries are presented . Spectroscopy is underway to confirm the physical nature of the candidates and to measure radial velocities .We also discuss the effects of large Poisson errors and crowding on our findings , and we prove that the seen spatial volume of VLMBs is compatible with previous catalogs .",
        "rewrite_text": "We present the findings of the second half of our LuckyCam experiment aimed at studying Very Low Mass Binaries (VLMBs). The experiment was conducted using the 5.1m Magellan/Baade Telescope located at the Las Campanas Observatory in Chile. In this segment of the investigation, we photographed a 22.7 square-degree area of the southern Scorpius constellation using both $i$ and $z$ filters. This process was conducted with total sensitivity periods of 10.4 and 5.6 hours, respectively. We utilized a trapezoidal selection function to pinpoint L/T transition white dwarfs, where L and T refer to the high and low-temperature components of the L/T shift. Additionally, we provide equatorial parameters, including $J$, $H$, and $K_s$ 2MASS photometry, along with optical and far-infrared colors for 13 verified binaries. Spectroscopy is currently underway to verify the physical nature of the identified candidates and measure their radial velocities. Furthermore, we discuss the impact of significant Poisson errors and crowding on our findings, confirming that the observed spatial volume of VLMBs aligns with previous catalogs.",
        "ori-fast-z-score": -0.7878385971583353,
        "water-fast-z-score": 4.556611884328835,
        "rewrite-fast-z-score": 0.9561828874675149
    },
    {
        "original_text": "The VIMOS VLT Deep Survey (VVDS) is a survey conducted with the VLT Survey Telescope (VST) deepening the census of the stellar mass in galaxies, from the young to the old universe. The total sample includes 980 early type galaxies and 1069 late type galaxies between redshifts 0.2 and 5, drawn from 44 independent fields (VVDS-44) in 32,000 arcmin2 of the sky uniformly distributed over three sky releases (VVDS-007, VVDS-Europe and VVDS-Omega). The VVDS-44 observations have already been presented in the single fields (hereafter SF) papers. These data are used to investigate the assembly history of galaxies by measuring the evolution with time of the scaling relations between the stellar mass, colour and environment, between 0.2<z<2.3. These are compared with local relations and with high-redshift analogues computed from mock galaxy catalogues built from hydrodynamical simulations, to infer the role of merging and of environmental processes in shaping the evolution of galaxies. The VVDS-007, VVDS-Europe and VVDS-Omega surveys cover 32, 24 and 18 % of the sky respectively. The paper describes the survey strategy, the dataset and the measurements made, as well as the analysis methods. The paper concludes with an overview of the main results and of the progress in the early data analysis of the wider VVDS-Infinity survey, which will study the evolution of galaxies as a function of their luminosity and rest-frame colour out to z = 6.",
        "watermark_text": "The VIMOS VLT Deep Survey ( VVDS ) is a survey conducted with the VLT Survey Telescope ( VST ) deepening the census of the stellar mass in galaxies , from the young to the ancient universe . The total sample comprises 980 earliest type galaxies and 1069 late type galaxies between redshifts 0 . 2 and 5 , chosen from 44 separate areas ( VVDS - 44 ) in 32 , 000 arcmin2 of the heavens uniformly scattered over three sky releases ( VVDS - 007 , VVDS - Europe and VVDS - Omega ) .The VVDS - 44 observed have already been presented in the single fields ( hereafter SF ) articles . These data are using to examine the assembly history of galaxies by monitoring the evolution with time of the scaling relations between the stellar mass , colour and environment , between 0 . 2 < z < 2 . 3 .These are compared with local relations and with high - redshift analogues computed from simulated star catalogues created from hydrodynamical simulations , to infer the importance of combining and of environmental mechanisms in shaping the evolution of galaxies . The VVDS - 007 , VVDS - Europe and VVDS - Omega studies cover 32 , 24 and 18 % of the heavens respectively .The paper explains the survey strategy , the dataset and the calculations made , as also as the evaluation technique . The paper closes with an overview of the main results and of the development in the early data analysis of the broader VVDS - Infinity survey , which will research the evolution of stars as a function of their luminosity and rest - frame colour out to z = 6 .",
        "rewrite_text": "The VIMOS VLT Deep Survey (VVDS) is an investigation performed utilizing the VLT Survey Telescope (VST) to expand the inventory of stellar mass in galaxies, spanning from the young to the ancient universe. The comprehensive sample comprises 980 early-type galaxies and 1069 late-type galaxies within redshift ranges of 0.2 to 5, selected from 44 distinct regions (VVDS-44) covering an area of 32,000 arcmin2 evenly distributed across three sky releases (VVDS-007, VVDS-Europe, and VVDS-Omega). The observations from VVDS-44 have already been presented in individual field (SF) articles. These data are utilized to investigate the assembly history of galaxies by monitoring the temporal evolution of scaling relationships between stellar mass, color, and environment, specifically within the range of 0.2 < z < 2.3. These relationships are compared to local connections and high-redshift analogues derived from simulated star catalogs created through hydrodynamic simulations. This comparison aims to elucidate the significance of combining and environmental mechanisms in shaping galaxy evolution. The VVDS-007, VVDS-Europe, and VVDS-Omega studies cover 32%, 24%, and 18% of the sky respectively. This paper elucidates the survey methodology, dataset, and calculations, as well as the evaluation technique. It concludes with an overview of the main findings and advancements in the early data analysis of the broader VVDS-Infinity survey, which will explore the evolution of stars based on their luminosity and rest-frame color up to z = 6.",
        "ori-fast-z-score": -0.10369516947304253,
        "water-fast-z-score": 5.775958979049243,
        "rewrite-fast-z-score": 2.3763541031440183
    },
    {
        "original_text": "Two neutron stars (NS) are believed to form via different mechanisms. For example, isolated NS form by the isolated formation of the NS core followed by a supernova explosion, whereas binary NS form via a binary star merger. In this work, we report the first detection of a double NS (DNS) system that can be utilized to distinguish between the two formation channels. We find that the DNS system in M22 has a median period of P=7.1days and an e-folding timescale of τF=1.8 Myr for the accretion rate of 2.1×10-9M⊙s-1, which are consistent with predictions from isolated NS formation. In contrast, the DNS system in NGC6440 has a longer period of P=14.2h and an e-folding timescale of τF=4.4 Myr for the accretion rate of 4.2×10-9M⊙s-1, which are consistent with predictions from binary NS formation. We find that both DNS systems have low, relatively constant X-ray luminosities (L_X∼1031-1033 erg s−1). These X-ray properties are also consistent with the predictions from binary NS formation. Our analysis suggests that the DNS system in M22 formed via isolated NS formation, whereas the DNS system in NGC6440 formed via binary NS formation. We propose that the DNS population can be used to study NS formation channels and infer the rates of both channels in the universe. We also analyze the optical counterparts of both DNS systems and suggest that the two NS in M22 may have a common envelope. This is contrary to a previous report, and may help to explain the longer observed period in M22.",
        "watermark_text": "Two neutron galaxies ( NS ) are said to form via different processes . For instance , isolated NS form by the isolated formation of the NS core followed by a supernova explosion , whereas binary NS form via a binary star separation .In this research , we publish the first detection of a double NS ( DNS ) scheme that can be employed to distinguish between the two formation channels . We see that the DNS system in M22 has a median period of P = 7 . 1days and an e - folding timescale of τF = 1 . 8 Myr for the accretion rate of 2 . 1×10 - [UNK] - 1 , which are compatible with predictions from isolated NS formation .In contrast , the DNS system in NGC6440 has a greater period of P = 14 . 2h and an e - folding timescale of τF = 4 . 4 Myr for the accretion rate of 4 . 2×10 - [UNK] - 1 , which are compatible with predictions from binary NS formation . We see that both DNS components have lowest , fairly constant X - ray luminosities ( L _ [UNK] - 1033 erg s−1 ) .These X - ray characteristics are also consistent with the estimates from binary NS formation . Our study implies that the DNS system in M22 arose via isolated NS formation , whereas the DNS system in NGC6440 formed via binary NS formation .We suggest that the DNS population can be used to study NS formation channels and infer the rates of both channels in the universe . We additionally analyze the optical cousins of both DNS components and suggest that the two NS in M22 may have a common envelope .This is contrary to a earlier report , and may assist to explain the longer observed period in M22 .",
        "rewrite_text": "Two neutron star (NS) galaxies are believed to form through distinct processes. Specifically, isolated NSs are believed to form through the independent development of the NS core, followed by a supernova explosion, while binary NSs form via the separation of binary stars. In this research, we report the initial detection of a double neutron star (DNS) system that can differentiate between these two formation paths.\n\nOur observations indicate that the DNS system in M22 has a median orbital period of 7.1 days and an e-folding timescale of τF = 1.8 million years for an accretion rate of 2.1×10-x-1, which aligns with predictions from isolated NS formation. In contrast, the DNS system in NGC6440 exhibits a longer period of 14.2 hours and an e-folding timescale of τF = 4.4 million years for an accretion rate of 4.2×10-x-1, which is more consistent with binary NS formation predictions. Furthermore, both DNS components exhibit consistently low X-ray luminosities (L_x ~ 1033 erg s-1), characteristics that are also in agreement with estimates from binary NS formation.\n\nOur findings suggest that the DNS system in M22 arose through isolated NS formation, while the DNS system in NGC6440 formed via binary NS formation. We propose that the DNS population can be utilized to investigate NS formation channels and to estimate the rates of both channels in the universe. Additionally, we analyze the optical counterparts of both DNS components and suggest that the two NSs in M22 may share a common envelope, contrasting with earlier reports and potentially offering an explanation for the observed longer period in M22.",
        "ori-fast-z-score": 2.288585537482975,
        "water-fast-z-score": 7.860793802658915,
        "rewrite-fast-z-score": 1.3348476249438292
    },
    {
        "original_text": "The SSS phase of RS Ophiuchi was observed with Chandra and XMM-Newton. Using the data obtained from the analysis of these observations, we report on the first detection of hard X-rays from the system and on the analysis of the temporal and spectral characteristics of the source. We also discuss the properties of the supersoft source and the dynamical status of the system based on our modeling of these data. We report the first detection of hard X-rays from the supersoft X-ray binary (SSS) system RS Ophiuchi. The observations were performed with the Chandra and XMM-Newton satellites. Using the data obtained from the analysis of these observations, we also discuss the properties of the supersoft source and the dynamical status of the system. Our analysis of the available X-ray data reveals hard X-ray emission from RS Oph. The observed spectrum can be fitted with a multicolor disk blackbody (diskbb) model with a hard component. The estimated coronal temperature of the source is in the range 0.3-0.8 keV. Assuming that the hard component originates from the accretion disk, we can estimate the inner radius of this disk using the corona temperature and the emitting area estimated from the hard component. We also discuss the properties of the supersoft source based on the modeling of these data. The analysis of the available X-ray data suggests that RS Oph is a dynamically stable system. We present the best-fitting parameters of the published eclipse model based on the observed spectroscopic and photometric variability of the source. Using these parameters, we simulate the light curves assuming different scenarios for the system and determine the regions of parameter space that can be excluded based on our analysis of the available X-ray data. This is an Open Access article distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0), which permits unrestricted use, distribution, and reproduction in any medium, provided the original work is properly cited. This paper describes the analysis of the available X-ray data from Chandra and XMM-Newton satellites of the supersoft X-ray binary system RS Ophiuchi. Using the data obtained from the analysis of these observations, we report on the first detection of hard X-rays from the system and on the analysis of the temporal and spectral characteristics of the source. The results of our analysis suggest that RS Oph is a dynamically stable system and contains a hot corona located in the outer region of the accretion disk. We also discuss the properties of the supersoft source and the dynamical status of the system. RS Ophiuchi is a low-mass binary system containing a white dwarf and a late type star. The latter is transferring material onto the former via an accretion disk. The spectral class of the optical star is K5Ve, which corresponds to a late type star of spectral type",
        "watermark_text": "The SSS phase of RS Ophiuchi was seen with Chandra and XMM - Newton . Using the information obtained from the examination of these observations , we publish on the first detection of hard X - radiation from the system and on the evaluation of the temporal and spectral traits of the source .We also discuss the properties of the supersoft source and the dynamical status of the system using on our modeling of these information . We report the first measurement of hard X - radiation from the supersoft X - ray binary ( SSS ) scheme RS Ophiuchi .The surveys were performed with the Chandra and XMM - Newton satellites . Using the information obtained from the analysis of these observations , we also discuss the properties of the supersoft source and the dynamical status of the system .Our study of the provided X - ray data reveals hard X - ray radiation from RS Oph . The observed spectrum can be fit with a multicolor disk blackbody ( diskbb ) model with a hard component .The estimated coronal temperature of the source is in the range 0 . 3 - 0 . 8 keV . Assuming that the hard component originates from the accretion disk , we can calculate the inner diameter of this disk using the corona temperature and the emitting area estimated from the hard component .We also discuss the properties of the supersoft source based on the modeling of these information . The evaluation of the provided X - ray data suggests that RS Oph is a dynamically stable structure .We present the best - fitting characteristics of the published eclipse theory based on the observed spectroscopic and photometric variability of the source . Using these parameters , we simulate the light angles assuming different scenarios for the system and establish the regions of parameter space that can be excluded based on our analysis of the provided X - ray data .This is an Open Access article distributed under the terms of the Creative Commons Attribution License ( http : / / creativecommons . org / licenses / by / 4 . 0 ) , which allows unrestricted sale , distribution , and reproduction in any medium , provided the original book is properly cited . This paper explains the examination of the provided X - ray data from Chandra and XMM - Newton satellites of the supersoft X - ray binary system RS Ophiuchi .Using the information obtained from the evaluation of these observations , we publish on the first detection of hard X - radiation from the system and on the evaluation of the temporal and spectral qualities of the source . The results of our analysis suggest that RS Oph is a dynamically stable system and comprises a heated corona located in the outer sector of the accretion disk .We also discuss the properties of the supersoft source and the dynamical status of the system . RS Ophiuchi is a small - mass binary system containing a white dwarf and a late type star .The latter is moving material onto the former via an accretion disk . The spectral category of the optical star is K5Ve , which corresponds to a late type star of spectral type",
        "rewrite_text": "The Chandra and XMM-Newton satellites observed the SSS phase of RS Ophiuchi. By analyzing the gathered data, we present the initial detection of hard X-ray radiation from the system and assess the temporal and spectral characteristics of the source. We further explore the properties of the supersoft source and the system's dynamic state using our modeling of the collected information.\n\nOur study reports the first measurement of hard X-ray radiation from the supersoft X-ray binary system RS Ophiuchi. The surveys were conducted with the mentioned satellites. Using the analysis results, we discuss the characteristics of the supersoft source and the system's dynamic status. Our investigation of the provided X-ray data reveals the presence of hard X-ray radiation from RS Oph. The observed spectrum can be fitted with a multicolor disk blackbody model incorporating a hard component. The estimated coronal temperature ranges between 0.3 and 0.8 keV. Assuming that the hard component originates from the accretion disk, we can calculate the inner diameter of this disk using the corona temperature and the emitting area estimated from the hard component.\n\nWe also explore the properties of the supersoft source based on our modeling. The evaluation of the X-ray data suggests that RS Oph is a dynamically stable structure. We present the best-fitting characteristics of the published eclipse theory based on the observed spectroscopic and photometric variability of the source. Utilizing these parameters, we simulate light angles, considering various system scenarios, and establish regions of parameter space that can be excluded based on our analysis of the provided X-ray data.\n\nThis article is an open access publication distributed under the terms of the Creative Commons Attribution License (http://creativecommons.org/licenses/by/4.0/). This allows unrestricted sale, distribution, and reproduction in any medium, provided that the original work is properly cited. This paper details our examination of X-ray data from the Chandra and XMM-Newton satellites regarding the supersoft X-ray binary system RS Ophiuchi. Using the gathered information from these observations, we report on the initial detection of hard X-radiation from the system and assess its temporal and spectral qualities. Our analysis indicates that RS Oph is a dynamically stable system with a heated corona located in the outer sector of the accretion disk. We also discuss the properties of the supersoft source and the system's dynamic status. RS Ophiuchi is a low-mass binary system consisting of a white dwarf and a late-type star. The latter is transferring material to the former through an accretion disk. The optical star's spectral category is K5Ve, corresponding to a late-type star of a specific spectral type.",
        "ori-fast-z-score": 0.760885910252682,
        "water-fast-z-score": 8.093059227233073,
        "rewrite-fast-z-score": 2.9602420805325114
    },
    {
        "original_text": "The recent discovery of the Schrödinger Greene method (SGM) for calculating holographic entanglement entropy (EE) in vacuum gravity backgrounds has led to the formulation of a covariant holographic entanglement entropy proposal (CHSMEE). This proposal states that the quantum extremal surface (QES) EE is given by the (covariant) Donaldson-Schreiber (DS) gauge theory action evaluated on a relevant configuration, where the relevant configuration is defined as the boundary value of a gauge field on the interior of a globally minimal area deformation of the projected QES onto the surface. Assuming that the proposal holds for general vacuum gravity backgrounds, CHSMEE reduces to a prescription for calculating EE for asymptotically AdS geometries. The proposal is applicable to holographic theories with a Lagrangian description, and, at least in minimal surfaces examples, yields results that agree with direct calculations of EE from QES wavefunctions. The proposal also possesses a consistency requirement that the EE found this way is a property of the CFTstate and is independent of how the QES is constructed. In this paper we apply the proposal to various examples, including a toy model, and find that in all cases the proposal yields results that agree with direct calculations from QES wavefunctions. Furthermore, the proposal appears to be extensible to higher dimensions, at least for minimal surfaces examples. Finally, we discuss how the proposal can be extended to EE of covariant flux operators and propose tests of the proposal for such operators.",
        "watermark_text": "The recent discovery of the Schrödinger Greene technique ( SGM ) for calculating holographic entanglement entropy ( EE ) in vacuum gravity backgrounds has led to the formulation of a covariant holographic entanglement entropy proposal ( CHSMEE ) . This suggestion asserts that the quantum extremal surface ( QES ) EE is given by the ( covariant ) Donaldson - Schreiber ( DS ) gauge theory action evaluated on a relevant configuration , where the appropriate configuration is given as the boundary value of a gauge field on the interior of a globally minimal area deformation of the projected QES onto the surface .Assuming that the proposal holds for general vacuum gravity backgrounds , CHSMEE reduces to a prescription for calculating EE for asymptotically AdS geometries . The suggestion is applicable to holographic models with a Lagrangian model , and , at least in reduced surfaces examples , yields findings that agree with direct calculations of EE from QES wavefunctions .The suggestion also possesses a consistency requirement that the EE found this way is a property of the CFTstate and is independent of how the QES is built . In this paper we apply the proposal to several examples , notably a toy theory , and find that in all situations the proposal yields findings that agree with direct calculations from QES wavefunctions .Furthermore , the proposal shows to be extensible to higher dimensions , at least for minimal surfaces examples . Finally , we talk how the proposal can be generalized to EE of covariant flux operators and suggest tests of the proposal for such functions .",
        "rewrite_text": "In recent research, the Schrödinger Greene Method (SGM) for calculating Holographic Entanglement Entropy (EE) in the context of vacuum gravity backgrounds has resulted in the development of a Covariant Holographic Entanglement Entropy Proposal (CHSMEE). This proposal posits that the Quantum Extremal Surface (QES) EE can be determined by evaluating the (covariant) Donaldson-Schreiber (DS) gauge theory action on a specific configuration. This configuration corresponds to the boundary value of a gauge field within a globally minimal area deformation of the projected QES onto the surface.\n\nAssuming the validity of this proposal for general vacuum gravity backgrounds, CHSMEE simplifies to a method for calculating EE in asymptotically AdS geometries. The suggestion is applicable to holographic models with a Lagrangian framework, and in limited surface examples, it produces findings that align with direct calculations of EE from QES wavefunctions.\n\nMoreover, this proposal demands a consistency requirement: the EE derived in this way must be a property of the CFT state and independent of how the QES is constructed. In this paper, we apply this proposal to several examples, including a toy theory, and find that it yields results consistent with direct calculations using QES wavefunctions in all scenarios. Furthermore, the proposal demonstrates its extensibility to higher dimensions, at least in the context of minimal surface examples.\n\nFinally, we discuss how this proposal can be generalized to accommodate EE of covariant flux operators and suggest tests for this extension using such functions.",
        "ori-fast-z-score": 1.6,
        "water-fast-z-score": 5.8,
        "rewrite-fast-z-score": 2.65361388801511
    },
    {
        "original_text": "The Amati relation is the correlation between the width of the optical afterglow of a gamma-ray burst (GRB) and its fluence. The afterglow is produced by the forward shock ionizing the circumburst medium (CM). The fluence is the total energy emitted by the GRB in the gamma-ray band. It is a very useful tool to study the high-energy emissions of GRBs. The Amati relation is linear in the ordinary cosmological evolution scenario. The slope of the Amati relation is related to the circumburst density, which is an indicator of the the type of the progenitor of the GRB. If the Amati relation changes slope at high redshifts, this would imply that the circumburst density increases with redshift, i.e. that the progenitors of high-redshift GRBs are significantly different from the low-redshift ones. This is contrary to the expected cosmological evolution. The hypothesis of a selection effect, i.e. that high-redshift GRBs with larger energies are also more peculiar (i.e. have different progenitors), can explain the Amati relation changing slope at high redshifts.",
        "watermark_text": "The Amati relation is the relationship between the width of the optical afterglow of a beta - ray burst ( GRB ) and its fluence . The afterglow is produced by the front shock ionizing the circumburst medium ( CM ) .The fluence is the total energy emitted by the GRB in the alpha - ray band . It is a very useful tool to study the high - energy emitted of GRBs .The Amati relation is linear in the ordinary cosmological evolution scenario . The slope of the Amati relation is related to the circumburst density , which is an measure of the the kind of the progenitor of the GRB .If the Amati relation changes slope at high redshifts , this might imply that the circumburst density increases with redshift , i . e . that the progenitors of high - redshift GRBs are greatly different from the high - redshift ones .This is contrary to the expected cosmological evolution . The theory of a selection influence , i . e .that high - redshift GRBs with larger energies are also more unusual ( i . e . have different progenitors ) , can reason the Amati relation shifting slope at high redshifts .",
        "rewrite_text": "The Amati correlation pertains to the relationship between the width of an optical afterglow resulting from a beta-ray burst (GRB) and its fluence. This afterglow is generated by the front shock ionizing the surrounding burst medium (CM). Fluence represents the total energy emitted by the GRB in the alpha-ray spectrum, making it a valuable instrument for studying high-energy emissions of GRBs. Within the ordinary framework of cosmological evolution, the Amati correlation is linear. The slope of this correlation is linked to the circumburst density, which serves as a measure for the type of GRB progenitor. If the Amati correlation experiences a change in slope at high redshifts, it may suggest an increase in circumburst density with redshift, indicating that the progenitors of high-redshift GRBs differ significantly from those at lower redshifts. This contradicts the anticipated cosmological evolution. A theory of selection influence suggests that high-redshift GRBs with greater energy are also more exceptional (i.e., with different progenitors), which could explain the shifting slope of the Amati correlation at high redshifts.",
        "ori-fast-z-score": 2.0768805540571886,
        "water-fast-z-score": 6.573840933228048,
        "rewrite-fast-z-score": 1.5215349135496974
    },
    {
        "original_text": "Flavour-dependent type II Leptogenesis provides a possible explanation for the observed neutrino masses and bi-large neutrino mixing. The seesaw mechanism of type I implies the light neutrino masses are proportional to the smallness parameters in the Yukawa matrices, which can be understood from the perspective of flavoured Leptogenesis. In this theory, CP is violated in the see-saw process, producing a stochastic energy density perturbation in the effective potential for leptons, which is trans-Planckian and leads to a later problem of inadequate cosmological perturbation. However, the flavoured Leptogenesis has an additional weak symmetry which can ensure that the flavoured CP asymmetry vanishes at leading order, and a small but non-vanishing contribution to the net asymmetry can be achieved through higher order contribution. In this way, the trans-Planckian problem can be naturally avoided, and the necessary baryon asymmetry can be obtained. Type II Leptogenesis can thus explain the observed neutrino masses and bi-large neutrino mixing simultaneously.",
        "watermark_text": "Flavour - dependent type II Leptogenesis offers a possible reason for the seen neutrino masses and bi - large neutrino mixes . The seesaw mechanism of type I implies the light neutrino masses are proportional to the smallness parameters in the Yukawa matrices , which can be understood from the viewpoint of flavoured Leptogenesis .In this theoretical , CP is violated in the saw - saw process , creating a stochastic energy density perturbation in the effective potential for leptons , which is trans - Planckian and results to a later issue of poor cosmological perturbation . However , the flavoured Leptogenesis has an additional weak symmetry which can maintain that the flavoured CP asymmetry vanishes at leading order , and a small but non - vanishing contribution to the net asymmetry can be obtained through higher order contribution .In this way , the trans - Planckian issue can be naturally ignored , and the necessary baryon asymmetry can be obtained . Type II Leptogenesis can thus understand the seen neutrino masses and bi - large neutrino mixes simultaneously .",
        "rewrite_text": "Flavor-dependent Type II Leptogenesis provides a potential explanation for the observed neutrino masses and the bi-large neutrino mixing. The seesaw mechanism of Type I suggests that the light neutrino masses are directly proportional to the smallness parameters found in the Yukawa matrices, which can be explained through the lens of flavored Leptogenesis. In this theoretical framework, CP violation occurs in the saw-saw process, generating a stochastic energy density perturbation in the effective potential for leptons. This perturbation is trans-Planckian and can lead to issues with poor cosmological perturbations. However, flavored Leptogenesis possesses an additional weak symmetry that can ensure the flavored CP asymmetry vanishes at the leading order. A small but non-vanishing contribution to the net asymmetry can be achieved through higher-order effects. This allows the trans-Planckian issue to be naturally overlooked, and the necessary baryon asymmetry can be achieved. Consequently, Type II Leptogenesis can simultaneously explain the observed neutrino masses and bi-large neutrino mixing patterns.",
        "ori-fast-z-score": 0.1259881576697424,
        "water-fast-z-score": 4.572004572006858,
        "rewrite-fast-z-score": 0.4923659639173309
    },
    {
        "original_text": "Trans-Neptunian objects (TNOs) are a heterogeneous population of objects, most of which are likely scattered disc objects (SDOs) that formed within the trans-Neptunian region of the Solar System. Their existence was predicted in 1664 by Johann-Heinrich Schroter, and the first such object was identified in 1801. TNOs are considered a key testbed for theories of planet formation, as their current orbit distribution provides evidence of the processes by which the Solar System evolved over time. Recent high-resolution images from the Hubble Space Telescope (HST) have revealed that a significant fraction of TNOs are in fact in compact multiple systems. This fraction is particularly high for larger TNOs, with about 15% of 2499+ objects with diameter >50 km being in multiple systems, and as many as one third of trans-Neptunian objects (TNOs) with a diameter of 100 km or more being part of multiple systems. Although many of these observed companions are likely remnants of shattered embryonic asteroids, several systems with centaurs, comets, and classical Kuiper belt objects are also clearly identified. Chaos-assisted capture (CAC) has been proposed as a mechanism to form these companions through resonant interactions with the giant planets, but recent theoretical work suggests that resonant capture may not be sufficient to explain the existence of so many multi-chained systems. In this paper, we propose another mechanism by which TNOs in the trans-Neptunian region could have been members of multiple systems: fission. We show that the sudden merger of two SDOs in a near-contact configuration can lead to the formation of a third SDO thanks to an instability that creates an additional gravitational torque. This mechanism, which we dub chaos-assisted fission, has the advantage of explaining the high observed fraction of binary TNOs without the need for planets to be responsible for their origin. We show that this dynamical process is likely to occur frequently in the trans-Neptunian region, especially during close encounters. By conducting a series of N-body simulations, we show that the instability that creates a new companion tends to occur for SDOs with a diameter of 100-150 km that are dynamically caught in an inward migration region between 35 and 40 au. For the conditions typically found in the trans-Neptunian region, the instability occurs more frequently for SDOs with an initial orbital radius of 30 au than for those with an initial orbital radius of 50 au. Finally, we show that two SDOs with initial semi-major axes of 300 au and 1.5 au could have been members of a binary system through this process.",
        "watermark_text": "Trans - Neptunian elements ( TNOs ) are a heterogeneous community of bodies , most of which are likely dispersed disc bodies ( SDOs ) that created within the trans - Neptunian region of the Solar System . Their existence was anticipated in 1664 by Johann - Heinrich Schroter , and the first such object was described in 1801 .TNOs are considered a key testbed for explanations of planet development , as their recent orbit distribution presents evidence of the mechanisms by which the Solar System evolved over time . Recent high - resolution images from the Hubble Space Telescope ( HST ) have revealed that a substantial proportion of TNOs are in reality in compact multiple components .This fraction is especially high for larger TNOs , with about 15 % of 2499 + items with diameter > 50 km being in multiple systems , and as much as one third of trans - Neptunian objects ( TNOs ) with a diameter of 100 km or more being part of multiple components . Although many of these observed companions are likely relics of broken embryonic asteroids , several systems with centaurs , comets , and classical Kuiper belt elements are also clearly identified .Chaos - aided capture ( CAC ) has been proposed as a process to form these companions through resonant interactions with the giant planets , but recent theoretical work suggests that resonant capture may not be sufficient to explain the existence of so several multi - chained systems . In this paper , we propose another process by which TNOs in the trans - Neptunian region could have been members of multiple components : fission .We suggest that the sudden union of two SDOs in a close - contact arrangement can lead to the formation of a third SDO due to an instability that creates an additional gravitational torque . This mechanism , which we dubbed chaos - aided fission , has the advantage of describing the high observed percentage of binary TNOs without the necessity for planets to be responsible for their source .We indicate that this dynamical process is probably to arise frequently in the trans - Neptunian region , particularly during close encounters . By conducting a sequence of N - bodies simulations , we indicate that the instability that creates a new sister tends to arise for SDOs with a diameter of 100 - 150 kilometres that are dynamically caught in an inward movement region between 35 and 40 au .For the conditions typically found in the trans - Neptunian region , the instability occurs more frequently for SDOs with an initial orbital radius of 30 au than for those with an initial orbital radius of 50 au . Finally , we find that two SDOs with initial semi - major axes of 300 au and 1 . 5 au might have been members of a binary system through this process .",
        "rewrite_text": "Trans-Neptunian Objects (TNOs) constitute a diverse and heterogeneous community of celestial bodies. The majority of these bodies are likely to be dispersed disc objects (SDOs) that originated within the trans-Neptunian region of the Solar System. Johann-Heinrich Schroter anticipated their existence in 1664, and the first such object was documented in 1801.\n\nTNOs are regarded as crucial laboratories for examining planet development theories due to the recent orbit distribution offering evidence of the Solar System's evolutionary mechanisms over time. Recent high-resolution images from the Hubble Space Telescope (HST) have revealed that a significant proportion of TNOs are actually composed of compact multiple components. This is especially evident in larger TNOs, with approximately 15% of objects over 2499+ items with a diameter exceeding 50 km being part of multiple systems. Furthermore, nearly one-third of trans-Neptunian objects with a diameter of 100 km or more are made up of multiple components.\n\nWhile many of these observed companions may be remnants of broken embryonic asteroids, several systems with centaurs, comets, and classical Kuiper belt elements have also been clearly identified. Chaos-aided capture (CAC) has been proposed as a process for forming these companions through interactions with the giant planets via resonant capture. However, recent theoretical studies suggest that this may not be sufficient to explain the existence of numerous multi-chained systems.\n\nIn this paper, we propose an alternative process to explain how TNOs in the trans-Neptunian region may have been part of multiple components: fission. We suggest that the sudden union of two SDOs in a close-contact arrangement can lead to the formation of a third SDO due to an instability that generates an additional gravitational torque. We refer to this mechanism as chaos-aided fission, which offers an explanation for the high observed percentage of binary TNOs without requiring planets to be their source.\n\nThrough dynamical analysis and simulations, we indicate that this process is likely to occur frequently in the trans-Neptunian region, particularly during close encounters. By conducting N-body simulations, we find that an instability that leads to the creation of a new companion SDO is more likely to occur for objects with diameters between 100 and 150 kilometers that are dynamically trapped in an inward movement region between 35 and 40 astronomical units (au). Under typical conditions in the trans-Neptunian region, this instability occurs more frequently for SDOs with an initial orbital radius of 30 au compared to those with an initial orbital radius of 50 au.\n\nFinally, our research suggests that two SDOs with initial semi-major axes of 300 au and 1.5 au could have been part of a binary system through this fission process.",
        "ori-fast-z-score": 1.466471150213533,
        "water-fast-z-score": 8.896096927335073,
        "rewrite-fast-z-score": 2.750847901848533
    },
    {
        "original_text": "Radio waves traveling through the atmosphere scour the sky, revealing the shape of the celestial sphere. Because stars are warmer than their surroundings, they preferentially emit into these cooler regions, creating the well-known temperature inversion in their atmospheres. For the most part, AGB stars emit most strongly at shorter wavelengths, so this provides a way to detect them. We collected photometric data from the U.K.’s White Mountain Array and analyzed it with a machine learning algorithm trained on data from the IRAS satellite. We also developed a new approach to interpolate K-band photometry from WMA and 2MASS data, which makes it possible to detect temperatures as low as 2000 K. Using these two techniques, we compiled a catalog of nearly 100,000 AGB stars in the Milky Way. We then used this catalog to measure their radiative mass loss rates. We found a strong correlation between these rates and spatial extinction, which can be used to trace mass loss in three dimensions.",
        "watermark_text": "Radio beams traveling through the atmosphere scour the heavens , showing the form of the astronomical sphere . Because galaxies are warmer than their environment , they preferentially emit into these cooler regions , creating the better - famous temperature inversion in their atmospheres .For the most part , AGB stars emit most strongly at shorter wavelengths , so this provides a way to identify them . We collected photometric data from the U . K . ’ s White Mountain Array and evaluated it with a machine learning engine trained on evidence from the IRAS satellite .We later advanced a new approach to interpolate K - band photometry from WMA and 2MASS information , which makes it able to identify heat as low as 2000 K . Using these two strategies , we compiled a label of almost 100 , 000 AGB stars in the Milky Way . We then utilized this catalog to measure their radiative mass loss trends .We identified a powerful correlation between these rates and spatial extinction , which can be used to map mass loss in three dimensions .",
        "rewrite_text": "Radio waves penetrate the atmosphere, scanning the vast expanse of the cosmos, unveiling the shape of the astronomical sphere. Galaxies, warmer than their surroundings, emit preferentially into cooler regions, creating a notable temperature inversion in their atmospheres. In most cases, AGB stars emit most intensely at shorter wavelengths, making them identifiable through this characteristic.\n\nWe have gathered photometric data from the UK's White Mountain Array and evaluated it using a machine learning engine trained on IRAS satellite evidence. We have subsequently developed a new method to interpolate K-band photometry from WMA and 2MASS data, enabling us to detect heat down to 2000 K. Through these two strategies, we have compiled a catalog containing nearly 100,000 AGB stars in the Milky Way. We then used this catalog to measure their radiative mass loss trends, revealing a significant correlation between these rates and spatial extinction. This correlation can be utilized to map mass loss in three dimensions.",
        "ori-fast-z-score": -0.6108472217815261,
        "water-fast-z-score": 6.230641662171566,
        "rewrite-fast-z-score": 1.3643820804812932
    },
    {
        "original_text": "The paper presents the results of numerical simulation of the time-dependent Schrödinger equation for the array of Josephson wires (JW). Using the parameterized paraconducting-to-superconducting (SC) transition model we perform direct comparative study of the dynamics of the array in the presence and the absence of an external magnetic field. It is shown that in the case of a homogeneous magnetic field the system passes through periodicAnderson-like  metallic  and  insulating  phases in direct analogue with the behavior of 1D single JW. Contrary, the applied magnetic field spatially distributed in the form of  ring  transforms the system from an insulating to a metallic one. The paper presents the rigorous analysis of the origin of this effect and development of the phenomenological model that well describes the system dynamics in both limiting cases. It is shown that such behavior of the array is generic and corresponds to the universal paraconducting-to-superconducting phase diagram for the 1D arrays. Original paper: https://arxiv.org/abs/1908.00363",
        "watermark_text": "The paper offers the conclusion of computational simulation of the time - dependent Schrödinger equation for the array of Josephson wires ( JW ) . Using the parameterized paraconducting - to - superconducting ( SC ) transition model we perform direct comparative research of the dynamics of the array in the presence and the absence of an external magnetic force .It is demonstrated that in the case of a homogeneous magnetic force the system passes through periodicAnderson - like metallic and insulating phases in direct analogue with the dynamics of 1D single JW . Contrary , the applied magnetic force spatially distributed in the form of ring transforms the system from an insulating to a metallic one .The paper offers the thorough analysis of the origin of this effect and development of the phenomenological theory that better models the process mechanics in both limiting cases . It is demonstrated that such behavior of the array is generic and relates to the fundamental paraconducting - to - superconducting phase diagram for the 1D arrays .Original report : https : / / arxiv . org / abs / 1908 . 00363",
        "rewrite_text": "The study presents the computational simulation results of the time-dependent Schrödinger equation for an array of Josephson wires (JW). Utilizing a parameterized paraconducting-to-superconducting (SC) transition model, we conduct a direct comparative analysis of the array's dynamics in the presence and absence of an external magnetic force. It is shown that under a homogeneous magnetic force, the system transitions through periodic Anderson-like metallic and insulating phases, closely mirroring the dynamics of a single 1D Josephson wire. Conversely, a spatially distributed ring-shaped magnetic force transforms the system from an insulating state to a metallic one.\n\nThe paper provides a comprehensive analysis of the origin of this effect and develops a phenomenological theory that more accurately models the mechanics of the process in both limiting cases. It is demonstrated that this array's behavior is typical and relates to the fundamental paraconducting-to-superconducting phase diagram for 1D arrays. Original report: https://arxiv.org/abs/1908.00363.",
        "ori-fast-z-score": -1.1952286093343936,
        "water-fast-z-score": 3.1075943842694236,
        "rewrite-fast-z-score": 0.47809144373375745
    },
    {
        "original_text": "The Orion Nebula is one of the most famous regions in the sky, and a jumping off point for many observing runs with both amateur and professional telescopes. Originally thought to be an example of a low-mass star forming region, it was later found to actually be a compact stellar cluster with an apparent emission temperature of over 10,000 K. The primary luminosity of the region comes from two massive stars within the inner region, also known as the Trapezium, but it is this nearby that has lead to significant analysis of the region s dynamics, surrounding gas, and galactic background. The region has also been used as a testbed for many different astronomical techniques. The first infrared imaging study of the region was published in 1955 and in 1956, Russell H. Bowey described the region as  a tiny globular star cluster  with a density between 1022 and 1023 m−3. In 1962, van den Bergh was the first to suggest that the cluster was in fact in an early stage of gravitational collapse. In 1964, Daniel Kerr described the region as  a concentration of some 50,000 stars... forming a beautiful, expanding cluster of nebulosity... almost certainly once a part of Orion s Nebula.  In 1968, David Whitmore detected an infrared counter-part to the Trapezium stars and in 1970, Walter Baade and Fritz Zwicky independently suggested that the Orion Nebula was in fact a star cluster. In 1973, Whitmore published the first deep survey of the region in visible light, detecting of an additional 60 stars, and in 1974, the first infrared survey of the region was published, reaching a limit of 15 Myr and approximately 2000 stars. In 1977, Alfred Landoldt and W. J. Moons independently calculated the age of the cluster to be between 2.1 and 2.5 million years, in close agreement with the most recent calculations. The Orion Nebula is now regarded as an example of a very young, massive star forming region and this has significant implications for its subsequent interaction with the interstellar medium. The region is within 1,200 light years of Earth and the Orion Molecular Cloud Complex, the nearest large molecular cloud, is approximately 1,000 light years across. This close interaction has profound implications for the radiation and winds from the Trapezium stars, the feedback this has on the region s interstellar environment, and the potential for disruption of the entire region.",
        "watermark_text": "The Orion Nebula is one of the most famous areas in the heavens , and a jumping off place for numerous observing runs with both professional and commercial telescopes . Originally considered to be an instance of a small - mass star producing zone , it was later revealed to really be a compact stellar cluster with an apparent emission temperature of over 10 , 000 K . The main luminosity of the region originates from two huge stars within the inner region , sometimes called as the Trapezium , but it is this nearby that has resulted to significant examination of the region s dynamics , surrounding gas , and galactic background .The region has additionally been used as a testbed for numerous varied astronomical techniques . The first infrared imaging study of the region was released in 1955 and in 1956 , Russell H . Bowey identified the region as a small globular star cluster with a density between 1022 and 1023 m−3 .In 1962 , van den Bergh was the first to propose that the cluster was in reality in an early stage of gravitational failure . In 1964 , Daniel Kerr described the region as a concentration of some 50 , 000 stars . . . forming a beautiful , evolving cluster of nebulosity . . . almost likely once a part of Orion s Nebula .In 1968 , David Whitmore detected an infrared counter - part to the Trapezium stars and in 1970 , Walter Baade and Fritz Zwicky independently suggested that the Orion Nebula was in actually a star cluster . In 1973 , Whitmore conducted the first depth survey of the region in apparent light , detecting of an additional 60 stars , and in 1974 , the first infrared survey of the region was publication , reaching a limit of 15 Myr and roughly 2000 stars .In 1977 , Alfred Landoldt and W . J . Moons separately predicted the age of the cluster to be between 2 . 1 and 2 . 5 million years , in good agreement with the most current estimates . The Orion Nebula is now regarded as an instance of a very young , huge star producing region and this has significant implications for its subsequent interference with the interstellar medium .The region is within 1 , 200 light years of Earth and the Orion Molecular Cloud Complex , the nearest large molecular cloud , is approximately 1 , 000 light years across . This tight interaction has tremendous significance for the radiation and winds from the Trapezium stars , the feedback this has on the region s interstellar climate , and the possibilities for disturbance of the entire region .",
        "rewrite_text": "The Orion Nebula stands as a renowned region in the heavens, serving as a starting point for numerous observing sessions utilizing both professional and commercial telescopes. Originally perceived as a small-mass star formation zone, it has been revealed to be a compact cluster of stars with an apparent emission temperature exceeding 10,000 K. The primary luminosity of this area originates from two large stars within its inner region, often referred to as the Trapezium. This proximity has led to extensive examination of the region's dynamics, surrounding gas, and galactic background.\n\nThe region has been utilized as a testing ground for various astronomical techniques. The initial infrared imaging study of the area was released in 1955, and in 1956, Russell H. Bowey identified it as a small globular star cluster with a density ranging between 1022 and 1023 m−3. In 1962, van den Bergh was the first to suggest that the cluster was in an early stage of gravitational instability. Four years later, Daniel Kerr described the area as a concentration of approximately 50,000 stars, forming a captivating, evolving cluster of nebulosity, possibly once part of Orion's Nebula.\n\nIn 1968, David Whitmore discovered an infrared counterpart to the Trapezium stars, and in 1970, Walter Baade and Fritz Zwicky independently proposed that the Orion Nebula was actually a star cluster. A depth survey of the region in apparent light was conducted by Whitmore in 1973, revealing an additional 60 stars. The first infrared survey of the area was published in 1974, extending its reach up to 15 million years and encompassing roughly 2000 stars.\n\nIn 1977, Alfred Landoldt and W.J. Moons independently estimated the cluster's age to be between 2.1 and 2.5 million years, aligning well with current estimates. Nowadays, the Orion Nebula is regarded as a prime example of a young and vast star-forming region, with significant implications for its subsequent interaction with the interstellar medium.\n\nThis region is situated within 1,200 light years of Earth, while the Orion Molecular Cloud Complex, the nearest large molecular cloud, spans approximately 1,000 light years across. This close interaction holds tremendous significance for the radiation and winds emitted by the Trapezium stars, their feedback on the region's interstellar climate, and the potential for disrupting the entire area.",
        "ori-fast-z-score": -3.023117679867774,
        "water-fast-z-score": 5.187564749039544,
        "rewrite-fast-z-score": 2.042752923427804
    },
    {
        "original_text": "In this paper, we study a two-component symmetric exclusion process (ASEP) on a one-dimensional lattice with open boundaries. The boundary sites are injected and extracted with species-dependent rates. This system can be realized as a lattice gas model with one type of particles (ASEPs) that hop to the right and an other type of particles (holes) that hop to the left. In the ASEP without extraction, the system exhibits phase separation: ASEP particles form domains surrounded by a gas of holes. When the extraction rate exceeds a critical value, the system undergoes a phase transition to an active state: Active sites become depleted and holes rush to the boundaries and accumulate at the extraction sites. A pair of density and flux gap are identified at the transition point. The model with two components generalizes the ASEP to asymmetric exclusion processes (ASEP(2)) in which both ASEPs and holes hop to the right and left with different rates. The behavior in the open boundaries with two species of particles is much richer than the previously studied cases with one species. For example, the two-ASEP(2) system shows three phases: gas, coexisting liquid, and active liquid. The density and flux gaps exist only in the liquid phases. We identify the order of the phase transitions using spatially resolved kinetic Monte Carlo simulations.",
        "watermark_text": "In this paper , we study a two - component symmetric exclusion cycle ( ASEP ) on a one - dimensional crystal with open boundaries . The border sites are injected and extracted with species - dependent rates .This system can be realized as a lattice gas model with one sort of molecules ( ASEPs ) that hop to the right and an other kind of molecules ( holes ) that hop to the left . In the ASEP without extraction , the model shows phase splitting : ASEP atoms form domains bordered by a gas of holes .When the extraction rate exceeds a critical value , the process undergoes a phase shift to an active state : Active locations become depleted and holes flood to the boundaries and accumulate at the extraction sites . A couple of density and flux gap are identified at the transition point .The model with two parts generalizes the ASEP to asymmetric exclusion mechanisms ( ASEP ( 2 ) ) in which both ASEPs and holes hop to the right and left with varying rates . The behavior in the open boundaries with two species of molecules is much richer than the previously observed cases with one species .For instance , the two - ASEP ( 2 ) scheme sees three stages : gas , coexisting solid , and active fluid . The density and flux gaps appear only in the liquid stages .We determine the order of the phase transitions using spatially resolved kinetic Monte Carlo simulations .",
        "rewrite_text": "In this study, we explore a two-component symmetric exclusion cycle (ASEP) within a one-dimensional crystal with open boundary conditions. The boundary sites are dynamically injected and extracted at species-specific rates. This system can be conceptualized as a lattice gas model comprising of two types of molecules: one type (ASEPs) hopping to the right, and the other type (holes) hopping to the left.\n\nIn the absence of extraction in ASEP, the model demonstrates phase separation, where ASEP atoms form domains bordered by a gas of holes. However, when the extraction rate surpasses a critical value, the system undergoes a phase shift to an active state. In this state, active locations become depleted, and holes flood towards the boundaries, accumulating at the extraction sites. At the transition point, distinct gaps in density and flux are observed.\n\nThe model, with its two components, generalizes the ASEP to asymmetric exclusion mechanisms (ASEP(2)), in which both ASEPs and holes can hop to both the right and left, albeit with varying rates. The behavior within the open boundaries with two molecular species is significantly more diverse compared to previous cases involving one species. For instance, the two-component ASEP(2) scheme exhibits three stages: gas, coexisting solid, and active fluid. Density and flux gaps are exclusive to the liquid stages. We determine the order of phase transitions using spatially resolved kinetic Monte Carlo simulations.",
        "ori-fast-z-score": -0.8620436566990363,
        "water-fast-z-score": 5.076479311672102,
        "rewrite-fast-z-score": 2.752558187682247
    },
    {
        "original_text": "SCF methods, also known as self-consistent-field methods, are an important class of techniques in quantum chemistry for solving the electronic structure problem. The Hartree-Fock method, proposed by Paul Hartree and Robert F. Service  1  and later developed by Robert Marshak  2 , was the first SCF method and remains widely used. In this technique, the system is approximated as a collection of non-interacting electrons, or, more accurately, a set of orthogonal orbitals, called Slater determinants, that describe the system. TheHartree-Fock equation, wherein the energy of the system is expressed as a functional of the orbitals, is then iterated to self-consistency, that is, to a solution in which the orbitals do not change between iterations. Several generalizations of the Hartree-Fock method have since been developed. The coupled perturbed Hartree-Fock (CPHF) method, proposed by Gilbert  3  and developed by Freeman and Subbaswamy  4 , is one such generalization. In this approach, the orbitals are allowed to change between iterations, and the energy is still expressed as a functional of the orbitals. However, the functional is approximated by the sum of the Hartree energy and a Fock energy expressed as a functional of the orbitals. Variational iterated perturbation theory (VIPT) is a similar generalization, proposed by Edward  5  and developed by Hyberts and Wood  6 . VIPT also represents the energy as a functional of the orbitals, but differs from CPHF in the manner by which the functional is approximated. The energy is expressed as a sum of a Hartree energy and a  free-energy  functional, which involves only perturbation theory within a given order. Variational perturbation theory (VPT) is another generalization of the Hartree-Fock method. It is based on a similar functional, but the functional is minimized with respect to the orbitals, as opposed to being treated as a fixed parameter. All of these generalizations of the Hartree-Fock method share the feature that the energy of the system is expressed as a functional of the orbitals. This review focuses on the development of the Hartree-Fock SCF method, as well as some of its generalizations, over the past few decades. We begin with a detailed description of the original method, outlining the various approximations that underlie its derivation. Next, several variants of the method are described. We then move on to describe the development of related ideas. For example, the CPHF method and its generalizations are described, and connections between these methods and Hartree-Fock are established. Finally, some challenges for the Hartree-Fock SCF method and its generalizations are discussed, including the need to develop efficient parallel algorithms. Overall, this review is intended as a resource for researchers interested in quantum chemistry and computational physics. While much of the material is taken",
        "watermark_text": "SCF methods , sometimes called as self - coherent - field methods , are an important group of techniques in quantum chemistry for solving the electronic stability problem . The Hartree - Fock method , suggested by Paul Hartree and Robert F . Service 1 and later built by Robert Marshak 2 , was the first SCF method and continues extensively employed .In this methodology , the system is approximated as a collection of non - interacting electrons , or , more accurately , a setting of orthogonal orbitals , called Slater determinants , that explain the system . TheHartree - Fock formula , wherein the electricity of the system is expressed as a functional of the orbitals , is then iterated to self - consistency , that is , to a solution in which the orbitals do not change between iterations .Several generalizations of the Hartree - Fock method have since been constructed . The correlated perturbed Hartree - Fock ( CPHF ) method , suggested by Gilbert 3 and developed by Freeman and Subbaswamy 4 , is one such generalization .In this approach , the orbitals are allowed to shift between iterations , and the power is still expressed as a functional of the orbitals . However , the functional is approximated by the sum of the Hartree energy and a Fock power expressed as a functional of the orbitals .Variational iterated perturbation theory ( VIPT ) is a similar generalization , suggested by Edward 5 and developed by Hyberts and Wood 6 . VIPT also expresses the power as a functional of the orbitals , but varies from CPHF in the manner by which the functional is approximated .The energy is expressed as a sum of a Hartree energy and a free - energy functional , which requires only perturbation theory within a given order . Variational perturbation theory ( VPT ) is another generalization of the Hartree - Fock method .It is based on a analogous functional , but the functional is minimized with regard to the orbitals , as opposed to being treated as a fixed parameter . All of these generalizations of the Hartree - Fock method exchange the feature that the energy of the system is expressed as a functional of the orbitals .This publication emphasizes on the development of the Hartree - Fock SCF method , as well as some of its generalizations , over the previous few decades . We begin with a detailed description of the first method , outlining the various approximations that underlie its derivation .Next , various variants of the method are explained . We then go on to explain the development of related ideas .For instance , the CPHF method and its generalizations are explained , and links between these algorithms and Hartree - Fock are established . Finally , some challenges for the Hartree - Fock SCF method and its generalizations are discussed , notably the requirement to develop fast parallel techniques .Overall , this review is intended as a resource for researchers interested in quantum chemistry and theoretical physics . While much of the information is taken",
        "rewrite_text": "In quantum chemistry, Self-Consistent Field (SCF) methods—also known as self-coherent field methods—play a crucial role in addressing the electronic stability problem. The Hartree-Fock method, initially proposed by Paul Hartree and Robert F. Service, and later refined by Robert Marshak, represents the initial SCF technique that has since been extensively utilized.\n\nIn this approach, the system is approximated as a set of non-interacting electrons or, more precisely, a set of orthogonal orbitals known as Slater determinants that describe the system. The Hartree-Fock formula, where the electrical properties of the system are expressed as a functional of these orbitals, is then iteratively refined until self-consistency is achieved—that is, until the orbitals remain unchanged through iterations.\n\nSince then, several generalizations of the Hartree-Fock method have been developed. One such generalization is the Correlated Perturbed Hartree-Fock (CPHF) method, suggested by Gilbert and further developed by Freeman and Subbaswamy. In this method, the orbitals are allowed to shift between iterations, while the power is still expressed as a functional of the orbitals. However, this functional is approximated by the sum of the Hartree energy and a Fock power functional of the orbitals.\n\nAnother similar generalization is the Variational Iterated Perturbation Theory (VIPT), proposed by Edward and further developed by Hyberts and Wood. VIPT also expresses power as a functional of the orbitals but differs from CPHF in how the functional is approximated. The energy in VIPT is expressed as a combination of a Hartree energy and a free-energy functional, requiring only perturbation theory within a specific order.\n\nVariational Perturbation Theory (VPT) is another variant of the Hartree-Fock method. It relies on a similar functional but minimizes it with respect to the orbitals rather than treating it as a fixed parameter. All these generalizations share a common feature: they express the system's energy as a functional of the orbitals.\n\nThis publication highlights the evolution of the Hartree-Fock SCF method and some of its generalizations over the past few decades. We begin with a comprehensive description of the initial method, outlining the various approximations underlying its derivation. Then we explain different variants of the method and delve into the development of related concepts. For instance, we explain the CPHF method and its generalizations, establishing links between these algorithms and the Hartree-Fock method.\n\nFinally, we discuss some challenges facing the Hartree-Fock SCF method and its generalizations, particularly the need to develop fast parallel techniques. Overall, this review aims to serve as a resource for researchers in quantum chemistry and theoretical physics. While much of the information presented is based on previous research, it provides a comprehensive overview and clarification to help researchers better understand and apply these techniques in their work.",
        "ori-fast-z-score": -0.16116459280507606,
        "water-fast-z-score": 6.783738517974788,
        "rewrite-fast-z-score": 1.7692307692307692
    },
    {
        "original_text": "In this paper, we perform a detailed comparison between several low-temperature and high-temperature approximation schemes for the stationary solution of the Ornstein-Zernike equation involving friction and hard-sphere interaction. We compare these schemes with numerical solutions of the corresponding integral equation and with Monte Carlo simulations of an effective hard-sphere (H Obamacare) model. We also show that our results can be used to obtain low- and high-temperature approximations for the solution of the Lado-Swift-Hohenberg equation, which is widely used to study patterns formation in evaporating drops and films. The rest of the paper is organized as follows. In the next section, we present the main equations and approximation schemes. The results of the comparison between these schemes and numerical solutions of the integral equations as well as with Monte Carlo simulations are presented and discussed in section 3. We conclude the paper with a brief summary of the main results in section 4.",
        "watermark_text": "In this paper , we perform a detailed comparison between many lowest - temperature and low - temperature approximation schemes for the stationary solution of the Ornstein - Zernike equation regarding tension and hard - sphere interaction . We evaluate these schemes with numerical solutions of the associated integral equation and with Monte Carlo simulations of an efficient hard - sphere ( H Obamacare ) model .We also prove that our findings can be used to obtain high - and low - temperature approximations for the solve of the Lado - Swift - Hohenberg function , which is widely using to study patterns formed in evaporating drops and films . The rest of the paper is organized as follows .In the second chapter , we present the main equations and approximation schemes . The results of the comparison between these schemes and mathematical solutions of the integral equations as well as with Monte Carlo simulations are presented and discussed in section 3 .We end the paper with a brief summary of the main results in section 4 .",
        "rewrite_text": "In this study, we conduct a comprehensive comparison between various lowest-temperature and low-temperature approximation methods for the stationary solution of the Ornstein-Zernike equation, specifically focusing on tension and hard-sphere interactions. We assess these methods using numerical solutions of the associated integral equations and through Monte Carlo simulations utilizing an efficient hard-sphere (H Obamacare) model. Furthermore, we demonstrate that our findings can be applied to obtain high- and low-temperature approximations for solving the Lado-Swift-Hohenberg function, which is commonly used to investigate patterns formed in evaporating drops and films.\n\nThe structure of this paper is organized as follows: In Chapter 2, we introduce the main equations and approximation schemes. In Chapter 3, we present and discuss the results of our comparison between these schemes, mathematical solutions of the integral equations, and Monte Carlo simulations. Finally, in Section 4, we provide a brief summary of the key findings.",
        "ori-fast-z-score": 1.1470786693528088,
        "water-fast-z-score": 5.735393346764043,
        "rewrite-fast-z-score": 3.25493388482694
    },
    {
        "original_text": "The non-Gaussian characteristics of the cosmic density field induce scale-dependent effects that can be measured through the gravitational lensing of the Cosmic Microwave Background (CMB). These effects imprint themselves in the two-point statistics of the temperature and polarization maps. We quantify how the detectability of these effects is impacted by the intrinsic levels of non-Gaussianity measured by Planck in the Synchronous scenario as well as by the presence of additional cosmological parameters, in particular the presence of early dark energy. We find that, for a wide range of observable scales, the detection of these effects would improve the sensitivity to the amplitude of the amplitude of the primordial non-Gaussianity of a factor 2 for a detection significance of 4σ. Additionally, we find that the effects of non-Gaussianity are best seen on relatively large angular scales, namely those probed by S-Z data from the Atacama Cosmology Telescope (ACT), and that those data are particularly well suited to constrain the presence of early dark energy.",
        "watermark_text": "The non - Gaussian attributes of the cosmic density field induce scale - dependent effects that can be recorded through the gravitational lensing of the Cosmic Microwave Background ( CMB ) . These effects imprint themselves in the two - point predictions of the temperature and polarization images .We quantify how the detectability of these changes is impacted by the intrinsic rates of non - Gaussianity measured by Planck in the Synchronous scenario as also as by the presence of added cosmological factors , in instance the presence of early dark energy . We see that , for a broad variety of observable scales , the detection of these influences would improve the sensitivity to the frequency of the frequency of the primordial non - Gaussianity of a factor 2 for a detection significance of 4σ .Additionally , we find that the effects of non - Gaussianity are best shown on relatively huge angular scales , notably those probed by S - Z data from the Atacama Cosmology Telescope ( ACT ) , and that those data are particularly good suitable to constrain the presence of early dark energy .",
        "rewrite_text": "The non-Gaussian characteristics of the cosmic density field generate scale-dependent effects that can be recorded via gravitational lensing of the Cosmic Microwave Background (CMB). These effects leave an imprint in the two-point temperature and polarization image predictions. We quantify how the detectability of these changes is affected by the intrinsic rates of non-Gaussianity, as measured by Planck in a synchronous scenario, as well as by the presence of additional cosmological factors, such as early dark energy. We observe that, across a wide range of observable scales, detecting these influences would enhance the sensitivity to the frequency of primordial non-Gaussianity by a factor of two, with a detection significance of 4σ. Furthermore, we find that the effects of non-Gaussianity are most evident on relatively large angular scales, notably those probed by S-Z data from the Atacama Cosmology Telescope (ACT). These data are particularly well-suited to constrain the presence of early dark energy.",
        "ori-fast-z-score": 0.7385489458759964,
        "water-fast-z-score": 5.2532861073211246,
        "rewrite-fast-z-score": 3.048003048004572
    },
    {
        "original_text": "Information-theoretic proofs of entropy power and complementary entropies inequalities for discrete random vectors are presented. In particular, inequalities for the joint entropy of dependent random variables are obtained. The novel inequalities are employed to derive vector formulations of well known entropy power and complementary entropy relations in information theory. The new results provide a theoretical framework for estimation-theoretic characterizations of entropy power, entropy rate, and conditional entropy, and related quantities. The inequalities are relevant to theories of multivariate dependence and have potential applications in mathematics, information theory, and statistics. Here is an excerpt from the paper: A remarkable property of entropy is its monotonicity under statistical mechanisms. For example, if two random vectors are statistically independent, then their entropies add. More generally, if statistical mechanisms transform into , their entropies increase. The entropy power inequality (EPI) and the entropy rate (ER) entropy power inequality are fundamental and well known results in information theory. The former states that the entropy of a random vector cannot be greater than the sum of the entropies of its components, while the latter states that the entropy of a random vector cannot be greater than the entropy of its mean. Both these inequalities hold for any random vectors. Here we present information-theoretic proofs of the entropy power and entropy rate entropy power inequalities. Our starting point is the information inequality (FI), which gives a lower bound on the mutual information. Under suitable conditions, we obtain a new family of information inequalities, which imply the entropy power and entropy rate entropy power inequalities. We demonstrate the use of these general inequalities through vector forms of fundamental entropy power and entropy rate relations. The inequalities presented here complement existing approaches to proving the entropy power and entropy rate entropy power inequalities. These include information-theoretic proofs based on Young’s inequality, probability distributions and sufficiency. In this paper we also present information-theoretic proofs of some related entropy inequalities, including relations for the conditional entropy and the joint entropy of dependent random variables. We observe that the proofs are unchanged if joint distributions are replaced with distribution functions. The new results presented here have several important applications, including to theories of multivariate dependence, mathematics, information theory and statistics. For example, the new results are used to obtain a vector entropy power inequality for statistically dependent random variables, an application that has received considerable attention in the information theory literature but which has not previously been available via an information-theoretic approach. Another application of our results is to obtain a vector entropy rate inequality for time series. We also prove a vector joint entropy power inequality. We argue that the new vector forms of fundamental relations may be of independent interest. We present",
        "watermark_text": "Information - theoretic proofs of entropy energy and complementary entropies inequalities for linear random vectors are presented . In particular , inequalities for the joint entropy of dependent random factors are derived .The novel inequalities are applied to derive vector formulations of well established entropy energy and complementary entropy relations in information theory . The new results present a conceptual template for estimation - theoretic characterizations of entropy energy , entropy rate , and conditional entropy , and related quantities .The inequalities are applicable to theories of multivariate dependence and have potential applications in math , information theory , and statistics . Here is an excerpt from the paper : A peculiar property of entropy is its monotonicity under statistical mechanisms .For instance , if two random vectors are statistically independent , then their entropies add . More generally , if statistical mechanisms shift into , their entropies grow .The entropy energy inequality ( EPI ) and the entropy rate ( ER ) entropy energy inequality are fundamental and well famous results in information theory . The former states that the entropy of a random matrix cannot be greater than the sum of the entropies of its components , while the former states that the entropy of a random matrix must be greater than the entropy of its average .Both these inequalities hold for any random vectors . Here we present data - theoretic proofs of the entropy energy and entropy rate entropy power inequalities .Our starting point is the information inequality ( FI ) , which gives a smaller bound on the mutual information . Under sufficient situations , we obtain a new family of information inequalities , which confirm the entropy energy and entropy rate entropy energy inequalities .We test the using of these general inequalities through vector types of fundamental entropy energy and entropy rate relations . The inequalities presented here match existing techniques to proving the entropy energy and entropy rate entropy power inequalities .These include information - theoretic proofs based on Young ’ s inequality , probability distributions and sufficiency . In this paper we also present data - theoretic proofs of some related entropy inequalities , notably relations for the conditional entropy and the joint entropy of dependent random factors .We see that the proofs are unchanged if joint variables are replaced with distribution functions . The new results presented here have several important use , notably to theories of multivariate dependence , mathematics , information theory and statistics .For instance , the new results are using to obtain a matrix entropy energy inequality for statistically dependent random factors , an use that has garnered considerable scrutiny in the information physics literature but which has not originally been accessible via an information - theoretic approach . Another application of our findings is to obtain a matrix entropy rate inequality for time series .We also prove a vector joint entropy capacity inequality . We argue that the new vector types of fundamental relations might be of independent importance .We present",
        "rewrite_text": "Theoretical proofs regarding entropy energy and complementary entropies for linear random vectors are presented in an information-theoretic framework. Specifically, inequalities pertaining to the joint entropy of interdependent random factors have been derived. These novel inequalities are applied to derive vector formulations of established entropy energy and complementary entropy relationships in the field of information theory.\n\nThe new research outcomes offer a conceptual template for estimating entropy energy, entropy rate, conditional entropy, and related measures in estimation theory. These inequalities are applicable in multivariate dependence theories and hold potential for applications in mathematics, information theory, and statistics.\n\nAn excerpt from the paper states that a distinctive characteristic of entropy is its monotonicity under statistical mechanisms. For instance, when two random vectors are statistically independent, their entropies sum up. More generally, when statistical mechanisms shift, their entropies increase. The entropy energy inequality (EPI) and the entropy rate (ER) inequality are fundamental and well-known concepts in information theory. The former states that the entropy of a random matrix cannot exceed the sum of its component entropies, while the latter asserts that the entropy of a random matrix must be greater than the entropy of its average. Both these inequalities hold true for any random vectors.\n\nIn this study, we provide data-theoretic proofs for the entropy energy and entropy rate power inequalities. Our starting point is the information inequality (FI), which provides a tighter bound on mutual information. Under suitable circumstances, we obtain a new family of information inequalities that affirm the entropy energy and entropy rate power inequalities. We test these general inequalities through vector types of fundamental entropy energy and entropy rate relationships.\n\nThe presented inequalities align with existing techniques for proving the entropy energy and entropy rate power inequalities, including information-theoretic proofs based on Young's inequality, probability distributions, and sufficiency. This paper also presents data-theoretic proofs for related entropy inequalities, notably for the conditional entropy and joint entropy of dependent random factors. It is observed that the proofs remain unchanged when joint variables are substituted with distribution functions.\n\nThe new research findings have several significant applications, particularly in multivariate dependence theories, mathematics, information theory, and statistics. For instance, the new results have been utilized to derive a matrix entropy energy inequality for statistically dependent random factors, which has garnered considerable attention in information physics literature previously unreachable through an information-theoretic approach. Another application of our findings is to obtain a matrix entropy rate inequality for time series. We have also proven a vector joint entropy capacity inequality. We argue that these new vector types of fundamental relations may hold independent importance. We present these findings with confidence in their potential to advance our understanding in various fields.",
        "ori-fast-z-score": -0.6868028197434451,
        "water-fast-z-score": 7.728734264634367,
        "rewrite-fast-z-score": 4.161167542326951
    },
    {
        "original_text": "In this work we investigate the interplay between radio galaxies and their cluster environment. Cluster radio galaxies (CRG), namely radio-loud AGN located in the cluster core, show signs of ongoing strong activity that are usually absent in field galaxies. Several scenarios have been proposed to explain the presence of active CRG in clusters, that however cannot be fully representative of the entire cluster population. Cluster cold fronts, for instance, appear frequently in simulations of clusters in the intermediate stages of formation, but hardly ever in actual X-ray observations. This suggests that the fraction of active CRG could be different from what predicted by these scenarios. Observations of the soft X-ray emission from CRG counterparts also suggest the presence of large amounts of hot gas associated with the CRG, likely stripped from the cluster galaxies. Finally, observations of the large scale structure surrounding clusters suggest that most of them grow through the accretion of mass-poorer groups. Groups can strip part of the cluster galaxies, leaving the galaxies in the cluster cores. Since radio galaxies typically show strong radio emission, they are excellent tracers of the galaxy density distribution. We plan to employ a wide set of observations of CRG and their host clusters to study the interplay between the CRG and their environment.",
        "watermark_text": "In this project we investigate the interplay between radio nuclei and their cluster environment . Cluster radio nuclei ( CRG ) , particularly radio - loud AGN situated in the cluster core , exhibit signs of ongoing strong activity that are typically lacking in field galaxies .Several possibilities have been proposed to explain the presence of active CRG in clusters , that however cannot be fully representative of the entire cluster population . Cluster cool fronts , for instance , appear often in simulations of clusters in the intermediate steps of formation , but hardly ever in real X - ray observations .This implies that the fraction of active CRG might be changed from what anticipated by these scenarios . Observations of the deep X - ray radiation from CRG counterparts additionally indicate the presence of large quantities of hot gas associated with the CRG , likely stripped from the cluster clusters .Finally , observations of the huge scale organization surrounding clusters indicated that most of them develop through the accretion of mass - poorer categories . Groups can strip part of the cluster clusters , left the galaxies in the cluster cores .Since radio stars typically produce bright radio emission , they are excellent tracers of the galaxy volume distribution . We intend to use a broad group of measurements of CRG and their host clusters to study the interplay between the CRG and their environment .",
        "rewrite_text": "In this project, we are exploring the intricate relationship between radio nuclei and their cluster environment. Cluster radio nuclei (CRG), particularly the radio-loud active galactic nuclei (AGN) situated in the cluster cores, display signs of sustained intense activity that is often absent in field galaxies.\n\nSeveral hypotheses have been proposed to explain the presence of active CRG in clusters, but they may not fully represent the entire cluster population. For instance, cluster cool fronts frequently appear in simulations of clusters in their formative intermediate stages, yet they are rarely observed in real X-ray observations. This suggests that the proportion of active CRG may differ from what is anticipated by these scenarios.\n\nFurther observations of deep X-ray radiation from CRG counterparts indicate the presence of vast quantities of hot gas associated with the CRG, likely stripped from the cluster itself. Observations of the vast organization surrounding clusters suggest that most of them develop through the accretion of lower mass categories. Groups can strip portions of the cluster, leaving the galaxies at the cluster cores intact. Since radio stars typically produce bright radio emissions, they serve as excellent tracers of galaxy volume distribution.\n\nOur aim is to utilize a comprehensive range of measurements of CRG and their host clusters to investigate the dynamic interaction between CRG and their environment.",
        "ori-fast-z-score": -1.5255401427929478,
        "water-fast-z-score": 5.669467095138408,
        "rewrite-fast-z-score": 1.4985372985307104
    },
    {
        "original_text": "A framework for modifying or affecting quantum walks via localized scattering interactions is developed. The general framework covers not only modifications that preserve the unitary nature of the quantum walk, but also introduces opportunities for altering the dynamics to introduce dissipation or otherwise break detailed balance. The two-node scattering approach is applied to several paradigmatic examples including robust group shifting, extension, and amplification. Examples are given for both random and tailored scattering networks. These results provide a solid foundation for future works on applications to quantum algorithms, simulation of quantum many-body dynamics, and experimental quantum walks, all of which are of considerable interest to the quantum information and computational complexity communities. This paper develops a framework for modifying or affecting quantum walks via localized scattering interactions. The framework covers not only modifications that preserve the unitary nature of the quantum walk, but also introduces opportunities for altering the dynamics to introduce dissipation or otherwise break detailed balance. The two-node scattering approach is applied to several paradigmatic examples including robust group shifting, extension, and amplification. Examples are given for both random and tailored scattering networks.",
        "watermark_text": "A template for modifying or affecting particle walks via localized scattering effects is developed . The basic framework covers not only alterations that maintain the unitary existence of the quantum walk , but also provides opportunities for altering the dynamics to introduce dissipation or otherwise break detailed balance .The two - node scattering approach is applied to several paradigmatic instance including efficient band shifting , extension , and amplification . Examples are given for both random and tailored scattering networks .These data provide a solid foundation for future publications on uses to quantum algorithms , simulation of quantum several - bodies dynamics , and theoretical particle walks , all of which are of large interest to the quantum information and mathematical complexity groups . This paper develops a framework for modifying or affecting particle walks via localized scattering effects .The framework covers not only modifications that maintain the unitary existence of the quantum walk , but also provides opportunities for altering the dynamics to introduce dissipation or otherwise break detailed balance . The two - node scattering approach is applied to several paradigmatic cases including robust band shifting , extension , and amplification .Examples are given for both random and tailored scattering networks .",
        "rewrite_text": "A framework has been devised to modify or influence the behavior of particle walks through localized scattering effects. This basic structure encompasses alterations that preserve the unitary nature of the quantum walk while also offering opportunities to alter its dynamics, introducing dissipation or disrupting detailed balance in other ways. The two-node scattering technique is utilized in several exemplary cases, such as efficient band shifting, expansion, and amplification. We present examples from both random and custom-designed scattering networks. These findings establish a robust foundation for future research in various fields, including quantum algorithms, simulations of multi-body quantum dynamics, and theoretical particle walks. These topics are highly relevant to the fields of quantum information and mathematical complexity. This paper specifically develops a system that allows for modifying particle walks via localized scattering effects. The system encompasses changes that maintain the integrity of the quantum walk while also providing opportunities to modify its dynamics, potentially introducing dissipation or disrupting detailed balance. Additionally, it applies the two-node scattering approach to several exemplary cases, including robust techniques for band shifting, expansion, and amplification. Illustrative examples are provided for both randomly generated and tailored scattering networks.",
        "ori-fast-z-score": 1.2339053944782488,
        "water-fast-z-score": 7.118684968143743,
        "rewrite-fast-z-score": 0.5570860145311556
    },
    {
        "original_text": "In this paper, we propose a statistical method for testing whether one sample path of a Wiener process is strictly double-sidededly continuous with respect to the previsible filtration. Under some conditions, we show that the test statistic converges in distribution to the Tracy-Widom distribution, which is well known in the literature of random matrix theory. An application to a fractional Brownian motion with Hurst parameter H<0.5 is given to demonstrate the proposed method. Our proposed test is based on the maximum likelihood estimator of the Hurst parameter, and the LSEE for the underlying diffusion process is derived. An exponential inequality for the LSEE is also established. Numerical studies show that the empirical size of the test is well controlled and the power is close to one under various scenarios. A real data example is also provided to demonstrate the applicability of the proposed method. Click here to access the paper on arXiv https://arxiv.org/abs/2004.00965 Zhenyu Bai Zheng Liu June 2023 Version: 1.0 Discrete-time fractional Brownian motion and its long-time behavior As one of the classical processes, fractional Brownian motion (fBm) has found many applications in different areas. For example, it was observed that certain physical systems, such as earthquakes and wind gusts, exhibit long-range power-law correlations that are similar in form to fBm. As a consequence, methods for testing for fBm have found use in a wide range of areas, including financial market analysis, human activity analysis, astrophysics, and geophysics. On the other hand, there are many scenarios in which it is desired to test the hypothesis that a sample path of a stochastic process is either fBm or an alternative process. As a typical example, when analyzing empirical data, one may suspect that the sample paths exhibit fBm behavior for some time periods, but then exhibit a different behavior for other time periods. It is therefore of interest to have a statistical test for determining whether a sample path of a stochastic process is fBm or an alternative. Such a test should be reliable (have accurate size control) and have power approaching one. The test that is most closely related to the present work was proposed in  5 . There, the hypothesis that a sample path of a stochastic process is fBm was tested using the maximum likelihood estimator (MLE) of the Hurst parameter, and the LSEE was used to approximate the sample path. Under some regularity conditions, it was shown that the test had correct size and that its power approached one. The primary drawback of this approach is that the regularity conditions were very restrictive, and it was therefore difficult to extend this approach to more general settings. In this work, we propose an improved test for testing the hypothesis that a sample path of a stochastic process is fB",
        "watermark_text": "In this paper , we propose a statistical method for evaluating whether one sample path of a Wiener process is strictly double - sidededly continuous with regard to the previsible filtration . Under some conditions , we prove that the test statistic converges in distribution to the Tracy - Widom distribution , which is well famous in the literature of random matrix theory .An application to a fractional Brownian movement with Hurst constant H < 0 . 5 is given to demonstrate the suggested method . Our proposed study is based on the maximum likelihood estimator of the Hurst parameter , and the LSEE for the fundamental diffusion system is calculated .An exponential inequality for the LSEE is also established . Numerical surveys demonstrate that the empirical size of the test is well controlled and the power is next to one under various scenarios .A real data example is also provided to test the applicability of the suggested method . Click here to access the paper on arXiv https : / / arxiv . org / abs / 2004 . 00965 Zhenyu Bai Zheng Liu June 2023 Version : 1 . 0 Discrete - time fractional Brownian movement and its long - time response As one of the classical processes , fractional Brownian movement ( fBm ) has found several uses in different areas .For instance , it was seen that particular physical structures , such as earthquakes and breeze gusts , exhibit long - range power - law correlations that are comparable in form to fBm . As a consequence , methods for testing for fBm have discovered using in a broad variety of fields , notably financial market analysis , human action analysis , astrophysics , and geophysics .On the other hand , there are many scenarios in which it is desired to test the assumption that a sample path of a stochastic mechanism is either fBm or an alternative process . As a typical example , when examining empirical data , one may suspect that the sample paths show fBm behavior for some time periods , but then exhibit a different activity for other time periods .It is consequently of interest to have a statistical check for determining whether a sample path of a stochastic process is fBm or an alternative . Such a measurement should be reliable ( have accurate size regulation ) and have power approaching one .The test that is most closely related to the present work was suggested in 5 . There , the assumption that a sample path of a stochastic process is fBm was tested using the maximum likelihood estimator ( MLE ) of the Hurst parameter , and the LSEE was used to approximate the sample path .Under some regularity situations , it was shown that the test had correct size and that its capacity approached one . The main drawback of this methodology is that the regularity situations were very restrictive , and it was thus difficult to apply this methodology to more general environments .In this research , we propose an better study for checking the hypothesis that a sample path of a stochastic process is fB",
        "rewrite_text": "In this study, we present a statistical approach to assess whether a specific sample path of a Wiener process is strictly double-sidedly continuous in relation to the previsible filtration. Certain conditions are met, we demonstrate that the test statistic converges in distribution to the Tracy-Widom distribution, which is widely recognized in the literature of random matrix theory.\n\nTo illustrate the proposed method, we provide an application to a fractional Brownian motion with a Hurst constant H less than 0.5. Our research is based on the maximum likelihood estimator of the Hurst parameter and calculates the LSEE for the fundamental diffusion system. Furthermore, we establish an exponential inequality for the LSEE.\n\nNumerical simulations indicate that the empirical size of the test is well controlled and its power approaches one in various scenarios. To demonstrate the applicability of our method, we provide a real data example. Click here to access the paper on arXiv: https://arxiv.org/abs/2004.00965\n\nZhenyu Bai, Zheng Liu June 2023 Version: 1.0\n\nDiscrete-time fractional Brownian motion and its long-term response serve as a classical process with multiple applications in various fields. For instance, certain physical structures, such as earthquakes and wind gusts, exhibit long-range power-law correlations that resemble fractional Brownian motion. Consequently, methods for testing fractional Brownian motion have found their way into a wide range of disciplines, including financial market analysis, human behavior analysis, astrophysics, and geophysics.\n\nIn various scenarios, it is essential to test whether a sample path of a stochastic mechanism follows fractional Brownian motion or an alternative process. As a typical example, when analyzing empirical data, one may suspect that sample paths exhibit fractional Brownian behavior during certain periods but demonstrate different activity during others. Therefore, it is crucial to have a reliable statistical test to determine whether a sample path of a stochastic process is fractional Brownian motion or an alternative process. Such a test should be accurate in size regulation and have a high power value.\n\nThe test most closely related to our current work was proposed in a previous study. There, the assumption of fractional Brownian motion for a sample path of a stochastic process was tested using the maximum likelihood estimator (MLE) of the Hurst parameter, and the LSEE was utilized to approximate the sample path. Under certain regularity conditions, it was shown that the test had an accurate size and approached a power value of one. However, the main drawback of this methodology was that the regularity conditions were highly restrictive, making it challenging to apply this approach in more general settings.\n\nIn this research, we propose an enhanced study to examine the hypothesis that a sample path of a stochastic process could be fractional Brownian motion.",
        "ori-fast-z-score": 0.1421338109037403,
        "water-fast-z-score": 8.763387148512887,
        "rewrite-fast-z-score": 1.0476454436543672
    },
    {
        "original_text": "The Kohn-Sham (KS) scheme is one of the most important and widely used methods in theoretical chemistry and quantum physics to solve the Schrödinger equation for the system with Coulomb potential. The efficiency of KS scheme relies on the choice of the approximate electron-electron interaction. For strong interaction, one has to use more elaborate KS-potential, which is numerically expensive. One of the efficient and low-cost approximations for the KS potential is the so-called exact exchange (EX) potential. This potential is constructed as a functional derivative of the exact exchange energy with respect to the electron density. The density obtained from the KS equations with the EX potential as a part of the exchange-correlation (XC) potential is very close to the actual one. However, the KS equation with the EX potential as the KS-potential yields a very large gap in the excitation spectrum, which does not correspond to the experimentally observed excitability of the systems. This discrepancy is referred to as the paradox of the correlated electrons. It was shown by Vydrov and Scuseria in 2008 that the gap in the excitation spectrum of the KS system with the EX potential can be made closer to the experimental one by introducing an artificial dependence of the potential on density, which they called the non-local dependence (NLD). In this work we show that the existence of NLD in KS potential is a consequence of the fact that it is obtained as a solution of the KS equations with the XC potential containing a non-local term. The non-local term appears due to the dependence of the Fock operator on the electron density. We derive the formula for the NLD term that takes this into account. We present the KS equations with the new XC potential and prove that the solutions of the equations coincide with those with the potentials used before and with the KS equation with the EX potential as the KS potential. This solution eliminates the gap in the excitation spectrum and reconciles the correlated electron systems described by the KS scheme with the Exact exchange potential and the Fock operator with the reasonable density dependence of the Coulomb potential.",
        "watermark_text": "The Kohn - Sham ( KS ) scheme is one of the most important and well used techniques in computational chemistry and quantum science to correct the Schrödinger equation for the system with Coulomb potential . The efficiency of KS scheme relies on the selection of the approximate electron - electron behavior .For strong coupling , one has to use more elaborate KS - potential , which is numerically expensive . One of the efficient and low - cost approximations for the KS potential is the so - called exact exchange ( EX ) potential .This potential is formed as a functional derivative of the exact exchange energy with regard to the electron density . The density derived from the KS coefficients with the EX potential as a portion of the transfer - correlation ( XC ) potential is very close to the actual one .However , the KS equation with the EX potential as the KS - potential yields a very huge gap in the excitation spectrum , which does not correspond to the experimentally seen excitability of the systems . This discrepancy is referred to as the paradox of the interacting electrons .It was shown by Vydrov and Scuseria in 2008 that the gap in the excitation spectrum of the KS system with the EX potential can be made nearer to the empirical one by using an synthetic dependence of the potential on density , which they called the non - local dependence ( NLD ) . In this research we prove that the existence of NLD in KS potential is a outcome of the fact that it is found as a solution of the KS equations with the XC potential containing a non - local term .The non - local term becomes due to the dependence of the Fock operator on the electron distribution . We derive the formula for the NLD term that takes this into consideration .We introduce the KS coefficients with the new XC potential and assume that the answers of the equations correspond with those with the potentials used before and with the KS equation with the EX potential as the KS potential . This solving eliminates the gap in the excitation spectrum and reconciles the interacting ion configurations presented by the KS scheme with the Exact exchange potential and the Fock operator with the reasonable density dependence of the Coulomb potential .",
        "rewrite_text": "The Kohn-Sham (KS) scheme is a pivotal and widely utilized technique in computational chemistry and quantum science. It serves to correct the Schrödinger equation for systems with Coulomb potential. The effectiveness of the KS scheme relies on the selection of an approximate electron-electron behavior. For systems with strong coupling, a more intricate KS potential is required, which comes at a numerically expensive cost. An efficient and cost-effective approximation for the KS potential is the so-called exact exchange (EX) potential. This potential is derived as a functional derivative of the exact exchange energy with respect to the electron density. When the density, derived from KS coefficients with the EX potential as a component of the transfer-correlation (XC) potential, closely resembles the actual one. However, utilizing the EX potential as the KS potential in the KS equation results in a significant gap in the excitation spectrum, which does not align with experimentally observed system excitability. This discrepancy is referred to as the paradox of interacting electrons.\n\nIn 2008, Vydrov and Scuseria demonstrated that by introducing a synthetic density-dependent relationship to the EX potential, known as the non-local dependence (NLD), the gap in the excitation spectrum of the KS system can be brought closer to empirical values. In this research, we establish that the presence of NLD in the KS potential arises from its solution to the KS equations with an XC potential containing a non-local term. This non-local term arises from the dependence of the Fock operator on the electron distribution. We derive a formula for the NLD term that accounts for this dependency. We introduce revised KS coefficients with the new XC potential, assuming that the solutions to the equations align with those obtained using previous potentials and with the KS equation using the EX potential as the KS potential. This approach eliminates the gap in the excitation spectrum and harmonizes the interacting ion configurations presented by the KS scheme with the Exact exchange potential and the Fock operator, while maintaining a reasonable density dependence on the Coulomb potential.",
        "ori-fast-z-score": -1.2632278815997784,
        "water-fast-z-score": 5.30555710271907,
        "rewrite-fast-z-score": 0.5734623443633283
    },
    {
        "original_text": "A resonating valence bond (RVB) state, which is a variational wavefunction for the quantum Heisenberg antiferromagnet, is proposed for the first time for the quantum disk, a plausible precursor of the familiar quantum Hall state at filling factor 1. The flux quantum in the quantum disk is treated as a perturbing parameter, and low-lying spin singlet excitations are identified. The triplet excitation spectrum is also calculated and found to be highly unstable against strong spin correlations. The ground state energy is computed in the sector with no triplet excitations, and it is shown to be lower than that of the conventional Néel state by a resonating valence bond solid (RVB) energy, α²q² - (α-q²)q, with α = 0.32 and q = 0.75. Furthermore, the susceptibility of this state to various perturbations is also computed and is shown to be very robust. The proposed resonating valence bond quantum disk is observed to have the correct low-energy behavior for all calculable quantities. The emergence of the resonating valence bond state from the conventional Néel state is analogous to the onset of superconductivity from a Fermi liquid, and there are some tantalizing similarities between these two transitions. Furthermore, there is now strong evidence that high-temperature superconductivity emerges from a near-Fermi-liquid state, and a spin-gap insulator with superconducting fluctuations may well describe the Mott insulator cuprate superconductors. In a similar manner, it may be that the quantum Hall state at ν=1 arises from a near-Fermi-liquid quantum disk and an associated resonating valence bond quantum disk is a natural subsequent phase. The intriguing idea of resonating valence bonds between Quantum Hall edges is already a fact, with Jain’s heterogenous electron gas as the most famous example. We predict a heterogenous valence bond solid of Quantum Hall edges, where half of the electrons remain unscreened.",
        "watermark_text": "A resonating valence bond ( RVB ) state , which is a variational wavefunction for the quantum Heisenberg antiferromagnet , is proposed for the first time for the quantum disk , a plausible precursor of the usual quantum Hall state at filling factor 1 . The flux quantum in the quantum disk is treated as a perturbing parameter , and low - lying spin singlet excitations are identified .The triplet excitation spectrum is also determined and found to be highly weak against strong spin correlations . The ground state energy is computed in the sector with no triplet excitations , and it is demonstrated to be less than that of the standard Néel state by a resonating valence bond solid ( RVB ) energy , α²q² - ( α - q² ) q , with α = 0 . 32 and q = 0 . 75 .Furthermore , the susceptibility of this state to several perturbations is also computed and is demonstrated to be very robust . The proposed resonating valence bond quantum disk is observed to have the appropriate low - energy properties for all calculable quantities .The emergence of the resonating valence bond state from the standard Néel state is analogous to the emergence of superconductivity from a Fermi solid , and there are some tantalizing similarities between these two transitions . Furthermore , there is now clear proof that high - temperature superconductivity arises from a near - Fermi - fluid state , and a spinning - gap insulator with superconducting fluctuations might well describe the Mott insulator cuprate superconductors .In a analogous way , it could be that the quantum Hall state at ν = 1 arises from a close - Fermi - fluid quantum disk and an associated resonating valence bond quantum disk is a natural subsequent phase . The intriguing thought of resonating valence bonds between Quantum Hall edges is already a fact , with Jain ’ s heterogenous electron gas as the most famous example .We predict a heterogenous valence bond solid of Quantum Hall edges , where half of the electrons stay unscreened .",
        "rewrite_text": "For the first time, a resonating valence bond (RVB) state, serving as a variational wavefunction for the quantum Heisenberg antiferromagnet, has been proposed for the quantum disk. This state is considered as a possible precursor to the typical quantum Hall state at a filling factor of 1. The quantum flux in the disk is treated as a perturbing parameter, enabling the identification of low-lying spin-singlet excitations. Additionally, the triplet excitation spectrum is determined to be highly subdued by strong spin correlations.\n\nThe ground-state energy is computed within the sector lacking triplet excitations and is found to be less than that of the standard Néel state by an RVB energy formula: α²q² - (α - q²)q, with α set to 0.32 and q to 0.75. Moreover, the state's susceptibility to various perturbations has been calculated and shown to be highly resilient.\n\nThe proposed RVB quantum disk exhibits suitable low-energy characteristics for all measurable properties. The emergence of the RVB state from the Néel state mirrors the transition to superconductivity in a Fermi solid, sharing tantalizing similarities between these two transitions. Importantly, there is now conclusive evidence that high-temperature superconductivity arises from a near-Fermi fluid state, with a spinning-gap insulator with superconducting fluctuations potentially describing Mott insulator cuprate superconductors.\n\nSimilarly, it is possible that the quantum Hall state at ν=1 originates from a close-to-Fermi fluid quantum disk. An associated RVB quantum disk represents a natural subsequent phase. The concept of resonating valence bonds between Quantum Hall edges is already a reality, with Jain's heterogeneous electron gas serving as a prominent example. We predict the existence of a heterogeneous valence bond solid of Quantum Hall edges, where half of the electrons remain unscreened.",
        "ori-fast-z-score": 1.5583874449479593,
        "water-fast-z-score": 6.141879930089016,
        "rewrite-fast-z-score": 1.6570343122169822
    },
    {
        "original_text": "Asteroseismology has proven to be a powerful tool for determining the properties of stars. The observed frequency spectrum of a star is a characteristic of its interior: the stellar mass, the arrangement and composition of its interior pieces, and the possible presence of rotation. The rotational splitting of starcoseismic modes can be used to detect the presence of surface rotation and thus map the differential rotation of the star. Here, we report seismology of the roAp star alpha Centauri A using observations spanning 4.5 years. We detect two triplet starcoseismic modes. From this we are able to measure the differential rotation of alpha Cen A and conclude that it has a complex structure, with a pole-to-equator gradient in the equator-on component of differential rotation of nearly 3%. This is the first detection of differential rotation in a binary system and demonstrates that stars are not zombies following rigid, global rotation.",
        "watermark_text": "Asteroseismology has proven to be a powerful tool for determining the properties of stars . The observed frequency spectrum of a star is a trait of its interior : the stellar mass , the organization and configuration of its interior pieces , and the possible presence of rotation .The rotational separation of starcoseismic modes can be used to identify the presence of exterior rotation and therefore mapping the differential rotation of the star . Here , we study seismology of the roAp star alpha Centauri A using observations spanning 4 . 5 years .We detect two triplet starcoseismic modes . From this we are able to measure the differential rotation of α Cen A and find that it has a complex shape , with a pole - to - equator gradient in the equator - on component of differential rotation of almost 3 % .This is the first measurement of differential rotation in a binary system and demonstrates that stars are not zombies following rigid , global rotation .",
        "rewrite_text": "Asteroseismology has emerged as a highly effective means to determine the properties of stars. The observed frequency spectrum of a star serves as a diagnostic of its internal characteristics, revealing aspects such as stellar mass, the structure and organization of its internal components, and the possible presence of rotational dynamics. The separation of starcoseismic modes due to rotation can be utilized to identify the presence of external rotational patterns, thereby enabling the mapping of differential rotation on the star. In this study, we explore the seismological analysis of the roAp star alpha Centauri A based on observations spanning over 4.5 years. We have detected two triplet starcoseismic modes, which have enabled us to measure the differential rotation of α Cen A. The results indicate a complex shape, with a notable pole-to-equator gradient in the equator-on component of differential rotation, nearly reaching 3%. This is the first measurement of differential rotation in a binary system, demonstrating that stars do not undergo rigid, global rotation but exhibit more complex dynamics.",
        "ori-fast-z-score": 1.4342743312012722,
        "water-fast-z-score": 4.85071250072666,
        "rewrite-fast-z-score": 1.865992419824736
    },
    {
        "original_text": "Zero-temperature phase of the XY spin glass in two dimensions: Genetic embedded matching heuristic April 20, 2023 In two dimensions, we find a zero-temperature phase of the two-dimensional (2D) XY spin glass that exhibits a spontaneous rotational symmetry breaking and concomitant long-range order in the chirality degree of freedom, namely, the direction of the net chiralities of the spins in each small volume of the system. This zero-temperature phase is characterized by a non-zero value of the chirality Edwards-Anderson order parameter θ_{EA}, as well as a non-zero amplitude A_{chiral} of the chiral spiraling correlations, which decay algebraically with distance. Both θ_{EA} and A_{chiral} are computed via efficient genetic algorithms, embedded within the standard replica-exchange Monte Carlo algorithm. Our findings suggest the possible existence of a discontinuous phase transition to a disorder-dominated phase at nonzero temperature. The direct calculation of θ_{EA} and A_{chiral} has not been possible so far in two dimensions, owing to the extreme computational cost of existing exact techniques. Our genetic algorithms yield the first estimates of θ_{EA} and A_{chiral}, and of the chiral glass exponent z=2.21(4). We expect that similar techniques will allow us to prove the existence of this phase and of its associated critical exponents, as well as of its continuum limit, in the general case of d dimensions and general lattice models of correlated electrons. The chirality is a discrete, global degree of freedom that has not been taken into account so far in spin glass models. It is a (net) rotation angle of the spin structure (e.g. left or right spin). As the spins are coupled in pairs in the standard XY model, it is natural to consider the chirality of each spin separately. Chirality appears as a local order parameter in models with discrete degrees of freedom. In equilibrium statistical physics, the chirality is a useful tool to detect spin glass order. For classical spin glasses, the chirality behaves in many ways as an ordinary spin (in a random Ising magnet, the chirality only distinguishes up and down spins). The XY spin glass is a classical, random spin model. The chirality can also be seen as a discrete, global order parameter in XY models. In continuum models, its detection via usual equilibrium tools (e.g. correlation functions) is impossible because of the critical character of the transition: long range order is absent at criticality. However, we found that, in the XY model on a lattice, chirality spontaneously appears at zero temperature, with very good accuracy. It would be interesting to prove this zero temperature chirality in the continuum limit. The chirality degree of freedom appears also in certain low temperature phases of higher dimensions and different lattice models (e.g. Heisenberg and Potts models). The",
        "watermark_text": "Zero - temperature phase of the XY spin glass in two dimensions : Genetic embedded matching heuristic April 20 , 2023 In two dimensions , we find a zero - temperature phase of the two - dimensional ( 2D ) XY spin glass that exhibits a spontaneous rotational symmetry breaking and concomitant extended - range order in the chirality degree of liberty , namely , the direction of the net chiralities of the spins in each tiny volume of the system . This zero - temperature phase is characterized by a non - zero value of the chirality Edwards - Anderson order parameter θ _ { EA } , as well as a non - zero amplitude A _ { chiral } of the chiral spiraling correlations , which decay algebraically with distance .Both θ _ { EA } and A _ { chiral } are computed via efficient genetic methods , embedded within the standard replica - transfer Monte Carlo algorithm . Our findings show the suggested existence of a discontinuous phase shift to a disease - dominated phase at nonzero temperature .The direct calculation of θ _ { EA } and A _ { chiral } has not been possible so far in two dimensions , owing to the high computational expensive of older exact procedures . Our genetic methods yield the first calculations of θ _ { EA } and A _ { chiral } , and of the chiral glass exponent z = 2 . 21 ( 4 ) .We predict that related techniques will able us to test the existence of this phase and of its associated essential exponents , as well as of its continuum limit , in the general case of d dimensions and general lattice models of coupled electrons . The chirality is a discrete , global degree of liberty that has not been took into consideration so far in spin mirror models .It is a ( net ) rotation angle of the spin geometry ( e . g . left or left spin ) .As the spins are coupled in couples in the standard XY model , it is common to consider the chirality of each spin separately . Chirality appears as a local order parameter in theories with discrete degrees of freedom .In optimal statistical mechanics , the chirality is a helpful tool to identify spin glass order . For classical spin glasses , the chirality behaves in variety manner as an everyday spin ( in a random Ising magnet , the chirality only distinguishes up and down spins ) .The XY spin glass is a classical , random spin theory . The chirality can also be viewed as a finite , global order parameter in XY models .In continuum models , its recognition via usual equilibrium tools ( e . g . correlation functions ) is useless because of the critical character of the transition : long distance order is lacking at criticality .However , we proved that , in the XY model on a lattice , chirality spontaneously emerges at zero temperature , with very high sensitivity . It would be nice to confirm this zero temperature chirality in the continuum limit .The chirality extent of liberty exists additionally in different small temperature phases of higher dimensions and varying lattice models ( e . g . Heisenberg and Potts theories ) .The",
        "rewrite_text": "The Phase of Zero Temperature in the XY Spin Glass in Two Dimensions: Genetic Embedded Matching Heuristic, April 20th, 2023\n\nIn two-dimensional space, we have discovered a zero-temperature phase of the 2D XY spin glass that demonstrates spontaneous rotational symmetry breaking and concomitant extended-range order in the chirality degree of freedom. This refers to the directionality of the net chiralities of spins within each tiny volume of the system. This zero-temperature phase is characterized by a non-zero value of the chirality Edwards-Anderson order parameter θ_EA, as well as a non-zero amplitude A_chiral of the chiral spiraling correlations, which decay algebraically with distance. Both θ_EA and A_chiral have been computed efficiently using genetic methods integrated into the standard replica-transfer Monte Carlo algorithm.\n\nOur findings suggest the existence of a discontinuous phase transition into a temperature-dominated phase at non-zero temperatures. The direct calculation of θ_EA and A_chiral has not been feasible in two dimensions due to the high computational cost of older exact methods. However, our genetic methods provide the first calculations of θ_EA, A_chiral, and the chiral glass exponent z = 2.21 (4). We predict that with related techniques, we can test the existence of this phase and its associated critical exponents, as well as its continuum limit, in the general case of d dimensions and various lattice models of coupled electrons.\n\nChirality, a discrete, global degree of freedom, has not been previously considered in spin mirror models. It represents a (net) rotation angle of the spin geometry, such as a left or right spin. In the standard XY model, where spins are coupled in pairs, it is common to consider the chirality of each spin individually. Chirality emerges as a local order parameter in theories with discrete degrees of freedom. In optimal statistical mechanics, chirality is a useful tool for identifying spin glass order. For classical spin glasses, chirality behaves in various ways compared to conventional spins (in a random Ising magnet, chirality only distinguishes between up and down spins). The XY spin glass is a classical, random spin theory, and chirality can also be viewed as a finite, global order parameter in XY models.\n\nIn continuum models, conventional equilibrium tools such as correlation functions are ineffective for recognizing chirality due to the critical nature of the transition; long-distance order is absent at criticality. Nevertheless, we have demonstrated that in the XY model on a lattice, chirality spontaneously emerges at zero temperature with high sensitivity. It would be beneficial to confirm this zero-temperature chirality in the continuum limit. Additionally, the extent of chirality freedom exists in different low-temperature phases of higher dimensions and varying lattice models, such as Heisenberg and Potts theories.",
        "ori-fast-z-score": -1.0277830647412975,
        "water-fast-z-score": 7.090812425461597,
        "rewrite-fast-z-score": 4.1012193308819755
    },
    {
        "original_text": "Noise can drastically change the behavior of a system. For example, in a system with feedback, a weak noise can induce instability, while a strong noise can promote stability. Here we report the experimental observation of a third regime: the suppression of growth by multiplicative white noise. We demonstrate that the addition of infinitesimal amplitude noise to a damped driven nonlinear system can stabilize the system. By performing direct numerical simulations and theoretical analysis, we identify this unusual phenomenon to be associated with the parametric resonance. Our work highlights the different noise strategies that can be employed to stabilize a dynamical system and could have potential implications for the stabilization of real world systems. In this study, we experimentally observe that the addition of infinitesimal amplitude noise to a damped driven nonlinear system can stabilize the system. We demonstrate that the addition of infinitesimal amplitude noise can suppress the growth of zero-mean oscillations in a finite range of frequencies. This suppression of growth is quantified by using the mean-square amplitude of the oscillations. The addition of infinitesimal amplitude noise can stabilize the system. This unusual phenomenon is highlighted in our work by using direct numerical simulations and theoretical analysis. The addition of infinitesimal amplitude noise can suppress the growth of zero-mean oscillations in a finite range of frequencies. This suppression of growth is quantified by using the mean-square amplitude of the oscillations. This unusual phenomenon is highlighted in our work by using direct numerical simulations and theoretical analysis. The addition of infinitesimal amplitude noise can stabilize the system.",
        "watermark_text": "Noise can drastically shift the dynamics of a system . For instance , in a system with feedback , a weak noise can induce instability , while a strong noise can encourage stability .Here we note the empirical observation of a third regime : the suppression of growth by multiplicative white sound . We suggest that the adding of infinitesimal intensity sound to a damped driven nonlinear system can stabilize the system .By conducting continuous mathematical simulations and theoretical analysis , we identify this uncommon phenomenon to be identified with the parametric resonance . Our research shows the different noise strategies that can be employed to stabilize a dynamical system and could have potential significance for the stabilization of real life systems .In this study , we experimentally learn that the adding of infinitesimal intensity sound to a damped driven nonlinear system can stabilize the system . We see that the adding of infinitesimal intensity sound can suppress the development of zero - mean oscillations in a finite variety of frequencies .This suppression of growth is quantified by using the mean - square amplitude of the oscillations . The addition of infinitesimal intensity sound can stabilize the system .This unusual phenomenon is highlighted in our work by using direct numerical simulations and theoretical analysis . The addition of infinitesimal amplitude sound can suppress the development of zero - mean oscillations in a finite variety of frequencies .This suppression of growth is quantified by using the mean - square amplitude of the oscillations . This unusual phenomenon is highlighted in our work by using direct numerical simulations and theoretical analysis .The addition of infinitesimal amplitude sound can stabilize the process .",
        "rewrite_text": "The dynamics of a system can be significantly altered by noise. For instance, in a feedback-based system, a slight noise can lead to instability, whereas a stronger noise can promote stability. In our research, we observe an empirical third regime: the suppression of growth through the introduction of multiplicative white sound. We propose that introducing sound with infinitesimal intensity into a damped, driven nonlinear system can stabilize it.\n\nThrough continuous mathematical simulations and theoretical analysis, we identify this unique phenomenon with parametric resonance. Our findings demonstrate various noise strategies that can be used to stabilize a dynamic system, potentially leading to significant implications for stabilizing real-world systems.\n\nIn this study, we experimentally discover that the introduction of sound with infinitesimal intensity into a damped, driven nonlinear system can indeed stabilize it. We observe that this sound can effectively suppress the development of zero-mean oscillations across a range of frequencies. This growth suppression is quantified using the mean-square amplitude of the oscillations.\n\nFurthermore, our work highlights this remarkable phenomenon through direct numerical simulations and theoretical analysis. The introduction of sound with infinitesimal amplitude can also suppress the emergence of zero-mean oscillations in various frequencies. This phenomenon is also emphasized in our study using direct numerical simulations and theoretical analysis. As a result, introducing sound with infinitesimal amplitude can effectively stabilize the process.",
        "ori-fast-z-score": 0.8250286473253902,
        "water-fast-z-score": 7.302967433402215,
        "rewrite-fast-z-score": 2.942389786832747
    },
    {
        "original_text": "In this paper, we propose a physically motivated approach to generating entangled graph states with spin chains that undergo Gaussian noise and decay. The noise and decay allow us to engineer various strengths of interactions, topologies of the underlying graphs, and fidelity of our desired target states. We show how to efficiently simulate the spin chain using a classical Ising model and develop an analog Hamiltonian engine to generate target states with high fidelity. We numerically optimize our strategy and show that by harnessing dissipation, thermal noise, and imperfect gates, we can produce highly entangled graph states in a probabilistic manner with fidelities above 0.97. These results provide a path to generating high-fidelity graph states with dissipative spin-chain engineering, a technology that is readily implementable with current experimental capabilities. This paper presents a physically motivated approach to generating entangled graph states with spin chains that undergo Gaussian noise and decay. We first show how to efficiently simulate these spin chains using a classical Ising model. We then develop an analog Hamiltonian engine to generate target states with high fidelity. We numerically optimize our strategy and show that by harnessing dissipation, thermal noise, and imperfect gates, we can produce highly entangled graph states in a probabilistic manner with fidelities above 0.97.",
        "watermark_text": "In this paper , we propose a physically motivated method to generating entangled graph states with spin networks that undergo Gaussian sound and decay . The noise and decay allow us to engineer different strengths of interactions , topologies of the underlying graphs , and fidelity of our intended target states .We see how to easily simulate the spin network utilizing a traditional Ising model and develop an analog Hamiltonian motor to create target states with high fidelity . We numerically optimize our approach and suggest that by harnessing dissipation , thermal noise , and imperfect gates , we can generate strongly entangled graph states in a probabilistic manner with fidelities above 0 . 97 .These data provide a path to generating high - fidelity graph states with dissipative spin - chain technology , a technique that is readily implementable with current experimental capabilities . This paper offers a physically motivated method to generating entangled graph states with spin chains that suffer Gaussian distortion and decay .We first learn how to easily simulate these spin networks using a traditional Ising model . We then develop an analog Hamiltonian generator to create target states with high fidelity .We numerically optimize our approach and suggest that by harnessing dissipation , thermal noise , and imperfect gates , we can generate strongly entangled graph states in a probabilistic manner with fidelities above 0 . 97 .",
        "rewrite_text": "In this study, we propose a method that utilizes physical principles to generate entangled graph states with spin networks that experience Gaussian noise and decay. The presence of noise and decay enables us to tailor various interaction strengths, underlying graph topologies, and the fidelity of our intended target states. We demonstrate how to effortlessly simulate the spin network using a traditional Ising model and develop a corresponding Hamiltonian engine to produce target states with high fidelity. Through numerical optimization, we suggest that by harnessing dissipation, thermal noise, and imperfect gates, we can probabilistically generate strongly entangled graph states with fidelities exceeding 0.97.\n\nThese findings provide a path forward for generating high-fidelity graph states using dissipative spin-chain technology, which is a viable technique with current experimental capabilities. This paper offers a practical approach for generating entangled graph states with spin chains that experience Gaussian distortions and decay. We begin by learning how to conveniently model these spin networks using a traditional Ising model. Subsequently, we develop an analog Hamiltonian generator to create target states with high accuracy. Our numerically optimized approach suggests that by effectively managing dissipation, thermal noise, and imperfect gates, we can produce strongly entangled graph states in a probabilistic manner with fidelities exceeding the threshold of 0.97.",
        "ori-fast-z-score": -1.9802950859533488,
        "water-fast-z-score": 4.554678697692702,
        "rewrite-fast-z-score": 2.203000456008648
    },
    {
        "original_text": "Graphene, a single layer of carbon atoms in a hexagonal lattice, is one of the most exciting two-dimensional materials. Its electronic band structure presents two points of non-dispersibility, the so-called Dirac points, which are directly related to its intriguing transport properties. Recently, this material has attracted a great deal of attention due to its response to strong magnetic fields, which is reminiscent of the Aharonov-Bohm (AB) effect. In particular, the transmission coefficient of charge carriers in graphene rings exposed to a uniform magnetic field exhibits a step-like dependence as a function of the Fermi energy, which is a characteristic signature of the AB effect. Similar behavior has also been observed in other carbon allotropes such as bilayer graphene and boron nitride. Despite its potential interest, to date, there have been few rigorous works exploring the response of graphene to a nonuniform magnetic field. In this paper, we show that a magnetic flux generated by a solenoid with a large radius, compared to the magnetic length, leads to a strongly deformed band structure in graphene, without splitting the Dirac points. Furthermore, when the radius of the solenoid is comparable to the magnetic length, new degrees of freedom, namely, the so-called topological valley, emerge. We show that the conductance spectrum is completely different from the AB effect, even in the limit of a strong magnetic field. Finally, we extend our findings to propose a realistic device that implements the AB effect in graphene.",
        "watermark_text": "Graphene , a single layer of carbon atoms in a hexagonal lattice , is one of the most exciting two - dimensional materials . Its optical band structure offers two points of non - dispersibility , the so - called Dirac points , which are directly related to its intriguing transport properties .Recently , this metal has garnered a considerable deal of awareness due to its response to powerful magnetic fields , which is reminiscent of the Aharonov - Bohm ( AB ) effect . In particular , the propagation constant of charge carriers in graphene belts attracted to a uniform magnetic force exhibits a step - like dependence as a function of the Fermi energy , which is a typical signature of the AB effect .Similar behavior has also been observed in other carbon allotropes such as bilayer graphene and boron nitride . Despite its potential interest , to date , there have been few rigorous studies investigating the response of graphene to a nonuniform magnetic force .In this paper , we prove that a magnetic flux produced by a solenoid with a large radius , compared to the magnetic width , leads to a strongly deformed band structure in graphene , without dividing the Dirac points . Furthermore , when the radius of the solenoid is equal to the magnetic width , new degrees of liberty , namely , the so - called topological region , appear .We see that the conductance spectrum is completely different from the AB effect , even in the limit of a weak magnetic force . Finally , we stretch our findings to propose a realistic device that implements the AB effect in graphene .",
        "rewrite_text": "Graphene, consisting of a single layer of carbon atoms arranged in a hexagonal lattice, is among the most intriguing two-dimensional materials. Its optical band structure features two points of non-dispersibility, known as Dirac points, which are closely linked to its fascinating transport properties. Recently, this material has gained significant attention due to its response to powerful magnetic fields, reminiscent of the Aharonov-Bohm (AB) effect.\n\nIn particular, the propagation constant of charge carriers within graphene sheets, when subjected to a uniform magnetic force, demonstrates a step-like dependence on the Fermi energy - a hallmark of the AB effect. A similar behavior has also been observed in other carbon allotropes, such as bilayer graphene and boron nitride. However, despite its potential significance, there have been few rigorous investigations exploring graphene's response to nonuniform magnetic forces.\n\nIn this paper, we establish that a magnetic flux generated by a solenoid with a large radius in comparison to the magnetic width results in a significantly deformed band structure in graphene, without splitting the Dirac points. Furthermore, when the radius of the solenoid is equivalent to the magnetic width, novel degrees of freedom, referred to as the topological region, emerge.\n\nOur findings reveal that the conductance spectrum differs significantly from the AB effect, even in the presence of a weak magnetic force. Ultimately, we extend our research to propose a practical device that can realize the AB effect in graphene.",
        "ori-fast-z-score": 0.4745789978762495,
        "water-fast-z-score": 5.480484858633795,
        "rewrite-fast-z-score": 2.777696227141339
    },
    {
        "original_text": "We show that the Bohmian mechanics of a quantum system whose wavefunction is entangled admits local de Broglie-Bohm trajectories in the limit of an infinitesimal wavelength. More precisely, we show that the Bohmian position of a test particle is a local constant of the motion for any wavefunction that is a nontrivial entangled discrete or continuous superposition of product states. We give an explicit formula for this local de Broglie-Bohm wave in terms of the original wavefunction, and we apply our result to delocalized solutions of the one-dimensional quantum many-body dynamical Yang-Lee model, which includes the 1D Bose gas, the 1D hard-core gas, and the spin 1/2 XX model. We establish this result by first considering a discretized version of the original wavefunction, in which the original wavefunction is a nontrivial entangled superposition of product states. We then show that a similar result holds for the continuous limit of this discretized wavefunction. The proof proceeds by showing that the Bohmian position is a constant of the motion for any wavefunction that is a product state of the original wavefunction and its complex conjugate. We show that the real and imaginary parts of the continuous limit of the discretized wavefunction each have this property, and then prove the result by proving that the real and imaginary parts of the original wavefunction satisfy this property separately. We also show that the continuous limit of the discretized wavefunction converges to the original wavefunction in the limit of an infinitesimal wavepacket. The proof also gives us an explicit formula for the local de Broglie-Bohm wave in terms of the original wavefunction. We demonstrate the versatility of this formula by applying it to localize delocalized solutions of the dynamical Yang-Lee model. Our result generalizes a recent proof of the existence of local de Broglie-Bohm trajectories for single qubits that are in a product state of eigenstates of the z-axis component of the spin operator and its adjoint operator. We show that this result extends to the full quantum many-body dynamical Yang-Lee model, which cannot be solved by existing numerical techniques. Our result has implications for experiments with quantum systems. For example, we show that it may be possible to measure local de Broglie-Bohm trajectories for entangled many-body quantum systems using imaging techniques such as electron microscopy and light microscopy in combination with conventional Bohmian quantum mechanics experiments. We also suggest an application to quantum magnetometry. We show that the spatial structure of the Bohmian local de Broglie-Bohm wave reveals information about the local values of the angle of the total spin in the dynamical Yang-Lee model, which may be used to image this angle. We also discuss an application to quantum computation. We show that if the total wavefunction of a quantum computer algorithm is a nontrivial entangled superposition of",
        "watermark_text": "We see that the Bohmian physics of a quantum system whose wavefunction is entangled admits local de Broglie - Bohm trajectories in the limit of an infinitesimal wavelength . More specifically , we prove that the Bohmian position of a test particle is a local constant of the movement for any wavefunction that is a nontrivial entangled continuous or constant superposition of product states .We get an explicit formula for this local de Broglie - Bohm wave in terms of the original wavefunction , and we apply our consequence to delocalized solutions of the one - dimensional quantum several - bodies dynamical Yang - Lee model , which includes the 1D Bose gas , the 1D hard - core gas , and the spin 1 / 2 XX theory . We establish this result by first considering a discretized version of the original wavefunction , in which the original wavefunction is a nontrivial entangled superposition of product states .We then show that a analogous result holds for the continuous limit of this discretized wavefunction . The proof proceeds by showing that the Bohmian position is a constant of the movement for any wavefunction that is a product state of the actual wavefunction and its complex conjugate .We see that the real and imaginary parts of the discrete limit of the discretized wavefunction each have this property , and then confirm the result by proving that the real and imaginary parts of the original wavefunction satisfy this property separately . We additionally prove that the discrete limit of the discretized wavefunction converges to the original wavefunction in the limit of an infinitesimal wavepacket .The proof also provides us an explicit formula for the local de Broglie - Bohm wave in terms of the actual wavefunction . We test the versatility of this formula by using it to localize delocalized solutions of the dynamical Yang - Lee model .Our result generalizes a recent proof of the existence of local de Broglie - Bohm trajectories for single qubits that are in a product state of eigenstates of the z - axis part of the spin operator and its adjoint operator . We see that this consequence extends to the full quantum several - bodies dynamical Yang - Lee model , which cannot be solved by existing numerical technology .Our result has implications for observations with quantum systems . For instance , we prove that it could be possible to measure local de Broglie - Bohm trajectories for entangled many - bodies quantum systems using optical techniques such as electron microscopy and light microscopy in combination with typical Bohmian quantum mechanics research .We also suggest an use to quantum magnetometry . We see that the spatial shape of the Bohmian local de Broglie - Bohm wave reveals information about the local values of the angle of the total spin in the dynamical Yang - Lee model , which may be used to map this angle .We also discuss an use to quantum computation . We see that if the total wavefunction of a quantum computer algorithm is a nontrivial entangled superposition of",
        "rewrite_text": "We observe that the Bohmian mechanics of a quantum system with an entangled wavefunction permits local de Broglie-Bohm trajectories in the limit of an infinitesimal wavelength. Specifically, we demonstrate that the Bohmian position of a test particle remains a constant of motion for any nontrivial entangled continuous or constant superposition of product states wavefunction. We derive an explicit formula for this local de Broglie-Bohm wave in terms of the original wavefunction.\n\nOur findings are applicable to delocalized solutions of the one-dimensional quantum many-body dynamical Yang-Lee model, encompassing the 1D Bose gas, the 1D hard-core gas, and the spin 1/2 XX theory. To establish this result, we first consider a discretized version of the original wavefunction, where it is composed of nontrivial entangled product states superpositions. We then show that a similar result holds for the continuous limit of this discretized wavefunction.\n\nThe proof proceeds by showing that the Bohmian position is a conserved property of motion for any wavefunction that is a product state of the actual wavefunction and its complex conjugate. We observe that both the real and imaginary parts of the discrete limit of the discretized wavefunction share this characteristic, which we confirm by separately proving that the real and imaginary parts of the original wavefunction satisfy this property.\n\nFurthermore, we prove that the discrete limit of the discretized wavefunction converges to the original wavefunction in the limit of an infinitesimal wavepacket. This proof provides us with an explicit formula for the local de Broglie-Bohm wave in terms of the actual wavefunction.\n\nTo demonstrate the versatility of this formula, we apply it to localize delocalized solutions of the dynamical Yang-Lee model. Our findings generalize recent proofs on the existence of local de Broglie-Bohm trajectories for single qubits in a product state of eigenstates related to the spin operator's z-axis part and its adjoint operator. This extends to the complete quantum many-body dynamical Yang-Lee model, which remains unsolvable by existing numerical techniques.\n\nOur research has implications for observations in quantum systems. For instance, we establish that it may be possible to measure local de Broglie-Bohm trajectories in entangled many-body quantum systems using optical techniques like electron microscopy and light microscopy, combined with traditional Bohmian quantum mechanics research. We also suggest applications in quantum magnetometry, where the spatial shape of the Bohmian de Broglie wave can reveal information about the local values of the total spin angle in the Yang-Lee model, potentially mapping this angle. Additionally, we discuss potential uses in quantum computation, where we note that if the total wavefunction of a quantum computer algorithm is a nontrivial entangled superposition, our findings could have significant implications for quantum computing applications as well.",
        "ori-fast-z-score": -0.36563621206356534,
        "water-fast-z-score": 6.654579059556888,
        "rewrite-fast-z-score": 2.0530596102989462
    },
    {
        "original_text": "Quasars are highly luminous point sources in the universe, primarily found in the nuclei of large galaxies. It is generally accepted that active galactic nucleis (AGN) are powered by material falling into a supermassive black hole at the center of the host galaxy. As material falls into the black hole, it generates bright lights called quasars. These quasars can be observed over a broad range of distances from the host galaxy, corresponding to various stages in the growth of the black hole, and are valuable tools for understanding how black holes and their host galaxies form and evolve. Using a large cluster catalog from the Sloan Digital Sky Survey data release 11, we select a sample of 43 quasars at a redshift of 0.3 < z < 2.6 that are located in the projected vicinity of a cluster, with a projected separation of less than 1 Mpc. We compare the radial distribution of these quasars to that of a control sample of 43 matched quasars at similar redshifts that are not in the vicinity of a cluster. We use a maximum likelihood method to fit for the mean separation between quasars and their host clusters, and find no significant radial offset between the two samples. We conclude that quasars do not preferentially reside in the centers of their clusters, and that any preference for quasars to be found near the centers of clusters is less than 5% at the 99% confidence level. These results are consistent with previous studies that have either examined significantly smaller samples of quasars, or have not separated quasars into central and satellite galaxies. Our results indicate that satellite quasars are not primarily being fueled by the hot gas in the clusters, but are instead forming their own massive reservoirs of cold gas, which are likely the primary source of fuel for future growth.",
        "watermark_text": "Quasars are extremely luminous point sources in the universe , primarily discovered in the nuclei of large galaxies . It is usually agreed that active galactic nucleis ( AGN ) are powered by material falling into a supermassive black hole at the center of the host universe .As material drops into the dark hole , it generates bright flashes called quasars . These quasars can be viewed over a broad variety of distances from the host galaxy , corresponding to several stages in the development of the dark hole , and are valuable tools for knowledge how white holes and their host galaxies form and evolve .Using a large cluster catalog from the Sloan Digital Sky Survey data release 11 , we choose a sample of 43 quasars at a redshift of 0 . 3 < z < 2 . 6 that are situated in the projected vicinity of a cluster , with a projected separation of fewer than 1 Mpc . We relate the radial distribution of these quasars to that of a control sample of 43 paired quasars at comparable redshifts that are not in the vicinity of a cluster .We use a maximum likelihood technique to fit for the mean separation between quasars and their host clusters , and find no considerable horizontal offset between the two specimens . We suggest that quasars do not preferentially reside in the centers of their clusters , and that any desire for quasars to be found near the centers of clusters is less than 5 % at the 99 % confidence rate .These conclusions are compatible with previous researchers that have either analyzed significantly smaller specimens of quasars , or have not differentiated quasars into central and satellite galaxies . Our results show that satellite quasars are not primarily being fueled by the cool gas in the clusters , but are merely creating their own huge reservoirs of cold energy , which are likely the primary source of gasoline for future development .",
        "rewrite_text": "Quasars are exceptionally bright point sources in the universe, primarily discovered within the cores of large galaxies. It is widely accepted that active galactic nuclei (AGN) are powered by material collapsing into a supermassive black hole at the center of the host galaxy. As this material plummets into the black hole, it generates brilliant flashes known as quasars. These quasars can be observed from a wide range of distances from the host galaxy, corresponding to various stages of the dark hole's development. They serve as valuable tools for understanding the formation and evolution of white holes and their host galaxies.\n\nUsing a large cluster catalog from the Sloan Digital Sky Survey's data release 11, we have selected a sample of 43 quasars with a redshift range of 0.3 < z < 2.6. These quasars are located in close proximity to a cluster, with a projected separation of less than 1 million light-years. We compare the radial distribution of these quasars to a control sample of 43 paired quasars at comparable redshifts but not in cluster proximity. Utilizing a maximum likelihood technique, we estimate the mean separation between quasars and their host clusters. Our findings reveal no significant horizontal disparities between the two samples. We propose that quasars do not preferentially reside in the centers of their clusters, and the likelihood of finding quasars near cluster centers is less than 5% with a 99% confidence level.\n\nThese conclusions align with previous studies that have analyzed smaller samples of quasars or have not distinguished between central and satellite galaxies hosting quasars. Our results indicate that satellite quasars are not primarily fueled by cool gas within clusters, but rather create their own vast reservoirs of cold energy, which likely serves as the primary fuel source for future development.",
        "ori-fast-z-score": -0.7016464154456235,
        "water-fast-z-score": 7.077002858954407,
        "rewrite-fast-z-score": 1.0366421106976322
    },
    {
        "original_text": "We introduce the dynamical discrete web, a framework that formalizes the idea that interactions between objects can be modeled as a web, where nodes represent objects and links model interactions between objects. We apply this framework to the study of networks of interacting items. We show how the discrete web, a discrete dynamical system, can be used to analyze networks of interacting items. We study random discrete webs and characterize their spectral properties. We apply spectral analysis to the study of various real world networks. We then study how local modifications to a discrete web can induce global dynamics on the underlying network. We define and analyze the local resonance and globalization principles. We conclude with a discussion of some open problems. The dynamical discrete web (DDW) framework formalizes the idea that interactions between objects can be modeled as a web, where nodes represent objects and links model interactions between objects. We apply this framework to the study of networks of interacting items. We study random discrete webs and characterize their spectral properties. We apply spectral analysis to the study of various real world networks. We then study how local modifications to a discrete web can induce global dynamics on the underlying network. We define and analyze the local resonance and globalization principles. We conclude with a discussion of some open problems. Some open problems for the DDW framework: 1. Develop effective algorithms to find/construct low dimensional discrete webs. 2. Develop efficient numerical algorithms to simulate discrete dynamical systems. 3. Develop and analyze conserved quantities for discrete dynamical systems. 4. Develop and analyze the global resonance and globalization principles. 5. Develop and analyze stochastic versions of the discrete web framework. 6. Develop and analyze effective methods to cluster and separate discrete webs. We define and analyze the local resonance and globalization principles. We show that local modifications that conserve local resonances are globalization mechanisms. We conclude with a discussion of some open problems for the DDW framework. Many networks of interacting items arise in physical, biological and social systems. These networks range in size from the web of human sexual contacts, the network of scientific collaborations, protein interaction networks, neural networks, food webs, ecological networks, word networks, power grids and many others. Recent developments in network science and complex systems have provided new insights into these networks. A fundamental framework for studying these networks is the complex network framework, where nodes represent objects and links model interactions between objects. This framework has been applied to study networks of scientific collaborations, protein interaction networks and neural networks. The dynamical systems point of view of networks has also proven to be powerful. In this framework, nodes and links are represented by dynamical systems. Links represent the interactions between the dynamical systems represented by the nodes. The interactions between links induce dynamics on the networks. We can study the local and global dynamics on networks from this point of view. This dynamical systems point of view has been used to study networks of human sexual contacts,",
        "watermark_text": "We introduce the dynamical discrete web , a framework that formalizes the idea that behaviors between objects can be described as a network , where vertices represent items and links model interactions between objects . We use this framework to the study of networks of interacting elements .We see how the discrete web , a discrete dynamical system , can be used to analyze networks of interacting elements . We study random continuous webs and characterize their spectral properties .We use spectral study to the study of several real life networks . We then explore how local modifications to a discrete web can induce global dynamics on the underlying network .We define and analyze the local resonance and globalization concepts . We continue with a debate of some open problems .The dynamical discrete web ( DDW ) approach formalizes the idea that behaviors between objects can be described as a network , where vertices represent items and links model interactions between objects . We application this framework to the study of networks of interacting elements .We explore random finite webs and characterize their spectral properties . We use spectral study to the study of several real life networks .We then research how local modifications to a discrete web can induce global dynamics on the underlying network . We define and analyze the local resonance and globalization concepts .We end with a debate of some open problems . Some open problems for the DDW framework : 1 .Develop effective methods to find / create low dimensional discrete webs . 2 .Develop effective numerical models to simulate discrete dynamical systems . 3 .Develop and solve conserved quantities for discrete dynamical systems . 4 .Develop and consider the global resonance and globalization concepts . 5 .Develop and analyze stochastic variants of the discrete web framework . 6 .Develop and consider effective methods to group and separate discrete webs . We define and understand the local resonance and globalization concepts .We suggest that regional modifications that conserve local resonances are globalization mechanisms . We continue with a review of some open problems for the DDW framework .Many networks of communicating items occur in physical , biological and social systems . These systems range in scale from the web of human female contacts , the network of research collaborations , protein communication networks , neural systems , food webs , ecological systems , word networks , electricity grids and many others .Recent developments in system studies and complex systems have provided new understanding into these networks . A basic framework for studying these networks is the complex network framework , where nodes describe objects and links model interactions between objects .This framework has been used to study networks of research collaborations , protein behavior connections and neural connections . The dynamical systems point of view of networks has also demonstrated to be powerful .In this framework, nodes and links are represented by dynamical systems.Links represent the interactions between the dynamical systems represented by the nodes.The relationships between connections induce dynamics on the connections . We can investigate the local and international dynamics on networks from this point of view .This dynamical systems point of view has been used to study networks of human sexual contacts ,",
        "rewrite_text": "We present the dynamical discrete web as a framework that formalizes the notion of describing behavioral interactions between objects as a network. In this framework, vertices symbolize entities, while links model the interactions between them. We employ this structure to investigate networks composed of interacting elements. The discrete web, seen as a discrete dynamical system, can be utilized to analyze such networks. We explore random continuous webs and delve into their spectral properties.\n\nSpectral analysis is applied to the study of several real-world networks. Furthermore, we investigate how local alterations in the discrete web can induce global changes in the underlying network. We define and examine the concepts of local resonance and globalization. Subsequently, we engage in a discussion regarding some unresolved issues.\n\nThe DDW approach standardizes the idea that object interactions can be portrayed as a network, with vertices as item representations and links as interactions between objects. We apply this framework to the study of networks with interactive elements. We investigate random finite webs and characterize their spectral properties. We also utilize spectral analysis to study various real-world networks.\n\nMoving forward, we research how minor modifications to the discrete web can trigger significant shifts in the network's dynamics. We introduce and analyze the concepts of local resonance and globalization. Finally, we conclude with a discussion on outstanding challenges within the DDW framework.\n\nSome open problems within the DDW framework are:\n\n1. Develop effective methods for identifying and creating low-dimensional discrete webs.\n2. Develop reliable numerical models for simulating discrete dynamical systems.\n3. Explore and solve conserved properties in discrete dynamical systems.\n4. Develop and consider global resonance and globalization concepts further.\n5. Explore stochastic variations of the discrete web framework and analyze them thoroughly.\n6. Develop effective methods for grouping and differentiating discrete webs.\n\nWe have defined and comprehended the notions of local resonance and globalization. It is suggested that local modifications that preserve resonances act as mechanisms for globalization. We continue with a review of some outstanding issues within the DDW framework.\n\nNumerous networks of communicating entities exist in physical, biological, and social systems, ranging from human social contact networks to protein communication networks and neural systems. Recent advancements in system studies and complex systems research have provided new insights into these networks. A fundamental framework for studying these networks is the complex network framework, where nodes represent objects and links model interactions between them. This framework has been utilized to explore research collaboration networks, protein behavior connections, and neural connections. The perspective of dynamical systems in networks has also proven its potency, where nodes and links are represented as dynamical systems with interactions between them leading to dynamic changes in the connections. We can investigate both local and global dynamics on networks from this perspective, making it a powerful tool for studying networks such as human sexual contact networks.",
        "ori-fast-z-score": 1.0954451150103324,
        "water-fast-z-score": 11.277906564189482,
        "rewrite-fast-z-score": 3.117795338581311
    },
    {
        "original_text": "The vibrational spectrum of nitrous oxide has been studied in solution, for the first time, using an infrared (IR) laser as the source of vibration. The OH and NH2 anti-solvent refrigeration technique was used to produce samples of nitrous oxide in diethyl ether solution. A range of data analysis methods were used to analyse the spectra, and it was found that the global minimum nitrous oxide conformer in solution has a gamma-proton resonance as a single hydrogen bond and a symmetric CH2O hydrogen bond between the two molecules. This is in contrast to the vapour, where the global minimum has two strong NH...O hydrogen bonds, leading to the resonance form. The reorganisation energy of the gamma-proton resonance in solution is 18 cm-1, very close to the value in the gas phase. The calculated am1g-frequencies for the two strongest NH...O and gamma-proton hydrogen bonds are 3650 and 3300 cm-1 respectively, and are found in the infrared spectrum at 3520 and 3220 cm-1 respectively. The calculated lifetime of the gamma-proton hydrogen bond in solution is 44 ps, and is found from the linewidth of the frequency range corresponding to this hydrogen bond, which is 10 cm-1. The linewidth is used to calculate the vibrational lifetime of the nitrous oxide solution molecules. The nitrous oxide solution has a weaker gamma-proton hydrogen bond than in the gas phase or other solution conformers, and this has been proposed to be one of the causes of the strong inversion, general anaesthesia, produced by inhalation of the solution. The gamma-proton hydrogen bond in solution has a lifetime which is less than that in the gas phase or other solution conformers, and this has implications for the theory of general anaesthesia produced by inhalation of nitrous oxide solution. The nitrous oxide spectrum in solution has been found to be in good agreement with previous gas phase studies, with a combination of methods for analysing IR spectra. The global minimum nitrous oxide solution conformer has a much lower energy than the observed gamma-proton resonance form in the gas phase, and this may be one of the reasons why general anaesthesia is only produced by inhalation of the gas form. The results could be relevant to understanding nitrous oxide general anaesthesia, and the differences could be used to characterise solvents and correlate with trends in anaesthetic potency.",
        "watermark_text": "The vibrational spectrum of nitrous oxide has been studied in solution , for the first time , using an infrared ( IR ) laser as the source of vibration . The OH and NH2 anti - solvent refrigeration procedure was used to produce samples of nitrous oxide in diethyl ether solution .A range of data analysis methods were used to analyse the spectra , and it was shown that the global minimum nitrous oxide conformer in solution has a beta - proton resonance as a single hydrogen bond and a symmetric CH2O hydrogen bond between the two compounds . This is in comparison to the vapour , where the global minimum has two positive NH . . . O hydrogen bonds , leading to the resonance shape .The reorganisation energy of the gamma - proton resonance in solution is 18 cm - 1 , very close to the value in the gas phase . The measured am1g - frequencies for the two strongest NH . . . O and gamma - proton oxygen bonds are 3650 and 3300 cm - 1 respectively , and are found in the infrared spectrum at 3520 and 3220 cm - 1 respectively .The measured life of the alpha - proton hydrogen bond in solution is 44 ps , and is found from the linewidth of the frequency range corresponding to this hydrogen bond , which is 10 centimeters - 1 . The linewidth is utilized to estimate the vibrational life of the nitrous oxide solution molecules .The nitrous oxide solution has a weaker gamma - proton hydrogen bond than in the gas phase or other solution conformers , and this has been proposed to be one of the causes of the strong inversion , general anaesthesia , created by inhalation of the solution . The gamma - proton oxygen bond in solution has a life which is less than that in the gas phase or other solution conformers , and this has implications for the principle of general anaesthesia produced by inhalation of nitrous oxide solution .The nitrous oxide spectrum in solution has been shown to be in good agreement with previous gas phase studies , with a combination of methods for analysing IR spectra . The global low nitrous oxide solution conformer has a far lower energy than the reported gamma - proton resonance shape in the gas phase , and this might be one of the explanations why general anaesthesia is only created by inhalation of the gas form .The results could be applicable to describing nitrous oxide general anaesthesia , and the differences might be used to characterise solvents and correlate with patterns in anaesthetic potency .",
        "rewrite_text": "For the first time, the vibrational spectrum of nitrous oxide has been explored in solution using an infrared (IR) laser as the driving force of vibration. The process of OH and NH2 anti-solvent refrigeration was employed to generate nitrous oxide samples in diethyl ether solution. A range of data analysis techniques were utilized to scrutinize the spectra, revealing that the global minimum nitrous oxide conformer in solution features a beta-proton resonance with a single hydrogen bond and a symmetric CH2O hydrogen bond between the two compounds. In contrast, the global minimum in the vapor phase involves two positive NH...O hydrogen bonds, leading to a specific resonance shape. The reorganization energy for the gamma-proton resonance in solution is 18 cm-1, closely aligning with its value in the gas phase.\n\nThe measured am1g-frequencies for the two strongest NH...O and gamma-proton oxygen bonds are 3650 and 3300 cm-1, respectively, with corresponding infrared spectrum peaks at 3520 and 3220 cm-1. The lifespan of the alpha-proton hydrogen bond in solution has been determined to be 44 ps, deduced from the linewidth of the frequency range associated with this hydrogen bond, which measures 10 cm-1. This linewidth aids in estimating the vibrational lifespan of nitrous oxide solution molecules.\n\nIt has been observed that the nitrous oxide solution exhibits a weaker gamma-proton hydrogen bond compared to its gas phase or other solution conformers. This is proposed as a contributing factor to the strong inversion and general anesthesia induced by inhaling the solution. Furthermore, the lifespan of the gamma-proton oxygen bond in solution is shorter than in the gas phase or other conformers, which has implications for the principle of general anesthesia produced by inhaling nitrous oxide solution.\n\nThe nitrous oxide spectrum in solution has been found to align well with previous gas phase studies, thanks to a combination of methods used to analyze IR spectra. The global low nitrous oxide solution conformer possesses significantly lower energy than the reported gamma-proton resonance shape in the gas phase. This may explain why general anesthesia is exclusively achieved through inhalation of the gas form. These findings could be instrumental in describing nitrous oxide-induced general anesthesia, and the identified differences could be used to characterize solvents and correlate with patterns in anesthetic potency.",
        "ori-fast-z-score": 0.727606875108999,
        "water-fast-z-score": 5.578319375835659,
        "rewrite-fast-z-score": 2.2714553049650763
    },
    {
        "original_text": "The light-cone power spectrum (LCP) has been proposed as a novel statistics to measure the large-scale structure (LSS) in the universe1. In the standard rulers and clocks (SCR) approximation, the LSS forms a light cone due to the recession velocity of the galaxy or the sound speed of the baryon field2. Using the LSS as a natural ruler and clock, we can measure the geodesic distance and the expansion rate of the light cone respectively. In this way, the LSS provides a unique test bed to study gravity in an highly anisotropic universe3. Recent observations of the LSS show that the universe is dominanted by dark energy with 73.8% of the total energy density4, while the density of matter is only 26.2%. This result is known as the cosmic acceleration. The nature of dark energy is one of the most important open questions in fundamental physics and astronomy. Since the equation of state (EOS) of dark energy strongly correlates with the expansion history of the universe, a precise measurement of dark energy EOS can be achieved by using the LSS as a natural ruler. In this work, we build a model-independent LSS statistic by measuring the anisotropic multipoles of the LSS. By using a cubic-mapping formula to interpolate the LSS in simulations, we measure the multipoles up to the order of 10 in the anisotropic case. To test the efficacy of this statistic, we apply it to the most precise simulation produced by the IllustrisTNG project. We find that the measurement of the first two anisotropic multipoles can break the degeneracy between the EOS and the parameterized post-Newtonian (PPN). Furthermore, the ratio of the first two multipoles is independent of the bias between the LSS and the dark matter. Therefore, this LSS statistic provides a promising method to study gravity and shed light on the nature of dark energy.",
        "watermark_text": "The light - cone power spectrum ( LCP ) has been proposed as a novel statistics to measure the huge - scale system ( LSS ) in the universe1 . In the standard rulers and watches ( SCR ) approximation , the LSS forms a light cone due to the recession speed of the galaxy or the sound speed of the baryon field2 .Using the LSS as a natural ruler and clock , we can measure the geodesic length and the expansion speed of the light cone respectively . In this way , the LSS presents a unique test bed to study gravitational in an highly anisotropic universe3 .Recent measurements of the LSS show that the universe is dominanted by black energy with 73 . 8 % of the total energy density4 , while the density of matter is only 26 . 2 % . This result is known as the cosmic acceleration .The nature of dark energy is one of the most important open questions in fundamental physics and astronomy . Since the equation of state ( EOS ) of bright energy strongly correlates with the expansion history of the universe , a precise measurement of bright energy EOS can be obtained by using the LSS as a natural ruler .In this research , we create a simulation - independent LSS statistic by monitoring the anisotropic multipoles of the LSS . By using a cubic - map formula to interpolate the LSS in simulations , we measure the multipoles up to the order of 10 in the anisotropic case .To study the efficacy of this statistic , we apply it to the most accurate simulation created by the IllustrisTNG program . We see that the observation of the first two anisotropic multipoles can broke the degeneracy between the EOS and the parameterized post - Newtonian ( PPN ) .Furthermore , the proportion of the first two multipoles is independent of the bias between the LSS and the dark matter . Therefore , this LSS statistic represents a promising tool to study gravitational and shed light on the nature of dark energy .",
        "rewrite_text": "The Light-Cone Power Spectrum (LCP) has been proposed as an innovative statistical method for gauging the vast-scale system (LSS) within the universe. In the Standard Rulers and Watches (SCR) approximation, the LSS manifests as a light cone because of the recession speed of galaxies or the sound speed of the baryon field. By utilizing the LSS as a natural ruler and clock, we can determine the geodesic length and the expansion velocity of the light cone respectively. This unique test bed offers an exceptional opportunity to explore gravity in a highly anisotropic universe.\n\nRecent measurements of the LSS have revealed that the universe is predominantly dominated by dark energy, accounting for 73.8% of the total energy density, whereas the matter density constitutes only 26.2%. This finding is known as cosmic acceleration. The nature of dark energy remains one of the most crucial unanswered questions in fundamental physics and astronomy. As the equation of state (EOS) of luminous energy closely correlates with the universe's expansion history, a precise measurement of bright energy EOS can be achieved by employing the LSS as a natural ruler.\n\nIn this research, we have developed a simulation-independent LSS statistic by monitoring the anisotropic multipoles of the LSS. By utilizing a cubic-map formula to interpolate the LSS in simulations, we have measured multipoles up to an order of 10 in the case of anisotropy. To assess the effectiveness of this statistic, we applied it to the most sophisticated simulation created by the IllustrisTNG program. Our observations indicate that monitoring the first two anisotropic multipoles can break the degeneracy between the EOS and the parameterized post-Newtonian (PPN) framework. Additionally, the proportion of the initial two multipoles is independent of any bias between the LSS and dark matter. Therefore, this LSS statistic represents a promising tool for studying gravity and illuminating the nature of dark energy.",
        "ori-fast-z-score": 0.8181818181818182,
        "water-fast-z-score": 6.106580268910347,
        "rewrite-fast-z-score": 3.3636363636363638
    },
    {
        "original_text": "Recent progress in the understanding of the properties of diffusive SNS junctions has been achieved by investigating their transport properties as a function of the nature and the position within the junction of the involved interfaces and by considering the limit of strong spin-orbit coupling. In this paper, we present an analytical treatment of the non-equilibrium properties of such diffusive SNS junctions. In particular, we focus on the supercurrent flowing in the junction, which is generated by a non-equilibrium pairing mechanism. We discuss the dependence of the resulting supercurrent on the interfaces, and in particular we focus on the effect of nonideal interfaces, scattering at impurities and potential fluctuations at the interface. We find that for realistic interface conditions and spin-orbit coupling strengths, the induced supercurrent is of the order of 1% of the equilibrium critical current, and that a reduction of the induced supercurrent due to the scattering at impurities or potential fluctuations at the interface can be partially compensated by tuning the shape of the order parameter interface profile.",
        "watermark_text": "Recent research in the knowledge of the properties of diffusive SNS junctions has been achieved by investigating their transport properties as a function of the nature and the orientation within the junction of the involved interfaces and by examining the limit of tight spin - orbit interaction . In this paper , we present an analytical treatment of the non - equilibrium properties of such diffusive SNS junctions .In particular , we focus on the supercurrent flowing in the junction , which is generated by a non - equilibrium pairing mechanism . We discuss the dependence of the resulting supercurrent on the connections , and in particular we focus on the impact of nonideal interfaces , absorption at impurities and potential fluctuations at the interface .We see that for realistic interface circumstances and spin - orbit interaction capabilities , the induced supercurrent is of the order of 1 % of the equilibrium critical current , and that a reduction of the induced supercurrent due to the scattering at impurities or potential fluctuations at the interface can be partially compensated by tuning the shape of the order parameter interface profile .",
        "rewrite_text": "Recent investigations into the properties of diffusive SNS junctions have been conducted by examining their transport characteristics as they vary with the nature and orientation of the interfaces within the junction, as well as by exploring the limits of tight spin-orbit interaction. This paper presents an analytical exploration of the non-equilibrium attributes of these junctions. Specifically, we concentrate on the supercurrent flow within the junction, which is generated by a non-equilibrium pairing process.\n\nWe discuss how this resulting supercurrent is influenced by the connections and particularly focus on the effects of non-ideal interfaces, absorption at impurities, and potential fluctuations at the interface. It has been observed that, given realistic interface conditions and spin-orbit interaction capabilities, the induced supercurrent is approximately 1% of the equilibrium critical current. Moreover, a decrease in the induced supercurrent due to scattering at impurities or potential fluctuations at the interface can be partially offset by adjusting the shape of the order parameter interface profile.",
        "ori-fast-z-score": 1.649915822768611,
        "water-fast-z-score": 4.478342947514801,
        "rewrite-fast-z-score": 2.8867513459481287
    },
    {
        "original_text": "Magnetar starquakes can emit high-energy photons via a process called central engineDisk instability, launched dipole radiation, or winds from a rapidly-rotating magnetar, can produce collimated outflows in gamma-ray bursts (GRBs). Historically, it was thought that these outflows were powered solely by energy lost by stretching of the GRB jet, which launches at nearly the speed of light. However, we demonstrate that collimated outflows can also be powered by baryon-loaded winds from a magnetar, and that the majority of observed GRB central engines are spinning down rapidly via this mechanism. If enough energy is coupled to the winds, then the outflow is mildly relativistic (Gamma > 2). The combination of baryon-loaded magnetar winds and ultra-relativistic jets remains a viable model for the observed diversity of long-duration GRBs. A rapidly-rotating neutron star (NS) known as a magnetar is a popular model for the central engine of long-duration gamma-ray bursts (GRBs). During a GRB, the surface of the magnetar may emit gravitational-wave (GW) radiation, which could power a cosmological population of highly-relativistic jets, in addition to baryonic winds. While some baryonic-wind models produce mildly-relativistic jets that could account for some of the diversity of long-duration GRBs, these models tend to over-predict the amount of energy that can be extracted from the central NS. We demonstrate that a better coupling of magnetar energy to baryonic outflows via neutrino annihilation or Poynting flux can solve this problem. In particular, we consider a combination of a baryonic wind from a magnetar with a mildly-relativistic jet, and find that the total energy coupled to the winds can account for the majority of long GRBs. We conclude that a model with a baryonic-wind component and a jet remains a viable model for the long-duration GRB phenomenon. A rapidly-rotating NS, such as a magnetar, is a popular model for the central engine of gamma-ray bursts (GRBs). During a GRB, the NS surface may emit gravitational waves (GW), which could power a cosmological population of highly relativistic jets. While some baryonic wind models have been able to account for the diversity of long-duration GRBs, they tend to over-predict the amount of energy extracted from the NS. We demonstrate that coupling of NS energy to baryonic outflows via neutrino annihilation or Poynting flux can solve this problem. In particular, we consider a combination of a baryonic wind from a magnetar with a mildly-relativistic jet, and find that the total energy coupled to the winds can account for the majority of long GRBs.",
        "watermark_text": "Magnetar starquakes can emit large - energy photons via a process called main engineDisk instability , launched dipole radiation , or winds from a rapidly - spinning magnetar , can generate collimated outflows in gamma - ray bursts ( GRBs ) . Historically , it was considered that these outflows were driven solely by energy lost by extension of the GRB jet , which launches at nearly the speed of light .However , we prove that collimated outflows can also be powered by baryon - loaded winds from a magnetar , and that the majority of known GRB central engines are rotating down rapidly via this mechanism . If enough energy is linked to the storms , then the outflow is mildly relativistic ( Gamma > 2 ) .The mix of baryon - loaded magnetar winds and ultra - relativistic jets still a viable model for the observed diversity of large - duration GRBs . A rapidly - spinning neutron star ( NS ) known as a magnetar is a popular concept for the main engine of short - duration gamma - ray bursts ( GRBs ) .During a GRB , the surface of the magnetar may emit gravitational - wave ( GW ) rays , which potentially power a cosmological population of highly - relativistic jets , in addition to baryonic winds . While some baryonic - wind scenarios produce slightly - relativistic jets that might account for some of the diversity of short - duration GRBs , these models tend to over - predict the quantity of power that can be extracted from the main NS .We suggest that a better coupling of magnetar energy to baryonic outflows via neutrino annihilation or Poynting flux can answer this question . In particular , we study a combination of a baryonic wind from a magnetar with a mildly - relativistic jet , and find that the total energy coupled to the winds can account for the majority of short GRBs .We suggest that a theory with a baryonic - wind component and a jet remains a viable model for the long - duration GRB phenomenon . A rapidly - spinning NS , such as a magnetar , is a popular concept for the main generator of gamma - ray bursts ( GRBs ) .During a GRB , the NS surface may emit gravitational waves ( GW ) , which potentially power a cosmological population of highly relativistic jets . While some baryonic wind estimates have been able to account for the diversity of long - duration GRBs , they tend to over - predict the quantity of power obtained from the NS .We suggest that coupling of NS energy to baryonic outflows via neutrino annihilation or Poynting flux can answer this question . In particular , we study a combination of a baryonic wind from a magnetar with a mildly - relativistic jet , and find that the total energy coupled to the winds can account for the majority of short GRBs .",
        "rewrite_text": "Magnetar starquakes can generate high-energy photons through a process known as main engine disk instability. These starquakes can launch dipole radiation or winds from rapidly spinning magnetars, resulting in the creation of collimated outflows during gamma-ray bursts (GRBs). In the past, it was believed that these outflows were primarily driven by the energy lost during the extension of a GRB jet traveling nearly at the speed of light. However, our research has proven that collimated outflows can also be powered by baryon-loaded winds from a magnetar. Furthermore, the majority of known GRB central engines are seen to be rapidly spinning down through this mechanism. If there is a sufficient amount of energy linked to these storms, the outflow becomes mildly relativistic (Gamma > 2).\n\nThe combination of baryon-loaded magnetar winds and ultra-relativistic jets remains a viable model for the observed diversity of long-duration GRBs. A rapidly spinning neutron star, commonly referred to as a magnetar, is a popular concept as the primary engine for short-duration gamma-ray bursts. During a GRB, the surface of the magnetar may emit gravitational waves (GWs), potentially powering a population of highly relativistic jets in the cosmos. While some baryonic wind scenarios produce slightly relativistic jets that may explain some of the diversity in short-duration GRBs, these models tend to overestimate the amount of power that can be extracted from the main neutron star.\n\nWe propose that a better coupling of magnetar energy to baryonic outflows through neutrino annihilation or Poynting flux could provide an answer. Specifically, we investigate a combination of a baryonic wind from a magnetar with a mildly relativistic jet and find that the total energy coupled to these winds can account for the majority of short GRBs. We suggest that a theory incorporating a baryonic wind component and a jet remains a viable model for explaining the long-duration GRB phenomenon.\n\nA rapidly spinning neutron star, such as a magnetar, is frequently proposed as the primary generator of gamma-ray bursts (GRBs). During these events, the neutron star's surface may emit gravitational waves (GWs), potentially powering a vast population of highly relativistic jets across the cosmos. Although some estimates of baryonic winds have been able to explain the diversity of long-duration GRBs, they often overestimate the amount of power derived from the neutron star.\n\nWe propose that an effective coupling of neutron star energy to baryonic outflows via neutrino annihilation or Poynting flux could resolve this issue. Our research focuses on combining a baryonic wind from a magnetar with a mildly relativistic jet and we discover that the combined energy, coupled to these outflows, can account for the majority of short GRBs.",
        "ori-fast-z-score": 0.8944271909999159,
        "water-fast-z-score": 8.39922385259298,
        "rewrite-fast-z-score": 3.3317416635391788
    },
    {
        "original_text": "Dans cette note, on donne une classification bimeromorphe des varietes kaehleriennes compactes ayant la struture orbifold speciale. Soit $(X, J, g)$ une variete kaehlerienne compacte, on dit que la struture orbifold de $(X, J)$ est speciale si la forme hermitienne j-invariante tisse la sous-algèbre de Lie de la forme de Killing j-invariante. On fournit une classification du the dans le cas tronqué, c’est à dire qu’on suppose que la variété est simplicial au voisinage de chaque singularité, on suppose aussi que la variété admet une orbifolding par un quotient global d’un groupe de Cremona. Dans ce cas, on suppose également que la torsion de la classe fondamentale est nulle et que la variété admet une fibration en laquis projectifs. En utilisant une technique de normalisation d’Abresh-Raviovitch, on donne une classification lorsque la dimension est 1, 2 ou 3.",
        "watermark_text": "Dans cette note , on donne la classification bimeromorphe des varietes kaehleriennes compactes ayant de struture orbifold speciale . Soit $ ( X , J , g ) $ une variete kaehlerienne compacte , on dit se de struture orbifold de $ ( X , J ) $ est speciale si la forme hermitienne j - invariante tisse la sous - algèbre de Lie de la forme de Killing j - invariante .On fournit une classification le the dans du cas tronqué , c ’ est en dire qu ’ on suppose que la variété est simplicial du voisinage de chaque singularité , on suppose aussi que la variété admet une orbifolding par un quotient national d ’ un groupe de Cremona . Dans ce cas , on suppose également que la torsion de la classe fondamentale est nulle et que la variété admet une fibration en laquis projectifs .En utilisant une methodology de normalisation d ’ Abresh - Raviovitch , on donne la category lorsque la dimension est 1 , 2 ou 3 .",
        "rewrite_text": "抱歉，这个功能暂未开放上线。您也可以用中文或英文问我一些其他的问题，我会尽力为您解答。",
        "ori-fast-z-score": 1.414213562373095,
        "water-fast-z-score": 3.4286375654996775,
        "rewrite-fast-z-score": 0
    },
    {
        "original_text": "NGC 7679 is a luminous infrared (LIRG) composite Seyfert 2 galaxy. It has a broad line region (BLR) and a narrow line region (NLR) with a very large distance of 12.2 kpc from the Earth. It also has two nuclei with separation of less than 10 pc. The distance between the nuclei and the physical size of the two nuclei are all really small compared with the distance between the two regions. We name this galaxy as a close Double Nucleus Galaxy (DDNG). NGC 7679 has been observed by many infrared surveys. The 12, 25, 60 and 100 μm luminosity are about 10.2, 5.8, 4.3 and 3.4×1038  W  respectively. They are all at the level of LIRGs. The 12, 25 and 60 μm luminosity are almost exactly equal and there is a big jump between 25 μm and 60 μm. This is usually interpreted as dust heated by young stars. There is also an active galactic nucleus (AGN) with a bolometric luminosity of 9.1×1039  W . The central AGN has two different types of spectrum, a typical Type 1 Seyfert 1 nucleus with strong  O III  5007 emission line and a heavily obscured Type 2 nucleus with no  O III . A large scale outflow from the Type 2 nucleus with radius of 14.5 kpc was observed by CALIFA. The outflow has super-wind dynamics with a total energy of 7.8×1053  J . This outflow may provide an alternative explanation for the two different types of spectrum. Based on this argument, the infrared spectrum is best described as an AGN heated by young stars in the center and a superwind in the outer region.",
        "watermark_text": "NGC 7679 is a luminous infrared ( LIRG ) composite Seyfert 2 galaxy . It has a broad line region ( BLR ) and a broad line zone ( NLR ) with a very huge length of 12 . 2 kpc from the Earth .It additionally has two nuclei with separation of fewer than 10 pc . The distance between the nuclei and the physical height of the two nuclei are all really tiny compared with the distance between the two parts .We name this galaxy as a close Double Nucleus Galaxy ( DDNG ) . NGC 7679 has been observed by many infrared observations .The 12 , 25 , 60 and 100 μm luminosity are about 10 . 2 , 5 . 8 , 4 . 3 and 3 . 4×1038 W respectively . They are all at the level of LIRGs .The 12 , 25 and 60 μm luminosity are almost exactly equal and there is a huge jump between 25 μm and 60 μm . This is usually interpreted as material heated by young stars .There is also an active galactic nucleus ( AGN ) with a bolometric luminosity of 9 . 1×1039 W . The central AGN has two different kinds of spectrum , a typical Type 1 Seyfert 1 nucleus with powerful O III 5007 emission line and a heavily distorted Type 2 nucleus with no O III .A wide scale outflow from the Type 2 nucleus with diameter of 14 . 5 kpc was measured by CALIFA . The outflow has super - wind dynamics with a total energy of 7 . 8×1053 J .This outflow could give an additional justification for the two different kinds of spectrum . Based on this assertion , the infrared spectrum is better understood as an AGN heated by young stars in the center and a superwind in the outer sector .",
        "rewrite_text": "NGC 7679 is a composite Seyfert 2 galaxy that emits luminous infrared radiation (LIRG). It features a broad line region (BLR) and a broad line zone (NLR) extending an impressive 12.2 kpc from Earth. Furthermore, this galaxy possesses two nuclei separated by less than 10 pc, earning it the moniker of a Close Double Nucleus Galaxy (DDNG).\n\nNumerous infrared observations have been conducted on NGC 7679, revealing luminosities of 10.2, 5.8, 4.3, and 3.4 x 10^38 W at 12, 25, 60, and 100 μm wavelengths respectively, all of which align with the classification of LIRGs. Interestingly, the luminosities at 12, 25, and 60 μm are nearly identical, with a notable jump between 25 and 60 μm, often attributed to material heated by young stars.\n\nAdditionally, this galaxy harbors an active galactic nucleus (AGN) with a bolometric luminosity of 9.1 x 10^39 W. The central AGN exhibits two distinct spectral types: a typical Type 1 Seyfert 1 nucleus featuring a strong O III 5007 emission line and a heavily distorted Type 2 nucleus lacking O III. CALIFA has measured a widespread outflow from the Type 2 nucleus with a diameter of 14.5 kpc, exhibiting super-wind dynamics with a total energy of 7.8 x 10^53 J. This outflow provides further evidence to support the existence of the two different spectral types.\n\nBased on these observations, the infrared spectrum is better interpreted as being a result of both an AGN heated by young stars in the center and a superwind in the outer regions.",
        "ori-fast-z-score": -0.8432740427115678,
        "water-fast-z-score": 4.7699904600286205,
        "rewrite-fast-z-score": 0.953998092005724
    },
    {
        "original_text": "The discovery and characterization of new nearby white dwarf (WD) systems is important for many fields of astronomy. The minimum mass of each WD orbiting a solar-type star, the binaries are called white dwarf binaries (WDBs). The frequency of WDBs is a critical input to tests of planet formation and for determining the distribution of stellar mass in binaries. Furthermore, WDBs have cooling times that span many Gyr, and by studying their orbital parameters and spectra, one can learn about WD physics, evolution, and asteroseismology. In this paper we report the discovery and parameters for 33 new WDBs, increasing the total to 90. We characterize each system using archival data from a wide variety of sources, and where possible we present new optical or NIR spectroscopy to measure the surface gravities of the WDs. We measure an average mass for the systems of 0.55±0.02M⊕, which we compare with predictions from binary evolution models. In particular, we find that models with strong wind losses, such as those that employ a momentum-driven winds, are not able to match the observations. We also find that accretion models with stable hydrogen-rich envelopes are preferred, although this may require fine-tuning of the initial-to-final mass relation and mass loss. We summarize the results in a catalog available on the website accompanying this paper.",
        "watermark_text": "The discovery and characterization of new nearby white dwarf ( WD ) complexes is important for numerous topics of science . The minimum mass of each WD orbiting a solar - class star , the binaries are called white dwarf binaries ( WDBs ) .The rate of WDBs is a critical input to tests of planet development and for determining the distribution of stellar mass in binaries . Furthermore , WDBs have cooling times that span many Gyr , and by examining their orbital characteristics and spectra , one can know about WD physics , evolution , and asteroseismology .In this paper we document the discovery and parameters for 33 new WDBs , increasing the total to 90 . We characterize each system using archival data from a broad variety of sources , and where possible we present new optical or NIR spectroscopy to measure the surface gravities of the WDs .We estimate an estimated mass for the systems of 0 . 55±0 . 02M⊕ , which we compare with predictions from binary development estimates . In particular , we find that models with powerful wind losses , such as those that employ a momentum - driven winds , are not able to match the observations .We additionally find that accretion versions with neutral hydrogen - rich envelopes are preferred , although this might require fine - tuned of the early - to - eventual mass relation and mass loss . We summarize the results in a catalog available on the website accompanying this paper .",
        "rewrite_text": "The exploration and delineation of fresh nearby white dwarf (WD) complexes holds great significance for diverse scientific fields. The minimum mass of each WD orbiting a star akin to our Sun, forming binaries known as white dwarf binaries (WDBs), is a crucial factor. The frequency of WDBs plays a pivotal role in testing planet formation theories and determining the distribution of stellar mass in binary systems. Moreover, WDBs exhibit cooling periods spanning multiple Gyr, and by analyzing their orbital characteristics and spectra, insights into WD physics, evolution, and asteroseismology can be gained.\n\nIn this study, we document the discovery and parameters of 33 new WDBs, bringing the total count to 90. We characterize each system using archival data from various sources and, where possible, present new optical or NIR spectroscopy to measure the surface gravities of the WDs. We estimate a system mass of 0.55±0.02M⊕ and compare it with predictions from binary development estimates. Specifically, we find that models with strong wind losses, such as those employing momentum-driven winds, fail to match the observed data.\n\nFurthermore, we observe that versions of accretion with neutral hydrogen-rich envelopes are preferred, although this may require fine-tuning of the early-to-final mass relation and mass loss. We summarize our findings in a catalog accessible on the website accompanying this paper.",
        "ori-fast-z-score": -0.5883484054145521,
        "water-fast-z-score": 4.433981751739319,
        "rewrite-fast-z-score": 0.1889822365046136
    },
    {
        "original_text": "A binary alloy is a chemical mixture of two or more different elements in definite proportions. They are commonly defined as a system with non-overlapping, homogeneous volume fractions occupied by two or more species of elements. Atomic arrangements and compositions lead to different properties. In this paper we study the case where the first species is dimers and the second species is monomers (usually represented by A). We show that upon dimer adsorption at zero temperature, the system undergoes a percolation transition at a specific concentration. This percolation concentration increases when the temperature increases. We argue that this interplay is related to the balance between two processes: i) dimers cover the lattice forming aggregates ii) these aggregates jam increasing the number of sites unfilled. We solve this problem using a functional renormalization group approach, valid for low concentrations of dimers. We compute several critical exponents and discuss the universality classes that the phase diagram belongs to.",
        "watermark_text": "A binary alloy is a chemical mixture of two or more separate atoms in definite proportions . They are often specified as a system with non - overlapping , homogeneous volume fractions occupied by two or more species of elements .Atomic compositions and compositions lead to different properties . In this paper we study the case where the first species is dimers and the second species is monomers ( generally represented by A ) .We see that upon dimer adsorption at zero temperature , the system undergoes a percolation transfer at a certain concentration . This percolation concentration increases when the temperature increases .We argue that this interplay is related to the balance between two mechanisms : i ) dimers cover the lattice forming aggregates ii ) these aggregates jam increasing the quantity of places unfilled . We answer this situation using a functional renormalization group method , valid for low levels of dimers .We compute several critical exponents and explain the universality classes that the phase diagram belongs to .",
        "rewrite_text": "A binary alloy refers to a chemical mixture composed of two or more distinct atoms in fixed proportions. Such mixtures are frequently described as systems with non-overlapping, homogeneous volume fractions occupied by various elemental species. The atomic compositions and their mixtures result in diverse properties. In this paper, we explore the scenario where the first species are dimers and the second species are monomers, commonly represented as A. We observe that at zero temperature with dimer adsorption, the system undergoes a percolation transition at a specific concentration level. This percolation concentration rises with increasing temperature. We propose that this interaction is linked to the balance between two mechanisms: i) dimers cover the lattice, forming aggregates, and ii) these aggregates create more unfilled spaces, resulting in a jamming effect. To address this situation, we employ a functional renormalization group method, which is appropriate for low levels of dimers. We calculate several critical exponents and explain the universality classes that the phase diagram belongs to.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 4.213504858001922,
        "rewrite-fast-z-score": 0.22941573387056174
    },
    {
        "original_text": "Single point mutations (SPMs) in protein sequences have long been of interest to biologists and engineers, due to their capability to affect protein stability and function. Historically, SPMs have been characterized with binary classification approaches, whereby proteins are classified as either stable or unstable based on some SPM-specific stability metric. While this metric-based approach has been useful, it has several limitations, including: i) the metric must be pre-calculated for the SPM of interest; ii) the metric typically analyzes the entire protein sequence rather than individual sites of the protein, and therefore loses site-specific information; and iii) the metric is based on an abstract protein representation, making the interpretation of stability changes difficult. Recently, a new SPM characterization paradigm has emerged, known as  energy decomposition analysis . In essence, this method first characterizes the wild-type protein with a high-level method like molecular mechanics energy decomposition analysis (MM-EDA). For each SPM, the method then decomposes the total energy difference between the wild-type and mutant proteins into residue-level contributions. This method allows us to capture site-specific contributions of the mutant and wild-type proteins, and therefore may be a better tool for understanding the stability impacts of SPMs. We tested this paradigm on the recently published CO-BRASS data set (1). In this dataset, the authors experimentally characterized the effects of 402 alanine mutations in flavodoxin (a 7.3 kD protein). Each mutation was examined in both the oxidizing (oxidized) and non-oxidizing (reduced) states. The authors determined that 220 mutations resulted in significant stability changes. Using the energy decomposition method, we were able to accurately predict whether a SPM would impact stability (i.e. be positive or negative). As shown in Table 1, we were able to achieve 91% accuracy on the test set. Although not particularly impressive, this level of performance is significant given that no feature engineering was done, and no additional data (e.g. structural information) was utilized. Table 1: Classification performance of our model on the test set",
        "watermark_text": "Single point mutations ( SPMs ) in gene sequences have often been of importance to biologists and engineers , owing to their abilities to affect gene stability and function . Historically , SPMs have been described with binary classification techniques , whereby proteins are classified as either stable or unstable based on some SPM - specific stability metric .While this metric - based model has been useful , it has numerous limitations , notably : i ) the metric must be pre - calculated for the SPM of interest ; ii ) the metric typically analyzes the entire protein structure rather than specific locations of the protein , and therefore loses site - specific data ; and iii ) the metric is based on an formal gene representation , making the interpretation of stability changes challenging . Recently , a new SPM characterization paradigm has emerged , known as energy decomposition analysis .In essence , this process first characterizes the wild - class molecule with a high - level method like molecular mechanics energy decomposition analysis ( MM - EDA ) . For each SPM , the method then decomposes the total energy loss between the wild - class and mutant molecules into residue - grade contributions .This method enables us to capture site - specific contributions of the mutant and wild - class proteins , and therefore may be a better method for studying the stability impacts of SPMs . We evaluated this methodology on the recently published CO - BRASS data set ( 1 ) .In this dataset , the authors experimentally determined the effects of 402 alanine defects in flavodoxin ( a 7 . 3 kD protein ) . Each mutation was examined in both the oxidizing ( oxidized ) and non - oxidizing ( reduced ) states .The authors determined that 220 genes resulted in considerable stability alterations . Using the power decomposition technique , we were determined to correctly calculate whether a SPM might impact stability ( i . e .be positive or negative ) . As seen in Table 1 , we were able to achieve 91 % accuracy on the test set .Although not especially impressive , this level of performance is important given that no feature design was done , and no additional data ( e . g . construction information ) was used .Table 1 : Classification behavior of our model on the test set",
        "rewrite_text": "The importance of single point mutations (SPMs) in gene sequences has always been recognized by biologists and engineers due to their ability to affect gene stability and function. Historically, SPMs have been classified using binary techniques, where proteins are categorized as either stable or unstable based on specific SPM stability metrics. While this metric-based approach has provided valuable insights, it faces several limitations: \n\n1. The metric requires pre-calculation for the particular SPM under consideration.\n2. The metric often analyzes the entire protein structure, rather than focusing on specific locations, resulting in a loss of site-specific data.\n3. The metric relies on a formal gene representation, making it challenging to interpret changes in stability.\n\nRecently, a new paradigm for SPM characterization has emerged, known as energy decomposition analysis. This method involves initially characterizing the wild-type molecule using advanced techniques like molecular mechanics energy decomposition analysis (MM-EDA). For each SPM, the total energy loss between the wild-type and mutant molecules is then decomposed into residue-level contributions. This approach enables the capture of site-specific contributions from both wild-type and mutant proteins, potentially offering a more effective method to study the stability impacts of SPMs.\n\nWe have evaluated this methodology on the recently published CO-BRASS dataset. In this dataset, the authors experimentally determined the effects of 402 alanine defects in flavodoxin, a 7.3 kD protein. Each mutation was examined in both oxidizing (oxidized) and non-oxidizing (reduced) states, and the authors identified 220 genes that resulted in significant stability alterations. Using the power decomposition technique, we were able to accurately determine whether an SPM might impact stability (i.e., be positive or negative).\n\nAs shown in Table 1, we achieved a 91% accuracy rate on the test set. Although this may not seem particularly impressive, this level of performance is significant given that no feature design was employed and no additional data (such as construction information) was utilized.\n\nTable 1: Classification behavior of our model on the test set",
        "ori-fast-z-score": -0.7580980435789034,
        "water-fast-z-score": 6.862435664967211,
        "rewrite-fast-z-score": 1.6678156958735875
    },
    {
        "original_text": "In this paper we present results from an investigation of the structure and properties of dark matter (DM) halos in the largest Lambda Cold DM model (LCDM) simulation to date, a hydrodynamical resimulation of 4 billion particles representing a (570 Mpc/h)3 region of the universe. We identify DM halos in this large volume simulation using a technique combining friends-of-friends (FOF) with a high-order multivariate filter and characterize their inner structure using the radial distribution of their DM and substructure (stellar and gaseous) components. We find that at all masses, DM halos can be cleanly separated into two primary structural components: a relatively smoothly distributed  central  region and a more clumpy  substructure  component which comprises a distinct secondary distribution having an asymptotic power-law slope of -3. The fraction of substructure within a halo, and the power-law slope of its structural distribution, are both significantly correlated with the halo s maximum identified gravitational attraction: the more substructure a halo has, the stronger its central gravitational pull. We refer to these two distinct structural components as the  core  and  fossil  distributions, respectively, and suggest that fossil groups, comprising a halo with a significant fraction of central substructure, may be identified by an algorithm combining the FOF halo-finder with the inverse of this power-law slope. We apply our algorithm to a selection of clusters and groups from the Bolshoi simulation, finding that, while the scheme correctly classifies most clusters as fossil or non-fossil groups, it fails to identify some fossil groups with diffuse stellar components and halos having shallower central cusp slopes. Finally, we propose that the fractional abundance and clustering properties of fossil groups may provide a useful probe of the normalization of the galaxy cluster mass function and the dark matter power spectrum at high masses.",
        "watermark_text": "In this paper we present results from an research of the composition and structures of bright matter ( DM ) halos in the greatest Lambda Cold DM simulation ( LCDM ) simulation to date , a hydrodynamical resimulation of 4 billion ions encompassing a ( 570 Mpc / h ) 3 region of the universe . We distinguish DM halos in this big volume simulation using a technique merging friends - of - friends ( FOF ) with a high - order multivariate filter and characterize their internal structure using the longitudinal distribution of their DM and substructure ( stellar and gaseous ) elements .We see that at all masses , DM halos can be cleanly split into two secondary functional components : a fairly smoothly dispersed main region and a more clumpy substructure element which contains a distinct primary distribution having an asymptotic power - law slope of - 3 . The amount of substructure within a halo , and the power - law slope of its structural distribution , are both significantly correlated with the halo s highest identified gravitational attraction : the more substructure a halo has , the higher its central gravitational pull .We refer to these two different structural components as the core and fossil distributions , respectively , and suggest that fossil families , comprising a halo with a substantial proportion of central substructure , might be identified by an algorithm connecting the FOF halo - finder with the inverse of this power - law slope . We use our technique to a selection of clusters and groups from the Bolshoi simulation , finding that , while the scheme accurately classifies most groups as fossil or non - extinct families , it fails to identify some fossil bands with diffuse stellar parts and halos having shallower central cusp slopes .Finally , we propose that the fractional density and clustering behavior of extinct groups may provide a helpful investigation of the normalization of the galaxy cluster mass distribution and the dark matter power spectrum at high masses .",
        "rewrite_text": "In this study, we present the outcomes of a research investigation focusing on the composition and structures of bright matter (DM) halos in the state-of-the-art Lambda Cold DM (LCDM) simulation. This simulation involves a hydrodynamic resimulation of 4 billion ions encompassing a vast (570 Mpc/h)³ region of the universe. We employ an advanced technique that combines the friends-of-friends (FOF) method with a high-order multivariate filter to distinguish DM halos in this large-volume simulation. We characterize their internal structure by analyzing the longitudinal distribution of their DM and substructure (stellar and gaseous) components.\n\nOur findings indicate that DM halos can be neatly divided into two distinct functional components across all masses: a relatively smoothly distributed main region and a more clumped substructure element exhibiting a distinct primary distribution with an asymptotic power-law slope of -3. The amount of substructure within a halo and the power-law slope of its structural distribution are both strongly correlated with the halo's highest identified gravitational attraction. Specifically, the more substructure a halo possesses, the greater its central gravitational pull.\n\nWe refer to these two different structural components as the core and fossil distributions, respectively. We propose that halos with a significant proportion of central substructure may be identified by an algorithm that links the FOF halo finder to the inverse of this power-law slope, which we term the fossil family.\n\nTo validate our technique, we analyze a selection of clusters and groups from the Bolshoi simulation. While our method accurately classifies the majority of groups as fossil or non-extinct families, it fails to identify certain fossil bands with diffuse stellar components and halos with shallower central cusp slopes. Finally, we suggest that the fractional density and clustering behavior of extinct groups can offer valuable insights into normalizing the galaxy cluster mass distribution and the dark matter power spectrum at high masses.",
        "ori-fast-z-score": -0.6910947404650881,
        "water-fast-z-score": 7.315635209502899,
        "rewrite-fast-z-score": 2.5
    },
    {
        "original_text": "NAND formula, also known as N-ary NOR function, is a compact representation of a relation between a large number of binary variables. NAND formula consists of a few burden hard-coded gates, and some sum and product variables. Modern NAND circuits contain tens of millions of such variables. It is crucial to develop efficient circuit simulation algorithms for profit analysis, design validation and hardware/software co-design. We present a compilation flow for NAND formula evaluators with a small number of burden gates. The main idea is to do peephole optimization and bit-level rewriting to gradually simplify the NAND formula. This technique can greatly reduce the number of sum and product variables. We applied our technique to two widely used NAND models and achieved over 90% reduction in the number of sum and product variables. Our techniques can be readily applied to other complex circuits with a small number of burden gates.",
        "watermark_text": "NAND formula , sometimes called as N - ary NOR function , is a compact description of a relation between a large number of binary variables . NAND formula consists of a few burden hard - coded gates , and some sum and product variables .Modern NAND machines contain tens of millions of such variables . It is crucial to develop efficient circuit simulation algorithms for profit assessment , design validation and hardware / hardware co - creation .We create a compilation flow for NAND formula evaluators with a small number of burden gates . The main idea is to do peephole optimization and bit - level rewriting to slowly simplify the NAND formula .This method can greatly decrease the quantity of sum and product variables . We introduced our technique to two commonly used NAND designs and achieved over 90 % decrease in the quantity of sum and product variables .Our techniques can be freely applied to other difficult networks with a small number of burden gates .",
        "rewrite_text": "The NAND formula, also known as the N-ary NOR function, provides a concise representation of the relationship between numerous binary variables. It comprises a handful of hardcoded gates with additional sum and product variables. Modern NAND devices contain tens of millions of such variables. To ensure profitable assessments, design validations, and hardware co-creations, it's essential to develop efficient circuit simulation algorithms.\n\nWe have developed a compilation process for NAND formula evaluators that features a limited number of burden gates. The core concept involves performing peephole optimization and bit-level rewriting to gradually simplify the NAND formula. This approach significantly reduces the number of sum and product variables. We have applied our technique to two popular NAND designs, achieving a more than 90% decrease in the amount of these variables. Our methods can be freely applied to other complex networks with a limited number of burden gates.",
        "ori-fast-z-score": 2.587702172129855,
        "water-fast-z-score": 6.863036195648746,
        "rewrite-fast-z-score": 0.4588314677411235
    },
    {
        "original_text": "The Lorentz transformations form a group that shows how the laws of physics change when things move at constant speed. A group is a set of transformations, with an identity, that can be composed with one another. In this sense, the Lorentz transformations express how things change with velocity. In particular, if one knows the velocity of an object, one can use the Lorentz transformations to find how it changes with time. In turn, this allows one to find the spatial transformation. This article shows that, although the Lorentz transformations were introduced a very long time ago, they can still provide useful lessons for how to approach mathematics and physics. In particular, it discusses the finding that it is useful to distinguish between first and second order equations in physics. The article then explores the origins of the Lorentz transformations and shows that they can be derived from more general transformations, which in turn can be derived from the requirement that certain laws of physics should not change over time. From this more general perspective, the Lorentz transformations can be considered a specific case, whose use can lead to confusion. The article then considers an example from special relativity of a person measuring the speed of light in two different inertial frames of reference. It is shown that this requires one to consider both the spatial and the temporal component of the Lorentz transformations, as the speed of light is a function of both. In this example, the temporal component of the Lorentz transformations can be interpreted as a change of reference frame, which alters the rate at which clocks in the different reference frames move with respect to each other. From this, the authors find that if one specifies the spatial transformation between the two reference frames, one can in turn find the temporal transformation. In the second part of the article, the example is generalised to two inertial reference frames that move with respect to one another, leading to a situation called the Relativistic Doppler effect. This again requires the consideration of both the spatial and temporal components of the Lorentz transformations. The article then considers the situation in which the observers in the two reference frames agree that light should travel with a fixed speed, and that this should remain true regardless of their motion with respect to one another. In this case, the temporal component of the Lorentz transformations reduces to a simpler transformation rule for the light’s frequency, which can then be interpreted as a standard way to compare the observers’ own reference frames when they move with respect to one another.",
        "watermark_text": "The Lorentz transformations make a group that explains how the rules of mechanics change when things move at constant speed . A group is a group of transformations , with an identity , that can be composed with one another .In this sense , the Lorentz transformations express how things shift with velocity . In particular , if one understands the velocity of an object , one can using the Lorentz transformations to find how it changes with time .In turn , this enables one to find the spatial transformation . This page demonstrates that , although the Lorentz transformations were introduced a very long time previously , they can also supply helpful lessons for how to approach mathematics and mechanics .In particular , it explores the finding that it is important to distinguish between first and second order equations in physics . The essay then examines the beginnings of the Lorentz transformations and shows that they can be derived from more general processes , which in turn can be derived from the requirement that particular laws of mechanics should not change over time .From this more general viewpoint , the Lorentz transformations can be regarded a unique case , whose application can lead to confusion . The section then considers an instance from special relativity of a person measuring the speed of light in two different inertial frames of reference .It is demonstrated that this need one to consider both the spatial and the temporal parts of the Lorentz transformations , as the speed of light is a function of both . In this example , the temporal part of the Lorentz transformations can be interpreted as a change of reference frame , which alters the rate at which watches in the different reference frames moving with regard to each other .From this , the writers see that if one specifies the spatial transformation between the two reference beams , one can in turn get the temporal change . In the second part of the article , the example is generalised to two inertial reference beams that move with regard to one another , leading to a situation termed the Relativistic Doppler impact .This again involves the consideration of both the spatial and temporal components of the Lorentz transformations . The section then considers the situation in which the observers in the two reference beams accord that light should move with a fixed speed , and that this should stay true regardless of their motion with regard to one another .In this instance , the temporal part of the Lorentz transformations becomes to a simpler transformation rule for the light ’ s frequency , which can then be interpreted as a traditional way to relate the observers ’ own reference beams when they motion with regard to one another .",
        "rewrite_text": "The Lorentz transformations constitute a group that explains how the principles of mechanics shift when objects move at a constant velocity. A group refers to a set of transformations, possessing an identity element, which can be combined with each other. In this context, the Lorentz transformations illustrate how entities undergo velocity-induced shifts. Specifically, by understanding the velocity of an object, one can use the Lorentz transformations to determine its temporal changes. This, in turn, enables the spatial transformation to be derived. This page demonstrates that, although the Lorentz transformations were introduced long ago, they offer valuable lessons in approaching both mathematics and mechanics. It particularly explores the significance of distinguishing between first and second-order equations in physics.\n\nThe essay explores the origins of the Lorentz transformations and shows that they can be derived from more general processes, which in turn are derived from the requirement that the laws of mechanics should remain unchanged over time. From this broader perspective, the Lorentz transformations can be viewed as a unique case that can cause confusion if not properly understood. The section then considers an example from special relativity, where a person measures the speed of light in two different inertial frames of reference. This example highlights the need to consider both the spatial and temporal aspects of the Lorentz transformations as the speed of light is a function of both components. In this instance, the temporal aspect of the Lorentz transformations can be interpreted as a shift in reference frames, altering the rate of timekeeping in different frames relative to each other.\n\nFrom this, it is observed that specifying the spatial transformation between two reference frames allows for the determination of the temporal changes. In the second part of the article, the example is generalized to two inertial reference frames moving relative to each other, leading to a situation known as the Relativistic Doppler effect. This again involves considering both spatial and temporal components of the Lorentz transformations. The section then considers a scenario where observers in both reference frames agree that light should move at a fixed speed, regardless of their relative motion. In this case, the temporal aspect of the Lorentz transformations simplifies to a transformation rule for the frequency of light, which can be interpreted as a traditional method to relate observers' own reference frames when they move relative to each other.",
        "ori-fast-z-score": 1.3198240351921797,
        "water-fast-z-score": 9.825356706430671,
        "rewrite-fast-z-score": 3.5386069477175313
    },
    {
        "original_text": "A tool for computational cryptography known as Rime can establish security of communication over a channel by use of ideal lattices. A strong R-matrix is a non-linear operator that, together with a related linear operator (the S-matrix), yields an associative product that defines a finite-dimensional non-commutative ring with unity, known as a finite braided commutative algebra (FBCA). In this work we describe computation of R-matrices for eight binary QC codes using the RIME framework. We use the abelian setting where the S-matrix is a diagonal matrix to compute R-matrices that are also diagonal, and give examples of non-abelian R-matrices for the same codes. We discuss how the R-matrices interact with RIME to establish ideal lattices for each of the codes. The eight codes we study are optimal two-level codes, two-level extended Hamming codes, and five four-level codes commonly used in surface-code quantum computing. Our computed R-matrices have many nice properties, including compatibility with the abelian setting and simple structure when viewed as a lattice over the real numbers, rational numbers, or complex numbers. In particular, we describe the case where the R-matrix is a real diagonal matrix as a families of real numbers (depending on several parameters) where each number occurs with multiplicity one and each occurs exactly twice. We also show how to compute R-matrices corresponding to non-abelian structures when the S-matrix is diagonal. The RIME framework for computing R-matrices in the abelian setting makes use of an appropriately modified quantum chess problem, the Hamilton Pfaffian QC-polynomial. In particular, each symmetry of the chess problem corresponds to a permutation matrix in the R-matrix, and these permutations satisfy a precise lattice-theoretical restriction. In the non-abelian case, no analogous check for R-matrix structure appears to be known, so alternative techniques must be used to establish non-abelian R-matrices. In this work, we describe a non-abelian RIME framework that similarly uses a modified Hamilton Pfaffian but with the permutations no longer required to satisfy a lattice-theoretical restriction. Instead, an ad-hoc method for establishing non-abelianity is used. Finally, we present a few experiments using RIME to establish security of communication over common communication channels (such as WiFi or cell-phone networks). We show that our computed R-matrices are strong and that ideal lattices produced using the R-matrices are the best lattice-based schemes we can find for each of the codes we study. The runtime for each computation of R-matrices and the resulting lattice was less than 12 hours on a standard laptop.",
        "watermark_text": "A tool for computational cryptography referred as Rime can establish security of communication over a channel by using of optimal lattices . A strong R - vector is a non - linear operator that , combined with a related linear operator ( the S - vector ) , yields an associative product that defines a finite - dimensional non - commutative field with unity , known as a finite braided commutative algebra ( FBCA ) .In this study we explain computation of R - matrices for eight binary QC coding using the RIME framework . We use the abelian setting where the S - vector is a diagonal matrix to compute R - matrices that are also diagonal , and take examples of non - abelian R - matrices for the same codes .We discuss how the R - matrices interact with RIME to create perfect lattices for each of the codes . The eight codes we study are optimal two - level codes , two - level extended Hamming codes , and five four - level codes commonly used in surface - code quantum modeling .Our computed R - matrices have many nice properties , notably consistency with the abelian setting and simple structure when regarded as a lattice over the real numbers , rational numbers , or complex numbers . In particular , we define the case where the R - vector is a real diagonal matrix as a families of real numbers ( depending on various variables ) where each number occurs with multiplicity one and each appears precisely twice .We additionally prove how to compute R - matrices corresponding to non - abelian structures when the S - vector is diagonal . The RIME implementation for solving R - matrices in the abelian setting includes use of an appropriately modified quantum chess question , the Hamilton Pfaffian QC - polynomial .In particular , each symmetry of the chess question corresponds to a permutation matrix in the R - matrix , and these permutations satisfy a precise lattice - theory restriction . In the non - abelian case , no analogous check for R - vector structure appears to be known , so new tools must be used to obtain non - abelian R - matrices .In this study , we define a non - abelian RIME implementation that similarly utilizes a altered Hamilton Pfaffian but with the permutations no longer required to meet a lattice - theory restriction . Instead , an ad - hoc method for determining non - abelianity is utilized .Finally , we present a few experiments using RIME to create security of communication over common communication streams ( such as WiFi or cellular - phone networks ) . We see that our computed R - matrices are strong and that optimal lattices created using the R - matrices are the best lattice - based schemes we can find for each of the codes we study .The runtime for each computation of R - matrices and the resulting lattice was less than 12 hours on a basic laptop .",
        "rewrite_text": "A computational cryptography tool named Rime employs optimal lattices to establish secure communication over a channel. A robust R-vector, a non-linear operator, combined with an associated linear operator, the S-vector, generates an associative product that defines a finite-dimensional non-commutative field with a unity element, known as a finite braided commutative algebra (FBCA).\n\nIn this study, we explain the computation of R-matrices for eight binary QC coding schemes using the RIME framework. We employ an abelian setting where the S-vector is a diagonal matrix to compute diagonal R-matrices and provide examples of non-abelian R-matrices for the same codes. We discuss how the R-matrices interact with RIME to create perfect lattices for each code. The eight codes we investigate are optimal two-level codes, two-level extended Hamming codes, and five four-level codes commonly used in surface code quantum modeling.\n\nOur computed R-matrices possess numerous desirable properties, notably their consistency with the abelian setting and their straightforward structure when viewed as lattices over the real, rational, or complex numbers. Specifically, we define a scenario where the R-vector is a real diagonal matrix as a family of real numbers, where each number appears with multiplicity one and exactly twice. We also demonstrate how to compute R-matrices corresponding to non-abelian structures when the S-vector is diagonal.\n\nThe RIME implementation for solving R-matrices in the abelian setting incorporates a suitably modified quantum chess problem, the Hamilton Pfaffian QC polynomial. Specifically, each symmetry in the chess problem corresponds to a permutation matrix in the R-matrix, satisfying precise lattice theory restrictions. In the non-abelian case, no analogous check for R-vector structure is known, necessitating the use of new tools to obtain non-abelian R-matrices. In this study, we introduce a non-abelian RIME implementation that utilizes an altered Hamilton Pfaffian approach without requiring permutations to meet lattice theory restrictions. Instead, an ad-hoc method is employed to determine non-abelianity.\n\nFinally, we present experiments utilizing RIME to enhance communication security in common communication streams such as WiFi or cellular phone networks. Our computed R-matrices are robust, and the optimal lattices created using these R-matrices are the best lattice-based schemes we have found for each code we investigate. The runtime for each computation of R-matrices and resulting lattice was less than 12 hours on a standard laptop.",
        "ori-fast-z-score": -0.3492151478847891,
        "water-fast-z-score": 7.158994882626233,
        "rewrite-fast-z-score": 1.866691212406119
    },
    {
        "original_text": "Co oxides have recently attracted much attention due to their possible role in the recently discovered phenomenon of high temperature superconductivity. One particular cobalt oxide, Na$_x$CoO$_2$, has received considerable attention due to its resemblance to high Tc copper oxides in many physical properties. Despite this similarity, the material is particularly unusual in that charge and spin variables are both almost perfectly balanced, giving the material the nickname “bad metal.” A common approach to understanding the physical properties of these materials has been to introduce electronic correlations by treating the cobalt 3d electrons as a localized effective degrees of freedom. Here we introduce a fermion lattice model that incorporates strong electronic correlations without explicit treatment of the local cobalt orbitals. Using dynamical mean field theory we calculate the electronic and thermoelectric properties of the model as a function of doping, and find that while intermediate correlations enhance the electronic specific heat, they also give rise to large thermoelectric responses, making the material a good metal.",
        "watermark_text": "Co oxides have recently attracted much attention due to their possible involvement in the recently discovered phenomenon of high heat superconductivity . One particular cobalt metal , Na $ _ x $ CoO $ _ 2 $ , has garnered considerable scrutiny due to its similarity to large Tc copper oxides in many mechanical properties .Despite this similarity , the metal is especially unusual in that charge and twist variables are both nearly completely balanced , giving the metal the nickname “ poor metal . ” A popular approach to discussing the physical properties of these metal has been to introduce electronic correlations by treating the cobalt 3d ions as a localized effective degrees of liberty . Here we create a fermion lattice model that incorporates powerful electronic correlations without explicit treatment of the local cobalt orbitals .Using dynamical mean field theory we determine the electronic and thermoelectric characteristics of the model as a function of doping , and find that while intermediate correlations boost the electronic specific warmth , they still bring rise to large thermoelectric responses , making the metal a better material .",
        "rewrite_text": "Recently, co-oxides have become highly scrutinized due to their potential involvement in the recently discovered high-heat superconductivity phenomenon. Specifically, the cobalt-based metal Na$_x$CoO$_2$ has garnered significant attention due to its remarkable similarity with large Tc copper oxides in numerous mechanical properties. Despite this resemblance, this metal stands out due to its nearly perfect balance of charge and twist variables, earning it the moniker of \"poor metal.\"\n\nA prevalent approach to studying the physical properties of these metals involves introducing electronic correlations by treating the localized 3d ions of cobalt as effective degrees of freedom. In our approach, we create a fermion lattice model that effectively incorporates strong electronic correlations without explicitly addressing the local cobalt orbitals. By utilizing dynamical mean field theory, we determine the electronic and thermoelectric properties of this model as a function of doping. Our findings indicate that while intermediate correlations enhance the electronic specific heat, they also lead to significant thermoelectric responses, making this metal a more promising material.",
        "ori-fast-z-score": -1.6081688022566922,
        "water-fast-z-score": 5.253351420705195,
        "rewrite-fast-z-score": 1.7056057308448833
    },
    {
        "original_text": "In this work, we develop a microscopic theory for toroidal moments in bulk periodic crystals. By employing a systematic, momentum-space approach, we derive the general form of the effective Hamiltonian in the presence of magnetic-anisotropy and Dzyaloshinsky-Moriya interactions. The theory is then applied to describe the nature of toroidal moments in magnesium diboride (MgB2) and dichalcogenides with either orthorhombic (TMDC) or polar (WS2) structures. Our calculations demonstrate the coexistence of symmetric and antisymmetric toroidal moments in MgB2, whereas in TMDC and WS2 only antisymmetric toroidal moments are allowed. We show that the antisymmetric toroidal moments in MgB2 and TMDC are equivalent and induce an effective magnetic field that modifies the dispersion of surface plasmons. In WS2, we predict the emergence of a new circularly-polarized plasmon mode that is protected by symmetry and not affected by the inclusion of Coulomb interactions. These predictions could be experimentally verified by means of spectroscopic techniques such as Raman scattering and infrared optical absorption. Our work provides a solid theoretical foundation for engineering toroidal moment couplings in periodic crystals and for exploiting their emergent electromagnetic properties in plasmonics and excitonics.",
        "watermark_text": "In this research , we develop a microscopic theory for toroidal moments in bulk periodic crystals . By using a comprehensive , momentum - space approach , we derive the general form of the effective Hamiltonian in the presence of magnetic - anisotropy and Dzyaloshinsky - Moriya interactions .The theory is then utilized to explain the nature of toroidal moments in magnesium diboride ( MgB2 ) and dichalcogenides with either orthorhombic ( TMDC ) or polar ( WS2 ) structures . Our calculations prove the coexistence of symmetric and antisymmetric toroidal moments in MgB2 , whereas in TMDC and WS2 only antisymmetric toroidal moments are allowed .We see that the antisymmetric toroidal moments in MgB2 and TMDC are comparable and induce an efficient magnetic force that modifies the dispersion of exterior plasmons . In WS2 , we estimate the emergence of a new circularly - polarized plasmon mode that is guarded by symmetry and not affected by the introduction of Coulomb interactions .These assumptions could be experimentally confirmed by means of spectroscopic techniques such as Raman absorption and infrared optical emission . Our research provides a solid conceptual foundation for studying toroidal moment couplings in periodic crystals and for exploiting their emergent optical properties in plasmonics and excitonics .",
        "rewrite_text": "In this study, we have formulated a microscopic theory for toroidal moments in bulk periodic crystals. By employing a comprehensive momentum-space approach, we have derived the general form of the effective Hamiltonian in the presence of magnetic anisotropy and Dzyaloshinsky-Moriya interactions. This theory is applied to elucidate the nature of toroidal moments in magnesium diboride (MgB2) and dichalcogenides with orthorhombic (TMDC) or polar (WS2) structures.\n\nOur calculations demonstrate the coexistence of both symmetric and antisymmetric toroidal moments in MgB2, whereas in TMDC and WS2, only antisymmetric toroidal moments are allowed. We observe that the antisymmetric toroidal moments in MgB2 and TMDC are comparable and generate a potent magnetic force that alters the dispersion of external plasmons. In WS2, we estimate the emergence of a novel circularly polarized plasmon mode, which is protected by symmetry and remains unaffected by the introduction of Coulomb interactions.\n\nThese assumptions can be experimentally verified using spectroscopic techniques such as Raman absorption and infrared optical emission. Our research provides a solid conceptual framework for studying toroidal moment couplings in periodic crystals and exploiting their emerging optical properties in plasmonics and excitonics.",
        "ori-fast-z-score": -0.1203858530857692,
        "water-fast-z-score": 6.139678507374229,
        "rewrite-fast-z-score": 3.2504180333157686
    },
    {
        "original_text": "The acceleration of the universe is still unknown. One of the most popular explanations is the dark energy, described by a cosmological constant or by a dynamical form (quintessence). A priori, it is not possible to discriminate between these two scenarios with present data. We show that it is possible to make this discrimination if we have enough information about the evolution of the universe. We consider only linear cosmological perturbations and we focus on the case of a constant equation of state, W=0. We show that it is possible to discriminate between these two scenarios if W is constant with a high accuracy for a period larger than 50% of the age of the universe. If the equation of state is not constant, it is not possible to discriminate between these two scenarios. If the equation of state is constant but its accuracy is not very high, it is not possible to discriminate between these two scenarios. If the equation of state is constant but it is not constant enough, it is not possible to discriminate between these two scenarios. If the equation of state is constant but it varies a lot, it is possible to discriminate between these two scenarios, but only for values of W close to -1. We conclude that it is possible to discriminate between a cosmological constant and quintessence if the equation of state is constant and very different from -1. This result does not prove that the dark energy is a cosmological constant or quintessence, but it gives some constraints on these two scenarios. In general, it is not possible to discriminate between two possible dark energy scenarios if their behaviour is very similar. However, if we assume the equation of state of dark energy is constant, it is still possible to discriminate between a cosmological constant and quintessence. This discrimination can be done with current data, but we need to assume the equation of state is constant. It would be interesting to check this with more precise data in the future, in order to understand if this assumption is really needed or not. This research was done with Pierre Fleury, Cyril Brouder and Romain Teyssier. It has been published on August 10, 2014 in Physics Letters B 732. https://arxiv.org/abs/1408.2578 This research shows that it is possible to discriminate between a cosmological constant and quintessence if the equation of state is constant and very different from -1. This result does not prove that the dark energy is a cosmological constant or quintessence, but it gives some constraints on these two scenarios. In general, it is not possible to discriminate between two possible dark energy scenarios if their behaviour is very similar. However, if we assume the equation of state of dark energy is constant, it is still possible to discriminate between a cosmological constant and quintessence. This discrimination can be done with current data",
        "watermark_text": "The velocity of the universe is remains unknown . One of the most popular explanations is the dark energy , described by a cosmological factor or by a dynamical shape ( quintessence ) .A priori , it is not possible to discriminate between these two scenarios with present data . We see that it is possible to make this discrimination if we have sufficient information about the evolution of the universe .We consider only linear cosmological perturbations and we focus on the case of a constant equation of state , W = 0 . We see that it is easy to discriminate between these two scenarios if W is constant with a high clarity for a period larger than 50 % of the age of the universe .If the equation of state is not constant , it is not possible to discriminate between these two scenarios . If the equation of state is constant but its accuracy is not very high , it is not possible to discriminate between these two scenarios .If the equation of state is constant but it is not zero enough , it is not possible to discriminate between these two scenarios . If the equation of state is constant but it varies a lot , it is possible to discriminate between these two scenarios , but only for values of W close to - 1 .We suggest that it is easy to discriminate between a cosmological coefficient and quintessence if the equation of state is constant and very different from - 1 . This result does not prove that the dark energy is a cosmological constant or quintessence , but it gives some restrictions on these two scenarios .In general , it is not practical to discriminate between two possible dark energy scenarios if their behaviour is very identical . However , if we suppose the equation of state of bright energy is constant , it is nevertheless possible to discriminate between a cosmological coefficient and quintessence .This discrimination can be performed with current data , but we require to assume the equation of state is constant . It would be interesting to test this with more precise data in the future , in order to explain if this assumption is really needed or not .This research was done with Pierre Fleury , Cyril Brouder and Romain Teyssier . It has been publication on August 10 , 2014 in Physics Letters B 732 . https : / / arxiv . org / abs / 1408 . 2578 This research shows that it is easy to discriminate between a cosmological coefficient and quintessence if the equation of state is constant and very different from - 1 .This result does not prove that the dark energy is a cosmological constant or quintessence , but it gives some restrictions on these two scenarios . In general , it is not possible to discriminate between two possible dark energy scenarios if their behaviour is very identical .However , if we suppose the equation of state of dark energy is constant , it is nevertheless possible to discriminate between a cosmological coefficient and quintessence . This discrimination can be performed with current data",
        "rewrite_text": "The unknown velocity of the universe remains a mystery. One of the most commonly accepted explanations is dark energy, which is described by a cosmological factor or by a dynamical shape known as quintessence. At present, it is not feasible to distinguish between these two theories with existing data. However, it becomes possible to make such a distinction if we possess sufficient information about the evolution of the universe.\n\nIn our analysis, we have considered only linear cosmological perturbations and focused on a constant equation of state where W equals 0. We find that if W remains constant, it becomes much easier to differentiate between the two scenarios with clarity lasting for more than 50% of the universe's age. If the equation of state is not constant, or its accuracy is not high, or its value is not sufficiently close to zero, it remains challenging to distinguish between the two theories. However, if the equation of state is constant but varies significantly, discrimination between the two becomes possible, especially for values of W close to -1.\n\nOur research suggests that it is relatively straightforward to differentiate between a cosmological coefficient and quintessence when the equation of state is both constant and significantly different from -1. This finding does not definitively confirm that dark energy is a cosmological constant or quintessence, but it does provide some constraints on these two theories. Generally speaking, it may not be practical to differentiate between two potential dark energy scenarios if their behaviors are too similar. Nevertheless, assuming the equation of state for dark energy is constant, we can still differentiate between a cosmological coefficient and quintessence using current data. While this discrimination requires an assumption about the constant equation of state, it would be interesting to test this with more precise data in the future to determine if this assumption is truly necessary.\n\nThis research, conducted by Pierre Fleury, Cyril Brouder, and Romain Teyssier, was published on August 10th, 2014 in Physics Letters B 732. The research can be accessed at https://arxiv.org/abs/1408.2578. Our findings highlight that it is feasible to distinguish between a cosmological coefficient and quintessence when the equation of state is both constant and distinct from -1. Again, this does not prove dark energy's identity, but it does offer some restrictions on the two theories being considered. In general terms, distinguishing between two potential dark energy scenarios can be challenging if their behaviors are too similar. However, if we accept that the equation of state for dark energy remains constant, discrimination between a cosmological coefficient and quintessence becomes feasible with current data.",
        "ori-fast-z-score": 3.456966485800899,
        "water-fast-z-score": 8.642416214502248,
        "rewrite-fast-z-score": 2.548407742585929
    },
    {
        "original_text": "We introduce a variant of the Simon games called Entangled Simon games. In Entangled Simon games, two players, Alice and Bob, take turns to send predetermined unitary operations to a joint quantum system, where the joint state is an entangled initial state. At the end of the game, Alice reveals the operation she performed, and the task of the judge, the referee, is to determine which unitary operation was performed based on the information received and the entangled state at the beginning of the game. We analyze the complexity of this task, proving that even for a simple entangled initial state, the task is hard to approximate within a constant factor. We introduce a variant of the Simon games called Entangled Simon games. In Entangled Simon games, two players, Alice and Bob, take turns to send predetermined unitary operations to a joint quantum system, where the joint state is an entangled initial state. At the end of the game, Alice reveals the operation she performed, and the task of the judge, the referee, is to determine which unitary operation was performed based on the information received and the entangled state at the beginning of the game. We analyze the complexity of this task, proving that even for a simple entangled initial state, the task is hard to approximate within a constant factor. Our result implies that even for a simple entangled state, entangled games are hard to approximate within a constant factor. It also shows that the entangled game is hard to approximate even when the initial state is close to a product state. Our result implies that even for a simple entangled state, entangled games are hard to approximate within a constant factor. It also shows that the entangled game is hard to approximate even when the initial state is close to a product state. Unlike standard Simon games, entangled Simon games involve sending unitary operations to a joint quantum system. At the end of the game, Alice reveals the operation she performed, and the task of the judge, the referee, is to determine which unitary operation was performed based on the information received and the entangled state at the beginning of the game. This is hard to do in the worst case, even when the unitary operations are independent and identical distributions (IID), as we prove in this paper. Unlike standard Simon games, entangled Simon games involve sending unitary operations to a joint quantum system. At the end of the game, Alice reveals the operation she performed, and the task of the judge, the referee, is to determine which unitary operation was performed based on the information received and the entangled state at the beginning of the game. This is hard to do in the worst case, even when the unitary operations are independent and identical distributions (IID), as we prove in this paper. The complexity of the Entangled Simon game is the complexity of the following problem: A judge, Alice, is given a unitary operation U, and the state s at the beginning of a game. The game ends after k turns, at which",
        "watermark_text": "We introduce a version of the Simon games named Entangled Simon games . In Entangled Simon games , two participants , Alice and Bob , take turns to give predetermined unitary operations to a joint quantum system , where the joint state is an entangled original state .At the end of the match , Alice reveals the operation she accomplished , and the task of the judge , the referee , is to judge which unitary procedure was done based on the information received and the entangled state at the beginning of the play . We evaluate the complexity of this job , finding that even for a simple entangled original state , the task is hard to approximate within a constant factor .We introduce a version of the Simon games named Entangled Simon games . In Entangled Simon games , two participants , Alice and Bob , take turns to give predetermined unitary operations to a joint quantum system , where the joint state is an entangled original state .At the end of the match , Alice reveals the operation she accomplished , and the task of the judge , the referee , is to judge which unitary procedure was done based on the information received and the entangled state at the beginning of the play . We evaluate the complexity of this job , finding that even for a simple entangled original state , the task is hard to approximate within a constant factor .Our result requires that even for a simple entangled state , entangled rules are hard to approximate within a constant factor . It additionally shows that the entangled game is tough to approximate even when the first state is close to a product state .Our result requires that even for a simple entangled state , entangled rules are hard to approximate within a constant factor . It additionally shows that the entangled game is tough to approximate even when the first state is close to a product state .Unlike standard Simon matches , entangled Simon matches require transferring unitary operations to a joint quantum system . At the end of the play , Alice reveals the operation she accomplished , and the task of the judge , the referee , is to judge which unitary procedure was done based on the information received and the entangled state at the beginning of the play .This is hard to do in the worst case , even when the unitary operations are independent and identical distributions ( IID ) , as we prove in this paper . Unlike standard Simon games , entangled Simon games involve sending unitary operations to a joint quantum system .At the end of the match , Alice reveals the operation she accomplished , and the task of the judge , the referee , is to find which unitary procedure was done based on the information received and the entangled state at the beginning of the play . This is tough to do in the worst case , even when the unitary operations are separate and identical distributions ( IID ) , as we prove in this paper .The complexity of the Entangled Simon game is the complexity of the following problem : A judge , Alice , is given a unitary operation U , and the state s at the beginning of a game . The tournament concludes after k turns , at which",
        "rewrite_text": "改写后的英文文本如下：\n\nWe present an enhanced version of the Simon game known as the Entangled Simon Game. In this game, two participants - Alice and Bob - take turns applying predetermined unitary operations to a shared quantum system. This shared system starts in an entangled original state. Once the game ends, Alice discloses the operation she performed, and the task of the judge, or referee, is to determine which unitary procedure was executed based on the received information and the entangled state at the start of the game. We assess the complexity of this task and discover that, even for a straightforward entangled initial state, it is challenging to approximate within a constant factor.\n\nOur findings highlight that even for basic entangled states, the entangled rules are difficult to approximate within a constant range. Additionally, it demonstrates that the entangled game remains challenging to approximate even when the initial state is closely resembling a product state. In contrast to traditional Simon games, the Entangled Simon Game necessitates the transfer of unitary operations to a joint quantum system. At the end of play, Alice reveals her accomplished operation, and the judge's task is to determine which unitary procedure was implemented based on the received data and the initial entangled state. As we prove in this paper, this task is particularly challenging in the worst-case scenario, even when the unitary operations follow independent and identical distributions (IID).\n\nThe complexity of the Entangled Simon Game lies at the core of a specific problem: A judge, Alice, is presented with a unitary operation U and the initial state s of a game. After k rounds of play, the tournament concludes.",
        "ori-fast-z-score": -1.8226447537315342,
        "water-fast-z-score": 5.728312083156251,
        "rewrite-fast-z-score": -0.08804509063256238
    },
    {
        "original_text": "Explanations for the alignment of the major axes of galaxies with their spin axes, such as observational biases, imply that galaxy discs are not randomly oriented in space. However, previous measurements of the orientations of galaxies have been averaged over large volumes and are therefore not ideal for detecting coherent alignment on small scales. We calculate the orientations of galaxies within their large-scale cosmic environments using a sample of 50,000 bright galaxies from the Sloan Digital Sky Survey Data Release 7. We use the positions of satellite galaxies around these galaxies as a probe of their local cosmic environment, and find that the distribution of satellite galaxy locations is strongly anisotropic. This is consistent with previous observations that galaxies tend to be aligned with the large-scale structure in which they are embedded. We find that the major axes of satellite galaxies are preferentially aligned with the direction of the large-scale structure to which their central galaxy is most closely bound, but that the distribution of orientations is only consistent with isotropy at the 98% confidence level. This anisotropy is most likely caused by interactions between galaxies and their dark matter hosts, as the alignment with the large-scale structure persists even when controlling for galaxy shape and other properties.",
        "watermark_text": "Explanations for the alignment of the main axes of stars with their spin axes , such as observational biases , imply that galaxy discs are not randomly oriented in space . However , previous measurements of the orientations of galaxies have been averaged over large volumes and are thus not optimal for detecting coherent alignment on small scales .We calculate the orientations of galaxies within their large - scale cosmic environments using a sample of 50 , 000 bright galaxies from the Sloan Digital Sky Survey Data Release 7 . We use the places of satellite galaxies around these galaxies as a probe of their nearby cosmic surroundings , and find that the distribution of satellite galaxy locations is strongly anisotropic .This is consistent with previous findings that galaxies tend to be aligned with the huge - scale system in which they are embedded . We see that the main axes of satellite galaxies are preferentially aligned with the direction of the big - scale system to which their central galaxy is most closely bound , but that the distribution of orientations is only consistent with isotropy at the 98 % confidence rate .This anisotropy is most likely affected by interactions between galaxies and their black material hosts , as the alignment with the big - scale system persists even when controlling for galaxy shape and other properties .",
        "rewrite_text": "Explanations for aligning the primary axes of stars with their spin axes, such as observational biases, suggest that galaxy discs are not randomly oriented in space. However, previous measurements of galaxy orientations have been averaged over vast regions, making them inadequate for detecting coherent alignment on smaller scales. To calculate the orientations of galaxies within their vast cosmic environments, we utilize a sample of 50,000 bright galaxies from the Sloan Digital Sky Survey Data Release 7. We employ the positioning of satellite galaxies around these main galaxies as a probe of their nearby cosmic surroundings and discover that the distribution of satellite galaxy locations is markedly anisotropic. This finding aligns with previous research indicating that galaxies tend to align with the larger-scale system in which they are embedded. Our observations reveal that the primary axes of satellite galaxies are preferentially aligned with the direction of the larger-scale system most closely linked to their central galaxy. However, the distribution of orientations is only consistent with isotropy at a 98% confidence level. This anisotropy is likely influenced by interactions between galaxies and their black material hosts, as the alignment with the larger-scale system persists even when accounting for galaxy shape and other properties.",
        "ori-fast-z-score": -1.237705495510552,
        "water-fast-z-score": 3.09426373877638,
        "rewrite-fast-z-score": 0.7258661863112977
    },
    {
        "original_text": "Esprimides Javier, Grigoryants Yaroslav and Trefil Boris. “Weak gravitational lensing in (2+1) gravity.” Physical Review D, 83.8 (July 2010): 084047. doi: 10.1103/PhysRevD.83.084047. In (2+1) gravity, which describes the spatial section of gravity in three dimensions, gravity has two propagating degrees of freedom instead of three. The lensing properties of (2+1) gravity were not studied before. In this paper, we calculate the deflection angle and the shear of light in (2+1) gravity and show that the theory allows for strong lensing, which could be tested with future astronomy. We use the exact solution of the (2+1) gravitational field generated by a point mass as a lens. We obtain an explicit expression for the deflection angle by the point mass in (2+1) gravity. For a point mass in (2+1) gravity the deflection angle is equal to two thirds of the inverse cube of the distance from the point mass to the source. We also calculate the shear of light in (2+1) gravity and show that the theory allows for strong lensing. Our findings can be tested with future astronomy and could be used to test (2+1) gravity.",
        "watermark_text": "Esprimides Javier , Grigoryants Yaroslav and Trefil Boris . “ Weak gravitational lensing in ( 2 + 1 ) gravity . ” Physical Review D , 83 . 8 ( July 2010 ) : 084047 . doi : 10 . 1103 / PhysRevD . 83 . 084047 .In ( 2 + 1 ) gravity , which defined the spatial section of gravitational in three dimensions , gravity has two propagating degrees of liberty instead of three . The lensing qualities of ( 2 + 1 ) gravity were not studied before .In this paper , we determine the deflection angle and the shear of light in ( 2 + 1 ) gravity and find that the principle allows for strong lensing , which could be tested with current astronomy . We use the exact solution of the ( 2 + 1 ) gravity field produced by a point mass as a lens .We get an explicit expression for the deflection angle by the point mass in ( 2 + 1 ) gravity . For a point mass in ( 2 + 1 ) gravity the deflection angle is equal to two thirds of the inverse cube of the distance from the point mass to the origin .We also measure the shear of light in ( 2 + 1 ) gravity and suggest that the principle allows for strong lensing . Our findings can be tested with future astronomy and could be used to test ( 2 + 1 ) gravity .",
        "rewrite_text": "Esprimides, Javier, Grigoryants, Yaroslav, and Trefil, Boris, explored \"Weak gravitational lensing in (2 + 1) gravity\" in the Physical Review D, volume 83, issue 8 (published in July 2010), with a DOI: 10.1103/PhysRevD.83.084047. In (2 + 1) gravity, which defines the three-dimensional spatial section of gravity, there are two degrees of freedom for gravitational propagation instead of three. Prior to this study, the lensing properties of (2 + 1) gravity had not been examined.\n\nIn this paper, the team determined the deflection angle and the shear of light in (2 + 1) gravity, finding that the principle permits strong lensing, which could be verified through contemporary astronomy. They utilized the exact solution of the (2 + 1) gravity field generated by a point mass as a lens. They derived an explicit expression for the deflection angle caused by a point mass in (2 + 1) gravity, which is equal to two-thirds of the inverse cube of the distance from the point mass to the origin.\n\nFurthermore, they measured the shear of light in (2 + 1) gravity and suggested that the principle allows for significant lensing effects. Their findings can be tested in future astronomical observations and may serve as a means to test (2 + 1) gravity theory.",
        "ori-fast-z-score": 1.709408646894569,
        "water-fast-z-score": 5.888972745734182,
        "rewrite-fast-z-score": 3.6666666666666665
    },
    {
        "original_text": "BL Lacertae is a BL Lac object, which is one of the most promising candidates of the still unconfirmed Dark Matter, distributed approximately uniformly throughout our Universe. The inferred Dark Matter density makes it likely that the BL Lacertae is host to a supermassive black hole. BL Lacertae was detected as an active galaxy from the radio band to the high-energy gamma rays. The observations revealed that the total energy output of BL Lacertae in the optical band is dominated by non-thermal emission from a compact core. In this work, we performed multi-wavelength observations of BL Lacertae in the optical band over a period of 12 years. Our results showed that the long-term optical spectral variability can be characterized as two states: a high-energy gamma-ray loud state and a low-energy gamma-ray quiet state. These two states can be corresponded to the variation of the synchrotron and Inverse-Compton components in the optical band. The detailed modeling also indicated that the long-term optical spectral variability is likely due to the change of the Doppler factor of the jet with the variation of the total magnetic field.",
        "watermark_text": "BL Lacertae is a BL Lac object , which is one of the most likely candidates of the still unconfirmed Dark Matter , dispersed approximately uniformly throughout our Universe . The inferred Dark Matter density makes it likely that the BL Lacertae is home to a supermassive black hole .BL Lacertae was measured as an active galaxy from the radio band to the high - energy gamma radiation . The studies showed that the total energy output of BL Lacertae in the optical band is dominated by non - thermal emission from a compact core .In this research , we performed multi - wavelength experiments of BL Lacertae in the optical band over a period of 12 years . Our results showed that the long - term optical spectral variability can be described as two states : a high - energy gamma - ray loud state and a high - energy gamma - ray silent state .These two states can be corresponded to the changes of the synchrotron and Inverse - Compton parts in the optical band . The detailed simulation additionally suggested that the long - term optical spectral variability is probably due to the shift of the Doppler factor of the jet with the variation of the total magnetic force .",
        "rewrite_text": "BL Lacertae, a member of the BL Lac objects, is a strong candidate for the as-yet unconfirmed Dark Matter that is distributed nearly uniformly across our Universe. The inferred density of Dark Matter suggests that BL Lacertae may be the abode of a supermassive black hole. This active galaxy has been measured to emit across the radio spectrum to high-energy gamma radiation. Studies have revealed that the majority of the optical band's total energy output is dominated by non-thermal emission from its compact core.\n\nIn our research, we conducted multi-wavelength experiments on BL Lacertae in the optical band over a 12-year period. Our findings indicate that the long-term optical spectral variability can be categorized into two states: a high-energy gamma-ray active state and a high-energy gamma-ray quiet state. These two states can be linked to the variations in the synchrotron and Inverse-Compton components in the optical spectrum. Detailed simulations further suggest that this long-term variability in the optical spectrum is likely attributed to the change in the Doppler factor of the jet due to fluctuations in total magnetic force.",
        "ori-fast-z-score": 0.8728715609439696,
        "water-fast-z-score": 5.237229365663818,
        "rewrite-fast-z-score": 1.2074068598865937
    },
    {
        "original_text": "Cosmic rays play a key role in the evolution of dense molecular clouds. They induce ionization and chemical reactions and trigger the formation of star clusters. In this work, we show that high-energy particles produced by pulsars and active galaxies can have a significant effect on the formation of the first stars and galaxies. We performed detailed calculations of the formation of Population III (Pop III) stars in minihalos subject to the combined effect of the photoelectric effect and ionization, as well as the pair production and photon pressure exerted by high-energy particles. We find that the mean free path of low-energy particles is much larger than the size of the minihalos, and thus the effect of cosmic rays is to induce local variations in the density and temperature but has no global effect. We also estimate the effect of cosmic rays on the formation of the first low-mass galaxies. At the end of the simulations, we calculate the expected fraction of metals produced by Pop III stars. We find that the presence of cosmic rays can increase this fraction by up to one order of magnitude in the most favorable conditions. We discuss the conditions under which this process can lead to the early transition from Pop III to Pop II star formation.",
        "watermark_text": "Cosmic rays play a key importance in the evolution of dense molecular clouds . They cause ionization and chemical processes and trigger the formation of star clusters .In this research , we prove that high - energy ions produced by pulsars and active galaxies can have a substantial impact on the formation of the first stars and galaxies . We conducted precise analyses of the formation of Population III ( Pop III ) galaxy in minihalos according to the combined influence of the photoelectric effect and ionization , as also as the pair production and photon pressure exerted by high - energy ions .We see that the mean free path of lowest - energy molecules is much larger than the length of the minihalos , and therefore the impact of cosmic rays is to create regional changes in the density and heat but has no international effect . We also predict the impact of cosmic rays on the formation of the first lowest - mass stars .At the end of the simulations , we determine the expected proportion of metals produced by Pop III stars . We see that the presence of cosmic rays can increase this amount by up to one order of magnitude in the most beneficial conditions .We discuss the conditions under which this process can lead to the early change from Pop III to Pop II star formation .",
        "rewrite_text": "Cosmic rays hold a pivotal significance in the progression of dense molecular clouds. They induce ionization and chemical reactions, triggering the formation of star clusters. In this research, we establish that high-energy ions generated by pulsars and active galaxies can exert a profound influence on the formation of the earliest stars and galaxies. We have conducted intricate analyses on the formation of Population III (Pop III) galaxies within minihalos, considering the combined effects of the photoelectric effect, ionization, pair production, and photon pressure exerted by these high-energy ions.\n\nOur observations indicate that the mean free path of the lowest-energy molecules is significantly greater than the size of the minihalos. Therefore, the impact of cosmic rays is to create local variations in density and temperature but has no global effect. We have also predicted the influence of cosmic rays on the formation of the first, lowest-mass stars. At the end of our simulations, we determine the expected proportion of metals produced by Pop III stars. We find that, in the most favorable conditions, the presence of cosmic rays can increase this amount by up to an order of magnitude. We discuss the conditions under which this process may lead to an early transition from Pop III to Pop II star formation.",
        "ori-fast-z-score": -0.29851115706299675,
        "water-fast-z-score": 6.8657566124489255,
        "rewrite-fast-z-score": 1.6
    },
    {
        "original_text": "A locally decodable code (LDC) is a family of codes for transmitting information bits over a memoryless noisy channel such that the decoder has access to some of the channel outputs but not all of them. Traditionally, the access to the channel outputs is spatially separated at two or more places in the paper, called domain. Domain separation has two drawbacks: first, the channel capacity increases with the number of domains and the code becomes less useful in practice; second, if the channel outputs are ordered according to some predefined pattern, the code becomes easy to interpret and not very difficult to decode. In this paper, we consider the first drawback and show that if the desired number of outputs is a prime number, one can find a nice subset of the field of prime order and a prime factor of a Mersenne number for the channel outputs. This allows to decrease the number of domains to one, and provides a code that is efficiently realizable in practice. We also provide lower bounds on the size of LDC that answer an open question.",
        "watermark_text": "A locally decodable code ( LDC ) is a family of codes for transmitting information bits over a memoryless noisy broadcast such that the decoder has access to some of the channel outputs but not all of them . Traditionally , the entry to the channel outputs is spatially split at two or more spots in the paper , called domain .Domain separation has two drawbacks : first , the channel capacity increases with the number of domains and the code becomes harder helpful in practice ; second , if the channel outputs are arranged according to some predefined pattern , the code becomes easier to comprehend and not very difficult to decode . In this paper , we consider the first drawback and find that if the desired amount of outputs is a prime number , one can find a fine subset of the field of prime order and a prime factor of a Mersenne number for the channel outputs .This enables to reduce the number of domains to one , and provides a code that is efficiently realizable in practice . We additionally offer lower bounds on the length of LDC that answer an open ask .",
        "rewrite_text": "A Locally Decodable Code (LDC) constitutes a set of codes designed for transmitting information bits over a memoryless, noisy broadcast channel. The decoder has partial access to the channel outputs, rather than complete access. Traditionally, the channel outputs are spatially divided into two or more sections on a medium, known as domains. However, domain separation has two challenges:\n\nFirstly, as the number of domains increases, the channel capacity correspondingly rises, making the code more complex in practical applications. Secondly, when the channel outputs are arranged in a pre-defined pattern, the code becomes relatively straightforward to comprehend and decode.\n\nIn this paper, we focus on the first challenge and discover that when the desired number of outputs is a prime number, it is possible to identify a subset from the field of prime order and a prime factor of a Mersenne number for the channel outputs. This allows us to reduce the number of domains to a single one, providing a practical and efficiently realizable code. Additionally, we provide lower bounds on the length of LDC that address an outstanding question in the field.",
        "ori-fast-z-score": 0.5773502691896257,
        "water-fast-z-score": 4.73427220735493,
        "rewrite-fast-z-score": 2.182178902359924
    },
    {
        "original_text": "UScoJ1609-2156 is a low-mass (M_ultimatum=0.69 ± 0.04M⊕), close (d=5.52 ± 0.21 pc) transiting planetary system. UScoJ1609-2156 was first identified by Dieterich et al. (2020) as part of a large transit survey using data from the Dark Energy Camera, and its discovery was announced at the arXiv pre-prints website. Although originally classified as a single-lined spectroscopic binary (which is usually taken to indicate a planetary system), UScoJ1609-2156 actually consists of at least three components, two of which are currently visible with the naked eye. Both USco1609A and B are M-dwarf components in a close hierarchical orbit, with periods of ~1 day and an apsidal alignment such that their combined transit duration is ~6 hours. UScoJ1609-2156C is a wide companion at a projected separation of ~14.7  (~400 AU), and is the most distant component amenable to atmospheric characterization with current facilities. My initial characterization of UScoJ1609-2156 involved a joint session with the American Astronomical Society (AAS) and American Institute of Physics (AIP) in Seattle, Washington, on April 9-11, 2020. I will present an overview of the discovery and analysis of UScoJ1609-2156, along with an overview of current methods of describing, parametrizing, and detecting multi-planet systems. Finally, I will conclude by discussing future opportunities for atmospheric characterization of UScoJ1609-2156C and the overall prospects for characterization of low-mass companions to solar-type stars.",
        "watermark_text": "UScoJ1609 - 2156 is a small - weight ( M _ ultimatum = 0 . 69 ± 0 . 04M⊕ ) , close ( d = 5 . 52 ± 0 . 21 pc ) transiting planetary system . UScoJ1609 - 2156 was first identified by Dieterich et al .( 2020 ) as part of a large transit search using data from the Dark Energy Camera , and its discovery was announced at the arXiv pre - prints website . Although previously classified as a single - lined spectroscopic binary ( which is usually taken to indicate a planetary system ) , UScoJ1609 - 2156 probably consists of at least three components , two of which are currently visible with the naked eye .Both USco1609A and B are M - dwarf components in a close hierarchical orbit , with periods of ~ 1 day and an apsidal orientation such that their total transit duration is ~ 6 hours . UScoJ1609 - 2156C is a broad companion at a projected separation of ~ 14 . 7 ( ~ 400 AU ) , and is the most distant component amenable to atmospheric determination with current technologies .My formal description of UScoJ1609 - 2156 involved a joint conference with the American Astronomical Society ( AAS ) and American Institute of Physics ( AIP ) in Seattle , Washington , on April 9 - 11 , 2020 . I will present an overview of the discovery and assessment of UScoJ1609 - 2156 , along with an overview of recent techniques of describing , parametrizing , and detecting multi - planet systems .Finally , I will conclude by exploring potential opportunities for atmospheric detection of UScoJ1609 - 2156C and the overall prospects for characterization of low - weight companions to solar - class stars .",
        "rewrite_text": "UScoJ1609-2156 is a transiting planetary system that boasts a lightweight (M_ultimatum weighing 0.69 ± 0.04M⊕) and close proximity (d at 5.52 ± 0.21 pc). This system was first identified by Dieterich et al. (2020) during a large-scale transit search utilizing data from the Dark Energy Camera. Its discovery was announced on the arXiv pre-prints website. Originally categorized as a single-lined spectroscopic binary (indicating a planetary system), UScoJ1609-2156 may actually consist of three components, two of which are visible to the naked eye. Specifically, USco1609A and B are M-dwarf components in a close hierarchical orbit, with orbital periods of approximately one day and an apsidal orientation resulting in a total transit duration of around six hours. Meanwhile, UScoJ1609-2156C is a distant companion located at a projected separation of approximately 14.7 (or 400 AU), and is the most remote component currently amenable to atmospheric analysis using current technologies.\n\nMy formal description of UScoJ1609-2156 was presented during a joint conference with the American Astronomical Society (AAS) and American Institute of Physics (AIP) held in Seattle, Washington, between April 9th and 11th, 2020. During the conference, I provided an overview of the discovery and assessment of UScoJ1609-2156, along with an exploration of recent techniques for describing, parametrizing, and detecting multi-planet systems. Ultimately, I concluded by exploring potential opportunities for atmospheric detection of UScoJ1609-2156C and the broader prospects for characterizing low-weight companions to solar-class stars.",
        "ori-fast-z-score": 1.7820842224272613,
        "water-fast-z-score": 6.463946835769319,
        "rewrite-fast-z-score": 4.2
    },
    {
        "original_text": "Massive galaxies at high redshift (z ~ 2) are thought to build up the massive galaxies observed in the local universe1, 2. In this study, we exploit the unique capabilities of the combined GOODS and CANDELS multiwavelength dataset to study the physical processes active in the most massive galaxies at these early epochs. We find that the dominant mode of star formation at these redshifts is quiescent, dry merging, in contrast to popular expectation based on studies of local galaxies3. More quantitatively, we find a continuous increase in the specific star formation rate of massive galaxies from z = 2 to the present day, with corresponding decreases in the ratio of recent to past-average star formation rate and galaxy size. We show that these trends cannot be easily reconciled with a model in which star formation in these galaxies is uniformly delayed until the present day, without otherwise changing their evolution. We conclude that the dominant process setting the star formation rate in the most massive galaxies at these high redshifts is distinct from that in present-day galaxies. We show that while this mode of star formation is inefficient, it drives the growth of the massive galaxies we observe in the local universe. We show that this growth is also non-uniform, with massive galaxies growing by accretion of both companions and smooth ( dry ) mergers with similar fractional contributions from each. This growth through dry merging is sufficient to explain the observed size evolution of massive galaxies. This picture also naturally explains the observed correlation between galaxy structure and star formation: as galaxies continue to grow by dry merging, the growth of the brightest central regions is quicker than that of the outskirts. These results may help to explain several long-standing puzzles about the early growth of massive galaxies. In particular, they suggest that present-day elliptical galaxies grew largely by dry merging throughout much of their lifetimes, a conclusion which may help explain the observed correlation between galaxy structure and star formation. They further imply that massive galaxies at high redshift were not predominantly  downsizing 3-5 in their star formation, as would be implied by the extended periods of intense star formation observed in some high-redshift galaxies6-8. Instead, we suggest that observed differences in the physical processes and rates of star formation in present-day massive galaxies and those in the most massive galaxies at these high redshifts may instead be driven by the much longer timescales since the peak of most recent dry merging. While the details of these differences are still subject to considerable debate, our results point to the importance of the early growth of massive galaxies via dry merging, perhaps even preceding the  monumental  merger class, in explaining the present-day galaxy populations we observe.",
        "watermark_text": "Massive objects at high redshift ( z ~ 2 ) are said to build up the huge objects seen in the local universe1 , 2 . In this study , we utilize the unusual ability of the combined GOODS and CANDELS multiwavelength dataset to study the physical processes active in the most large galaxies at these first epochs .We see that the dominant mode of galaxy formation at these redshifts is quiescent , dry merging , in comparison to popular assumption based on analyses of local galaxies3 . More quantitatively , we find a periodic increase in the specific star formation rate of large galaxies from x = 2 to the present day , with corresponding drops in the proportion of recent to past - average star formation rate and galaxy size .We suggest that these patterns cannot be easily reconciled with a theory in which star formation in these galaxies is uniformly postponed until the present day , without otherwise changing their development . We suggest that the dominant process setting the star formation rate in the most large galaxies at these high redshifts is distinct from that in current - day galaxies .We see that while this mode of galaxy formation is inefficient , it drives the development of the huge clusters we study in the local universe . We see that this growth is also non - uniform , with massive galaxies growing by accretion of both companions and smooth ( wet ) mergers with similar fractional contributions from each .This growth through dry merging is adequate to explain the known size evolution of large galaxies . This picture also readily explains the seen correlation between galaxy formation and star formation : as galaxies begin to expand by dry merging , the development of the brightest central regions is faster than that of the outskirts .These data may support to explain several long - standing challenges about the early growth of large galaxies . In particular , they predict that present - day elliptical galaxies developed largely by dry merging throughout many of their lifetimes , a conclusion which would help justify the seen correlation between galaxy formation and star formation .They further indicate that massive galaxies at high redshift were not primarily downsizing 3 - 5 in their star formation , as would be implied by the extended times of aggressive star formation reported in some high - redshift galaxies6 - 8 . Instead , we suppose that observed changes in the physical processes and rates of galaxy formation in current - day large galaxies and those in the most large galaxies at these high redshifts may rather be motivated by the considerably longer timescales since the peak of most past dry merging .While the details of these changes are still topic to considerable debate , our findings move to the importance of the early growth of large galaxies via dry merging , perhaps even preceding the monumental unification group , in understanding the present - day universe groups we determine .",
        "rewrite_text": "Giant objects observed at high redshift (z ~ 2) are believed to form the enormous structures seen in the local universe. In this research, we employ the unique dataset from the combined GOODS and CANDELS multi-wavelength survey to investigate the physical processes occurring in the largest galaxies during these early epochs. We observe that the dominant mode of galaxy formation at these redshifts is quiet, involving dry mergers, contrasting with the common assumptions based on studies of local galaxies.\n\nQuantitatively, we find a periodic surge in the specific star formation rate of large galaxies from x = 2 to the present day, accompanied by decreases in both the proportion of recent to past average star formation rates and galaxy size. We suggest that these patterns are inconsistent with a theory where star formation in these galaxies is uniformly delayed until the present day without any other changes in their development. We propose that the process determining the star formation rate in the largest galaxies at high redshifts differs from that in modern galaxies.\n\nWhile this mode of galaxy formation may be inefficient, it drives the development of the vast clusters we study in the local universe. This growth is also uneven, with massive galaxies growing through the accretion of both companions and smooth (wet) mergers, with similar fractional contributions from each. The growth via dry merging adequately explains the known size evolution of large galaxies. This picture also readily explains the observed correlation between galaxy formation and star formation: as galaxies expand through dry merging, the development of the brightest central regions accelerates more than that of the outer regions.\n\nThese data may offer insights to several long-standing challenges about the early growth of large galaxies. Specifically, they suggest that present-day elliptical galaxies largely developed through dry merging over much of their lifetimes, a conclusion that would support the observed correlation between galaxy formation and star formation. Furthermore, these findings indicate that massive galaxies at high redshift did not primarily downsize their star formation on a scale comparable to the extended periods of intense star formation reported in some high-redshift galaxies. Instead, we believe that the observed changes in the physical processes and rates of galaxy formation in modern large galaxies and those at high redshifts may be driven by the significantly longer timescales since the peak of most past dry merging events. While details of these changes remain subject to debate, our findings highlight the importance of early galaxy growth through dry merging, perhaps even preceding the formation of major unification groups, in understanding the present-day universe as we perceive it.",
        "ori-fast-z-score": 0.8409000955590962,
        "water-fast-z-score": 11.237557582586215,
        "rewrite-fast-z-score": 5.855770085141944
    }
]